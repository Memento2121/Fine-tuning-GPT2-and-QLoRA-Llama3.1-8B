{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to fine tune GPT-2 on a dataset of shakespear text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running this notebook on Google Colab run this cell to clone the repository\n",
    "# !git clone https://github.com/Memento2121/Fine-tuning-GPT2.git\n",
    "# %cd Fine-tuning-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are gonna do a QLoRA fineturning on the shakespear dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.ln_f\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Colin MININI\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"attn.c_attn\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA parameters found: ['base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight', 'base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight']\n"
     ]
    }
   ],
   "source": [
    "# Function to verify the presence of LoRA parameters\n",
    "def verify_lora_parameters(model):\n",
    "    lora_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_A\" in name or \"lora_B\" in name:\n",
    "            lora_params.append(name)\n",
    "    if lora_params:\n",
    "        print(f\"LoRA parameters found: {lora_params}\")\n",
    "    else:\n",
    "        print(\"No LoRA parameters found. Please check the target_modules configuration.\")\n",
    "\n",
    "# Verify the presence of LoRA parameters\n",
    "verify_lora_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 294912\n",
      "Total parameters: 124734720\n",
      "Percentage of trainable parameters: 0.24%\n"
     ]
    }
   ],
   "source": [
    "# Define function to print trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "    print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Print trainable parameters\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is a text file of shakespear text\n",
    "\n",
    "with open('input.txt', 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11706\n",
      "torch.Size([1, 338025])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "\n",
    "dataset = tokenizer.encode(data, return_tensors='pt')\n",
    "\n",
    "print(len(set(dataset[0].tolist())))\n",
    "\n",
    "print(dataset.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation sets\n",
    "\n",
    "n = 0.95\n",
    "\n",
    "train_size = int(dataset.size()[1] * n)\n",
    "\n",
    "train_dataset = dataset[:, :train_size]\n",
    "val_dataset = dataset[:, train_size:]\n",
    "\n",
    "# parameters of GPT2 model\n",
    "\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "\n",
    "# get the block size of the model\n",
    "\n",
    "block_size = config.n_positions\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Subtract self.block_size + 1 to avoid going out of bounds\n",
    "        return self.data.size()[1] - self.block_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input sequence is from idx to idx+self.block_size\n",
    "        input_sequence = self.data[:, idx:idx+self.block_size]\n",
    "        # Target sequence is shifted by one token to the right\n",
    "        target_sequence = self.data[:, idx+1:idx+self.block_size+1]\n",
    "        return input_sequence, target_sequence\n",
    "    \n",
    "\n",
    "train_dataset = TextDataset(train_dataset, block_size)\n",
    "\n",
    "val_dataset = TextDataset(val_dataset, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          num_workers=4,\n",
    "                          prefetch_factor=2,\n",
    "                          batch_size=2, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        num_workers=4,\n",
    "                        prefetch_factor=2,\n",
    "                        batch_size=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160049\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "lr = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "print(total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './model_checkpoints'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per batch: 25.1420 seconds\n",
      "Estimated total training time: 1117.76 hours\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to measure time per batch\n",
    "def measure_time_per_batch(dataloader, num_batches=10):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for i, (input_seq, target_seq) in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        outputs = model(input_seq, labels=target_seq)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    end_time = time.time()\n",
    "    avg_time_per_batch = (end_time - start_time) / num_batches\n",
    "    return avg_time_per_batch\n",
    "\n",
    "# Measure the average time per batch\n",
    "avg_time_per_batch = measure_time_per_batch(train_loader)\n",
    "print(f\"Average time per batch: {avg_time_per_batch:.4f} seconds\")\n",
    "\n",
    "# Calculate total training time\n",
    "total_iterations = len(train_loader) * epochs\n",
    "estimated_total_time = total_iterations * avg_time_per_batch\n",
    "print(f\"Estimated total training time: {estimated_total_time / 3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Profiling within the training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Start profiling\n",
    "    with profiler.profile(record_shapes=True, use_cuda=True) as prof:\n",
    "        for i, (input_seq, attention_mask) in enumerate(train_loader):\n",
    "            if i >= 10:  # Profile only the first 10 iterations\n",
    "                break\n",
    "            \n",
    "            input_seq = input_seq.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with profiler.record_function(\"forward\"):\n",
    "                outputs = model(input_seq, attention_mask=attention_mask, labels=input_seq)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            with profiler.record_function(\"backward\"):\n",
    "                loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if i % 10 == 0:  # Adjust the logging frequency as needed\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Iteration {i+1}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Training Loss: {total_loss / (i+1)}\")\n",
    "    \n",
    "    # Export profiling results\n",
    "    prof.export_chrome_trace(f\"profile_epoch_{epoch+1}.json\")\n",
    "\n",
    "# Download the profiling results\n",
    "filename = f\"profile_epoch_{epoch+1}.json\"\n",
    "files.download(filename)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor epoch in range(epochs):\\n    model.train()\\n    total_loss = 0\\n    total_val_loss = 0\\n    for i, (input_seq, target_seq) in enumerate(train_loader):\\n        input_seq = input_seq.to(device)\\n        target_seq = target_seq.to(device)\\n        outputs = model(input_seq, labels=target_seq)\\n        loss = outputs.loss\\n        loss.backward()\\n        optimizer.step()\\n        optimizer.zero_grad()\\n        total_loss += loss.item()\\n        if i % 1e6 == 0 and i > 0:\\n            model.eval()\\n            print(f\"Epoch {epoch} Iter {i} Loss: {loss.item()}\")\\n            for j, (input_seq, target_seq) in enumerate(val_loader):\\n                total_val_loss = 0\\n                input_seq = input_seq.to(device)\\n                target_seq = target_seq.to(device)\\n                with torch.no_grad():\\n                    outputs = model(input_seq, labels=target_seq)\\n                loss = outputs.loss\\n                total_val_loss += loss.item()\\n            print(f\"Epoch {epoch} Iter {i} Validation Loss: {total_val_loss}\")\\n            checkpoint_path = os.path.join(save_path, f\\'checkpoint_epoch_{i}.pt\\')\\n            model.save_pretrained(checkpoint_path)\\n            tokenizer.save_pretrained(checkpoint_path)\\n            model.train()\\n    print(f\"Epoch {epoch} Total Loss: {total_loss}\")\\n\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "\"\"\"\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_val_loss = 0\n",
    "    for i, (input_seq, target_seq) in enumerate(train_loader):\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        outputs = model(input_seq, labels=target_seq)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        if i % 1e6 == 0 and i > 0:\n",
    "            model.eval()\n",
    "            print(f\"Epoch {epoch} Iter {i} Loss: {loss.item()}\")\n",
    "            for j, (input_seq, target_seq) in enumerate(val_loader):\n",
    "                total_val_loss = 0\n",
    "                input_seq = input_seq.to(device)\n",
    "                target_seq = target_seq.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_seq, labels=target_seq)\n",
    "                loss = outputs.loss\n",
    "                total_val_loss += loss.item()\n",
    "            print(f\"Epoch {epoch} Iter {i} Validation Loss: {total_val_loss}\")\n",
    "            checkpoint_path = os.path.join(save_path, f'checkpoint_epoch_{i}.pt')\n",
    "            model.save_pretrained(checkpoint_path)\n",
    "            tokenizer.save_pretrained(checkpoint_path)\n",
    "            model.train()\n",
    "    print(f\"Epoch {epoch} Total Loss: {total_loss}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be.\n",
      "\n",
      "I don't know what to say to that. I'm not going to tell you what you should or shouldn't do, but I do know that you have to do something about it. It's not something you can just sit back and let it go. You've got to get out there and do what's right for you, and that's what I've been doing for the last couple of years. And I think it's important for us to\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "# Generate text\n",
    "\n",
    "model.eval()\n",
    "prompt = \"To be or not to be\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "output = model.generate(input_ids, \n",
    "                        max_length=100,\n",
    "                        no_repeat_ngram_size=2,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True,\n",
    "                        num_return_sequences=1)\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
