{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques et verrous technologiques\n\n#### Projet 1 : Étude du vieillissement des joints sur gyrolaser\n- **Objectif** : Étude exploratoire de l’origine du vieillissement prématuré des joints et modélisation pour planifier la maintenance.\n- **Problème identifié** : Usure des joints par oxydation critique pour le fonctionnement, entraînant des opérations de maintenance plus fréquentes que prévu.\n- **Conséquences** : Impact négatif sur l’opérationnalité du système et prévision des stocks.\n- **Verrou principal** : Complexité des contacts et conditions de fonctionnement rendant une modélisation théorique inenvisageable.\n- **Approche adoptée** : Développement d’un modèle empirique.\n- **Limitation des mesures** : Les mesures de maintenance corrective ne quantifient pas l’usure réelle en raison de faibles amplitudes de défauts, entraînant des erreurs de mesure.\n- **Travaux réalisés** : Modélisation des dégradations basée sur les retours d’expérience du client.\n- **Problèmes rencontrés** : Incertitudes de mesures affectant les résultats et taille réduite de l’échantillon.\n\n#### Projet 2 : Évaluation de la sécurité du système de dégivrage du MA700\n- **Objectif** : Évaluation de la sécurité et de la fiabilité du système de dégivrage.\n- **Approche** : Modélisation de la défaillance et vérification des performances par rapport aux normes.\n- **Verrou principal** : Absence de modèle pertinent pour la défaillance des systèmes pneumatiques.\n- **Défi** : Identifier les grandeurs physiques traduisant l’usure et modéliser leur évolution sous sollicitations.\n- **Limitations** : Système électromécanique, seule une partie était testable, nécessitant des hypothèses sur l’ensemble du système.\n- **Problème de données** : Taille de l’échantillon insuffisante pour représenter le phénomène d’usure à modéliser.\n\n#### Projet 3 : Conception d’un système de stockage et distribution de fuel pour hélicoptères\n- **Objectif** : Concevoir un système avec des contraintes d’accélération plus importantes pour une nouvelle gamme d’hélicoptères.\n- **Conception** : Réservoir à parois souples proposé par SAFRAN, intervention en amont pour évaluer fiabilité et sûreté.\n- **Enjeu principal** : Modéliser pour prédire le vieillissement et évaluer la probabilité de défaillance.\n- **Verrous rencontrés** : Complexité du système et de l’environnement d’opération.\n- **Aléas expérimentaux** : Présence d’électricité statique sur les conduites de fuel lors du remplissage sous pression."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques et verrous technologiques\nDans le cadre de notre collaboration avec SAFRAN, nous avons été impliqués dans trois projets.\n\nLe premier projet consistait en une étude exploratoire de l’origine du vieillissement prématuré de joints embarqués sur un gyrolaser, et sa modélisation afin de planifier les opérations de maintenance de cet équipement. L’usure de ces joints par oxydation ayant un caractère critique pour le fonctionnement du système, celui-ci nécessite des opérations de dépose pour maintenance plus récurrentes que prévues, influant ainsi négativement sur l’opérationnalité du système et sur la prévision des stocks. Un premier verrou découle de la complexité des contacts mis en œuvre et des conditions de fonctionnement qui rendent, de fait, une modélisation purement théorique, inenvisageable. Nous nous sommes alors orientés vers le développement d’un modèle empirique. Toutefois, les mesures effectuées à l’occasion des opérations de maintenance corrective ne permettent pas de quantifier l’usure réelle du joint du fait des faibles amplitudes des défauts, sources d’erreurs de mesures. Dans ce contexte, nos travaux consistaient ainsi à modéliser les dégradations dans le but de pouvoir estimer l’évolution de la taille des dégradations en nous basant sur les données issues des retours d’expérience de notre client. Là encore, nous avons été confrontés à des problèmes découlant des incertitudes de mesures entachant les résultats obtenus ainsi que la taille réduite de l’échantillon.\n\nLe second projet visait l’évaluation de la sécurité et de la fiabilité du système de dégivrage des ailes du MA700. Notre approche consistait en une modélisation de la défaillance du système et une vérification des performances atteintes vis-à-vis des normes en vigueur. Le principal verrou auquel nous avons été confrontés était l’absence de modèle pertinent concernant la défaillance de ce type de système pneumatique : nous devions ainsi identifier les principales grandeurs physiques capables de traduire l’usure et modéliser leur évolution sous les sollicitations qu’elles subissent en utilisation. De plus, le système que nous étudions étant électromécanique, seule une partie du système était testable. Nous avons ainsi dû poser des hypothèses sur l’ensemble du système que nous devrons confronter aux mesures expérimentales. Or, la taille de l’échantillon ne nous permet pas de disposer d’une base de résultats suffisante pour être représentative du phénomène d’usure que nous souhaitons modéliser.\n\nEnfin, le troisième projet réalisé en collaboration avec SAFRAN impliquait également AIRBUS, dans l’objectif de concevoir un système de stockage et de distribution de fuel pour une nouvelle gamme d’hélicoptères avec des contraintes d’accélération plus importantes que les autres systèmes. La conception innovante proposée par SAFRAN étant un réservoir à parois souple, nous sommes intervenus très en amont de la conception du système dans le but d’évaluer la fiabilité et la sureté d’un tel système dans un environnement complexe et très contraint. L’enjeu principal était donc de modéliser en vue de prédire le vieillissement d’un tel équipement afin d’en évaluer la probabilité de défaillance. Les verrous que nous avons rencontrés découlaient de la complexité d’un tel système et de l’environnement dans lequel il opère. Des aléas ont également été rencontrés lors de l’expérimentation et de la campagne de mesure, notamment en raison de la présence d’électricité statique sur les conduites de fuel lors du remplissage sous pression."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte du projet**: \n  - Projet Smart Maintenance CIR axé sur la maintenance prédictive dans l'industrie 4.0.\n  - Objectif : modéliser et prédire les pannes sur des équipements de pointe pour des solutions durables économiquement, techniquement et industriellement.\n  - Collaboration interne et avec des partenaires sur des équipements spécifiques.\n\n- **Focus de l'étude en 2022**:\n  - Étude des moteurs électriques sans balai (brushless), utilisés dans l'industrie du futur (servo-mécanismes, robotique).\n  - Avantages : performance supérieure par rapport aux moteurs à balais.\n\n- **Complexité du projet**:\n  - Difficulté à suivre une approche centrée sur les données en raison du vieillissement des équipements.\n  - Nécessité d'alimenter le modèle de défaillance par des mesures d'usure en fonctionnement pour estimer le temps d'utilisation restant.\n  - Besoin de concilier fiabilité du modèle de défaillance et contexte de vieillissement des équipements industriels.\n\n- **Travail de recherche**:\n  - Recherche de modèles de défaillance sur des organes mécaniques non étudiés.\n  - Développement de modèles synthétiques d'usure pour une compréhension approfondie des scénarios de dégradation.\n  - Évolution vers une méthodologie globale pour une maintenance prédictive robuste à l’échelle du système.\n\n- **Collaboration avec MECALAC**:\n  - Développement de modèles d’usure pour l'engin de terrassement Mecalac e12, moteur électrique alimenté par batterie lithium.\n  - Objectif : garantir la sûreté de l'engin en présence d'humains.\n\n- **Défis scientifiques et techniques**:\n  - Modélisation des performances du moteur électrique avec batterie lithium pour assurer une puissance suffisante.\n  - Maintien de pressions hydrauliques élevées nécessitant un pack de batteries complet, posant des défis de conception et de sécurité.\n  - Acquisition de connaissances centrée sur la modélisation du comportement du moteur pour optimiser performances hydrauliques et sécurité fonctionnelle.\n\n- **Collaboration avec KNDS (Nexter)**:\n  - Développement d'un modèle d’usure pour les véhicules Griffon et Serval, visant à couvrir 80% des pannes des équipements de climatisation EBMR.\n  \n- **Verrous scientifiques identifiés**:\n  - Manque de concordance entre données réelles et données des capteurs, nécessitant une classification par famille de données.\n  - Déficit de cadre normatif pour la maintenance prédictive, avec un saut technologique non comblé depuis dix ans.\n  - Complexité opérationnelle due à l'absence de consolidation des données exploitables et des équations des modèles.\n  - Gestion de multiples configurations existantes au sein des véhicules.\n\n- **Approche hybride**:\n  - Combinaison de modélisation numérique (création d’un « digital twin ») et d'orientation métier axée sur la maintenance et la « data science ».\n  - Objectif : atteindre un stade de reproductibilité pour exploiter les données et développer des modèles d’usure pour des systèmes similaires."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques et verrous technologiques\nLe projet Smart Maintenance CIR se concentre sur la maintenance prédictive dans le contexte de l’industrie 4.0 qui fait référence à une transformation numérique dans le domaine de la fabrication et de l'industrie. Cette initiative qui a connu des avancées significatives en 2022 où l’objectif est de modéliser et prédire les pannes sur des équipements/technologies de pointe afin d’apporter des solutions pérennes du point de vue économique, technique et industrielle est menée en interne ainsi qu’en collaboration avec des partenaires sur leurs équipements spécifiques.\nEn 2022, notre étude a porté sur les moteurs électriques sans balai (moteur brushless) qui sont largement utilisés dans « l'industrie du futur » notamment dans les servo-mécanismes des machines-outils et en robotique en raison de leur performance supérieure par rapport aux moteurs traditionnels à balais. La complexité du projet réside dans son approche qui ne peut pas suivre une démarche centrée sur les données en raison du vieillissement des équipements, alors que le modèle de défaillance doit être alimenté par des mesures d'usure en fonctionnement pour calculer le temps d'utilisation restant. Cela impose une réflexion approfondie sur la manière de concilier ces deux aspects, pour parvenir à une solution qui maximise la fiabilité du modèle de défaillance tout en prenant en considération le contexte spécifique du vieillissement des équipements industriels.\nLe travail de recherche implique la recherche de modèles de défaillance sur divers organes mécaniques jusque-là jamais étudiés, permettant une compréhension approfondie des scénarios de dégradation pour développer des modèles synthétiques d'usure. L'approche évolue de l'étude d'organes spécifiques vers le développement d'une méthodologie globale pour mettre en œuvre une maintenance prédictive robuste à l’échelle du système, apportant une amélioration substantielle dans la modélisation du vieillissement d'équipements révolutionnaires.\n\nDans le cadre de la thématique de maintenance prédictive que nous abordons avec notre partenaire MECALAC, l’objectif est d'entreprendre le développement de modèles d’usure d’un engin de terrassement révolutionnaire, le Mecalac e12, équipé d'un moteur électrique alimenté par une batterie lithium de pointe. Il s’agit d’une démarche inédite visant à garantir la sûreté de l'engin, en particulier en présence d'humains à proximité.\nLes principaux défis scientifiques et techniques du projet résident dans la modélisation des performances d'un moteur électrique doté d’une batterie lithium, en garantissant une puissance suffisante pour les opérations de terrassement. De plus, le maintien de pressions hydrauliques élevées pour certaines fonctions nécessite un pack de batteries complet, posant des défis supplémentaires en termes de conception et de sécurité.\nL'acquisition de connaissances dans le cadre du projet MECALAC s'est concentrée sur la modélisation du comportement du moteur électrique en vue de l'optimisation des performances hydrauliques et de la sécurité fonctionnelle.\n\nDans le cadre de notre collaboration dans le domaine de la défense avec KNDS (Nexter), nous cherchons à développer un modèle d’usure des véhicules Griffon et Serval afin de couvrir 80% des chemins de pannes de leurs équipements de climatisation EBMR.\nL'état de l'art actuel reposant sur des approches « data driven » générales, les verrous scientifiques identifiés comprennent dans notre contexte le manque de concordance entre les données réelles et les données des capteurs (nécessitant une classification par famille de données) et un déficit de cadre normatif pour la maintenance prédictive, avec un saut technologique non comblé au cours des dix dernières années. La complexité opérationnelle résulte aussi de l'absence de consolidation au sein des données exploitables et des équations caractérisant les modèles, et la gestion de plusieurs configurations existantes au sein de ces véhicules.\nCette approche inédite se veut hybride et combine la modélisation numérique (création d’un « digital twin ») élément de nouveauté dans le contexte de KNDS, une orientation métier axée sur la maintenance et la « data science » ; ce qui permet d’atteindre un stade de reproductibilité qui ouvre la voie à l'exploitation des données pour le développement de modèles d’usures de systèmes similaires."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Verrou de la Smart Maintenance**:\n  - Objectif : Développer une maintenance prédictive proche de la panne.\n  - Risques :\n    - Maintenance trop précoce : non optimisation de la durée de vie de l’équipement.\n    - Maintenance non planifiée : risque de panne et impact sur d'autres organes, entraînant des surcoûts et immobilisation du véhicule.\n\n- **Récupération des données sur les véhicules**:\n  - Problème : Multiplicité des standards de communication et protocoles spécifiques à chaque constructeur.\n  - Nécessité : Comprendre et intégrer les différences de communication pour relever les données sur le CAN.\n  - Problématiques rencontrées :\n    - Cohérence et compatibilité entre protocoles CAN, MQTT et JSON.\n\n- **Réduction des volumes de données**:\n  - Constat : Les données électroniques du réseau CAN peuvent atteindre des tailles de l’ordre du Téraoctet (10^12 octets).\n  - Solution envisagée : Stratégie de compression des données.\n  - Problèmes associés : Transmission des informations, choix du protocole de communication, structure des données.\n\n- **Transmission des données en temps réel**:\n  - Difficulté : Envoi des données des véhicules depuis le boîtier Dunasys vers un serveur pour analyse.\n\n- **Documentation et recherche**:\n  - Problème : Manque de documentation pertinente sur l’usure des plaquettes de frein.\n  - Défi : Concevoir une méthode générique applicable à tous les modèles de véhicules, indépendamment du style de conduite."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLa réalisation de notre projet de développement de la Smart Maintenance pour les plaquettes de frein a fait l’objet de la levée de plusieurs verrous. Nous pouvons en particulier citer le verrou qu’est la détermination du compromis de la Smart Maintenance que nous souhaitons développer : une maintenance prédictive au plus près de la panne, en amont de celle-ci. En effet, si la maintenance arrive trop tôt par rapport à l’occurrence de la panne, la durée de vie de l’équipement n’est pas optimisée puisqu’il aurait pu continuer à fonctionner avant son remplacement. Et si la maintenance n’est pas planifiée alors que l’équipement arrive en fin de vue, une panne survient et d’autres organes peuvent être impactés constituant ainsi un surcoût, en plus du coût lié à l’immobilisation du véhicule pour la maintenance corrective.  \nUne autre difficulté s’est posée sur la récupération des données sur les véhicules : en effet, différents standards de communication se côtoient et les protocoles de communication de chaque constructeur lui est spécifique. Nous avons ainsi dû comprendre puis prendre en compte les différences qui existent entre ces communications afin de pouvoir relever les données échangées sur le CAN des véhicules et travailler dessus. Nous avons par ailleurs été confrontés à des problématiques liées aux protocoles CAN, MQTT et JSON ; nous devions en particulier assurer la cohérence et la compatibilité entre eux.  \nUne problématique très importante dans nos travaux concernait la réduction des volumes des données. En effet, au sein d’un véhicule, les données électroniques issues du réseau CAN relié à l’ensemble des organes représentent des tailles de l’ordre du Téraoctet (10^12 octets), d’où l’intérêt d’une stratégie de compression. Ces grands volumes de données posent également des problèmes sur la transmission des informations relevées : choix du protocole de communication, structure des données…  \nPar ailleurs, la problématique de transmission des données se pose également dans la manipulation de données en temps réel. L’une des difficultés à adresser dans nos travaux portait sur l’envoi des données des véhicules depuis le boitier Dunasys vers un serveur où elles sont analysées.  \nEnfin, d’autres aléas ont été rencontrés au cours de nos travaux, notamment concernant la documentation sur laquelle nous appuyer. En effet, comme nous avons pu l’illustrer dans l’état de l’art, les seules publications qui abordent le sujet de l’usure des plaquettes s’intéressent à des cas très spécifiques. La difficulté dans notre cas est de concevoir une méthode générique, qui puisse être utilisée sur tous les modèles de véhicules, quel que soit le style de conduite de l’utilisateur."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Système d’Embrayage\n- **Difficulté principale** : Modélisation du système d’embrayage, en particulier l’identification des éléments mécaniques les plus soumis à l’usure.\n- **Modèle tribologique développé** : Deux composants du système d’embrayage.\n- **Modélisations à compléter** :\n  - Appui sur la pédale d’embrayage (position débrayée ou patinage).\n  - Contact entre le diaphragme et le disque d’embrayage.\n  - Usure des ressorts.\n- **Défi d’identification des signaux** : Identification des signaux pertinents du réseau CAN pour modéliser les sollicitations et estimer l’usure.\n- **Modèle mathématique nécessaire** : Développement d’un modèle pour définir l’implication de chaque paramètre.\n- **Problème d’évaluation du modèle d’usure** : Démontage intrusif du système d’embrayage complique la vérification des estimations d’usure.\n  - Nécessité d’attendre la casse de l’équipement pour réaliser des mesures.\n  - Taille d’échantillon de flotte limitée en raison de la durée de vie nominale des embrayages.\n\n#### Bande de Pantographe\n- **Difficulté de modélisation** : Modélisation des forces sur la bande carbone (direction, surface de sollicitation, force).\n- **Limites identifiées** :\n  - Surface de contact variable.\n  - Rugosité variable de la caténaire due aux conditions climatiques (ex. : gel).\n  - Effort de contact variable selon la vitesse de circulation du train.\n- **Défi de modélisation de l’usure** : Identification d’une pièce soumise à la même loi d’usure pour étudier la faisabilité d’une modélisation mécanique.\n- **Conception d’une solution** : Développement d’une méthode pour connaître l’épaisseur résiduelle des bandes carbone sans démontage, afin d’éviter les opérations de maintenance préventive.\n\n#### Brosse de Contact\n- **Objectif des travaux** : Détection de l’usure des brosses de contact.\n- **Solution développée** : Conception d’une brosse télescopique pour démultiplier les pièces d’usure et espacer les opérations de maintenance nécessitant un démontage.\n- **Besoin identifié lors des tests** : Développement d’une solution pour automatiser la descente des nouvelles lamelles de la brosse lors de l’expérimentation du prototype en conditions réelles."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nSystème d’embrayage\nLa principale difficulté rencontrée portait sur la modélisation du système d’embrayage, notamment l’identification des éléments mécaniques soumis à la plus grande usure.\nNous avons dans un premier temps choisi de développer un modèle tribologique de deux composants du système d’embrayage.\n\nNotre modélisation sera ultérieurement complétée par :\nLa modélisation de l’appui sur la pédale d’embrayage (position totalement débrayée ou patinage) ;\nLa modélisation du contact entre le diaphragme et le disque d’embrayage ;\nLa modélisation de l’usure des ressorts.\n\nUn autre défi que nous avons dû adresser portait sur l’identification des signaux pertinents du réseau CAN pour modéliser numériquement les sollicitations et estimer l’usure en conséquence. En effet, en plus de la difficulté d’identification des signaux pertinents, nous devons également développer un modèle mathématique permettant de définir l’implication de chaque paramètre.\n\nPar ailleurs, nous sommes confrontés à un problème que nous n’avons pas encore résolu pour l’évaluation du modèle d’usure par une mesure physique. En effet, le caractère intrusif de l’opération de démontage du système d’embrayage rend difficile la vérification de nos estimations de l’usure sans endommager les pièces manipulées. Nous sommes ainsi contraints d’attendre la casse de l’équipement pour réaliser nos mesures. Une autre difficulté découlant de cet obstacle se pose en termes de taille d’échantillon de flotte, la durée de vie nominale des embrayages étant relativement étendue.\nBande de pantographe\nLa première difficulté rencontrée était liée à la modélisation des forces auxquelles est soumise la bande carbone : direction, surface de sollicitation, force, etc.\nUne analyse approfondie nous a permis d’identifier différentes limites qui rendaient la modélisation de l’usure de la bande carbone impossible :\nLa surface de contact est variable ;\nLa caténaire, en raison des conditions climatiques, présente une rugosité variable (présence de gel) ;\nSelon la vitesse de circulation du train, l’effort de contact est variable.\n\nFace à l’impossibilité de modélisation de la loi d’usure des bandes carbone, notre défi était alors d’identifier une pièce soumise à la même loi d’usure et d’étudier la faisabilité d’une modélisation mécanique de ces phénomènes.\nEnfin, la dernière difficulté était liée à la conception d’une solution permettant, sans démontage, de connaître l’épaisseur résiduelle des bandes carbone afin d’éviter les opérations de maintenance préventive.\nBrosse de contact\nL’inconnue que ces travaux visaient à lever était la détection de l’usure des brosses de contact. Nos travaux de développement nous ont permis de concevoir une solution de brosse télescopique permettant de démultiplier les pièces d’usure afin d’espacer les opérations de maintenance ayant recours à un démontage de l’équipement. L’expérimentation du prototype de brosse en conditions réelles d’utilisation nous a cependant confrontés au besoin de développer une solution permettant d’automatiser la descente des nouvelles lamelles de la brosse."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Système d’Embrayage\n- **Continuation des travaux de 2017** : Recherche sur les mécanismes d’usure des systèmes d’embrayage.\n- **Collaboration avec NEXTER** : Développement d'une solution de mesure de l’usure pour camions et blindés.\n  - Avantage : Tests terrain sur des véhicules avec une durée de vie d’embrayage réduite.\n- **Difficulté principale (2018)** : Corrélation entre le modèle mathématique et l’usure réelle.\n  - Nécessité de connaître les variables des matériaux, souvent confidentielles.\n  - Solution : Relevés expérimentaux via campagnes de mesure.\n- **Objectifs des mesures** :\n  - Alimenter la base de données.\n  - Créer des abaques pour améliorer le modèle.\n- **Contraintes des relevés** :\n  - Caractère invasif et intervalles temporels éloignés.\n  - Nécessité de mesurer à différents moments du cycle de vie des pièces.\n\n#### Usure des Plaquettes de Freins\n- **Utilisation du module D-Car** : Collecte de données via le réseau CAN sur la flotte de véhicules ARVAL.\n- **Objectif** : Développer un modèle mathématique prédictif de l’usure des plaquettes de freins.\n- **Verrous identifiés** :\n  - Mise en œuvre des formules de tribologie, nécessitant un traitement complexe des données.\n  - Sélection des informations pertinentes parmi le flux de données CAN.\n\n#### Usure Moteur : Huiles et Batteries\n- **Appel à projet ENGIE** : Étude de l’usure moteur axée sur les huiles et les batteries.\n- **Modélisation mathématique** : Représentation des facteurs d’influence sur l’usure.\n- **Difficultés** :\n  - Aspects chimiques des huiles et batteries difficilement modélisables.\n  - Absence de modèle global pour la durée de vie des batteries.\n- **Paramètres à considérer** : Température, stockage, cycles de charge/décharge, humidité, etc.\n- **Objectif** : Identifier et modéliser les paramètres les plus pertinents.\n\n#### Banc de Contrôle des Essieux\n- **Programme SNCF A Plein Rég’IM** : Amélioration de la fiabilité opérationnelle et réduction des coûts de maintenance.\n- **Verrous identifiés** :\n  - Difficulté à collecter des données fiables pour l’étude préliminaire.\n  - Identification des points de mesure représentatifs pour l’usure des essieux.\n- **Types d’usure** : Usure due au roulage et reprofilage, nécessitant un banc de mesure adapté.\n- **Contraintes** : Hétérogénéité des types d’essieux, absence de standardisation.\n\n#### Usure des Pignons de Pods\n- **Appel à projet THALES** : Objectif de prolonger la durée de vie des équipements de 10 à 30 ans.\n- **Problématique principale** : Difficulté à confronter le modèle mathématique à l’usure réelle.\n- **Nécessité de mesures** : Échantillon représentatif pour obtenir des résultats fiables.\n- **Obstacles** :\n  - Immobilisation fréquente et prolongée du système pour les mesures.\n  - Prise en compte de la lubrification dans le modèle, modifiant les coefficients de frottement.\n- **Solution** : Détermination expérimentale des coefficients de frottement."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\nSystème d’embrayage  \nDans la continuité des travaux menés en 2017, A2L a poursuivi ses recherches sur les mécanismes d’usure des systèmes d’embrayage. Les travaux initiés en interne ont été prolongés par des travaux réalisés en collaboration avec la société NEXTER qui souhaitait développer une solution de mesure de l’usure des embrayages d’une flotte de camions ou de blindés. Cette collaboration permet d’adresser le besoin de NEXTER qui souhaite développer une solution de mesure des embrayages d’une flotte de camions blindés, et nous permet également de confronter le modèle que nous avons développé à des tests terrain. En effet, la flotte de véhicule NEXTER est particulièrement intéressante pour nous car leurs blindés sont confrontés à une plus faible durée de vie de leur système d’embrayage. Cela nous permet donc de contourner la difficulté du démontage invasif tout en ayant une occurrence de casse suffisante pour réaliser des statistiques de données empiriques.\n\nLa principale difficulté rencontrée en 2018 concernant le système d’embrayage résidait dans la corrélation entre le modèle mathématique et l’usure réelle des composants. Par ailleurs, cette modélisation faisant intervenir différentes variables liées aux caractéristiques intrinsèques à chaque matériau ou alliage, il nous est nécessaire de connaître ces variables ; nous nous heurtons cependant à la confidentialité des fabricants à ce sujet. De ce fait, la seule solution nous permettant de définir ces variables est d’effectuer des relevés expérimentaux via des campagnes de mesure. Ces mesures permettront, en outre, d’alimenter notre base de données et de réaliser des abaques des différentes grandeurs d’intérêt pour les matériaux rencontrés, afin d’améliorer, au fil du temps, la robustesse de notre modèle. Les résultats obtenus via celui-ci devant être confrontés aux données obtenues de manière expérimentale. Pour ce faire, nous devons donc mener des campagnes de mesure suivant un protocole strict afin de collecter suffisamment de données pouvant être interprétées. Les relevés effectués sur les organes du système d’embrayage ayant un caractère invasif et intervenant à des intervalles temporels éloignés, la caractérisation des éléments nécessite une durée particulièrement longue ainsi que la probable destruction des pièces du systèmes. Or, si nous souhaitons mesurer de manière précise l’évolution de l’usure des organes en fonction du temps, nos relevés doivent intervenir à différents instants du cycle de vie des pièces. \n\nUsure des plaquettes de freins  \nA l’aide des données récoltées grâce au module D-Car, développé par Dunasys, nous permettant la collecte des données via le réseau CAN, nous avons souhaité mettre en expérimentation ce système sur la flotte de véhicule de la société ARVAL, filiale de BNP Paribas spécialisée dans la location de véhicule auprès de professionnels. Cette collecte de données nous permettra, par la suite, de développer un modèle mathématique prédictif de l’usure des plaquettes de freins. En outre, le partenariat avec la société ARVAL nous permet de bénéficier d’un échantillon de véhicules suffisamment conséquent pour obtenir une quantité de données suffisante et représentative permettant la corrélation du modèle mathématique développé avec les essais en conditions réelles. En effet, bien que de nombreux véhicules soient aujourd’hui équipés d’un voyant indiquant la nécessité de changer les plaquettes de freins, il s’avère que ceux-ci sont basés sur un paradigme de maintenance préventive. En effet, l’usage des plaquettes est donné, à l’avance, pour un nombre de kilomètres préconisé par le fabricant. Une fois cette distance atteinte, le voyant notifie le conducteur de la nécessité de changer ses plaquettes.\n\nConcernant ce point, les principaux verrous identifiés sont la mise en œuvre des formules de tribologie, qui nécessitent le traitement et la sélection de nombreuses données rendant son application particulièrement difficile. Par ailleurs, la sélection des informations pertinentes parmi le flux de données remonté via le réseau CAN nécessite une connaissance particulièrement pointue des lois d’usure et de la tribologie. \n\nUsure moteur : huiles et batteries  \nDans le cadre de la réponse à un appel à projet lancé par la société ENGIE, nous avons initié des travaux d’étude de l’usure d’un moteur. Cette usure pouvant provenir d’une multitude de sources, nous avons décidé d’orienter nos recherches sur deux axes principaux : L’usure des huiles moteur et L’usure des batteries. En 2018, nos travaux ont principalement consisté à étudier de manière approfondie les différents facteurs d’influence et établir une modélisation mathématique représentative de la réalité, permettant de monitorer l’état de ces organes en temps réel. L’étude de l’usure des batteries, tout comme celle de la dégradation des huiles, fait intervenir des aspects chimiques difficilement modélisables, nous devrons ainsi formuler des hypothèses de modélisation permettant de remédier à ces limites.\n\nPar ailleurs, il n’existe, à ce jour, aucun modèle prenant en compte l’ensemble des paramètres de durée de vie d’une batterie. Les premiers paramètres que nous pouvons citer sont notamment la température, le stockage, la non-utilisation, le nombre de cycle de charge et de décharge, le nombre de décharge complète, l’augmentation de la charge résistive, la défaillance du circuit de charge, l’humidité, etc. L’objectif de nos travaux sera de faire le tri parmi ces différents paramètres, identifier les plus pertinents et modéliser leur évolution vis-à-vis de l’usure de l’organe étudié.\n\nBanc de contrôle des essieux  \nDans le cadre du programme A Plein Rég’IM, programme de transformation mené par la SNCF, depuis 2017, dans le but d’améliorer la fiabilité opérationnelle, d’augmenter la disponibilité du matériel et de diminuer les coûts de maintenance, le groupe s’est rapproché d’A2L afin de lui confier la conception d’une solution de diagnostic de l’usure des essieux. Concernant ce point, les principaux verrous sont, d’une part la difficulté à collecter des données fiables pour la réalisation de l’étude préliminaire permettant d’identifier une tendance dans les résultats de l’analyse statistique. D’autre part, l’identification des points de mesures représentatifs du seuil de non-conformité de l’essieux représente également une difficulté particulière de par les différents types d’usures en jeu, l’usure due au roulage et celle due aux opérations de reprofilage. En effet, pour des raisons de sécurité, les essieux doivent conserver une forme circulaire. Cependant, l’usure de roulage provoque au fil du temps une déformation qui, au-delà d’un certain seuil, s’avère critique et nécessite alors une opération de reprofilage via un usinage de l’essieu, enlevant donc une épaisseur de matière. Le suivi de ces deux types d’usure nécessite donc la mise en place d’un banc de mesure des essieux et la prise en compte des deux composantes dans le modèle prédictif. Notre modèle devra considérer un cas limite dans lequel l’usure est due au roulement ainsi que la prise en compte de corrections de la formule liées aux reprofilages subis. Nous nous heurtons également à l’hétérogénéité des types d’essieux en circulation. En effet, il n’existe pas de standardisation à ce niveau et chaque modèle de train peut potentiellement présenter un type d’essieux aux caractéristiques qui lui sont propres, entrainant une contrainte sur le dimensionnement du banc de mesure.\n\nUsure des pignons de pods  \nL’étude de vieillissement des pods intervient dans le cadre d’un appel à projet lancé par la société THALES. Le but de cet appel à projet est de permettre d’actionner des leviers permettant une gestion efficiente de la maintenance afin de passer d’une durée de vie du matériel de 10 ans à une durée de vie de 30 ans, sans perte significative de disponibilité ni de performance. Cette problématique se traduit par la nécessité de fiabiliser l’équipement sur une durée de 30 ans. La mesure de l’usure des pignons de pods nécessite la modélisation d’une loi d’usure. Cependant, la principale problématique intervenant à ce sujet est la difficulté à confronter le modèle mathématique à l’usure réelle des pièces. En effet, l’obtention de résultats nécessite la mise en place de protocoles de mesures sur un échantillon suffisamment élevé pour être représentatif de l’usure réelle. Ces mesures sont très difficilement réalisables car elles nécessiteraient l’immobilisation du système de manière récurrente et relativement longue. Par ailleurs, la prise en compte de la lubrification dans le modèle mathématique de base représente également un obstacle important. En effet, la présence d’une couche de lubrifiant modifie considérablement les coefficients de frottement en jeu et il n’existe pas d’abaque tenant compte de cette lubrification nous permettant de les estimer. Ces coefficients devront donc être déterminés expérimentalement."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte général :**\n  - Projets de recherche et développement sur la fiabilité de composants innovants sans modèles existants.\n  - Nécessité de nouvelles modélisations basées sur des mesures réelles.\n  \n- **Problèmes de mesure :**\n  - Instruments de mesure peu fiables entraînant des problèmes de conversion des signaux électriques.\n  - Mesures inexactes de la taille réelle des fissures.\n  - Présence de bruit de fond lorsque aucun défaut n'est détecté, rendant difficile la détection de petits défauts.\n  - Instruments pas assez précis pour mesurer les petits défauts, entraînant des données censurées.\n  - Les données obtenues sont des intervalles plutôt que des valeurs précises.\n  - Faible nombre d'équipements livrés en aéronautique, limitant la quantité de données disponibles.\n  - Fissures détectées lors d'inspections successives sont censurées en temps, car l'instant d’amorçage est inconnu.\n\n### Projets avec SAFRAN\n\n1. **Premier projet : Étude du vieillissement des joints sur gyrolaser**\n   - Objectif : Modélisation pour planifier les opérations de maintenance.\n   - Verrous :\n     - Complexité des contacts et conditions de fonctionnement rendant une modélisation théorique inenvisageable.\n     - Mesures de maintenance corrective ne quantifiant pas l’usure réelle en raison de faibles amplitudes des défauts.\n\n2. **Deuxième projet : Évaluation du système de dégivrage des ailes du MA700**\n   - Objectif : Évaluer la sécurité et fiabilité du système.\n   - Verrous :\n     - Absence de modèle pertinent pour la défaillance du système pneumatique.\n     - Système électromécanique, testabilité limitée à une partie du système.\n     - Taille de l’échantillon insuffisante pour une base de résultats représentative.\n\n3. **Troisième projet : Modélisation du vieillissement d’un réservoir d’hélicoptère innovant**\n   - Objectif : Prédire la probabilité de défaillance.\n   - Verrous :\n     - Complexité du système et de son environnement d’opération.\n     - Aléas lors de l’expérimentation dus à l’électricité statique sur les conduites de fuel.\n\n### Autres Collaborations\n\n- **Collaboration avec GME Défense : Conception d’une combinaison NRBC**\n  - Verrous :\n    - Complexité de l’équipement.\n    - Matériaux impliqués et types de sollicitations en utilisation.\n\n- **Collaboration avec la SNCF : Modélisation des besoins en installations de maintenance**\n  - Objectif : Assurer la disponibilité du matériel suite à l'augmentation du nombre de trains.\n  - Incertitudes :\n    - Suffisance des installations de maintenance.\n  - Développement d’un modèle mathématique empirique (algorithme) avec plusieurs verrous :\n    - Fiabilité des données nécessaires à l’algorithme dépendant des langages métier.\n    - Estimation de la charge et capacité des installations.\n    - Allocation des machines comme second verrou.\n    - Modèle de charge avec incertitudes sur la maintenance prédictive et corrective.\n    - Limites de scalabilité de l’algorithme, nécessitant des recherches sur la parallélisation des calculs et remaniement des fonctions."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours de nos projets de recherche et développement concernant l’évaluation de la fiabilité de composants innovants et dont il n’existe pas actuellement de modèles, nous nous retrouvons face au besoin de développer de nouvelles modélisations, basée sur des mesures réelles. Or, les instruments de mesure ne sont pas toujours très fiables et entraînent différents problèmes : la conversion des signaux électriques en ondes et réciproquement, n’est pas parfaite, et peut faire perdre de la précision et de l’information. Les mesures effectuées ne correspondent pas exactement à la taille réelle de la fissure. \nDe plus, lorsqu’il n’y a pas de défaut, l’instrument de mesure « présente » un bruit de fond. Par conséquent, une faible mesure provenant d’un défaut très petit peut être considérée comme du bruit, et le défaut ne sera pas détecté. \nAussi, les instruments de mesure n’étant pas assez précis, les petits défauts sont difficilement mesurables. On constate ainsi de nombreuses données censurées. Les données obtenues ne sont pas des valeurs mais des intervalles dans lesquels doivent se trouver les tailles de fissure. \nEnfin, pour les projets en aéronautique ci-dessous, le nombre d’équipements livrés étant faible, il y a donc peu de données. De plus, les fissures détectées lors d’un contrôle alors qu’elles ne l’avaient pas été au contrôle précédent, sont censurées en temps, car l’instant d’amorçage du défaut est inconnu : il se situe entre les deux inspections.\n\nDans le cadre de notre collaboration avec SAFRAN, nous avons été impliqués dans trois projets.\n\nLe premier projet consistait en une étude exploratoire de l’origine du vieillissement prématuré de joints embarqués sur un gyrolaser, et sa modélisation afin de planifier les opérations de maintenance de cet équipement. Un premier verrou découle de la complexité des contacts mis en œuvre et des conditions de fonctionnement qui rendent, de fait, une modélisation purement théorique inenvisageable. De plus, un deuxième verrou résidait dans le fait que les mesures effectuées à l’occasion des opérations de maintenance corrective ne permettaient pas de quantifier l’usure réelle du joint du fait des faibles amplitudes des défauts, sources d’erreurs de mesures. \n\nLe second projet visait l’évaluation de la sécurité et de la fiabilité du système de dégivrage des ailes du MA700. Le principal verrou auquel nous avons été confrontés était l’absence de modèle pertinent concernant la défaillance de ce type de système pneumatique. De plus, le système que nous étudions étant électromécanique, seule une partie du système était testable. Enfin, une autre incertitude fût que la taille de l’échantillon ne nous permettait pas de disposer d’une base de résultats suffisante pour être représentative du phénomène d’usure que nous souhaitons modéliser.\n\nEnfin, dans le troisième projet en collaboration avec SAFRAN et AIRBUS, nous avions pour objectif de modéliser en vue de prédire le vieillissement d’un équipement de réservoir d’hélicoptère innovant à paroi souples afin d’en évaluer la probabilité de défaillance. Les verrous que nous avons rencontrés découlaient de la complexité d’un tel système et de l’environnement dans lequel il opère. Des aléas ont également été rencontrés lors de l’expérimentation et de la campagne de mesure, notamment en raison de la présence d’électricité statique sur les conduites de fuel lors du remplissage sous pression.\n\nEn parallèle de ces projets avec SAFRAN, nous avons également collaboré avec GME Défense, aux côtés d’autres partenaires, dans un projet de conception d’une combinaison NRBC. Les verrous découlent de la complexité de l’équipement, des matériaux impliqués et des types de sollicitations qui s’exercent sur l’équipement en utilisation. \n\nEnfin, un dernier projet réalisé en collaboration avec la SNCF visait la modélisation des besoins en installations de maintenance, vue à travers le spectre de la logistique.\nL’une des principales incertitudes qui s’est posée est, suite à une augmentation du nombre de trains, de s’assurer du nombre suffisant d’installations de maintenance pour s’assurer de la disponibilité du matériel. Nous avons ainsi dû développer un modèle mathématique empirique, sous forme d’algorithme, qui nous a conduit à de nombreux verrous technologiques. Pour premier verrou, les données, nécessaires pour la construction de cet algorithme n’était pas toutes fiables et dépendaient des langages métier. De plus, nous avons ensuite dû estimer la charge et la capacité des installations, l’allocations des machines a constitué notre deuxième verrou et nous avons dû développer un modèle de scoring. Pour ce qui est du modèle de charge, les incertitudes soulevées étaient en lien avec la prise en compte de la maintenance prédictive et la modélisation de la maintenance corrective. Enfin, nous avons atteint des limites quant à la scalabilité de l’algorithme, nous avons alors lancé des recherches sur la parallélisation des calculs, mais aussi dû remanier nos fonctions."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\n- **Absence de modèles pertinents** : Nécessité de développer de nouvelles modélisations basées sur des mesures réelles pour étudier des systèmes complexes.\n  \n- **Défis techniques en R&D** : Confrontation à divers défis dans la détection de défaillances et l'évaluation de la fiabilité d'équipements novateurs et complexes.\n\n- **Complexité des systèmes multi-composants** :\n  - Conception d'algorithmes de maintenance prédictive pour des systèmes dynamiques multi-composants.\n  - Singularité de chaque composant et complexité des interactions augmentent les défis de modélisation.\n  - Manque de techniques adaptées dans la littérature pour gérer cette complexité.\n\n- **Limites des capteurs** :\n  - Capteurs spécifiques à certains types de défaillances, limitant la détection de multiples défaillances.\n  - Problèmes de précision et d'information lors de la conversion des signaux électriques en ondes et vice versa.\n  - Imprécision des instruments pouvant entraîner des mesures inexactes des défauts.\n  - Faibles amplitudes de défauts souvent confondues avec le bruit, entraînant une détection insuffisante.\n  - Mesures souvent censurées, fournissant uniquement des intervalles de taille de fissure au lieu de valeurs précises.\n\n- **Exploitation du Big Data** :\n  - Difficulté à gérer des sources de données hétérogènes et multiples.\n  - Identification des données pertinentes au sein d'une masse de données superflue (concept de Small Data).\n  - Échantillons de données souvent insuffisants pour représenter les phénomènes d'usure.\n  - Utilisation intelligente des données pertinentes (Smart Data) représente un défi supplémentaire."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\nActuellement, l'absence de modèles pertinents pour les systèmes complexes que nous souhaitons étudier nous oblige à entreprendre le développement de nouvelles modélisations basées sur des mesures réelles. Dans nos travaux de recherche et développement axés sur la détection de défaillances et l'évaluation de la fiabilité impliquant des équipements novateurs et complexes, nous sommes confrontés à divers défis techniques.\n\nDifficulté liée à la complexité intrinsèque des systèmes multi-composants dynamiques : la conception d'algorithmes de maintenance prédictive pour les systèmes multi-composants confrontés à une variété de défaillances sur diverses configurations représente un défi majeur, en raison de la singularité de chaque composant et de la complexité globale des systèmes dynamiques. La présence de plusieurs composants et de leurs interdépendances augmente la complexité et la difficulté de modélisation d'un algorithme de maintenance prédictive. Comme on l’a vu dans la section précédente, la littérature ne propose pas de techniques adaptées pour ce qui rajoute à la complexité.\n\nLimites des capteurs devant la diversité des défaillances : un autre défi concerne l’utilisation des capteurs existants, qui sont spécifiques à la détection de certains types de défaillances. Cette spécialisation des capteurs crée des limitations dans la détection de multiples types de défaillances par chaque capteur. De plus, les instruments de mesure utilisés pour développer de nouvelles modélisations basées sur des mesures réelles présentent des problèmes tels que la perte de précision et d'information lors de la conversion des signaux électriques en ondes et vice versa. De plus, l'imprécision des instruments peut conduire à des mesures qui ne correspondent pas exactement à la taille réelle des défauts. Les faibles amplitudes des défauts peuvent être confondues avec le bruit des instruments, entraînant une détection insuffisante de petits défauts. La difficulté à mesurer précisément conduit souvent à des données censurées, où seuls des intervalles de taille de fissure sont obtenus au lieu de valeurs précises.\n\nDéfis liés à l'exploitation du Big Data : un troisième défi est lié à l'exploitation de sources de données hétérogènes et multiples, où la récupération et le stockage d'une quantité considérable d'informations sont possibles. Cependant, la difficulté réside dans l'identification des données pertinentes au sein de la masse de données superflue et hétérogène, concept connu sous le nom de Small Data. Et des échantillons de données insuffisants pour représenter les phénomènes d'usure, et des problèmes expérimentaux liés à l'environnement opérationnel. Une fois ces données pertinentes identifiées, leur utilisation intelligente, connue sous le terme de Smart Data, représente un autre défi."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Objectif du projet (2019)** : Développer un système de matching personnalisé entre un CV et un emploi.\n\n- **Première incertitude scientifique** : \n  - Problème d'extraction de modèles d'emplois et de CV.\n  - Nécessité de modèles numériques structurés pour calculer la similitude entre un emploi et un CV.\n  - Les JRS (Job Request Systems) demandent aux utilisateurs de saisir leurs informations dans des formulaires, mais :\n    - Les demandeurs d'emploi préfèrent télécharger leur CV directement.\n    - Les recruteurs préfèrent publier des descriptions de poste complètes sur des sites Web.\n  - Besoin d'extraire des informations structurées à partir de données non structurées (CV et descriptions de poste).\n\n- **Difficulté dans le calcul de la similitude** :\n  - La simple correspondance de mots-clés est insuffisante.\n  - Les descriptions de poste et les CV contiennent des mots complexes et des concepts variés.\n  - Certains concepts peuvent être exprimés de différentes manières, et d'autres peuvent avoir des relations étroites.\n\n- **Observation sur la correspondance de texte** :\n  - Les CV contiennent peu de mots communs avec les descriptions de poste.\n  - Un ingénieur expérimenté peut voir une correspondance étroite malgré le manque de mots communs (exemples : bases de données Oracle et MySQL, OOA/OOD par rapport à l'expérience en Java et C++, Tomcat et JBOSS comme serveurs d'applications Web Java).\n  - La correspondance de mots-clés ne produit pas de résultats solides dans des cas courants.\n  - Nécessité d'une approche améliorée pour évaluer la similitude entre concepts techniques différents."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de nos travaux de l’année 2019, nous souhaitions proposer à nos utilisateurs un système de matching entre un CV et un emploi de façon personnalisé. \n\nDans ce cadre, la première incertitude scientifique à laquelle nous avons été confrontés s’est concentrée sur le fait de trouver une solution pour savoir comment extraire des modèles d’emplois et de curriculum vitae. Effectivement, pour calculer la similitude entre un emploi et un CV, notre système CVJobs a besoin de modèles numériques structurés de chaque document. Pour obtenir les données structurées, certains JRS demandent aux demandeurs d'emploi de saisir leurs profils dans des formulaires champ par domaine, et le recruteur saisit ses descriptions de poste de la même manière. Cependant, les utilisateurs sont réticents à prendre le processus fastidieux. Les demandeurs d'emploi préfèrent télécharger leur curriculum vitae directement, et les recruteurs préfèrent publier l'intégralité des descriptions de poste sur des sites Web. Nous devons donc extraire les informations structurées d'une source de données non structurée, comme les curriculum vitae et les descriptions de poste.\n\nUne autre difficulté consistait à savoir comment calculer la similitude entre le rsum et les modèles d'emploi. Nous avons observé que la simple correspondance de mots-clés n'est pas une bonne mesure de similitude, car les descriptions de poste et les curriculum vitae contiennent tous deux des mots plus riches et plus complexes qui ne peuvent être décrits simplement par des mots-clés. Dans ces documents, certains concepts peuvent être écrits de différentes manières et d'autres concepts peuvent avoir des relations étroites. \n\nEnfin, une autre difficulté était liée au fait qu’en regardant simplement le texte, nous pouvons constater que le CV contient très peu de mots communs avec la description de poste. Mais du point de vue d'un ingénieur expérimenté, le candidat correspond étroitement au poste : les deux bases de données relationnelles Oracle et MySQL sont très similaires, OOA / OOD a le même sens que de nombreuses années d'expériences Java et C ++, et Tomcat et JBOSS sont tous les deux Serveurs d'applications Web Java. Si nous utilisons la correspondance de mots-clés, le système ne fournit pas de résultat de correspondance solide dans des cas très courants comme celui-ci. Nous avons donc besoin d'une meilleure approche pour calculer la similitude entre différents concepts techniques."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte de R&D (2013-2014)** :\n  - Confrontation à des contraintes techniques lors de la réalisation des travaux sur Akeneo.\n\n- **Évolution du schéma sans maintenance** :\n  - Produits dynamiques nécessitant des modifications fréquentes de la structure.\n  - Impossibilité de placer l'application en mode maintenance pour ces évolutions.\n\n- **Respect de l'environnement d'exécution PHP** :\n  - Akeneo est un outil Open Source, dépendant de l'adoption par la communauté.\n  - Environnement standard basé sur MySQL/PHP, essentiel pour l'intégration et l'utilisation par les développeurs.\n  - Nécessité d'éviter les dépendances non standards, comme les bases de données NoSQL.\n\n- **Utilisation d'un ORM** :\n  - Akeneo basé sur Symfony2 et Doctrine2.\n  - Symfony2 : framework PHP populaire pour des applications complexes.\n  - Doctrine2 : ORM le plus utilisé en PHP, permettant l'abstraction des systèmes de base de données relationnelles.\n  - Utilisation de DQL (Doctrine Query Language) pour remplacer SQL, avec conversion automatique au dialecte SQL cible.\n\n- **Interopérabilité avec les systèmes d'information des clients** :\n  - Akeneo doit s'intégrer dans des systèmes d'information souvent rigides.\n  - Nécessité de fonctionner avec des bases de données imposées par les clients.\n  - Limitation d'utilisation de systèmes NoSQL non validés par la DSI.\n\n- **Compatibilité de l'approche EAV avec l'ORM** :\n  - Besoin d'une capacité EAV pour des entités flexibles.\n  - Conflit entre le mapping symétrique de l'ORM et la structure éclatée de l'EAV.\n  - Risque d'impossibilité d'utiliser l'ORM en raison de cette incompatibilité.\n\n- **Manipulation de volumes de données importants** :\n  - Exploration de l'utilisation de NoSQL pour gérer des volumes de données produits et attributs.\n  - Verrou technologique lié à l'adaptation de modèles de données à des systèmes de stockage avec des paradigmes opposés.\n\n- **Travaux de R&D de 2015 : Nouvelle brique PAM (Product Asset Management)** :\n  - Complément de l'offre PIM pour traiter les spécificités liées aux assets.\n  - Confrontation à plusieurs verrous techniques :\n    - **Hétérogénéité des contenus des assets** :\n      - Difficulté d'indexation et de recherche due à la nature complexe des fichiers (ex. : images).\n    - **Volumétrie excessive des fichiers** :\n      - Complexité de stockage, notamment pour des fichiers haute définition.\n    - **Transformations et manipulations des assets** :\n      - Nécessité d'adapter les fichiers à différents supports (ex. : réduction et recompression d'images pour le web)."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons fait face lors de la réalisation de nos travaux de R&D de 2013 un ensemble de contraintes techniques, que nous détaillons dans la suite. \n\nL’évolution du schéma sans mise en maintenance de l’application : En effet, la typologie des produits étant hautement dynamique, les opérations de modification de la structure des produits seront très fréquentes. De ce fait, il n’est pas envisageable de placer l’application en mode maintenance pour faire évoluer le schéma. \nRespect de l’environnement d’exécution PHP : Akeneo étant un outil Open Source, son succès sera forcément lié au succès de son adoption par la communauté. Or l’environnement d’exécution standard attendu par la communauté PHP est basé sur le couple MySQL/PHP.\n\nDe façon à pouvoir pénétrer le monde PHP en laissant la possibilité aux futurs utilisateurs et développeurs de tester rapidement l’application, il faut donc que celle-ci puisse fonctionner sur cette plateforme MySQL/PHP.\n\nC’est donc ici une contrainte technologique forte lié à la communauté. Il n’est pas concevable de l’ignorer en ajoutant par exemple une dépendance avec un outil externe non standard, telle qu’une base de données NoSQL.\n\nUtilisation d’un ORM : Nous avons décidé de baser Akeneo sur le couple Symfony2 / Doctrine2. Symfony2 est un framework PHP développé par la société française Sensio Labs. C’est aujourd’hui un des frameworks les plus utilisés pour les applications à forte complexité fonctionnelle.\n\nDoctrine 2 est le système d’Object Relation Mapping le plus utilisé dans le monde PHP. Il s’intégrer parfaitement dans Symfony2. Il apporte également une couche d’abstraction qui permet aux applications de pouvoir s’exécuter sur différents systèmes de base de données relationnelles sans avoir à recourir à des adaptations côté applications.\n\nAinsi le langage SQL au sein de l’application est remplacé par le DQL (Doctrine Query Langage) qui sera convertit automatiquement à l’exécution dans le dialecte SQL correspondant à la base de données cible.\n\nLors la réalisation de nos travaux de R&D en 2013 et 2014 sur l’outil Akeneo, nous avons confronté et résolu un ensemble d’incertitudes scientifiques et de verrous techniques.\n\nLe premier verrou technique concerne l’interopérabilité de notre outil Akeneo avec les systèmes d’informations de nos clients. En effet, le PIM est un outil qui vient s’intégrer au sein du système d’information de l’entreprise. L’environnement est donc critique et très souvent figés par les Direction du Système d’information. L’application devra donc être capable de fonctionner sur des systèmes de base de données imposées en partie par le client, ou dans un environnement où les systèmes de gestion de données de type NoSQL n’ont pas été validées par la DSI, et sont donc inutilisables.\n\nPar ailleurs, étant donné les contraintes de fonctionnement sur système PHP/MySQL standards, il est nécessaire de fournir une capacité EAV aux entités flexibles. Néanmoins, nous avons d’un côté un ORM, qui va mapper d’une manière symétrique des objets à des tables (1 objet = 1 table). Or la structure même de l’EAV consiste à éclater l’information produit au sein de plusieurs tables. Nous avons donc confronté un deuxième verrou qui concerne la compatibilité d’approche, avec à la clef peut être l’impossibilité d’utiliser l’ORM.\n\nEn outre, de façon à pouvoir manipuler des volumes de données produits et attributs importants, l’utilisation en NoSQL s’est imposée comme piste sérieuse. Le principal verrou technologique auquel nous avons fait face, concerne l’adaptation d’un modèle de données à des systèmes de stockages possédant des paradigmes radicalement opposés.\n\nL’enjeu de nos travaux de R&D de 2015 concerne un complément de notre offre d’une nouvelle brique nommée PAM (Product Asset Management). Cette dernière a pour vocation de compléter notre offre PIM sur les spécificités liées aux assets en y apportant un ensemble d’éléments permettant de traiter un ensemble de problématiques (cf. section 1.1). Nous avons fait face lors de ces travaux de R&D un ensemble de verrous techniques, que nous citons dans la suite :\n\nHétérogénéité des contenu des assets qui sont également difficilement indexables (et par conséquence difficilement recherchables par les utilisateurs) de par leur nature « fermée » (complexité de définir le contenu d’un fichier image ou d’identifier ce qu’il représente). \nVolumétrie excessives des fichiers et la complexité de stocker ces derniers. En particulier lorsque nous sommes amenés à manipuler des fichiers haute définition ayant des volumes assez excessives. \nDes opérations de transformations et des manipulations des assets doivent régulièrement être effectués pour être adaptés à leur support de destination (canal). Par exemple une image HD pourra être utilisée pour un support papier mais devra être réduite et re-compressée pour devenir exploitable par un support web."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Évolution du schéma sans maintenance de l'application :**\n  - Typologie des produits dynamique, nécessitant des modifications fréquentes.\n  - Impossible de placer l'application en mode maintenance pour faire évoluer le schéma.\n\n- **Respect de l'environnement d'exécution PHP :**\n  - Akeneo est un outil Open Source, son succès dépend de l'adoption par la communauté.\n  - Environnement standard attendu : couple MySQL/PHP.\n  - L'application doit fonctionner sur MySQL/PHP pour faciliter les tests par les utilisateurs et développeurs.\n  - Contrainte technologique forte : pas de dépendance à des outils externes non standards (ex. : bases de données NoSQL).\n\n- **Utilisation d'un Object Relation Mapping (ORM) :**\n  - Plateforme basée sur Symfony2 (framework PHP) et Doctrine2 (ORM).\n  - Symfony2 : largement utilisé pour des applications complexes.\n  - Doctrine2 : permet l'exécution sur différents systèmes de bases de données relationnelles sans adaptations.\n  - Remplacement du SQL par DQL (Doctrine Query Language), converti automatiquement au dialecte SQL correspondant.\n\n- **Interopérabilité avec les systèmes d'information des clients :**\n  - Le PIM s'intègre dans le système d'information de l'entreprise, souvent figé par la DSI.\n  - L'application doit fonctionner sur des systèmes de bases de données imposés par le client.\n  - Inutilisabilité des systèmes de gestion de données NoSQL non validés par la DSI.\n\n- **Passage à un nouveau système de traitement des données :**\n  - Adoption d'une nouvelle méthode de gestion des données par les utilisateurs nécessaire.\n  - Intégration dans le système existant et réponse précise aux besoins de stockage.\n  - Utilisation d'Elastic-Search pour améliorer la récupération des données.\n\n- **Contraintes liées à la capacité EAV :**\n  - Nécessité de fournir une capacité EAV aux entités flexibles dans un environnement PHP/MySQL.\n  - ORM mappe des objets à des tables (1 objet = 1 table), tandis que l'EAV éclate l'information produit sur plusieurs tables.\n  - Confrontation à un verrou concernant la compatibilité d'approche, potentiellement rendant l'utilisation de l'ORM impossible.\n\n- **Adaptation à des systèmes de stockage avec paradigmes opposés :**\n  - Nécessité de manipuler de grands volumes de données produits et attributs.\n  - Exploration de l'utilisation de NoSQL comme solution sérieuse face aux verrous technologiques."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons fait face lors de la réalisation de nos travaux de R&D à un ensemble de verrous techniques et d’incertitudes scientifiques, que nous détaillons ci-après. \n\nL’évolution du schéma sans mise en maintenance de l’application : La typologie des produits étant hautement dynamique, les opérations de modification de la structure des produits seront très fréquentes. De ce fait, il n’est pas envisageable de placer l’application en mode maintenance pour faire évoluer le schéma. \n\nRespect de l’environnement d’exécution PHP : Akeneo étant un outil Open Source, son succès sera forcément lié au succès de son adoption par la communauté. Or l’environnement d’exécution standard attendu par la communauté PHP est basé sur le couple MySQL/PHP.\n\nDe façon à pouvoir pénétrer le monde PHP en laissant la possibilité aux futurs utilisateurs et développeurs de tester rapidement l’application, il faut donc que celle-ci puisse fonctionner sur cette plateforme MySQL/PHP. \n\nIl s’agit donc d’une contrainte technologique forte. Il n’est pas concevable de l’ignorer en ajoutant par exemple une dépendance avec un outil externe non standard, telle qu’une base de données NoSQL.\n\nUtilisation d’un Object Relation Mapping (ORM) : Nous avons décidé de baser notre plateforme sur le couple Symfony2 / Doctrine2. Symfony2 est un framework PHP développé par la société française Sensio Labs. C’est aujourd’hui un des frameworks les plus utilisés pour les applications à forte complexité fonctionnelle. \n\nDoctrine 2 est le système d’Object Relation Mapping le plus utilisé dans le monde PHP. Il apporte une couche d’abstraction qui permet aux applications de pouvoir s’exécuter sur différents systèmes de base de données relationnelles sans avoir à recourir à des adaptations côté applications. \n\nAinsi le langage SQL au sein de l’application est remplacé par le DQL (Doctrine Query Langage) qui sera convertit automatiquement à l’exécution dans le dialecte SQL correspondant à la base de données cible.\n\nL’interopérabilité de notre outil avec les systèmes d’informations de nos clients : Le PIM est un outil qui vient s’intégrer au sein du système d’information de l’entreprise. L’environnement est donc critique et très souvent figés par les Directions du Système d’information. L’application devra donc être capable de fonctionner sur des systèmes de base de données imposés en partie par le client, ou dans un environnement où les systèmes de gestion de données de type NoSQL n’ont pas été validés par la DSI, et sont donc inutilisables. \n\nLe passage à un nouveau système de traitement des données : Il faut prendre en compte le fait que notre solution suppose l’adoption d’une nouvelle méthode de gestion des données par nos utilisateurs. Il est donc nécessaire de l’intégrer à leur système et surtout s’assurer qu’elle répond avec précision à leurs besoins en termes de stockage. Pour surmonter ces problématiques, nous avons utilisé une méthode basée sur l’Elastic-Search pour améliorer notre solution. Cela facilite la récupération des données dans les bases de données. \n\nPar ailleurs, étant donné les contraintes de fonctionnement sur système PHP/MySQL standards, il est nécessaire de fournir une capacité EAV aux entités flexibles. Néanmoins, nous avons d’un côté un ORM, qui va mapper d’une manière symétrique des objets à des tables (1 objet = 1 table). Or la structure même de l’EAV consiste à éclater l’information produit au sein de plusieurs tables. Nous avons donc été confronté à un deuxième verrou qui concerne la compatibilité d’approche, avec à la clef peut être l’impossibilité d’utiliser l’ORM. \n\nEn outre, de façon à pouvoir manipuler des volumes de données produits et attributs importants, l’utilisation en NoSQL s’est imposée comme piste sérieuse. Le principal verrou technologique auquel nous avons fait face, concerne l’adaptation d’un modèle de données à des systèmes de stockages possédant des paradigmes radicalement opposés."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Développement d'un système unique de stockage :**\n  - Objectif : Surpasser les défauts des trois composantes du stockage hybride actuel.\n  - Exigences :\n    - Scalabilité verticale (augmentation du volume de données).\n    - Scalabilité horizontale (augmentation du nombre d'utilisateurs simultanés).\n    - Adaptabilité à tous les types et modèles de données du marché.\n    - Gestion performante des données produit.\n\n- **Modélisation des produits :**\n  - Défi : Capturer tous les cas d'utilisation existants sans compromettre la performance des opérations usuelles de gestion de base de données.\n  - Problème : La hiérarchie entre modèles de produits et variantes complique la mise à jour de la base de données et la consistance éventuelle.\n\n- **Création d'un format de données universel :**\n  - Objectif : Interface facile avec tous les formats existants sans nuire à l'intégrité des données ni à la performance.\n  - Problématique : Multiplicité des formats et modèles de données sur le marché.\n  - Solution envisagée : Développement d'un nouveau format universel et d'une API performante pour l'importation et l'exportation de données.\n\n- **Garantie de traitement des requêtes utilisateurs :**\n  - Exigence : Assurer la stabilité et la performance du système lors de pics de charge.\n  - Solution : Mise en place de l'auto-scaling pour un approvisionnement dynamique et automatique des ressources.\n\n- **Tests pour validation et intégration de nouveaux modules :**\n  - Défi : Réaliser des tests rapides sans nuire à la performance du système.\n  - Situation actuelle : Ajout constant de nouvelles fonctionnalités et nécessité de lancer des centaines de builds quotidiennement.\n  - Objectif : Supporter une charge 2 à 3 fois supérieure chaque année, un volume non documenté dans la littérature."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons fait face, dans le cadre de nos travaux R&D de 2017 sur notre système d’information pour la gestion de données produit, à un ensemble de verrous technologiques et d’incertitude scientifiques : \n\nPourrions-nous développer un système unique de stockage, dépassant les défauts des trois composantes de notre stockage hybride actuel ? En particulier, notre système doit être scalable aussi bien verticalement (volume de données) qu’horizontalement (nombre d’utilisateurs simultanés), être adapté à tous les types et modèles de données du marché, et permettre une gestion performante des données produit. \nPourrions-nous mettre au point une modélisation des produits capable de capturer tous les cas d’utilisation existants sans sacrifier la performance pour les opérations usuelles de gestion de base de données ? La notion de hiérarchie introduite entre les modèles de produits et les variantes implique une difficulté pour mettre à jour la base de données et atteindre ce qu’on appelle la consistance éventuelle. \nEst-il possible de créer un format de données universel, pouvant s’interfacer facilement avec tous les formats existants sans nuire à l’intégrité des données et à la performance du service ? Il existe sur le marché de nombreux formats et modèles de données, et nous ne pouvons pas développer de connecteurs pour tous. Nous devons développer un nouveau format universel et une API performante permettant l’importation et l’exportation de données depuis et vers tous les systèmes d’information. \nPourrions-nous garantir le traitement de toutes les requêtes des utilisateurs sans faille ainsi que la stabilité et la performance du système même en cas de forts pics de charge ? Notre solution doit permettre l’auto-scaling, c’est-à-dire l’approvisionnement dynamique et automatique de ressources pour pallier aux surcharges, que ce soit en nombre de données comme en nombre d’utilisateurs simultanés. \nEnfin, comment réaliser l’ensemble des tests nécessaires à la validation et l’intégration de nouveaux modules de manière rapide sans nuire à la performance du système ? Nous ajoutons sans cesse de nouvelles fonctionnalités et devons lancer des centaines de builds chaque jour pour les tester. Une telle volumétrie ne se trouve pas dans la littérature, et ainsi notre système doit pouvoir supporter 2 à 3 fois plus de charge chaque année."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Product Variation Modeling\n- **Objectif** : Enrichir automatiquement les variations du modèle sans affecter les performances.\n- **Questions clés** :\n  - Gestion des relations entre associations à tous les niveaux du modèle de données sans impact sur la performance et la scalabilité (lecture/écriture).\n  - Autorisation de modifications des structures et niveaux hiérarchiques des modèles produit tout en minimisant le délai de traitement dans un contexte de millions de références.\n\n#### Product Platform\n- **Objectif** : Outil compatible avec toutes les infrastructures clients.\n- **Questions clés** :\n  - Détection des modélisations et volumétries des catalogues produits on-premises pour fournir des indicateurs de performance et de scalabilité.\n  - Amélioration de la recherche intelligente de produit (smart search) pour un catalogue de 1 million de produits et 20 000 catégories.\n\n#### Reference Entities\n- **Objectif** : Modélisation flexible, dynamique et performante des référentiels sans intervention technique.\n- **Questions clés** :\n  - Développement d'une modélisation générique d’entités pour capturer tous les cas d’utilisation sans sacrifier la performance des opérations de gestion de base de données.\n  - Création d'une modélisation flexible pour des entités métier génériques, éditable par l’utilisateur final, garantissant de hautes performances.\n  - Assurance de la migration automatique et générique de modèles de données personnalisés vers la modélisation dynamique.\n\n#### Cloud Offer\n- **Objectif** : Gestion des plateformes de production avec des outils indépendants tout en maintenant l'opérationnalité.\n- **Questions clés** :\n  - Disponibilité de changements impactant l'infrastructure de production via une application web de manière sécurisée.\n  - Mise en place de l'automatisation sans refonte de l'infrastructure ni impact sur les clients."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nProduct Variation Modeling\nNotre moteur de règles doit permettre d’enrichir automatiquement les variations, quel que soit le niveau du modèle, sans impacter les performances :\nPourrions-nous gérer les relations entre ces associations, et ce quel que soit le niveau du modèle de données, sans impacter la performance et la scalabilité en lecture et écriture ?\nPourrions-nous autoriser les modifications de structures, de niveau hiérarchique des modèles produit ainsi que du niveau de ces propriétés, et ce, en limitant le délai de traitement au minimum dans un contexte de millions de références ?\nProduct Platform\nNotre outil doit pouvoir fonctionner avec toutes les infrastructures de nos clients. En particulier, nous devons détecter l’ensemble des modélisations et volumétries des catalogues produits des clients on-premises afin de fournir des indicateurs de performances et de scalabilité :\nPourrions-nous trouver une solution pour rendre la recherche intelligente de produit (smart search) plus performante sur un catalogue de 1M de produits et 20k catégories ?\nReference entities\nL’objectif de ce sous-projet est de permettre une modélisation flexible, dynamique et performante des différents référentiels sans nécessité la moindre action technique :\nPourrions-nous mettre au point une modélisation générique d’entités capable de capturer tous les cas d’utilisation existants, sans sacrifier la performance pour les opérations usuelles de gestion de base de données ?\nLes travaux des années précédentes nous ont permis de réaliser une modélisation de produit flexible et performante. L’idée ici est de créer une modélisation flexible pour des entités métier génériques ; une forme de méta-modélisation, éditable par l’utilisateur final, et garantissant de hautes performances.\nPourrions-nous assurer la migration automatique et générique de modèles de données personnalisés réalisés par des développeurs dans le cadre de projets clients vers notre modélisation dynamique ?\nCloud Offer\nLa gestion des plateformes de production s’appuie sur plusieurs outils indépendants, que nous souhaitons conserver pour ne pas changer trop profondément notre manière d'opérer. La mise en place d'automatisation demande une standardisation forte que nous n’avons pas sur l’ensemble de l'infrastructure :\nPourrions-nous rendre disponibles des changements impactant une infrastructure de production via une application web de manière sécurisée dans l’impact et dans ses accès ?\nEst-il possible de pouvoir mettre en place cette automatisation sans refondre l’infrastructure ni impacter nos clients ?"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### Asset Manager\n\n- **Moteur de règles** :\n  - Doit enrichir automatiquement les variations, indépendamment du niveau du modèle, sans affecter les performances.\n  \n- **Création d'un moteur** :\n  - Objectif : établir un lien automatique entre ressources et produits.\n  - Basé sur les informations des produits et des ressources.\n  \n- **Extraction dynamique d'informations** :\n  - Nécessité d'un format permettant l'extraction dynamique des informations.\n  - Besoin de définir des règles statiques pour la famille de ressources.\n  \n- **Utilisation d'une brique technologique existante** :\n  - Possibilité d'utiliser un \"Rule Engine\" pour enrichir l'information produit.\n\n- **Conventions de nommage des fichiers** :\n  - Exploration de l'utilisation des conventions de nommage pour enrichir automatiquement les ressources.\n\n- **Extraction d'informations depuis le code ou le nom de fichier** :\n  - Création d'un moteur capable d'extraire des informations pour enrichir automatiquement les ressources.\n  - Basé sur des règles métiers personnalisées par les utilisateurs.\n\n- **Migration automatique du modèle de données** :\n  - Possibilité d'assurer la migration générique du modèle de données de la brique technologique précédente (PAM) vers la nouvelle modélisation dynamique.\n\n#### Difficultés rencontrées lors de la migration automatique des ressources du PAM vers l'AM :\n\n1. **Définition d'une structure flexible** :\n   - Nécessité d'une structure flexible pour accueillir les ressources du PAM.\n\n2. **Migration des ressources** :\n   - Difficultés spécifiques liées à la migration effective des ressources."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAsset Manager \nNotre moteur de règles doit permettre d’enrichir automatiquement les variations, quel que soit le niveau du modèle, sans impacter les performances :\nSerions-nous capables de créer un moteur qui déterminerait un lien automatique entre ressource et produit en se basant à la fois sur les informations contenues dans les produits et les informations dans ces ressources. Existe-t-il un format nous permettant d’extraire ces informations de manière dynamique tout en définissant des règles statiques à la famille de ressource. Pourrions-nous tirer parti d’une brique technologique existante “Rule Engine” capable d’enrichir l’information produit pour accomplir cette tâche ?\nEst-il possible de tirer parti des conventions de nommage des fichiers des ressources pour enrichir automatiquement ces ressources ?\nNous serait-il possible de créer un moteur capable d’extraire des informations depuis le code ou le nom de fichier d’une ressource pour l’enrichir de manière automatique en nous basant sur des règles métiers utilisateurs personnalisées ?\nPourrions-nous assurer la migration automatique et générique du modèle de données provenant de la précédente brique technologique (PAM) vers notre nouvelle modélisation dynamique ?\n\nEn outre, lors de la migration automatique des ressources du PAM dans l’AM, nous avons été confrontés aux deux difficultés suivantes. La première difficulté est liée à la définition d’une structure flexible permettant de recevoir les ressources du PAM. La seconde difficulté se présente au niveau de la migration des ressources à proprement parlé."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### New Rules Engine\n- Introduction d'une action de concaténation pour combiner valeurs d'attributs et textes prédéfinis en une seule valeur.\n- Chaque valeur d'attribut peut être localisée et contenir des informations supplémentaires (ex. : format de la date).\n- Problématique de synchronisation entre actions utilisateurs et données soumises par le formulaire.\n- Utilisation de la fonctionnalité \"drag-and-drop\" nécessite le hook `useFieldArray` pour simplifier le déplacement d'éléments.\n- Le hook `useFieldArray` propose des méthodes comme `append`, `move`, et `insert` pour manipuler les tableaux de données.\n- Limitation : chaque action doit être déclenchée après affichage via la méthode `render`, nécessitant une pile d'actions dépilée par `render`.\n\n#### Product Quality\n- Frein à l'adoption : nécessité pour les clients de développer des familles de mesures personnalisées.\n- Question sur la capacité à modéliser l'expérience utilisateur dans le PIM pour définir des familles de mesures.\n- Problème de mise à jour des familles de mesures une fois intégrées dans le catalogue.\n\n#### Association quantifiées\n- Incertitude scientifique sur la quantification des relations d'association produit à produit.\n- Objectif : associer un produit à plusieurs versions d'un autre produit (ex. : une table à cinq chaises).\n- Problématique d'exportation des informations produits sous format de fichier plat : besoin de définir un format adéquat.\n\n#### Amélioration des exécutions de jobs (suivi et arrêt)\n- Question sur la possibilité de transmettre des informations sur l'exécution d'un job et d'estimer sa date de fin de manière fiable.\n- Besoin de transmettre un signal d'arrêt pour un job en cours d'exécution.\n\n#### Évaluation du niveau de grammaire d’un texte\n- Difficulté technique : développement d'un outil tiers avec connexion simple, sans sur-couche d’authentification, et gestion des coûts d’usage.\n\n#### Connection tracking\n- Transition d'information via API avec tâches backend de synchronisation, souvent nocturnes et invisibles pour les utilisateurs.\n- Intervention d'intégrateurs parfois nécessaire pour vérifier la bonne synchronisation.\n- Besoin des utilisateurs de confirmer la disponibilité des produits enrichis sur les canaux de vente.\n\n#### First PXM on the market\n- Transformation dépendante des développements spécifiques des clients effectués en dehors du code de l'application pour favoriser l'offre SaaS.\n- Problème : utilisateurs n'ont pas la main sur les synchronisations entre systèmes, généralement pilotées par des tâches backend programmées.\n- Délai de mise à jour des données après correction par l'équipe marketing peut entraîner une perte de valeur pour les clients.\n- Impact économique potentiel élevé en période de soldes dû à des erreurs visuelles ou des données sensibles incorrectes (ex. : dates de disponibilité)."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\nNew Rules Engine\n\nUne des fonctionnalités qui a été introduite est l’action qui permet de concaténer. Cette action permet de concaténer les valeurs d’attributs et les textes prédéfinis dans une seule valeur d’attribut. Chaque valeur d’attribut peut être localisée et contenir des informations supplémentaires (comme le format de la date par exemple).\n\nLa problématique est de pouvoir synchroniser les actions des utilisateurs avec la donnée qui sera soumise par le formulaire.\n\nPar exemple, lorsqu’on utilise la fonctionnalité de “drag-and-drop” (glisser-déplacer) sur une ligne du formulaire, cela veut dire qu’il faut déplacer un ou plusieurs éléments de la liste. Nous avons donc dû mettre en place le “hook” useFieldArray qui permet justement de simplifier ce genre d'opération. Ce hook fournit des méthodes comme `append`, `move` ou `insert` qui permettent ainsi de manipuler facilement les tableaux de données.\n\nUne autre limitation que nous avons dû adresser est le fait que chaque action ne peut pas être appelée à la suite. Chaque action est déclenchée après avoir affiché le résultat via la méthode `render` : il faut donc insérer un élément, puis l’afficher, et ensuite seulement le déplacer. A cet effet, nous avons ainsi mis en place une pile d’actions qui est dépilée une fois par `render`.\n\nProduct Quality\n\nL’un des freins les plus forts à l’adoption de la fonctionnalité provient de la nécessité pour nos clients de faire des efforts en développement logiciel supplémentaires pour modéliser des familles de mesures personnalisées. \nSerions-nous capables de modéliser une expérience utilisateur directement dans le PIM permettant la définition complète de familles de mesures ?\nQu’en est-il de la mise à jour de ces familles de mesures une fois utilisées dans le catalogue ?\n\nAssociation quantifiées\n\nL’incertitude scientifique réside dans le fait d’être capable d’apposer une quantité sur la relation d’association de produit à produit. Nous souhaitons être capable d’associer à un produit, une ou plusieurs versions d’un autre unique produit.\nPar exemple, nous souhaitons associer à une table cinq autres versions de la même chaise qui formerait un mobilier de jardin complet (dans cet exemple, nous souhaitons associer de manière quantifiée notre produit “table” et notre produit “chaise” avec une quantité de cinq).\nUne dernière problématique se pose au niveau de l’export sous format de fichier plat de ces informations produits. Serait-il possible de définir un format nous permettant de les modéliser ?\n\nAmélioration des exécutions de jobs (suivi et arrêt)\n\nNous sera-t-il possible de transmettre des informations liées à l'exécution d’un job et d’estimer la date de fin d’un job de manière fiable ? \nNous sera-t-il possible de transmettre le signal d’arrêt d’un job en cours d’exécution ?\n\nEvaluation du niveau de grammaire d’un texte\nLa principale difficulté technique est de développer un outil tiers proposant une connexion simple, sans sur-couche d’authentification ainsi que l’architecture à mettre en œuvre et les coûts d’usage associés.\nConnection tracking\nLa transition d'information se fait principalement au travers de l'API via des tâches backend de synchronisation souvent nocturnes et invisibles pour nos utilisateurs. Cela nécessite parfois l'intervention d'intégrateurs afin de vérifier que tout s'est bien passé.\nNos utilisateurs aimeraient savoir si les produits enrichis lors de leur journée de travail sont bien disponibles sur les différents canaux de vente.\nFirst PXM on the market\n\nL’une des clés de cette transformation repose sur le fait que les développements spécifiques de nos clients soient de plus en plus effectués à l’extérieur du code de notre application afin de favoriser l’offre SaaS.\n\nUn autre point important autour de cette problématique est que nos utilisateurs n’ont pas forcément la main sur les synchronisations effectuées entre les différents systèmes. Les synchronisations sont généralement effectuées par des tâches backend pilotées par des crons et s’exécutent à des intervalles réguliers pouvant aller d’une fois par jour à toutes les 15 minutes.\n\nNéanmoins, si un utilisateur de l’équipe marketing repère une erreur sur une fiche produit et la corrige, il devra attendre la prochaine synchronisation pour que ces données soient mises à jour sur le canal de vente. Ce délai peut créer une vraie perte de valeur pour certains clients car les acheteurs peuvent arrêter d’acheter un produit pendant ce laps de temps. L’impact économique pour l’entreprise peut être énorme en période de soldes par exemple.\n\nPlusieurs problèmes en ce sens nous ont été remontés par nos clients en rapport avec un mauvais visuel sur un produit ou encore des données sensibles incorrectes par rapport à des dates de disponibilité."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Actions de Modifications en Masse des Ressources\n- **Problème actuel** : Modification et suppression des ressources uniquement de manière unitaire via l’interface utilisateur, freinant l'adoption et la productivité.\n- **Conséquence** : Les erreurs d'importation nécessitent des corrections manuelles pour chaque ressource, ce qui est chronophage.\n- **Incertitudes à lever** :\n  - Possibilité d'utiliser un processus de mise à jour en masse similaire à celui de l’entité produit pour les ressources.\n  - Efficacité de la mise à jour et suppression en tâche de fond comme pour les produits.\n\n#### Suppression en Masse d’Enregistrements pour les Entités de Référence\n- **Problème actuel** : Suppression des enregistrements uniquement de manière unitaire, limitant l'adoption des entités de référence.\n- **Conséquence** : Processus de suppression très chronophage pour les utilisateurs.\n\n#### Table Attribute\n- **Problème précédent** : Utilisation d'un bundle non flexible et non maintenu pour le Table Attribute PIM.\n- **Risques** : Développement d'une solution trop complexe, risquant de devenir un fourre-tout ou de remplacer un tableur.\n- **Objectif principal** : Permettre une recherche puissante et aisée dans la table (exemples : produits avec plus de 20% de lait, allergènes, composants fabriqués en Espagne).\n- **Défis techniques** :\n  - Complexité du stockage des données sur MySQL et ElasticSearch.\n  - Limitation d’ElasticSearch à 1000 champs, nécessitant des limites sur les colonnes et lignes pour maintenir la réactivité et la fonctionnalité.\n\n#### Job d’Export Sur Mesure\n- **Problème actuel** : Ligne d'en-tête et colonnes d'export dépendent de la structure PIM et des configurations globales, peu personnalisables.\n- **Limitations** :\n  - Valeurs produits non sélectionnables par l'utilisateur (exemple : informations de collection de ressources).\n  - Absence de configurations pour des opérations simples à l’export (nettoyage de données, concaténation, conversion d'unités).\n  - Valeurs par défaut non configurables pour les cellules vides, rendant l'interprétation des données difficile.\n- **Problème de transmission** : Les utilisateurs doivent modifier manuellement les fichiers exportés pour répondre aux exigences spécifiques des partenaires.\n\n#### Process Tracker\n- **Problématique** : Mise à jour des statuts d'exécution sans intervention manuelle, l'écran de suivi des jobs étant limité par le composant ORO.\n- **Besoins** :\n  - Filtrage intuitif des exécutions par utilisateurs et types de jobs via des listes déroulantes.\n  - Homogénéité de l'expérience utilisateur dans les différentes sections du PIM.\n\n#### Free Trial\n- **Enjeu principal** : Développer une version cloud de la version communautaire tout en intégrant les fonctionnalités de l’édition entreprise (EE).\n- **Challenges techniques** :\n  - Rapidité de mise à disposition de l'instance malgré la complexité de déploiement.\n  - Garantir une expérience utilisateur avec un catalogue de produits préchargés représentatif.\n  - Présentation des fonctionnalités de l’édition Entreprise sans accès direct."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous présentons dans cette partie les incertitudes scientifiques et les verrous technologiques que nous devons lever afin de répondre aux objectifs cités précédemment : \n\nActions de modifications en masse des ressources\nDepuis la sortie de notre gestionnaire de ressource en 2020, la modification ainsi que la suppression des ressources ne peuvent se faire que de manière unitaire (ressource par ressource via l’interface utilisateur). Ceci est un frein à l’adoption du gestionnaire de ressource et à la productivité de nos utilisateurs. \nEn effet, toute erreur résultant d’un import de ressources (par fichier plats ou API) nécessite une action manuelle corrective passant par l’interface utilisateur pour chaque asset nécessitant une correction. \nDe plus, cette étape de correction reste très chronophage pour un utilisateur ayant pour charge de corriger des dizaines de ressources, voire de devoir supprimer des centaines de ressources une à une.\nLa mise à jour d’entités dépendant d’une structure flexible est déjà une fonctionnalité possible sur l’entité produit. Les entités de type ressources dépendent elles aussi d’une entité flexible. \nPour répondre à notre besoin, nous nous sommes confrontés à plusieurs incertitudes : \nPouvons-nous utiliser un processus de configuration de mise à jour en masse similaire à celui de l’entité produit pour les entités de type ressources ?\nEst-il possible d’effectuer cette mise à jour de manière efficiente en exécutant la mise jour et la suppression de produit dans un processus lancé en tâche de fond comme pour le produit ?\n\nSuppression en masse d’enregistrements pour les entités de référence\nDepuis la sortie des entités de références en 2019, la suppression des enregistrements ne peut se faire que de manière unitaire (enregistrement par enregistrement via l’interface utilisateur). Ceci est un frein à l’adoption des entités de références et à la productivité de nos utilisateurs. \nCette étape de suppression reste très chronophage pour un utilisateur ayant pour charge de supprimer des dizaines d’enregistrements uns à uns.\n\n\nTable attribute \nAvant nos travaux sur le Table Attribute PIM natif, il existait un bundle sur la marketplace qui permettait de contourner le problème, mais cette solution n’était pas flexible et ne permettait pas de répondre à tous les besoins. D’autre part, cette solution n'était pas maintenue par Akeneo et ne garantissait aucun niveau de service respectable. L’autre risque que présentait ce développement était de concevoir une solution trop complexe et qui devient une sorte de fourre-tout ou qui remplace un tableur comme Excel.\nFinalement, et ce qui correspond à la problématique principale, nous voulions que l’utilisateur puisse rechercher dans cette table aisément et de manière puissante, par exemple “tous les produits qui ont plus de 20% de lait”, “les produits avec des allergènes” ou “tous les produits qui ont au moins un composant fabriqué en Espagne”.\nTechniquement, la conception autour du stockage de la donnée était le plus compliquée, et notamment sur MySQL et ElasticSearch. Pour ce dernier, il existe une notion de mapping qui nous empêche de rajouter plus de 1000 champs. Par conséquent, lorsqu’un champ est ajouté, une ligne et une colonne le sont également, ce qui limite considérablement les options, sachant que nous souhaitons qu’ElasticSearch soit très réactif lorsque nous effectuons une recherche. Nous devons donc fixer des limites (colonnes par attribut, lignes par valeur de produit, options par colonne d’attribut) pour être sûr que le PIM reste fonctionnel, et que l'UI/UX ne soit pas dégradé non plus.\nJob d’export sur mesure\nLa ligne d'en-tête et le nom des différentes colonnes du fichier exportés dépendent à la fois de la structure du PIM, de ses attributs et du paramétrage de l’export des labels dans le profil d’export.\nDe la même manière, les valeurs produits exportées sont peu personnalisables et dépendent, pour certains types d’attributs, de configurations portées au niveau du profil d’export et restent globales à tout le fichier exporté et ne sont pas personnalisables à la colonne.\nDe même en fonction de l’attribut auxquelles elles sont rattachées, certaines valeurs produit sont en réalité composées de plusieurs valeurs exportables qui ne sont aujourd’hui pas sélectionnables par un utilisateur. Par exemple, pour une valeur de type collection de ressource on peut retrouver pour chaque ressource de cette collection les informations suivantes :\nFilekey: clé interne au PIM correspondant au fichier\nFilename: le nom du fichier\nLabel: libellé de la ressource\nAujourd’hui, il est seulement possible d’exporter la clé interne au PIM du fichier dans l’export produit. Impossible pour un utilisateur d’exporter le libellé ou le nom de fichier associé à l’une des ressources de la collection.\nLe PIM ne permet pas la configuration d’opérations simples à l’export comme :\nle nettoyage de données: suppression ou remplacement de caractères, suppression des balises HTMLs automatiques\nla concaténation de valeurs au sein d’une même colonne\nla conversion de données de mesures dans une unité différente\nDe même, le PIM ne permet pas de configurer des valeurs par défaut lorsque la valeur produit est vide. Ainsi, la cellule apparaît vide dans le fichier CSV ou XLSX. Il est impossible pour un utilisateur de discerner si la colonne pour cette cellule est vide car la famille du produit en question n’a pas l’attribut rattaché à la colonne ou alors s’il y a effectivement une valeur produit mais celle-ci est simplement vide.\nDe nos jours, la transmission d’information produits peut se faire selon une multitude de formats de tableaux sur lesquels les différentes parties doivent se mettre d’accord. Ainsi, lorsqu’un utilisateur doit transmettre des fichiers produits au format CSV, XLSX à un partenaire qui attend une structure de tableau qui lui est propre (Nom de colonne, format des valeurs particuliers), nos utilisateurs doivent effectuer manuellement ces changements sur le fichier exporté par le PIM pour que ce dernier soit conforme aux exigences du partenaire.\n\nProcess tracker \nLa problématique consiste à mettre à jour les statuts d'exécution sans action manuelle de l'utilisateur dans l'écran de suivi des jobs alors que cet écran est conçu techniquement avec un composant ORO ne permettant nativement la mise à jour de cellule de manière scalable.\nComment permettre un filtrage plus intuitif des exécutions au sein de ces jobs via les technologies modernes du PIM ?\nFiltrer sur les utilisateurs via une liste déroulante.\nFiltrer sur les types de jobs via une liste déroulante.\nComment proposer une expérience utilisateur homogène aux différents endroits du PIM ou la liste des jobs exécutions est affichée ?\nFree Trial\nLe principal enjeu de ce projet est de mettre en place une version cloud de la version communautaire tout en présentant les fonctionnalités de l’édition entreprise (EE), avec un accompagnement directement dans l’outil.\nLes challenges techniques à relever sont nombreux\nl’instance doit pouvoir être mise à disposition dans un délai rapide malgré la complexité de la chaîne de déploiement,\nl’expérience utilisateur doit être garantie par un catalogue de produits préchargés suffisamment fournis pour être représentatif de toutes les industries,\nles fonctionnalités de la version Entreprise doivent pouvoir être présentées sans y donner accès."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Amélioration du savoir-faire** : Projet axé sur l'amélioration de concepts ou technologies existants mais récents.\n  \n- **Verrous technologiques rencontrés** : De nombreux obstacles identifiés lors des travaux de 2021, nécessitant des recherches supplémentaires.\n\n- **Problème de données** :\n  - Focalisation sur l'investissement early stage dans des technologies de rupture.\n  - Données très éparses, avec un historique de 10 à 15 ans selon les technologies.\n  - Modèles de réseaux de neurones profonds nécessitant un large historique et une fréquence élevée de points de donnée.\n\n- **Phase de recherche et optimisation** :\n  - Longue phase de recherche, features engineering et post-processing pour optimiser les modèles.\n  - Mise en place de boucles d’optimisation pour définir les hyperparamètres des modèles adaptés.\n  - Objectif d'éviter l’overfitting en raison de la faible quantité de données.\n\n- **Hétérogénéité des modèles** :\n  - Mobilisation de l’état de l’art en informatique pour identifier des métriques d’évaluation appropriées.\n  - Comparaison des algorithmes et sélection des modèles les plus adaptés.\n\n- **Développement d'algorithmes** : Création d'algorithmes combinant les modèles existants pour mieux répondre aux besoins spécifiques du projet."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNotre travail consiste donc en une amélioration du savoir-faire concernant des concepts ou technologies existants mais récents.  \nEn effet malgré les technologies déjà existantes, au cours des travaux de l’année 2021, nous avons rencontré de nombreux verrous technologiques qui nous ont amenés à effectuer des recherches supplémentaires.  \n\nLe premier réside dans la donnée elle-même, puisque nous nous focalisons sur l'investissement early stage sur des technologies de rupture, les données sont très éparses. De même, notre dataset sur les investissements ne présente qu'entre 10 et 15 ans d'historique en fonction des technologies. Comme les modèles de réseaux de neurones profonds (utilisés en Deep Learning) requièrent un large historique de données et une fréquence importante dans l'occurrence de points de donnée, il a donc fallu procéder à une longue phase de recherche, de features engineering et post processing d'une manière générale pour optimiser les modèles testés.  \n\nDe plus, nous avons dû mettre en place de nombreuses boucles d’optimisation dans le but de définir les hyperparamètres des modèles les plus adaptés à la situation, n’ayant que très peu de données et voulant éviter l’overfitting.  \n\nAussi, face à l’hétérogénéité des modèles existants et testés, nous avons mobilisé l’état de l’art de la recherche en informatique dans le but de trouver des métriques d’évaluation adaptées pour comparer les algorithmes et choisir les modèles.  \nEnfin, les modèles existants n’étant pas parfaitement adaptés à nos besoins, nous avons dû développer des algorithmes combinant ceux-ci."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrou technologique principal :** \n  - Tenue des roulements et étanchéité des paliers à une accélération de 3500 g.\n  \n- **Problèmes rencontrés lors des tests internes :**\n  - Roulements à double rangée de rouleaux grillés.\n  - Précision nécessaire des pièces recevant les roulements non anticipée (tolérance de forme et rugosité).\n  - Surdosage de graisse initial, entraînant une évacuation incorrecte et un échauffement excessif à l'intérieur du roulement.\n\n- **Difficultés dans la conception de la version caséine :**\n  - Augmentation de la taille de la machine existante (F5000) de 1,7 fois pour atteindre la taille de la D6LN.\n  - Problèmes de dimensionnement au niveau du palier d’alimentation et des roulements.\n  - Nécessité d'une phase de réflexion importante sur la modélisation et le développement des pièces.\n\n- **Objectif final :**\n  - Trouver des solutions aux défis techniques pour développer une centrifugeuse D6LN efficace et économe en énergie."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLe verrou technologique principal de ce projet de recherche est la tenue des roulements et l’étanchéité des paliers à une accélération de 3500 g de la centrifugeuse. Au cours des tests réalisés en interne, les nouveaux roulements à double rangée de rouleaux que nous avons installés ont grillé. L’état de l’art et nos pré-études ne nous avaient pas permis d’anticiper la précision nécessaire des pièces qui reçoivent les roulements, que ce soit dans la tolérance de forme ou de rugosité notamment. De plus, nous nous sommes rendu compte que nous avions mis trop de graisse au départ. Cette dernière n’a pas pu s’évacuer correctement et cela a donc créé un échauffement trop important à l’intérieur du roulement.\n\nD’autre part, nous avons rencontré des difficultés dans la conception de la version caséine. A partir d’une machine existante de taille plus petite (F5000), l’augmentation de la taille (x 1,7) pour atteindre celle de la D6LN a engendré des problèmes de dimensionnement au niveau du palier d’alimentation et des roulements. Nous avons donc dû avoir une importante phase de réflexion sur la modélisation et le développement de ces pièces.\n\nTrouver les solutions à ces défis techniques nous permettra de développer une nouvelle centrifugeuse D6LN efficace et économe en énergie."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Problèmes de réduction de puissance** : Difficultés à concilier ajustements des pièces nouvellement conçues avec les performances d’une machine standard.\n  \n- **Centrifugeuse D5LX** :\n  - Problèmes d’équilibrage de la vis observés après réduction de la sortie liquide.\n  - Règles d’équilibrage habituelles inadaptées pour déterminer la position et la quantité des masses de compensation.\n  - Influence des niveaux d’air et d’eau sur les modalités d’équilibrage de la vis.\n  - Difficulté majeure : équilibrer la vis dans l’eau et l’air, alors que l’équilibreuse ne fonctionne que dans l’air.\n  - Solution développée : Outil de calcul pour simuler les filets de la vis et l’équilibrage.\n\n- **Diminution des diamètres de la vis** :\n  - Problématiques de fréquences propres et de torsion non anticipées.\n  - Changement de design trop important par rapport à une vis classique, nécessitant une prise en compte des sollicitations en torsion dans les calculs de fréquence et de résistance.\n  - Révision complète des méthodes de conception pour renforcer la vis et assurer un coefficient de sécurité suffisant.\n\n- **Conception d’un réducteur** :\n  - Objectif : Réducteur compact et léger en porte à faux à l’extérieur.\n  - Aléas techniques : Importantes vibrations subies par le réducteur lors de l’augmentation de vitesse de la centrifugeuse.\n  - Difficulté à monter un petit réducteur en raison des couples générés.\n  - Concentration des réflexions sur l’architecture du réducteur, notamment l’agencement des pignons."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nParmi les problèmes rencontrés pour réduire la puissance consommée tout en maintenant les performances d’une machine standard, nous pouvons citer les difficultés à allier ajustements des pièces nouvellement conçues et performances. \n\nAprès réduction de la sortie liquide, des problèmes d’équilibrage de la vis ont été observés après des essais de fonctionnement de la centrifugeuse D5LX. Nous nous sommes rendu compte que nos règles d’équilibrage habituelles n’étaient plus adaptées pour déterminer la position et  la quantité des masses de compensation. En effet, les niveaux d’air et d’eau au sein de la machine influent sur les modalités d’équilibrage de la vis. La difficulté majeure a été d’équilibrer la vis à la fois dans l’eau et l’air, sachant que ne l’équilibreuse ne fonctionne que dans l’air. Pour y remédier, nous avons développé un outil de calcul permettant de simuler les filets de la vis et l’équilibrage.\n\nLa diminution des diamètres aux extrémités de la vis a également engendré des problématiques de fréquences propres et surtout de torsion, que nous n’avions pas anticipé. En effet, le changement du design était trop important par rapport à une vis classique, que les sollicitations en torsion devaient être considérées dans les calculs de fréquence et de résistance de la vis et du bol. Nous avons donc dû revoir complètement nos méthodes de conception pour aboutir à un renforcement de la vis et à un coefficient de sécurité suffisant. \n\nLa conception d’un réducteur, compact et léger, en porte à faux à l’extérieur a soulevé de nombreux aléas techniques. En effet, lorsque la centrifugeuse monte en vitesse, le réducteur subit d’importantes vibrations qu’il est difficile de monter un petit réducteur en raison des couples. Nous avons ainsi concentré notre réflexion sur l’architecture du réducteur, en particulier au niveau de l’agencement des pignons entre eux."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- Difficultés rencontrées pour regrouper les trois dégrilleurs :\n  - Nécessité d'assurer la compatibilité des fonctionnalités et performances des chaînes et châssis préexistants avec une conception unique.\n  - Nécessité d'imaginer et expérimenter de nouvelles conceptions pour garantir la modularité (base et tête commune).\n  - Maintien du dégrillage par l’aval et spécificité sur le nettoyage des dents.\n\n- Problèmes lors des tests internes :\n  - Blocage des nouveaux maillons de chaîne.\n  - Casse de nombreuses dents du dégrilleur.\n  - Nécessité de redéfinir la conception et le montage des éléments.\n  - Analyse de la trajectoire de la chaîne d’écran et calculs de dimensionnement effectués.\n\n- Résultats des tests :\n  - Mise en évidence de pertes de charge non prévues lors du développement du dégrilleur.\n  - Hypothèse que ces pertes de charge sont dues au châssis, à confirmer."}, {"role": "assistant", "content": "Au cours du projet Aqualine, nous avons rencontré des difficultés pour regrouper les trois dégrilleurs car nous devions assurer la compatibilité des différentes fonctionnalités et performances des chaînes et des châssis préexistants avec une unique conception. Nous avons donc dû imaginer et expérimenter de nouvelles conceptions afin d’assurer une modularité (base et tête commune) tout en conservant le dégrillage par l’aval et notre spécificité sur le nettoyage des dents.\n\nLors des tests réalisés en interne, nous avons été confrontés à un blocage des nouveaux maillons de chaîne et à une casse de nombreuses dents du dégrilleur. Nous avons ainsi dû redéfinir la conception et le montage de ces éléments par une analyse de la trajectoire de la chaîne d’écran et des calculs de dimensionnement.\n\nPar ailleurs, ces tests ont également permis de mettre en évidence des pertes de charge que nous n’avions pas prévu lors du développement du dégrilleur. Nous supposons que ces pertes sont dues au châssis, cela restant à confirmer."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Difficultés rencontrées** : Problèmes pour assurer une qualité fiable et constante de l’huile d’olive obtenue.\n  \n- **Configuration des pales** : Utilisation de pales en forme de « plough shape » pour maîtriser le malaxage dans la décanteuse et optimiser l’extraction de l’huile.\n\n- **Problèmes identifiés** :\n  - Décharges solides discontinues en sortie de machine, affectant à la fois le côté solide et le côté huile.\n  - Problèmes de convoyage avec toutes les pales, entraînant un transfert en forme de bouchon avant la sortie de la centrifugeuse.\n  - Usure observée dans la partie cylindrique du bol.\n\n- **Solutions mises en œuvre** :\n  - Modification de la conception de la vis pour améliorer le convoyage et faciliter l’extraction de l’huile d’olive.\n  - Expérimentation de plusieurs variantes de vis sur les sites d’exploitation pour atteindre les performances attendues."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nAu cours de ce projet de recherche, nous avons rencontré des difficultés pour assurer une qualité fiable et constante de l’huile d’olive obtenue. \n\nLa configuration « plough shape » des pales de la vis avait pour objectif de maîtriser le malaxage dans la décanteuse de façon à extraire efficacement l’huile de la pâte d’olive. Cependant, au cours des essais réalisés sur site, nous avons été confrontés à des décharges solides discontinues en sortie machine, côté solide mais également côté huile.  \nDe plus, nous avons constaté des problèmes de convoyage avec toutes les pales, créant un transfert par forme de bouchon avant de sortir de la centrifugeuse.  \nEnfin, nous avons également relevé une usure dans la partie cylindrique du bol.\n\nAfin de pallier ces difficultés, la conception de la vis a été modifiée de manière à accroître le convoyage et faciliter l’extraction de l’huile d’olive de la pâte. Nous avons expérimenté sur les sites d’exploitation plusieurs variantes de vis jusqu’à ce que les performances obtenues soient celles attendues."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Développement de la Presse à Vis**:\n  - Nécessité d'acquérir des connaissances sur les performances de déshydratation.\n  - Manque d'informations suffisantes sur l'état de l'art.\n  - Incertitudes sur la corrélation entre les performances de l'outil et la qualité de la boue, notamment en ce qui concerne la floculation.\n\n- **Verrous Techniques Rencontrés**:\n  - **Fissurations**:\n    - Détection de fissurations sur la cuve de floculation au début des essais.\n  \n  - **Problèmes de Moteur**:\n    - Augmentation du couple et du courant électrique lors du démarrage du moteur de la presse à vis.\n    - Montée en température rapide du moteur entraînant un arrêt de sécurité de la machine.\n    - Nécessité d'actions pour identifier l'origine des blocages.\n\n- **Système d'Automatisation**:\n  - L'automatisme initialement construit selon le fonctionnement des presses à vis dans le secteur Pulp & Paper s'est révélé inadapté au marché du traitement des boues.\n  - Nécessité de modifier substantiellement l'approche initiale pour le système d'automatisation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement d’une presse à vis spécifique pour la déshydratation des boues organiques a nécessité en premier lieu un besoin d’acquisition de connaissances sur les performances de déshydratation, car nous ne disposions pas d’informations suffisantes avec l’état de l’art et des incertitudes subsistaient notamment sur la corrélation des performances atteintes avec cet outil et la qualité de la boue (travail sur la floculation entre autres).  \n\nOutre ces incertitudes, nous avons été confrontés à d’autres verrous techniques lors de nos travaux.\n\nAu début des essais réalisés, des fissurations ont été détectées sur la cuve de floculation. Nous avons également noté, lorsque le moteur de la presse à vis démarrait, une augmentation du couple et du courant électrique qui entraînaient une montée en température rapide du moteur, nécessitant un arrêt de sécurité de la machine. Des actions ont dues être menées pour déterminer l’origine de ces blocages.\n\nConcernant le système d’automatisation, nous nous sommes rendu compte que l’automatisme construit suivant le fonctionnement des presses à vis dans le Pulp & Paper, n’était pas adapté au marché du traitement des boues. Nous avons donc dû changer notre approche initiale basée sur le Pulp & Paper pour apporter des modifications substantielles au système d’automatisation."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Problème de surcharge du distributeur** :\n  - Retour de la boue entre le tube d’alimentation et la conduite.\n  - La boue passe par les ouïes d’alimentation, dégradant la qualité des centrats.\n\n- **Difficulté de normalisation de la CFC** :\n  - Hétérogénéité du comportement des boues testées.\n  - Complexité de la caractérisation des boues due à :\n    - Variations de composition selon l'origine et l'âge.\n    - Traitements antérieurs amplifiant ces variations.\n  - Impact direct sur le comportement mécanique lors de la déshydratation.\n\n- **Mesure de la siccité limite** :\n  - Nécessité d'une mesure rapide en laboratoire.\n  - Définition d'un critère pour arrêter l'essai et extrapoler les données à un temps infini.\n  - Établissement de corrélations entre mesures et siccité des boues étudiées.\n  - Étude des paramètres d'influence :\n    - Pression\n    - Épaisseur du gâteau\n    - Consommation de polymère\n    - Média filtrant\n  - Manque d'informations suffisantes dans la norme T97-001.\n\n- **Digestat de méthanisation** :\n  - Boue de composition complexe, plus concentrée en matières solubles que les boues d'épuration.\n  - Problème pour le calcul de la concentration en matière sèche.\n  - Mise en place d'une méthode de calcul pour obtenir la concentration en Matière en Suspension (MeS).\n\n- **Problèmes d'extraction lors des essais de déshydratation** :\n  - Bourrage de la vis d’extraction observé avec des boues faiblement concentrées.\n  - Augmentation du couple de la vis d’extraction par régulation :\n    - Diminution de la vitesse relative de la vis par rapport au bol.\n    - Risque de ralentissement excessif de la vis, entraînant une évacuation insuffisante du sédiment.\n  - Compaction de la boue entre les filets de la vis, formation d'un cylindre glissant contre les parois du bol.\n  - Conséquences : perte de couple de la vis et augmentation de la charge des centrats.\n  - Solution : utilisation d'une régulation par vitesse relative pour éviter le colmatage de la vis."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons rencontré un problème de surcharge du distributeur qui implique le retour de la boue entre le tube d’alimentation et sa conduite. Nous avons observé que la boue passe par les ouïes d’alimentation prévu pour le fonctionnement equicourant de la centrifugeuse et vient dégrader la qualité des centrats.\n\nConcernant la normalisation « unique » de la CFC, la principale difficulté que nous avons rencontrée est l’hétérogénéité du comportement des boues testées. De manière générale, la caractérisation des boues est complexe à réaliser. Les boues présentent d’importantes variations de composition selon leur origine ou leur âge, et les traitements qu’elles ont subis peuvent amplifier ces variations, ce qui influence directement leur comportement mécanique pendant l’étape de déshydratation.\n\nDe plus, la mesure de la siccité limite doit se faire en un temps raisonnable en laboratoire, ce qui nécessite la définition d’un critère pour arrêter l’essai et extrapoler les données à un temps infini. En éprouvant les protocoles d’expérimentation, les verrous techniques à lever ont été d’établir des corrélations entre les mesures relevées et la siccité de la boue étudiée. Nous avons ainsi concentré nos efforts sur l’étude des paramètres d’influence, tels que la pression, l’épaisseur du gâteau, la consommation polymère et le media filtrant, car nous ne disposions pas d’informations suffisantes dans la norme T97-001.\n\nLe digestat, déchet de la méthanisation, est une boue de composition très complexe. Cette boue arrive au laboratoire au même titre que les autres boues bien qu’elle soit beaucoup plus concentrée en matières solubles que des boues d’épuration. Cette forte concentration en soluble pose problème pour le calcul de la concentration en matière sèche. Nous avons donc dû mettre en place une méthode de calcul qui permet d’obtenir une valeur de concentration de Matière en Suspension (MeS).\n\nEnfin, au cours des essais de déshydratation sur boues faiblement concentrées, nous avons été confrontés à des problèmes d’extraction conduisant à un bourrage de la vis d’extraction. En effet, pour augmenter le couple de la vis d’extraction, la régulation agit en diminuant la vitesse relative de la vis par rapport au bol jusqu’à ce que celle-ci ne tourne plus assez vite pour évacuer le sédiment en dehors de la décanteuse. La vis tournant très lentement par rapport au bol, il en est de même pour la boue qu’elle transporte. La boue se compacte alors entre les filets de la vis et finit par former un cylindre glissant contre les parois du bol, entraînant ainsi une perte du couple de la vis et une forte augmentation de la charge des centrats. Nous avons donc été contraints à utiliser une régulation par la vitesse relative au lieu d’une régulation par couple pour éviter tout risque de colmatage de la vis."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrou technologique principal** : Tenue des roulements et étanchéité des paliers à une accélération de 3500 g de la centrifugeuse.\n  \n- **Problèmes rencontrés lors des tests internes** :\n  - Roulements à double rangée de rouleaux grillés pour l'application boue.\n  - Précision nécessaire des pièces recevant les roulements non anticipée (tolérance de forme et rugosité).\n\n- **Problème de lubrification** :\n  - Excès de graisse initial qui n'a pas pu s'évacuer correctement, entraînant un échauffement excessif à l'intérieur du roulement.\n\n- **Essais sur site** :\n  - Problèmes d’échauffement à l’intérieur des roulements lors des tests avec les versions caséine sanitaire et huile d’olive.\n  - Développement de procédures spécifiques de rodage et de lubrification pour ces deux applications.\n\n- **Difficultés de conception** :\n  - Problèmes de dimensionnement lors de l'augmentation de la taille de la machine (F5000 à D6LN, augmentation de 1,7 fois).\n  - Nécessité d'une phase de réflexion importante sur la modélisation et le développement des pièces, notamment au niveau du palier d’alimentation et des roulements.\n\n- **Objectif final** : Trouver des solutions aux défis techniques pour développer une nouvelle centrifugeuse D6LN efficace et économe en énergie."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe verrou technologique principal de ce projet de recherche est la tenue des roulements et l’étanchéité des paliers à une accélération de 3500 g de la centrifugeuse.  \nAu cours des tests réalisés en interne, les nouveaux roulements à double rangée de rouleaux que nous avons installés ont grillé pour l’application boue. L’état de l’art et nos pré-études ne nous avaient pas permis d’anticiper la précision nécessaire des pièces qui reçoivent les roulements, que ce soit dans la tolérance de forme ou de rugosité notamment.  \nDe plus, nous nous sommes rendu compte que nous avions mis trop de graisse au départ. Cette dernière n’a pas pu s’évacuer correctement et cela a donc créé un échauffement trop important à l’intérieur du roulement.  \nLors des essais sur site des versions caséine sanitaire et huile d’olive, nous avons également rencontré des problèmes d’échauffement à l’intérieur des roulements. Afin de pallier cela, nous avons développé des procédures spécifiques de rodage et de lubrification pour ces deux applications.  \n\nD’autre part, nous avons rencontré des difficultés dans la conception de la version caséine. A partir d’une machine existante de taille plus petite (F5000), l’augmentation de la taille (x 1,7) pour atteindre celle de la D6LN a engendré des problèmes de dimensionnement au niveau du palier d’alimentation et des roulements. Nous avons donc dû avoir une importante phase de réflexion sur la modélisation et le développement de ces pièces.  \n\nTrouver les solutions à ces défis techniques nous permettra de développer une nouvelle centrifugeuse D6LN efficace et économe en énergie."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Problèmes rencontrés :**\n  - Difficulté à réduire la puissance consommée tout en maintenant les performances d’une machine standard.\n  - Problèmes d’alliance entre ajustements des pièces nouvellement conçues et performances.\n\n- **Observations après essais de fonctionnement :**\n  - Problèmes d’équilibrage de la vis après réduction de la sortie liquide.\n  - Règles d’équilibrage habituelles inadaptées pour déterminer la position et la quantité des masses de compensation.\n\n- **Influences sur l'équilibrage :**\n  - Niveaux d’air et d’eau au sein de la machine affectent les modalités d’équilibrage de la vis.\n  - Difficulté à équilibrer la vis dans l’eau et l’air, l’équilibreuse ne fonctionnant que dans l’air.\n\n- **Solutions développées :**\n  - Création d’un outil de calcul pour simuler les filets de la vis et l’équilibrage.\n\n- **Problématiques liées à la conception :**\n  - Diminution des diamètres aux extrémités de la vis entraînant des problèmes de fréquences propres et de torsion non anticipés.\n  - Changement de design trop important par rapport à une vis classique nécessitant une prise en compte des sollicitations en torsion dans les calculs de fréquence et de résistance.\n\n- **Révisions nécessaires :**\n  - Révision complète des méthodes de conception pour renforcer la vis et assurer un coefficient de sécurité suffisant.\n\n- **Résultats des efforts de re-conception :**\n  - La vis s’est fortement endommagée lors de l’équilibrage, le modèle de calcul appliqué n’étant pas en phase avec la réalité.\n  - Réduction de l’épaisseur des pièces a permis une bonne résistance, mais a conduit à dépasser la limite élastique de certaines pièces en ce qui concerne la fréquence."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nParmi les problèmes rencontrés pour réduire la puissance consommée tout en maintenant les performances d’une machine standard, nous pouvons citer les difficultés à allier ajustements des pièces nouvellement conçues et performances.\n\nAprès réduction de la sortie liquide, des problèmes d’équilibrage de la vis ont été observés après des essais de fonctionnement de la centrifugeuse. Nous nous sommes rendu compte que nos règles d’équilibrage habituelles n’étaient plus adaptées pour déterminer la position et  la quantité des masses de compensation. En effet, les niveaux d’air et d’eau au sein de la machine influent sur les modalités d’équilibrage de la vis. La difficulté majeure a été d’équilibrer la vis à la fois dans l’eau et l’air, sachant que l’équilibreuse ne fonctionne que dans l’air. Pour y remédier, nous avons développé un outil de calcul permettant de simuler les filets de la vis et l’équilibrage.\n\nLa diminution des diamètres aux extrémités de la vis a également engendré des problématiques de fréquences propres et surtout de torsion, que nous n’avions pas anticipé. En effet, le changement du design était trop important par rapport à une vis classique et les sollicitations en torsion devaient être considérées dans les calculs de fréquence et de résistance de la vis et du bol. Nous avons donc dû revoir complètement nos méthodes de conception pour aboutir à un renforcement de la vis et à un coefficient de sécurité suffisant. Cependant, malgré nos efforts de re-conception, la vis s’est fortement endommagée à l’équilibrage, le modèle de calcul appliqué n’étant pas en phase avec la réalité. En réduisant l’épaisseur des pièces, la résistance était bonne mais, en ce qui concerne la fréquence, nous avions dépassé la limite élastique de certaines pièces."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Difficultés rencontrées** : Problèmes pour assurer une qualité fiable et constante de l’huile d’olive obtenue.\n  \n- **Problèmes techniques** :\n  - Centrifugeuses déchargeant de manière discontinue (côté solide et huile).\n  - Problèmes de convoyage au sein du bol, entraînant des bouchons de solides.\n\n- **Solutions apportées** :\n  - Modifications substantielles de la vis convoyeuse en corrigeant la configuration des pales « plough shape ».\n\n- **Résultats des modifications** :\n  - Atteinte des objectifs de performances avec la nouvelle vis.\n  - Problèmes d’abrasion des pales supérieurs aux attentes, causés par l’effet abrasif du produit.\n\n- **Facteurs influençant les performances** :\n  - Dépendance des performances au type de produit traité et à la conduite de l’installation par le client.\n  - Variabilité de la qualité de l’olive selon les saisons (composition en huile ou eau), impactant les performances des centrifugeuses.\n\n- **Difficultés d'analyse** :\n  - Complexité d'analyse des résultats de performances sur site.\n  - Difficulté à garantir un taux d’huile résiduel dans les grignons en sortie machine."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours de ce projet de recherche, nous avons rencontré des difficultés pour assurer une qualité fiable et constante de l’huile d’olive obtenue.\n\nLors des essais de notre première vis convoyeuse avec pales « plough shape », nous avons constaté que nos centrifugeuses déchargeaient de manière discontinue en sortie machine, que ce soit côté solide ou côté huile. Nous avons aussi noté des problèmes de convoyage au sein du bol avec toutes les pales, créant un transfert des solides par forme de bouchon avant de sortir de la machine. \nAfin de pallier ces difficultés, nous avons apporté des modifications substantielles à notre vis en corrigeant la configuration des pales « plough shape ».\n\nBien que nous sommes parvenus à atteindre nos objectifs de performances grâce à notre nouvelle vis, les essais menés sur les sites d’exploitation ont révélé des problèmes d’abrasion supérieure à ce que nous attendions au niveau des pales, l’effet abrasif étant dû au produit.\n\nPar ailleurs, dans le secteur de l’extraction de l’huile d’olive, les performances sont directement dépendantes du type de produit à traiter et de la conduite de l’installation par le client. La qualité de l’olive change à chaque saison (composition en huile ou eau) et cela impacte considérablement les performances des centrifugeuses. Il est ainsi compliqué d’analyser les résultats des performances obtenues sur site et de garantir un taux d’huile résiduel dans les grignons en sortie machine."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Acquisition de Connaissances**:\n  - Nécessité de comprendre les performances de déshydratation des boues organiques.\n  - Manque d'informations suffisantes sur l'état de l'art.\n  - Incertitudes sur la corrélation entre les performances de la presse à vis et la qualité de la boue, notamment en ce qui concerne la floculation.\n\n- **Verrous Techniques**:\n  - **Système d'Automatisation**:\n    - L'automatisme initial, basé sur le fonctionnement des presses à vis dans le secteur Pulp & Paper, s'est révélé inadapté au traitement des boues.\n    - Nécessité de modifier l'approche initiale pour le système d'automatisation.\n\n  - **Choix de l'Outil de Mélange**:\n    - Impact du choix de l'outil de mélange boue-polymère sur les performances de déshydratation.\n    - Difficulté à déterminer les paramètres optimaux pour le dosage en polymère (poudre, émulsion) et le mélangeur approprié.\n    - Importance de la qualité des flocs pour le rendement de la presse à vis : les flocs doivent résister à des contraintes mécaniques élevées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement d’une presse à vis spécifique pour la déshydratation des boues organiques a nécessité en premier lieu un besoin d’acquisition de connaissances sur les performances de déshydratation, car nous ne disposions pas d’informations suffisantes avec l’état de l’art et des incertitudes subsistaient notamment sur la corrélation des performances atteintes avec cet outil et la qualité de la boue (travail sur la floculation entre autres).  \n\nOutre ces incertitudes, nous avons été confrontés à d’autres verrous techniques lors de nos travaux.\n\nConcernant le système d’automatisation, nous nous sommes rendu compte que l’automatisme construit suivant le fonctionnement des presses à vis dans le Pulp & Paper, n’était pas adapté au marché du traitement des boues. Nous avons donc dû changer notre approche initiale basée sur le Pulp & Paper pour apporter des modifications substantielles au système d’automatisation.\n\nLe choix de l’outil de mélange boue – polymère influe sur les performances de déshydratation de la presse à vis. La difficulté a résidé dans la détermination des paramètres optimaux en termes de dosage en polymère (poudre, émulsion) et du mélangeur adéquat pour obtenir une qualité des flocs satisfaisante. En effet, celle-ci est très importante pour obtenir le meilleur rendement de la presse à vis : les flocs doivent être résistants à des contraintes mécaniques élevées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Objectif du Matériau**: Développer un matériau similaire à la boue pour tester les centrifugeuses dans des conditions proches de celles des sites industriels.\n  - Permet de simuler la prise de couple (efforts de transport solide).\n  - Facilite le travail avec un anneau profond, difficile à reproduire en laboratoire.\n\n- **Difficultés de Comparaison**: \n  - Évaluation complexe de l'aptitude d'un matériau A par rapport à un matériau B en termes de comportement similaire à la boue.\n  \n- **Verrou Technique Principal**: \n  - Établir un lien entre les résultats de laboratoire et les tests industriels.\n  - Taux de certitude faible lors des travaux en laboratoire, souvent contredits par des résultats en machine.\n\n- **Contraintes Externes**: \n  - Respect des normes ISO de rejet.\n  - Risque de pollution de la cuve de stockage d’eau, interdisant l'utilisation de produits polluants dans le développement du Synthetic Sludge.\n\n- **Normalisation de la CFC**: \n  - Difficulté due à l'hétérogénéité des boues testées.\n  - Caractérisation complexe des boues, avec des variations significatives selon leur origine, âge et traitements subis, influençant leur comportement mécanique lors de la déshydratation.\n\n- **Mesure de la Siccité Limite**: \n  - Nécessité d'effectuer la mesure dans un délai raisonnable en laboratoire.\n  - Définition d'un critère pour arrêter l'essai et extrapoler les données à un temps infini.\n\n- **Corrélations à Établir**: \n  - Identification des verrous techniques à lever pour établir des corrélations entre les mesures et la siccité de la boue.\n  - Étude des paramètres d’influence : pression, épaisseur du gâteau, consommation de polymère, média filtrant.\n  - Manque d'informations suffisantes dans la norme T97-001."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’intérêt de disposer d’un matériau qui ressemble à la boue est de pouvoir tester nos centrifugeuses dans des conditions de réglages similaires à ce qui est pratiqué sur les sites industriels, à savoir, la prise de couple (génération des efforts de transport solide) ou encore, la possibilité de travailler avec un anneau profond. Ce dernier point est extrêmement difficile à appréhender en laboratoire, car il n’est pas évident de déterminer si oui ou non un matériau A va être meilleur par rapport à un matériau B sur son aptitude à avoir un comportement similaire à la boue dans une centrifugeuse.\n\nOutre les difficultés de préparation des essais (mélanges), le verrou technique majeur auquel nous avons dû faire face est d’établir un lien entre le travail de laboratoire et les tests industriels. Il existe en effet un taux de certitude relativement faible à chaque fois que nous travaillons au niveau laboratoire : nous trouvons toujours un contre-exemple en machine.\n\nD’autre part, des contraintes externes ont également dues être considérées durant nos travaux concernant le respect des normes ISO de rejet. Il y a un risque de polluer notre cuve de stockage d’eau. L’utilisation de produits polluants est donc proscrite dans le développement du Synthetic Sludge.\n\nConcernant la normalisation « unique » de la CFC, la principale difficulté que nous avons rencontrée est l’hétérogénéité du comportement des boues testées. De manière générale, la caractérisation des boues est complexe à réaliser. Les boues présentent d’importantes variations de composition selon leur origine ou leur âge, et les traitements qu’elles ont subis peuvent amplifier ces variations, ce qui influence directement leur comportement mécanique pendant l’étape de déshydratation.\n\nDe plus, la mesure de la siccité limite doit se faire en un temps raisonnable en laboratoire, ce qui nécessite la définition d’un critère pour arrêter l’essai et extrapoler les données à un temps infini. En éprouvant les protocoles d’expérimentation, les verrous techniques à lever ont été d’établir des corrélations entre les mesures relevées et la siccité de la boue étudiée. Nous avons ainsi concentré nos efforts sur l’étude des paramètres d’influence, tels que la pression, l’épaisseur du gâteau, la consommation polymère et le media filtrant, car nous ne disposions pas d’informations suffisantes dans la norme T97-001."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Problématique générale** : Développement d'une nouvelle gamme de centrifugeuses multi-applications.\n  \n- **Verrous identifiés** :\n  - **Conception pour efficacité énergétique** : Nécessité de concevoir un réducteur différentiel pour utiliser des moteurs de puissance moindre.\n  - **Standardisation des pièces** : Limitation des dimensions du réducteur pour utiliser un bâti habituel.\n  \n- **Difficultés de dimensionnement** :\n  - Longueur de la queue du rotor : Nécessité de rapprocher le palier pour un fonctionnement à 3500g, mais cela compromet la compatibilité avec le bâti existant.\n  - Vitesse de rotation : Augmentation de la vitesse nécessite une tige de réducteur plus courte pour éviter la flexibilité excessive et les vibrations.\n  \n- **Solutions innovantes** : Conception d'une solution inédite pour résoudre les problèmes de dimensionnement et de compatibilité.\n\n- **Aléa des vibrations** :\n  - Réducteur en porte-à-faux intensifiant les vibrations.\n  - Étude vibratoire initiale pour identifier la fréquence propre de l’ensemble rotatif.\n  - Étude de faisabilité pour réduire la masse du réducteur tout en maintenant les performances au point de fonctionnement nominal.\n\n- **Objectif de modularité** : Développement d'un nouveau concept de centrifugeuses pour diverses applications, avec un accent particulier sur l'application alimentaire.\n\n- **Contraintes spécifiques à l'application alimentaire** :\n  - Nécessité d'une étanchéité suffisante, impliquant l'intégration de nombreux composants.\n  - Encombrement des composants limitant l'espace disponible pour le réducteur.\n  \n- **Choix de conception** : Adoption d'un réducteur différentiel plus compact pour répondre aux exigences d'étanchéité et d'espace."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nQ-type\nLes travaux réalisés concernant cette problématique ont soulevé de nombreux verrous que nous avons dû résoudre afin d’atteindre notre objectif de développement d’une nouvelle gamme de centrifugeuses multi-applications.\n\nL’une de ces difficultés était le type de conception à privilégier pour un gain en efficacité énergétique de la machine en utilisation. Nous avons ainsi exploré la piste de conception d’un réducteur différentiel permettant d’utiliser des moteurs de puissance moindre pour entrainer l’ensemble rotatif. La conception de ce réducteur a également été compliquée par notre objectif de standardisation des pièces utilisées sur nos différentes gammes. En effet, nous souhaitions utiliser un bâti habituel : nous avons ainsi été limités dans les dimensions du réducteur. Le dimensionnement en encombrement du réducteur a posé de nombreuses difficultés. En effet, si la queue du rotor est longue, nous avons besoin de rapprocher le palier pour que le réducteur puisse fonctionner à 3500g mais si nous rapprochons le palier, le bâti existant n’est plus compatible. En effet, si la vitesse augmente, nous devons nous restreindre à une tige de réducteur plus courte ; si la tige est trop longue, elle sera trop flexible et sera soumise à des vibrations d’autant plus importantes que la vitesse de rotation est grande. Nous avons ainsi dû concevoir une solution inédite pour adresser ce verrou.\n\nNous avons également rencontré un aléa concernant les vibrations de la machine en rotation. En effet, du fait de la configuration de celle-ci, le réducteur se retrouve en porte à faux, intensifiant ainsi les vibrations. Nous avons dans un premier temps dû mener une étude vibratoire pour identifier la fréquence propre de l’ensemble rotatif. Nous avons ensuite dû étudier la faisabilité d’une réduction de masse du réducteur, mais qui soit capable de travailler au point de fonctionnement nominal, notamment en termes de vitesse de rotation.\n\nLe principal verrou que nous avons dû adresser concernait le développement d’un nouveau concept de centrifugeuses compatibles dans diverses applications. C’est notamment cet objectif de modularité des machines développées qui était à l’origine de la conception du nouveau réducteur. C’est en particulier l’application alimentaire que nous souhaitions développer qui était la plus contraignante, notamment vis-à-vis de l’étanchéité. En effet, afin d’assurer une étanchéité suffisante pour l’application alimentaire, il est nécessaire d’intégrer de nombreux composants, dont l’encombrement viendrait limiter l’espace réservé au réducteur. Nous avons ainsi opté pour une conception avec un réducteur différentiel, plus compact."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Verrou Technologique Principal**: Tenue des roulements et étanchéité des paliers à une accélération de 3500 g de la centrifugeuse.\n  \n- **Problèmes Identifiés lors des Tests Internes**:\n  - Roulements à double rangée de rouleaux grillés pour l'application boue.\n  - Insuffisance des pré-études pour anticiper la précision nécessaire des pièces recevant les roulements (tolérance de forme et rugosité).\n\n- **Problème de Lubrification**:\n  - Excès de graisse initialement appliqué, entraînant une évacuation incorrecte et un échauffement excessif à l'intérieur du roulement.\n  \n- **Essais sur Site**:\n  - Problèmes d’échauffement dans les roulements lors des tests avec caséine sanitaire et huile d’olive.\n  - Développement de procédures spécifiques de rodage et de lubrification pour ces applications.\n\n- **Difficulté Technique Majeure**: \n  - Étanchéité des paliers d'alimentation.\n  - Système d’étanchéité initial utilisant des bagues de frottement en PTFE, sujettes à déformation et destruction rapide sous température et friction, entraînant un échauffement des roulements et de l'environnement.\n\n- **Solution Développée**: \n  - Nécessité de concevoir une alternative pour le système d'étanchéité.\n\n- **Difficultés dans la Conception de la Version Caséine**:\n  - Augmentation de la taille de la machine (x 1,7) par rapport à la machine existante (F5000) a causé des problèmes de dimensionnement pour le palier d’alimentation et les roulements.\n  - Nécessité d'une phase de réflexion importante sur la modélisation et le développement de ces pièces.\n\n- **Objectif Final**: \n  - Trouver des solutions aux défis techniques pour développer une nouvelle centrifugeuse D6LN efficace et économe en énergie."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe verrou technologique principal de ce projet de recherche est la tenue des roulements et l’étanchéité des paliers à une accélération de 3500 g de la centrifugeuse.  \nAu cours des tests réalisés en interne, les nouveaux roulements à double rangée de rouleaux que nous avons installés ont grillé pour l’application boue. L’état de l’art et nos pré-études ne nous avaient pas permis d’anticiper la précision nécessaire des pièces qui reçoivent les roulements, que ce soit dans la tolérance de forme ou de rugosité notamment.  \nDe plus, nous nous sommes rendu compte que nous avions mis trop de graisse au départ. Cette dernière n’a pas pu s’évacuer correctement et cela a donc créé un échauffement trop important à l’intérieur du roulement.  \nLors des essais sur site des versions caséine sanitaire et huile d’olive, nous avons également rencontré des problèmes d’échauffement à l’intérieur des roulements. Afin de pallier cela, nous avons développé des procédures spécifiques de rodage et de lubrification pour ces deux applications.  \nCependant, l’étanchéité des paliers alimentation respectifs reste la difficulté technique majeure à laquelle nous avons dû faire face. Nous avions un système d’étanchéité qui utilisait des bagues de frottement en PTFE. Or, sous l’effet de la température, ces bagues d’étanchéité ont tendance à se déformer en se rétractant, et se détruisent très rapidement sous l’augmentation de l’effort de friction sur l’arbre. Ce phénomène a aussi pour conséquence de faire chauffer le roulement placé à proximité et tout l’environnement avoisinant. Nous avons donc dû développer une solution d’étanchéité alternative.  \nD’autre part, nous avons rencontré des difficultés dans la conception de la version caséine. A partir d’une machine existante de taille plus petite (F5000), l’augmentation de la taille (x 1,7) pour atteindre celle de la D6LN a engendré des problèmes de dimensionnement au niveau du palier d’alimentation et des roulements. Nous avons donc dû avoir une importante phase de réflexion sur la modélisation et le développement de ces pièces.  \nTrouver les solutions à ces défis techniques nous permettra de développer une nouvelle centrifugeuse D6LN efficace et économe en énergie."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Qualité de l'huile d'olive** : Difficulté à assurer une qualité fiable et constante de l'huile d'olive obtenue.\n  \n- **Problèmes de la vis convoyeuse** :\n  - Essais de la première vis convoyeuse avec pales « plough shape » ont montré :\n    - Décharge discontinue des centrifugeuses (solides et huile).\n    - Problèmes de convoyage dans le bol, créant des bouchons de solides.\n\n- **Modifications apportées** :\n  - Corrections substantielles de la configuration des pales « plough shape ».\n  - Progrès vers les objectifs de performance, mais :\n    - Problèmes d'abrasion des pales supérieurs aux attentes, causés par le produit.\n\n- **Caractérisation de l'abrasivité** :\n  - Recherche d'études sur l'abrasivité d'une pâte d'olive n'a pas abouti.\n  - Identification des facteurs favorisant l'abrasivité et élaboration d'un protocole expérimental robuste.\n  - Incertitudes sur la relation entre la taille des particules et l'effet abrasif, avec variabilité des résultats d'une même récolte.\n\n- **Procédure de lavage** :\n  - Création d'une procédure de lavage régulier pour :\n    - Éliminer les sédiments résiduels.\n    - Réduire l'abrasivité due aux particules solides.\n    - Traiter les vibrations excessives de la machine.\n  - Verrou principal : Élaboration d'une procédure de nettoyage sans endommager les zones sensibles à l'exposition aux liquides.\n\n- **Difficultés d'évaluation des performances** :\n  - Incapacité des grandeurs existantes à déterminer rigoureusement les performances de séparation.\n  - Développement d'un nouvel outil pour réaliser des bilans de masse du processus de séparation pour calculer le taux d'huile d'extraction.\n\n- **Variabilité des performances** :\n  - Performances d'extraction directement dépendantes du type de produit et de la conduite de l'installation par le client.\n  - Variabilité de la qualité des olives selon la saison (composition, maturité, méthode de récolte).\n  - Impact significatif sur les performances des centrifugeuses et difficulté d'analyse des résultats sur site.\n  - Garantie d'un taux d'huile résiduel dans les grignons en sortie machine est complexe."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nCe projet de recherche est caractérisé par la difficulté d’assurer une qualité fiable et constante de l’huile d’olive obtenue.\n\nLors des essais de notre première vis convoyeuse avec pales « plough shape », nous avons constaté que nos centrifugeuses déchargeaient de manière discontinue en sortie machine, que ce soit côté solide ou côté huile. Nous avons aussi noté des problèmes de convoyage au sein du bol avec toutes les pales, créant un transfert des solides par forme de bouchon avant de sortir de la machine.  \nAfin de pallier ces difficultés, nous avons apporté des modifications substantielles à notre vis en corrigeant la configuration des pales « plough shape ».  \nBien que nous fussions presque parvenus à atteindre nos objectifs de performances grâce à notre nouvelle vis, les essais menés sur les sites d’exploitation ont révélé des problèmes d’abrasion supérieure à ce que nous attendions au niveau des pales, l’effet abrasif étant dû au produit.\n\nPour remédier à cela, nous avons cherché à caractériser l’abrasivité d’une pâte d’olive. Dans l’état de l’art, nous n’avons trouvé aucune étude spécifique sur ce point appliquée à l’huile d’olive. Nous avons donc dû identifier les facteurs pouvant favoriser l’abrasivité et élaborer un protocole expérimental robuste et reproductible. Cependant, nous avons des incertitudes quant à la possibilité d’établir une relation entre la taille des particules et l’effet abrasif du produit correspondant, surtout qu’une même récolte d’olives peut donner des résultats très différents.\n\nNous avons par ailleurs décidé de créer une procédure de lavage régulier qui permettrait d’éliminer les sédiments résiduels, dans le but de diminuer l’abrasivité due aux particules solides, mais aussi dans le but de trouver une solution aux vibrations excessives constatées sur la machine. Concernant le lavage, le principal verrou a été d’élaborer une procédure nettoyer toutes les zones de la centrifugeuse sans endommager les zones sensibles à l’exposition aux liquides de lavage.\n\nD’autres difficultés se sont posées, notamment lors des essais permettant de suivre les performances de nos machines. La difficulté rencontrée était l’incapacité des grandeurs existantes à déterminer les performances de séparation de manière rigoureuse. Pour calculer le taux d’huile d’extraction, nous avons ainsi dû développer un nouvel outil pour réaliser des bilans de masse du process de séparation. \n\nPar ailleurs, dans le secteur de l’extraction de l’huile d’olive, les performances sont directement dépendantes du type de produit à traiter et de la conduite de l’installation par le client. La qualité de l’olive change à chaque saison (composition en huile ou eau, maturité, méthode de récolte…) et cela impacte considérablement les performances des centrifugeuses. Il est ainsi compliqué d’analyser les résultats des performances obtenues sur site et de garantir un taux d’huile résiduel dans les grignons en sortie machine."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Risque de Contamination Croisée**:\n  - Problèmes de fuite lors de la séparation de l’eau et de l’huile dans la centrifugeuse.\n  - Nécessité d’éviter le re-mélange des deux liquides à la sortie.\n\n- **Fuites au Niveau de la Sortie Tangentielle**:\n  - Incertitudes de 2015 confirmées par des fuites dues à l'accumulation d'eau et à la perte de charge.\n  - Nécessité d'évaluer plusieurs pistes et de concevoir une nouvelle sortie.\n\n- **Qualité de l'Huile de Palme**:\n  - Difficultés à assurer une qualité fiable et constante de l'huile obtenue.\n\n- **Turbulences et Émulsions**:\n  - Fortes accélérations centrifuges entraînent des turbulences, générant des émulsions d'huile et d'eau.\n  - Émulsions diminuent le rendement de clarification.\n\n- **Étude sur l'Émulsion**:\n  - Complexité due à la multiplicité des facteurs générant l’émulsion (chambre d’accélération, rotor, sortie des phases liquides).\n  \n- **Solutions pour Réduire la Turbulence**:\n  - Réduction du volume de la chambre de distribution et conception d'un nouveau mode de distribution.\n  - Évaluation de l'ajout d'un disque flottant près de la chambre de distribution pour favoriser la séparation liquide/liquide, avec des risques de dégradation de la robustesse de la centrifugeuse.\n\n- **Compromis sur le Disque de Séparation**:\n  - Augmentation du diamètre du disque de séparation pour réduire l’émulsion à la sortie de la phase lourde.\n  - Problèmes d'équilibrage de la machine dus à l'augmentation du diamètre.\n  - Développement orienté vers l'agrandissement du disque baffle pour améliorer la robustesse de la centrifugeuse.\n  - Incertitude sur le diamètre maximal pouvant être entraîné par la boîte de vitesse, limitant l'augmentation des diamètres.\n\n- **Effets de l'Émulsion sur l'Écoulement**:\n  - Émulsion provoquant un écoulement turbulent avec vagues ou crêtes, rendant inintelligibles des données clés.\n  - Absence de modèle théorique de l'effet de crête dans l'application, nécessitant l'identification de paramètres pertinents et l'élaboration d'un modèle.\n\n- **Durabilité de la Centrifugeuse**:\n  - Réflexions technologiques pour réduire l'abrasion de la zone active et la consommation électrique.\n\n- **Complexité de la Configuration de la Centrifugeuse**:\n  - Machine fermée et tournante, rendant l'observation des phénomènes internes impossible lors du fonctionnement."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nConcernant le développement expérimental de la sortie gravitaire des phases liquides, la principale difficulté provient du risque de contamination croisée. Nous pouvons en effet avoir des problèmes de fuite une fois la séparation de l’eau et de l’huile réalisée au sein de la centrifugeuse. Or, il ne faut pas que ces deux liquides se re-mélange à la sortie.\nNos incertitudes de 2015 quant à la possibilité de fuites au niveau de la sortie tangentielle se sont confirmées puisque le prototype présentait des fuites du fait de l’accumulation d’eau de la soudaine perte de charge. Nous avons ainsi dû évaluer plusieurs pistes pour déterminer le problème à l’origine des fuites et avons procédé à une nouvelle conception de la sortie.\n\nNous avons également rencontré des difficultés pour assurer une qualité fiable et constante de l’huile de palme obtenue.\n\nLes fortes accélérations centrifuges rencontrées dans les centrifugeuses induisent de fortes turbulences qui peuvent générer une émulsion d’huile et d’eau. La formation d’émulsion diminue alors le rendement de clarification. \nNous avons décidé de mener une étude pour comprendre les origines du phénomène d’émulsion. Cette étude s’est avérée particulièrement complexe en raison de la multiplicité des facteurs pouvant être à l’origine de ce problème. L’émulsion peut être générée au niveau de la chambre d’accélération du produit entrant, à l’intérieur même du rotor (ensemble vis / bol en rotation haute vitesse), ou encore, au niveau de la sortie des phases liquides clarifiées.\n\nPour diminuer la turbulence initiale donnée au fluide, nous avons cherché tout d’abord à réduire le volume de la chambre de distribution et à concevoir un nouveau mode de distribution.\nNous avons également évalué la pertinence de l’ajout d’un disque flottant près de la chambre de distribution. Ce disque devrait permettre la création d’un nouvel écoulement et favoriser la séparation liquide / liquide, mais cet ajout présenterait des risques de dégradation de la robustesse de la centrifugeuse. \nEnfin, nous avons dû trouver un compromis entre deux solutions. En effet, nous avons décidé d’augmenter le diamètre du disque de séparation à l’extrémité liquide pour récupérer la phase aqueuse sur un plus grand diamètre, ce qui permettrait d’avoir moins d'émulsion à travers la sortie de la phase lourde. Cependant, l’augmentation de ce diamètre provoque des problèmes d’équilibrage de la machine. Nous avons ainsi orienté nos développements vers l’agrandissement d’un autre disque, le disque baffle, qui permettrait d’avoir une meilleure robustesse de la centrifugeuse. Une grande incertitude reste toutefois non adressée quant au diamètre maximal effectivement capable d’être entrainé par notre boite de vitesse. C’est principalement ce facteur qui a limité l’augmentation du diamètre de ces deux disques.\n\nPar ailleurs, l’émulsion créée provoque un écoulement turbulent, qui donne li à des vagues ou des crêtes. Ces dernières rendent inintelligibles des données clés, touchant notamment l’ajustement relatif des niveaux des phases liquides.\nNous nous sommes retrouvés face à une difficulté supplémentaire puisqu’aucun modèle théorique de l’effet de crête n’existait dans notre application. Nous avons ainsi dû identifier les paramètres pertinents et élaborer un modèle.\n\nD’autres complexités ont dû être résolues, notamment en termes de durabilité de notre centrifugeuse. Nous avons considéré différentes pistes de réflexion technologique pour réduire l’abrasion de la zone active de la machine, et réduire sa consommation électrique.\n\nAutant d’incertitudes technologiques à dépasser, avec une complexité accrue liée à la configuration propre d’une centrifugeuse. C’est une machine tournante, fermée qui ne permet pas d’observer les phénomènes qui se produisent à l’intérieur de la machine lors de son fonctionnement."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Acquisition de connaissances** :\n  - Nécessité d'informations sur les performances de déshydratation.\n  - Manque d'informations suffisantes sur l'état de l'art.\n  - Incertitudes sur la corrélation entre performances de la presse à vis et qualité de la boue, notamment en floculation.\n\n- **Choix des outils** :\n  - L'outil de mélange boue-polymère impacte les performances de déshydratation.\n  - Détermination des paramètres optimaux pour le dosage de polymère (poudre, émulsion) et le mélangeur est complexe.\n  - Importance de la qualité des flocs pour un rendement optimal de la presse à vis, flocs devant résister à des contraintes mécaniques élevées.\n\n- **Aléas techniques** :\n  - Conception de méthodes et outils de mesure pour surmonter les aléas rencontrés.\n\n- **Tests sur le lisier** :\n  - Procédé de déshydratation habituel inadapté au type de boue traité.\n  - Nécessité d'ajouter un coagulant avant le polymère, impact étudié.\n  - Difficultés dans la manipulation de boues très concentrées.\n  - Problèmes lors de la définition de la pression d'alimentation et de la contre-pression pneumatique, entraînant un colmatage de la presse à vis et une dégradation des performances.\n  - Résultats inhabituels, hypothèses émises sur la méthode de mesure des performances.\n\n- **Modifications de la presse à vis** :\n  - Dépendance des modifications à la force axiale de la vis convoyeuse, valeur non déterminée avec précision.\n  - Concentration de l'étude sur la pression interne en raison de cette incertitude, avec conception d'un outil de mesure spécifique.\n\n- **Difficultés liées aux roulements** :\n  - Concentration de contraintes sur une petite zone au niveau des roulements.\n  - Orientation vers un type de roulements avec une plus grande surface de contact entre les éléments roulants.\n\n- **Médias filtrants** :\n  - Difficulté à déterminer les paramètres pour une bonne déshydratation de la boue, en tenant compte de la pression limite et de la résistance mécanique de l'écran filtrant.\n  - Sur le site de Levroux, difficultés à évaluer l'impact du dosage de polymère avec l'écran wedge wire, performances médiocres relevées.\n  - Identification de la pression optimale pour la déshydratation de deux médias filtrants testés.\n  - Interprétation des écarts entre le taux de capture en opération et le taux de capture global, impact du maillage de l'écran filtrant sur le stockage de la boue.\n\n- **Conclusion** :\n  - L'ensemble des verrous techniques a nécessité des travaux de recherche pour répondre aux problématiques posées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLe développement d’une presse à vis spécifique pour la déshydratation des boues organiques a nécessité en premier lieu un besoin d’acquisition de connaissances sur les performances de déshydratation, car nous ne disposions pas d’informations suffisantes avec l’état de l’art et des incertitudes subsistaient notamment sur la corrélation des performances atteintes avec cet outil et la qualité de la boue. Nous pouvons citer en particulier le travail sur la floculation. Le choix de l’outil de mélange boue – polymère influe sur les performances de déshydratation de la presse à vis. La difficulté a résidé dans la détermination des paramètres optimaux en termes de dosage en polymère (poudre, émulsion) et du mélangeur adéquat pour obtenir une qualité des flocs satisfaisante. En effet, celle-ci est très importante pour obtenir le meilleur rendement de la presse à vis : les flocs doivent être résistants à des contraintes mécaniques élevées.\n\nOutre ces incertitudes, nous avons été confrontés à d’autres aléas techniques lors de nos travaux qui nous ont amenés à concevoir des méthodes et des outils de mesure pour les adresser.\n\nConcernant les tests menés sur le lisier, nous avons constaté que le procédé de déshydratation habituel n’était pas adapté au type de boue que nous traitions. Nous avons ainsi dû imaginer de nouvelles étapes, en particulier, l’ajout d’un coagulant en amont de l’ajout de polymère, dont nous avons étudié l’impact. Des difficultés supplémentaires se sont posées dans la détermination des paramètres d’expérimentation, notamment pour la manipulation de boues très concentrées. Nous avons en particulier rencontré des difficultés lors de la définition de la pression d’alimentation et de la contre-pression pneumatique : la presse à vis se retrouvait colmatée et les performances de déshydratation se dégradaient. Par ailleurs, nous avons obtenu des résultats très inhabituels. Nous avons émis des hypothèses et nous supposons que ces changements sont dus à la méthode utilisée pour la mesure des performances.\n\nLes modifications substantielles effectuées sur la presse à vis dépendaient d’une valeur clé, la force axiale de la vis convoyeuse. Nous ne sommes pas encore parvenus à la déterminer de façon précise. Cette incertitude a été le cœur des difficultés rencontrées, notamment concernant les travaux sur les paniers de filtration. Ne connaissant pas la valeur de cette grandeur, nous avons dû concentrer notre étude sur la pression interne. Nous avons pour cela conçu un outil de mesure spécifique.\n\nNous avons également rencontré des difficultés provenant d’une concentration de contraintes sur une très petite zone au niveau des roulements. Nous nous sommes alors orientés vers un autre type de roulements qui permettait une plus grande surface de contact entre les bagues intérieure et extérieure et les éléments roulants.\n\nConcernant les médias filtrants, la difficulté rencontrée était la détermination des paramètres permettant d’avoir une bonne déshydratation de la boue, tout en considérant la pression limite supportée et la résistance mécanique de l’écran filtrant testée. Une difficulté supplémentaire a été rencontrée sur le site de Levroux au niveau de la détermination de l’impact du dosage de polymère avec l’écran wedge wire dans les conditions expérimentales que nous avions fixées. Nous avons en effet récupéré de l’eau souillée et les performances relevées sur les points de fonctionnement étaient très médiocres. Nous avons ainsi la pression permettant une meilleure déshydratation de deux médias filtrants testés. Enfin, lors de la comparaison de ces deux médias filtrants, nous avons été amenés à interpréter les écarts plus ou moins grands entre le taux de capture en opération et le taux de capture global. Le maillage de l’écran filtrant a un impact direct sur le stockage de la boue au niveau des parois.\n\nAinsi, l’ensemble de ces verrous techniques a nécessité des travaux de recherche pour répondre aux problématiques posées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Normalisation unique de la CFC :**\n  - Difficulté principale : hétérogénéité du comportement des boues testées.\n  \n- **Caractérisation des boues :**\n  - Complexité due à :\n    - Variations importantes de composition selon l'origine et l'âge des boues.\n    - Traitements subis par les boues amplifiant ces variations.\n  - Impact direct sur le comportement mécanique lors de la déshydratation.\n\n- **Mesure de la siccité limite :**\n  - Nécessité d'effectuer la mesure dans un temps raisonnable en laboratoire.\n  - Besoin de définir un critère pour arrêter l’essai et extrapoler les données à un temps infini.\n\n- **Verrous techniques à lever :**\n  - Établir des corrélations entre les mesures et la siccité des boues.\n  \n- **Paramètres d’influence étudiés :**\n  - Pression.\n  - Épaisseur du gâteau.\n  - Consommation de polymère.\n  - Média filtrant.\n  \n- **Manque d'informations :**\n  - Insuffisance d'informations dans la norme T97-001 concernant les paramètres d'influence."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nConcernant la normalisation « unique » de la CFC, la principale difficulté que nous avons rencontrée est l’hétérogénéité du comportement des boues testées. De manière générale, la caractérisation des boues est complexe à réaliser. Les boues présentent d’importantes variations de composition selon leur origine ou leur âge, et les traitements qu’elles ont subis peuvent amplifier ces variations, ce qui influence directement leur comportement mécanique pendant l’étape de déshydratation.\n\nDe plus, la mesure de la siccité limite doit se faire en un temps raisonnable en laboratoire, ce qui nécessite la définition d’un critère pour arrêter l’essai et extrapoler les données à un temps infini. En éprouvant les protocoles d’expérimentation, les verrous techniques à lever ont été d’établir des corrélations entre les mesures relevées et la siccité de la boue étudiée. Nous avons ainsi concentré nos efforts sur l’étude des paramètres d’influence, tels que la pression, l’épaisseur du gâteau, la consommation polymère et le media filtrant, car nous ne disposions pas d’informations suffisantes dans la norme T97-001."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Verrou technologique principal** : Tenue des roulements et étanchéité des paliers à une accélération de 3500 g de la centrifugeuse.\n  \n- **Problèmes rencontrés lors des tests internes** :\n  - Roulements à double rangée de rouleaux grillés pour l'application boue.\n  - Précision nécessaire des pièces recevant les roulements non anticipée (tolérance de forme et rugosité).\n\n- **Problème de lubrification** :\n  - Excès de graisse initiale qui n'a pas pu s'évacuer correctement, entraînant un échauffement excessif à l'intérieur du roulement.\n  \n- **Problèmes d’échauffement lors des essais sur site** :\n  - Versions caséine sanitaire et huile d’olive ont également montré des problèmes d’échauffement dans les roulements.\n  - Développement de procédures spécifiques de rodage et de lubrification pour ces applications.\n\n- **Difficulté technique majeure** : Étanchéité des paliers d'alimentation.\n  - Système d’étanchéité initial : bagues de frottement en PTFE.\n  - Problème : déformation des bagues sous température, entraînant destruction rapide et échauffement des roulements adjacents.\n  - Nécessité de développer une solution d’étanchéité alternative.\n\n- **Difficultés dans la conception de la version caséine** :\n  - Augmentation de la taille de la machine (F5000 à D6LN, x 1,7) a causé des problèmes de dimensionnement au niveau du palier d’alimentation et des roulements.\n  - Nécessité d'une phase de réflexion importante sur la modélisation et le développement des pièces.\n\n- **Objectif final** : Trouver des solutions aux défis techniques pour développer une nouvelle centrifugeuse D6LN efficace et économe en énergie."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe verrou technologique principal de ce projet de recherche est la tenue des roulements et l’étanchéité des paliers à une accélération de 3500 g de la centrifugeuse.  \nAu cours des tests réalisés en interne, les nouveaux roulements à double rangée de rouleaux que nous avons installés ont grillé pour l’application boue. L’état de l’art et nos pré-études ne nous avaient pas permis d’anticiper la précision nécessaire des pièces qui reçoivent les roulements, que ce soit dans la tolérance de forme ou de rugosité notamment.  \nDe plus, nous nous sommes rendu compte que nous avions mis trop de graisse au départ. Cette dernière n’a pas pu s’évacuer correctement et cela a donc créé un échauffement trop important à l’intérieur du roulement.  \nLors des essais sur site des versions caséine sanitaire et huile d’olive, nous avons également rencontré des problèmes d’échauffement à l’intérieur des roulements. Afin de pallier cela, nous avons développé des procédures spécifiques de rodage et de lubrification pour ces deux applications.  \nCependant, l’étanchéité des paliers alimentation respectifs reste la difficulté technique majeure à laquelle nous avons dû faire face. Nous avions un système d’étanchéité qui utilisait des bagues de frottement en PTFE. Or, sous l’effet de la température, ces bagues d’étanchéité ont tendance à se déformer en se rétractant, et se détruisent très rapidement sous l’augmentation de l’effort de friction sur l’arbre. Ce phénomène a aussi pour conséquence de faire chauffer le roulement placé à proximité et tout l’environnement avoisinant. Nous avons donc dû développer une solution d’étanchéité alternative.  \nD’autre part, nous avons rencontré des difficultés dans la conception de la version caséine. A partir d’une machine existante de taille plus petite (F5000), l’augmentation de la taille (x 1,7) pour atteindre celle de la D6LN a engendré des problèmes de dimensionnement au niveau du palier d’alimentation et des roulements. Nous avons donc dû avoir une importante phase de réflexion sur la modélisation et le développement de ces pièces.  \nTrouver les solutions à ces défis techniques nous permettra de développer une nouvelle centrifugeuse D6LN efficace et économe en énergie."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Problèmes rencontrés** :\n  - Difficultés à réduire la puissance consommée tout en maintenant les performances d’une machine standard.\n  - Problèmes d'alliance entre ajustements des pièces nouvellement conçues et performances.\n\n- **Observations après essais de fonctionnement** :\n  - Problèmes d’équilibrage de la vis après réduction de la sortie liquide.\n  - Règles d’équilibrage habituelles inadaptées pour déterminer la position et la quantité des masses de compensation.\n\n- **Influences sur l’équilibrage** :\n  - Niveaux d’air et d’eau au sein de la machine influencent les modalités d’équilibrage de la vis.\n  - Difficulté majeure : équilibrer la vis dans l’eau et l’air, alors que l’équilibreuse ne fonctionne que dans l’air.\n\n- **Solutions développées** :\n  - Outil de calcul créé pour simuler les filets de la vis et l’équilibrage.\n\n- **Problématiques liées au design** :\n  - Diminution des diamètres aux extrémités de la vis entraînant des problèmes de fréquences propres et de torsion non anticipés.\n  - Changement de design trop important par rapport à une vis classique, nécessitant une prise en compte des sollicitations en torsion dans les calculs.\n\n- **Révisions nécessaires** :\n  - Revue complète des méthodes de conception pour renforcer la vis et assurer un coefficient de sécurité suffisant.\n\n- **Résultats des efforts de re-conception** :\n  - La vis s’est fortement endommagée lors de l’équilibrage.\n  - Modèle de calcul appliqué non en phase avec la réalité.\n  - Réduction de l’épaisseur des pièces a permis une bonne résistance, mais a dépassé la limite élastique de certaines pièces concernant la fréquence."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nParmi les problèmes rencontrés pour réduire la puissance consommée tout en maintenant les performances d’une machine standard, nous pouvons citer les difficultés à allier ajustements des pièces nouvellement conçues et performances.\n\nAprès réduction de la sortie liquide, des problèmes d’équilibrage de la vis ont été observés après des essais de fonctionnement de la centrifugeuse. Nous nous sommes rendu compte que nos règles d’équilibrage habituelles n’étaient plus adaptées pour déterminer la position et  la quantité des masses de compensation. En effet, les niveaux d’air et d’eau au sein de la machine influent sur les modalités d’équilibrage de la vis. La difficulté majeure a été d’équilibrer la vis à la fois dans l’eau et l’air, sachant que l’équilibreuse ne fonctionne que dans l’air. Pour y remédier, nous avons développé un outil de calcul permettant de simuler les filets de la vis et l’équilibrage.\n\nLa diminution des diamètres aux extrémités de la vis a également engendré des problématiques de fréquences propres et surtout de torsion, que nous n’avions pas anticipé. En effet, le changement du design était trop important par rapport à une vis classique et les sollicitations en torsion devaient être considérées dans les calculs de fréquence et de résistance de la vis et du bol. Nous avons donc dû revoir complètement nos méthodes de conception pour aboutir à un renforcement de la vis et à un coefficient de sécurité suffisant. Cependant, malgré nos efforts de re-conception, la vis s’est fortement endommagée à l’équilibrage, le modèle de calcul appliqué n’étant pas en phase avec la réalité. En réduisant l’épaisseur des pièces, la résistance était bonne mais, en ce qui concerne la fréquence, nous avions dépassé la limite élastique de certaines pièces."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques dans le Projet Aqualine\n\n- **Difficultés de regroupement des dégrilleurs :**\n  - Nécessité d'assurer la compatibilité des fonctionnalités et performances des chaînes et châssis préexistants avec une conception unique.\n  - Développement de nouvelles conceptions pour garantir la modularité (base et tête commune).\n  - Maintien du dégrillage par l’aval et spécificité sur le nettoyage des dents.\n\n- **Problèmes de capture :**\n  - Phase de réflexion et d’essais pour identifier les causes de la non-performance des appareils.\n  - Essais de colmatage des parties en amont n'ont pas amélioré le taux de capture.\n  - Conclusion : les déchets capturés en amont retombent dans le canal à cause d'un système de décharge inefficace.\n  - Nécessité de réévaluer complètement le système de décharge.\n\n- **Changement de position de la brosse :**\n  - Initialement en bas de la machine, la brosse a été repositionnée en haut pour améliorer le contact avec les tôles perforées.\n  - Objectif : améliorer le lavage de l’écran et réduire le rejet de déchets dans le canal en aval.\n  - Ce changement a nécessité une redéfinition de la conception de la structure de la machine, de l’évacuation des déchets et de la cinématique.\n  - Développement d'une rampe de lavage spécifique et résolution de problèmes d’étanchéité du châssis.\n\n- **Difficultés dans le développement du dégrilleur Aquadrum :**\n  - Évaluation des forces engendrées par l’effluent chargé sur le tambour.\n  - Conception d'une structure capable d'accepter un fort débit d'effluent.\n  - Garantie de l’étanchéité du tambour pour atteindre un taux de capture proche de 100% des solides malgré des débits élevés.\n  - Mise en rotation d'un tambour de 1800 mm de diamètre, soumis à une charge non constante, sans déformation.\n  - Développement d'un système de convoyage totalement hydraulique des déchets, en accord avec le pas de la vis.\n  - Risques de colmatage et nécessité de nettoyage du tambour identifiés comme difficultés connexes."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours du projet Aqualine, nous avons rencontré des difficultés pour regrouper les trois dégrilleurs car nous devions assurer la compatibilité des différentes fonctionnalités et performances des chaînes et des châssis préexistants avec une unique conception. Nous avons donc dû imaginer et expérimenter de nouvelles conceptions afin d’assurer une modularité (base et tête commune) tout en conservant le dégrillage par l’aval et notre spécificité sur le nettoyage des dents.\n\nLe manque de capture sur nos appareils nous a demandé une importante phase de réflexion et d’essais pour définir les causes exactes de cette non performance.  \n\nDes essais de colmatages des diverses parties en amont n'ont pas permis d'améliorer le taux de capture, donc la conclusion est que les déchets capturés en amont retombent du coté décharge dans le canal. Notre système de décharge n'était pas assez efficace. Nous avons alors dû remettre en cause complètement notre système de décharge en analysant chaque phase de cette étape.\n\nLe changement de position de la brosse de plaques perforées a également été une source de difficultés. Initialement située en bas de la machine, en-dessous du système d’entraînement, la brosse qui vient laver les écrans de l’Aqualine a été positionnée sur le haut de la machine pour avoir plus de contacts entre les tôles perforées du dégrilleur et la brosse. Le but est d’améliorer le lavage de l’écran et de moins rejeter de déchets dans le canal en aval de la machine. Cependant, ce changement de position a entraîné une redéfinition de la conception de la structure machine, l’évacuation des déchets, et donc la cinématique, n’étant plus du tout la même. Une rampe de lavage spécifique a également dû être développée et des problématiques d’étanchéité du châssis ont été rencontrées.\n\nConcernant le développement du dégrilleur Aquadrum, les principales difficultés auxquelles nous avons été confrontées ont été :\nEvaluer les forces engendrées par l’effluent chargé sur le tambour.\nConcevoir une structure permettant d'accepter un fort débit d'effluent.\nGarantir l’étanchéité du tambour pour s’approcher d’un taux de capture de 100% des solides malgré de très forts débits.\nMettre en rotation un tambour de diamètre 1800 mm, soumis à une charge non constante et cela sans qu'il ne se déforme.\nDévelopper un convoyage totalement hydraulique des déchets (sans recours à un outil mécanique de transport). Celui-ci doit en effet être en accord avec le pas de la vis. D’autres difficultés connexes se sont ajoutées telles que le risque de colmatage ou le nettoyage du tambour."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Qualité de l'huile d'olive** : Difficulté à assurer une qualité fiable et constante de l'huile d'olive obtenue.\n  \n- **Problèmes de centrifugeuses** :\n  - Décharge discontinue en sortie machine (solides et huile).\n  - Problèmes de convoyage au sein du bol, formation de bouchons de solides.\n\n- **Modifications apportées** :\n  - Corrections substantielles de la configuration des pales « plough shape » de la vis convoyeuse.\n  - Progrès vers les objectifs de performance, mais problèmes d'abrasion des pales supérieurs aux attentes.\n\n- **Caractérisation de l'abrasivité** :\n  - Recherche de caractérisation de l'abrasivité d'une pâte d'olive.\n  - Absence d'études spécifiques dans l'état de l'art sur l'abrasivité appliquée à l'huile d'olive.\n  - Identification des facteurs favorisant l'abrasivité et élaboration d'un protocole expérimental robuste.\n  - Incertitudes sur la relation entre la taille des particules et l'effet abrasif, variabilité des résultats selon les récoltes.\n\n- **Procédure de lavage** :\n  - Création d'une procédure de lavage régulier pour éliminer les sédiments résiduels.\n  - Objectifs : réduire l'abrasivité due aux particules solides et résoudre les vibrations excessives de la machine.\n  - Verrou principal : Élaboration d'une procédure de nettoyage sans endommager les zones sensibles de la centrifugeuse.\n\n- **Suivi des performances des machines** :\n  - Difficulté à évaluer rigoureusement les performances de séparation avec les grandeurs existantes.\n  - Développement d'un nouvel outil pour réaliser des bilans de masse du process de séparation afin de calculer le taux d'huile d'extraction.\n\n- **Dépendance des performances** :\n  - Performances d'extraction directement dépendantes du type de produit et de la conduite de l'installation par le client.\n  - Variabilité de la qualité des olives selon la saison (composition, maturité, méthode de récolte), impactant les performances des centrifugeuses.\n  - Complexité d'analyse des résultats de performances sur site et garantie d'un taux d'huile résiduel dans les grignons en sortie machine."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nCe projet de recherche est caractérisé par la difficulté d’assurer une qualité fiable et constante de l’huile d’olive obtenue.\n\nLors des essais de notre première vis convoyeuse avec pales « plough shape », nous avons constaté que nos centrifugeuses déchargeaient de manière discontinue en sortie machine, que ce soit côté solide ou côté huile. Nous avons aussi noté des problèmes de convoyage au sein du bol avec toutes les pales, créant un transfert des solides par forme de bouchon avant de sortir de la machine.  \nAfin de pallier ces difficultés, nous avons apporté des modifications substantielles à notre vis en corrigeant la configuration des pales « plough shape ».  \nBien que nous fussions presque parvenus à atteindre nos objectifs de performances grâce à notre nouvelle vis, les essais menés sur les sites d’exploitation ont révélé des problèmes d’abrasion supérieure à ce que nous attendions au niveau des pales, l’effet abrasif étant dû au produit.\n\nPour remédier à cela, nous avons cherché à caractériser l’abrasivité d’une pâte d’olive. Dans l’état de l’art, nous n’avons trouvé aucune étude spécifique sur ce point appliquée à l’huile d’olive. Nous avons donc dû identifier les facteurs pouvant favoriser l’abrasivité et élaborer un protocole expérimental robuste et reproductible. Cependant, nous avons des incertitudes quant à la possibilité d’établir une relation entre la taille des particules et l’effet abrasif du produit correspondant, surtout qu’une même récolte d’olives peut donner des résultats très différents.\n\nNous avons par ailleurs décidé de créer une procédure de lavage régulier qui permettrait d’éliminer les sédiments résiduels, dans le but de diminuer l’abrasivité due aux particules solides, mais aussi dans le but de trouver une solution aux vibrations excessives constatées sur la machine. Concernant le lavage, le principal verrou a été d’élaborer une procédure pouvant nettoyer toutes les zones de la centrifugeuse sans endommager les zones sensibles à l’exposition aux liquides de lavage.\n\nD’autres difficultés se sont posées, notamment lors des essais permettant de suivre les performances de nos machines. La difficulté rencontrée était l’incapacité des grandeurs existantes à déterminer les performances de séparation de manière rigoureuse. Pour calculer le taux d’huile d’extraction, nous avons ainsi dû développer un nouvel outil pour réaliser des bilans de masse du process de séparation.\n\nPar ailleurs, dans le secteur de l’extraction de l’huile d’olive, les performances sont directement dépendantes du type de produit à traiter et de la conduite de l’installation par le client. La qualité de l’olive change à chaque saison (composition en huile ou eau, maturité, méthode de récolte…) et cela impacte considérablement les performances des centrifugeuses. Il est ainsi compliqué d’analyser les résultats des performances obtenues sur site et de garantir un taux d’huile résiduel dans les grignons en sortie machine."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Développement d'une Presse à Vis**:\n  - Nécessité d'acquérir des connaissances sur les performances de déshydratation.\n  - Manque d'informations suffisantes dans l'état de l'art.\n  - Incertitudes sur la corrélation entre performances de l'outil et qualité de la boue.\n\n- **Floculation**:\n  - Importance du choix de l'outil de mélange boue-polymère sur les performances de déshydratation.\n  - Détermination des paramètres optimaux :\n    - Dosage en polymère (poudre, émulsion).\n    - Mélangeur adéquat pour obtenir une qualité de flocs satisfaisante.\n  - Flocs doivent être résistants à des contraintes mécaniques élevées pour un bon rendement.\n\n- **Aléas Techniques**:\n  - Conception de méthodes et outils de mesure pour surmonter les aléas techniques rencontrés.\n\n- **Essais sur Boues de Papeterie**:\n  - Incertitude technique majeure : extraction continue des boues sans blocage.\n  - Boues de papeterie génèrent un couple élevé, poussant les limites de fonctionnement de la presse à vis.\n\n- **Système d'Épaississement**:\n  - Développement d'un système spécifique en amont de la presse à vis pour augmenter la capacité massique.\n  - Combinaison presse à vis et table d'égouttage nouvelle dans cette application.\n  - Nécessité d'études et d'analyses pour identifier les paramètres critiques du processus et leur valeur optimum selon la qualité des boues.\n\n- **Modifications de la Presse à Vis**:\n  - Dépendance des modifications sur la force axiale de la vis convoyeuse.\n  - Difficulté à déterminer cette valeur au début des travaux, source de problèmes, notamment pour les paniers de filtration.\n  - Concentration de l'étude sur la pression interne en raison de l'incertitude sur la force axiale.\n  - Conception d'un outil de mesure spécifique pour cette détermination.\n  - Nécessité d'une expérimentation spécifique sur site pour finaliser la détermination de la force axiale.\n\n- **Travaux de Recherche**:\n  - Tous ces verrous techniques ont nécessité des travaux de recherche pour résoudre les problématiques identifiées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement d’une presse à vis spécifique pour la déshydratation des boues organiques a nécessité en premier lieu un besoin d’acquisition de connaissances sur les performances de déshydratation, car nous ne disposions pas d’informations suffisantes avec l’état de l’art et des incertitudes subsistaient notamment sur la corrélation des performances atteintes avec cet outil et la qualité de la boue.  \nNous pouvons citer en particulier le travail sur la floculation. Le choix de l’outil de mélange boue – polymère influe sur les performances de déshydratation de la presse à vis. La difficulté a résidé dans la détermination des paramètres optimaux en termes de dosage en polymère (poudre, émulsion) et du mélangeur adéquat pour obtenir une qualité des flocs satisfaisante. En effet, celle-ci est très importante pour obtenir le meilleur rendement de la presse à vis : les flocs doivent être résistants à des contraintes mécaniques élevées.  \n\nOutre ces incertitudes, nous avons été confrontés à d’autres aléas techniques lors de nos travaux qui nous ont amenés à concevoir des méthodes et des outils de mesure pour les adresser.  \n\nConcernant les essais menés sur les boues de papeterie à faible taux de fibre, notre plus forte incertitude technique était portée sur la possibilité d'extraction des boues en continu sans blocage dans la machine. Les boues de papeterie génèrent en effet beaucoup de couple, poussant les limites de fonctionnement de notre outil presse à vis au maximum.  \nDes difficultés supplémentaires se sont posées dans le développement du système d’épaississement spécifique en amont de la presse à vis pour augmenter sa capacité massique. La combinaison presse à vis et table d'égouttage étant nouvelle dans cette application, nous avons dû mener des études et des analyses pour identifier les paramètres critiques du process et déterminer leur valeur optimum en fonction de la qualité entrante des boues.  \n\nLes modifications substantielles effectuées sur la presse à vis dépendent d’une valeur clé, la force axiale de la vis convoyeuse. Nous n’étions pas encore parvenus à la déterminer de façon précise au début de nos travaux. Cette incertitude a été le cœur des difficultés rencontrées, notamment concernant les travaux sur les paniers de filtration. Ne connaissant pas la valeur de cette grandeur, nous avons dû concentrer notre étude sur la pression interne. Nous avons pour cela conçu un outil de mesure spécifique.  \nLa détermination de la force axiale a par la suite nécessité une expérimentation spécifique sur site.  \n\nAinsi, l’ensemble de ces verrous techniques a nécessité des travaux de recherche pour répondre aux problématiques posées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte du projet**: Développement d'une presse à vis pour la déshydratation des boues organiques.\n  \n- **Nécessité de connaissances**: \n  - Manque d'informations sur les performances de déshydratation.\n  - Incertitudes sur la corrélation entre performances de l'outil et qualité de la boue.\n\n- **Floculation**:\n  - Importance du choix de l'outil de mélange boue-polymère pour les performances de déshydratation.\n  - Détermination des paramètres optimaux :\n    - Dosage en polymère (poudre, émulsion).\n    - Sélection du mélangeur pour obtenir des flocs de qualité.\n  - Flocs doivent être résistants à des contraintes mécaniques élevées pour un meilleur rendement.\n\n- **Aléas techniques**:\n  - Conception de méthodes et outils de mesure pour surmonter les aléas rencontrés.\n\n- **Problématique principale**:\n  - La presse à vis n'est pas suffisamment capacitive pour le traitement des boues diluées.\n  - Nécessité d'étudier des solutions pour augmenter la capacité de l'outil.\n  - Absence d'informations dans l'état de l'art, entraînant une démarche expérimentale.\n\n- **Démarche expérimentale**:\n  - Exploration de plusieurs pistes de conception.\n  - Création de prototypes et réalisation de tests.\n  - Développement d'une vis modulaire pour tester différentes hypothèses de conception.\n\n- **Contraintes rencontrées**:\n  - Délai restreint pour mener des campagnes d'essais expérimentaux.\n  - Nécessité de maintenir la qualité de la boue stable durant les essais pour des résultats comparables.\n  - Intégration de différentes configurations prototypes sur un seul rotor tout en permettant des tests indépendants.\n\n- **Problématiques de conception**:\n  - Proposer des parties démontables tout en assurant la rigidité représentative des conditions réelles.\n  - Les supports des éléments mobiles ne doivent pas altérer la conception initiale de la vis pour conserver un état de référence.\n\n- **Développement des prototypes**:\n  - Travail sur la réduction du pas pour éviter le blocage du gâteau dans la machine.\n  - Risque de colmatage accru avec l'utilisation d'un double filet en raison de l'espace réduit pour la boue.\n  - Option des déflecteurs peut induire un bourrage de la machine."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement d’une presse à vis spécifique pour la déshydratation des boues organiques a nécessité en premier lieu un besoin d’acquisition de connaissances sur les performances de déshydratation, car nous ne disposions pas d’informations suffisantes avec l’état de l’art et des incertitudes subsistaient notamment sur la corrélation des performances atteintes avec cet outil et la qualité de la boue.  \nNous pouvons citer en particulier le travail sur la floculation. Le choix de l’outil de mélange boue – polymère influe sur les performances de déshydratation de la presse à vis. La difficulté a résidé dans la détermination des paramètres optimaux en termes de dosage en polymère (poudre, émulsion) et du mélangeur adéquat pour obtenir une qualité des flocs satisfaisante. En effet, celle-ci est très importante pour obtenir le meilleur rendement de la presse à vis : les flocs doivent être résistants à des contraintes mécaniques élevées.  \n\nOutre ces incertitudes, nous avons été confrontés à d’autres aléas techniques lors de nos travaux qui nous ont amenés à concevoir des méthodes et des outils de mesure pour les adresser.  \n\nSuite aux différentes hypothèses de conception de la vis, notre principale problématique était la suivante.  \nDans le cadre du traitement des boues diluées, la presse à vis n’est pas un outil suffisamment capacitif. Nous avons donc souhaité étudier les possibilités de le rendre davantage capacitif. Cependant, il n’existait pas d’information disponible à ce sujet dans l’état de l’art. De ce fait, nous avons dû mettre en œuvre une démarche expérimentale afin de collecter des informations techniques à travers l’exploration de plusieurs pistes de conception dont les résultats ne sont pas connus, la création de prototypes, la réalisation de tests et l’amélioration de nos prototypes. Cela nous a amené à développer une vis modulaire permettant de tester expérimentalement nos différentes hypothèses de conception.  \n\nPar ailleurs, nous avons dû faire face à différentes contraintes telles que la possibilité de mener des campagnes d’essais expérimentaux sur un délai relativement restreint. En effet, afin d’obtenir des résultats comparables, nous devons nous assurer que la qualité de la boue reste stable durant la durée complète des différents essais menés successivement. Cette contrainte nous a menés à la nécessité d’intégrer l’ensemble des différentes configurations prototypes sur un seul rotor tout en ayant la possibilité de les tester indépendamment les unes des autres. Dès lors, nous avons été confrontés à la problématique de proposer des parties démontables tout en assurant une rigidité représentative des conditions réelles de ces dernières. De plus, les parties supportant les éléments mobiles ne devaient pas modifier la conception initiale de la vis afin de permettre de conserver un état de référence permettant une comparaison des résultats issus des essais expérimentaux avec les résultats actuels.  \nLe développement de ces prototypes nous a également conduits à travailler sur la réduction du pas au blocage du gâteau dans la machine. En effet, l’utilisation d’un double filet induit un moindre espace pour la boue au cœur de la presse à vis et donc un risque de colmatage plus important. L’option des déflecteurs induit quant à elle un bourrage de la machine."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Problème principal** : Difficultés à réduire la puissance consommée tout en maintenant les performances d’une machine standard, notamment l'alliance entre ajustements des pièces nouvellement conçues et performances.\n\n- **Allègement du rotor** :\n  - Réduction de l’épaisseur des éléments du rotor (bol et vis convoyeuse).\n  - Viabilité théorique de la structure allégée confirmée par des travaux antérieurs.\n  - **Difficulté rencontrée** : Déformations importantes des vis convoyeuses dues aux contraintes induites par le soudage, qui est omniprésent dans l’assemblage.\n  - **Problème d’usinage** : Les bols subissent des vibrations importantes lors de l’usinage, nécessitant une redéfinition des procédés de fabrication.\n\n- **Problème de roulement** :\n  - L’allègement du rotor entraîne un sous-chargement du roulement, provoquant le glissement des rouleaux sur les pistes.\n  - Contraintes géométriques empêchent l'utilisation de roulements de plus petite taille pour résoudre le problème de sous-charge.\n\n- **Contraintes géométriques** :\n  - Empêchent l'application des technologies développées sur le nouveau prototype D5LX, qui est une évolution du prototype DU3LL, lui-même dérivé du prototype U3LL.\n\n- **Mauvaise connaissance du convoyage** :\n  - Manque de visibilité sur le brassage du mélange boue/polymère au sein de la centrifugeuse.\n  - Observation d'un by-pass de boue liquide vers la sortie d’évacuation de la boue déshydratée, non modélisé jusqu'à présent."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nParmi les problèmes rencontrés pour réduire la puissance consommée tout en maintenant les performances d’une machine standard, nous pouvons citer les difficultés à allier ajustements des pièces nouvellement conçues et performances.\n\nPour alléger le rotor, nous avons diminué l’épaisseur des éléments qui le composent : le bol et la vis convoyeuse. Nos travaux des années précédentes nous ont permis de d’attester de la viabilité théorique d’une telle structure dans le fonctionnement de la centrifugeuse. Néanmoins, nous avons rencontré une difficulté majeure lors de la conception des vis convoyeuses ainsi modifiées. En effet, le soudage est omniprésent dans l’assemblage de nos produits, or celui-ci induit des contraintes fortes dans les pièces soudées. Nous avons observé d’importantes déformations au niveau des vis dues à cette étape de soudage. De façon équivalente, les bols subissent une étape d’usinage avant d’être assemblés. Nous avons observé que les pièces à épaisseur réduites étaient sujettes à d’importantes vibrations lors de cette étape. Les procédés de fabrication des pièces en elles-mêmes sont donc à redéfinir.\n\nL’allègement du rotor a induit un autre problème. En raison de celui-ci, le roulement se retrouve sous chargé, ce qui provoque le glissement des rouleaux sur les pistes. De plus, à cause de contraintes géométriques, le recours à un roulement de plus petite taille pour résoudre le problème de sous charge est impossible. Ce sont également des contraintes géométriques qui nous ont empêché d’appliquer les technologies sur lesquelles nous travaillons depuis des années au nouveau prototype D5LX, évolution du prototype DU3LL (lui-même étant une évolution du prototype U3LL).\n\nUne dernière difficulté réside dans la mauvaise connaissance et visibilité que nous avons du convoyage des boues au sein de nos prototypes. En effet, il existe, de façon générale un brassage du mélange boue/polymère au sein de la centrifugeuse. Nous avons observé, dans un grand nombre de configurations différentes, un by-pass de boue liquide vers la sortie destinée à l’évacuation de la boue déshydratée, que nous ne sommes jusqu’ici pas parvenus à modéliser."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Développement de la Presse à Vis**:\n  - Nécessité d'acquérir des connaissances sur les performances de déshydratation des boues organiques.\n  - Manque d'informations suffisantes dans l'état de l'art concernant la corrélation entre les performances de l'outil et la qualité de la boue.\n\n- **Floculation**:\n  - Importance du choix de l'outil de mélange boue-polymère sur les performances de déshydratation.\n  - Difficulté à déterminer les paramètres optimaux : dosage en polymère (poudre, émulsion) et type de mélangeur.\n  - Qualité des flocs cruciale pour le rendement de la presse à vis, nécessitant des flocs résistants aux contraintes mécaniques élevées.\n\n- **Aléas Techniques**:\n  - Confrontation à divers aléas techniques nécessitant la conception de méthodes et d'outils de mesure.\n\n- **Problématique Principale**:\n  - Insuffisance capacitive de la presse à vis pour le traitement des boues diluées.\n  - Absence d'informations dans l'état de l'art sur les possibilités d'augmentation de la capacité.\n  - Mise en œuvre d'une démarche expérimentale pour collecter des informations techniques :\n    - Exploration de plusieurs pistes de conception.\n    - Création de prototypes.\n    - Réalisation de tests et amélioration des prototypes.\n  - Développement d'une vis modulaire pour tester les hypothèses de conception.\n\n- **Contraintes Expérimentales**:\n  - Nécessité de mener des campagnes d'essais expérimentaux dans un délai restreint.\n  - Importance de maintenir la qualité de la boue stable durant les essais pour obtenir des résultats comparables.\n  - Intégration de différentes configurations prototypes sur un seul rotor, avec possibilité de tests indépendants.\n  - Problématique de proposer des parties démontables tout en assurant la rigidité représentative des conditions réelles.\n  - Conservation de l'état de référence de la vis pour permettre la comparaison des résultats expérimentaux avec les résultats actuels.\n\n- **Développement des Prototypes**:\n  - Travail sur la réduction du pas pour éviter le blocage du gâteau dans la presse à vis.\n  - Incertitude technique majeure : possibilité d'extraction continue des boues sans blocage.\n  - Utilisation d'un double filet réduisant l'espace pour la boue, augmentant le risque de colmatage.\n  - Option des déflecteurs entraînant un bourrage de la machine.\n  - Nécessité d'études et d'analyses pour comprendre les phénomènes de convoyage et la répartition des boues à l'intérieur de la presse à vis."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement d’une presse à vis spécifique pour la déshydratation des boues organiques a nécessité en premier lieu un besoin d’acquisition de connaissances sur les performances de déshydratation, car nous ne disposions pas d’informations suffisantes avec l’état de l’art et des incertitudes subsistaient notamment sur la corrélation des performances atteintes avec cet outil et la qualité de la boue.  \nNous pouvons citer en particulier le travail sur la floculation. Le choix de l’outil de mélange boue – polymère influe sur les performances de déshydratation de la presse à vis. La difficulté a résidé dans la détermination des paramètres optimaux en termes de dosage en polymère (poudre, émulsion) et du mélangeur adéquat pour obtenir une qualité des flocs satisfaisante. En effet, celle-ci est très importante pour obtenir le meilleur rendement de la presse à vis : les flocs doivent être résistants à des contraintes mécaniques élevées.  \n\nOutre ces incertitudes, nous avons été confrontés à d’autres aléas techniques lors de nos travaux qui nous ont amenés à concevoir des méthodes et des outils de mesure pour les adresser.  \n\nSuite aux différentes hypothèses de conception de la nouvelle vis, notre principale problématique était la suivante.  \nDans le cadre du traitement des boues diluées, la presse à vis n’est pas un outil suffisamment capacitif. Nous avons donc souhaité étudier les possibilités technologiques pour le rendre davantage capacitif. Cependant, il n’existait pas d’information disponible à ce sujet dans l’état de l’art. De ce fait, nous avons dû mettre en œuvre une démarche expérimentale afin de collecter des informations techniques à travers l’exploration de plusieurs pistes de conception dont les résultats ne sont pas connus, la création de prototypes, la réalisation de tests et l’amélioration de nos prototypes. Cela nous a amenés à développer une vis modulaire permettant de tester expérimentalement nos différentes hypothèses de conception.  \n\nPar ailleurs, nous avons dû faire face à différentes contraintes telles que la possibilité de mener des campagnes d’essais expérimentaux sur un délai relativement restreint. En effet, afin d’obtenir des résultats comparables, nous devons nous assurer que la qualité de la boue reste stable durant la durée complète des différents essais menés successivement. Cette contrainte nous a menés à la nécessité d’intégrer l’ensemble des différentes configurations prototypes sur un seul rotor tout en ayant la possibilité de les tester indépendamment les unes des autres. Dès lors, nous avons été confrontés à la problématique de proposer des parties démontables tout en assurant une rigidité représentative des conditions réelles de ces dernières. De plus, les parties supportant les éléments mobiles ne devaient pas modifier la conception initiale de la vis afin de conserver un état de référence permettant une comparaison des résultats issus des essais expérimentaux avec les résultats actuels.  \n\nLe développement de ces prototypes nous a également conduits à travailler sur la réduction du pas au blocage du gâteau dans la presse à vis, notre plus forte incertitude technique étant portée sur la possibilité d'extraction des boues en continu sans blocage dans la machine. En effet, l’utilisation d’un double filet induit un moindre espace pour la boue au cœur de la presse à vis et donc un risque de colmatage plus important. L’option des déflecteurs induit quant à elle un bourrage de la machine.  \nCes solutions étant nouvelles dans l’application presse à vis, nous avons dû mener des études et des analyses pour mieux comprendre les phénomènes de convoyage et déterminer la répartition des boues à l’intérieur de la presse à vis."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte du projet (2019)** :\n  - Rencontres avec de nombreux verrous technologiques.\n  - Difficulté d'interprétation des résultats de performances faibles après la campagne d’essais.\n\n- **Problématique principale** :\n  - Identification des phénomènes responsables des performances faibles.\n  - Détermination des paramètres critiques à mesurer sur le prototype de centrifugeuse.\n\n- **Observations et analyses** :\n  - Pression d’alimentation observée : 1 à 3 bars (supérieure à la machine de référence : 0 à 0,5 bars).\n  - Nécessité d’une étude hydraulique pour comprendre l'origine de la surpression.\n\n- **Incertitude scientifique 1** :\n  - Simulation numérique des phénomènes à l’intérieur de la centrifugeuse.\n  - Difficulté d'observation des phénomènes physiques en raison de l'espace fermé.\n  - Modélisation des pièces en Conception Assistée par Ordinateur (CAO) avec maillage fin des zones d’intérêt.\n\n- **Incertitude technologique 2** :\n  - Simulation du comportement de la boue et paramétrage des résultats.\n  - Complexité liée à la présence de bulles d’air dans la boue.\n  - Variations de composition des boues selon leur origine, âge et traitements, influençant leur comportement mécanique.\n\n- **Incertitude scientifique 3** :\n  - Complexité des phénomènes à l’intérieur de la centrifugeuse due à la rotation du bol et du tube d’alimentation.\n  - Étude initiale de la centrifugeuse en statique, suivie de l'ajout de la rotation pour simplifier l’analyse.\n\n- **Difficultés d’interprétation** :\n  - Résultats des premières simulations nécessitant des études paramétriques.\n  - Hypothèses posées sur les effets de la vitesse de rotation, du débit et de la géométrie de la centrifugeuse.\n  - Importance de prendre en compte l'impact de la géométrie sur le flux.\n\n- **Hypothèses et vérifications** :\n  - Simulations sur des zones précises tout en paramétrant le débit du flux.\n  - Vérification d’hypothèses par calculs supplémentaires (ex. : effet de l’épaisseur de la sortie fluide sur les phénomènes)."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nDurant nos travaux de 2019, nous nous sommes heurtés à de nombreux verrous technologiques. \n\nDans un premier temps, le verrou principal qui a orienté tout le projet était la difficulté d'interprétation pour expliquer l'origine des résultats de performances faibles à la suite de la campagne d’essais début 2019. C’est dans le but de résoudre cette problématique que nous avons effectué les expérimentations et les simulations suivantes. \nCette problématique a entrainé une difficulté liée à la recherche de phénomènes potentiellement responsables et la détermination des paramètres critiques à mesurer sur notre centrifugeuse prototype pour confirmer ou infirmer nos hypothèses.\n\nAprès analyse de la situation, recherche dans nos anciens rapports d’observations liés au prototype et réalisation de mesures, une observation : la pression d’alimentation était très élevée, bien supérieure à la machine de référence (de 1 à 3 bars au lieu de 0 à 0,5 bars). La difficulté qui en a découlée est liée à la recherche et l’analyse des phénomènes provoquant cette surpression, donnant lieu à une étude hydraulique pour comprendre l'origine du phénomène.\n\nUne première incertitude scientifique lors de l’étude a été de simuler numériquement les phénomènes se produisant à l’intérieur de la centrifugeuse. En effet, la centrifugeuse étant un espace fermé, l’observation des phénomènes physiques par la mise en place de capteurs n’est pas possible ou, au cas échéant, influera grandement sur les résultats observés. Nous avons alors dû modéliser toutes les pièces des centrifugeuses en Conception Assistée par Ordinateur (CAO), et maillé suffisamment finement les zones d’intérêt dans le but de pouvoir visualiser les phénomènes tout en ayant une complexité de calcul raisonnable.\n\nPuis, à ces simulations, nous avons dû ajouter des paramètres indispensables pour notre étude : les caractéristiques de la boue. C’est alors que nous avons rencontré notre deuxième verrou technologique. Nous devions simuler numériquement le comportement de la boue et la paramétrer dans nos résultats, sachant qu’il peut y avoir des phénomènes complexes tels que des bulles d’air dans la boue, ce dont il faudra prendre en compte en comparant nos résultats aux mesures physiques de pression d’alimentation. D’autant plus que chaque boue testée se comporte différemment. Les boues présentent en effet d’importantes variations de composition selon leur origine ou leur âge, et les traitements qu’elles ont subis peuvent amplifier ces variations, ce qui influence directement leur comportement mécanique pendant l’étape de déshydratation.\n\nA ces phénomènes complexes, s’est ajoutée notre troisième incertitude, celle-ci concernant la complexité des phénomènes prenant place à l’intérieur de la centrifugeuse de par la rotation du bol mais aussi du tube d’alimentation. Pour réaliser nos études, nous avons étudié dans un premier temps la centrifugeuse en statique, sans rotation et ensuite ajouté la rotation de la géométrie dans le but de simplifier l’analyse des phénomènes, et de pouvoir accéder à la pression dynamique. \nNous avons aussi fait face à une difficulté d’interprétation : après les premières simulations, nous avons observé des résultats qui ont nécessité de nombreuses études paramétriques dans le but de comprendre s’ils étaient dus à la vitesse de rotation du bol, au débit, où à la géométrie de la centrifugeuse. Nous avons donc dû poser des hypothèses de remplissage des ouvertures dans le but d’avoir un flux continu et de négliger certains effets instables. \nDe plus, pour étudier les phénomènes sur des zones précises, nous avons effectué les simulations sur ces zones mais il est important de prendre en compte l’impact du reste de la géométrie sur le flux, et pour cela, de paramétriser le débit du flux.\nEnfin, dans le but de faire nos simulations, nous avons posé des hypothèses que nous avons vérifiées par calcul supplémentaire (par exemple, effet de l’épaisseur de la sortie fluide sur les phénomènes)."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Verrous Technologiques Identifiés (2020)**:\n  - Difficulté d'interprétation des résultats de performances faibles suite à la campagne d'essais de début 2019.\n  - Nécessité d'expérimentations et de simulations pour résoudre cette problématique.\n  - Identification des phénomènes responsables et des paramètres critiques à mesurer sur le prototype de centrifugeuse.\n\n- **Observation Clé**:\n  - Pression d'alimentation très élevée (1 à 3 bars) par rapport à la machine de référence (0 à 0,5 bars).\n  - Nécessité d'une étude hydraulique pour comprendre l'origine de cette surpression.\n\n- **Incertitudes Scientifiques**:\n  - **Simulation Numérique**:\n    - Difficulté à simuler les phénomènes à l'intérieur de la centrifugeuse en raison de son espace fermé.\n    - Mise en place de capteurs impossible sans influencer les résultats.\n    - Modélisation en Conception Assistée par Ordinateur (CAO) des pièces de la centrifugeuse avec un maillage fin dans les zones d'intérêt.\n\n  - **Comportement de la Boue**:\n    - Nécessité de simuler le comportement de la boue, incluant des phénomènes complexes comme des bulles d'air.\n    - Variations importantes de composition des boues selon leur origine, âge et traitements, influençant leur comportement mécanique lors de la déshydratation.\n\n  - **Complexité des Phénomènes Internes**:\n    - Étude de la centrifugeuse en statique avant d'ajouter la rotation pour simplifier l'analyse.\n    - Observation de résultats nécessitant des études paramétriques pour déterminer l'impact de la vitesse de rotation, du débit et de la géométrie.\n\n- **Hypothèses et Paramétrages**:\n  - Hypothèses de remplissage des ouvertures pour assurer un flux continu et négliger certains effets instables.\n  - Importance de paramétriser le débit du flux pour étudier les phénomènes sur des zones précises tout en tenant compte de l'impact de la géométrie environnante.\n  - Vérification des hypothèses par des calculs supplémentaires, par exemple, l'effet de l'épaisseur de la sortie fluide sur les phénomènes."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDurant nos travaux de 2020, nous nous sommes heurtés à de nombreux verrous technologiques. \n\nDans un premier temps, le verrou principal qui a orienté tout le projet était la difficulté d'interprétation pour expliquer l'origine des résultats de performances faibles à la suite de la campagne d’essais début 2019. C’est dans le but de résoudre cette problématique que nous avons effectué des expérimentations et des simulations.  \nCette problématique a entrainé une difficulté liée à la recherche de phénomènes potentiellement responsables et la détermination des paramètres critiques à mesurer sur notre centrifugeuse prototype pour confirmer ou infirmer nos hypothèses.\n\nAprès analyse de la situation, recherche dans nos anciens rapports d’observations liés au prototype et réalisation de mesures, une observation est ressortie : la pression d’alimentation était très élevée, bien supérieure à la machine de référence (de 1 à 3 bars au lieu de 0 à 0,5 bars). La difficulté qui en a découlée est liée à la recherche et l’analyse des phénomènes provoquant cette surpression, donnant lieu à une étude hydraulique pour comprendre l'origine du phénomène.\n\nUne première incertitude scientifique lors de l’étude a été de simuler numériquement les phénomènes se produisant à l’intérieur de la centrifugeuse. En effet, la centrifugeuse étant un espace fermé, l’observation des phénomènes physiques par la mise en place de capteurs n’est pas possible ou, le cas échéant, influera grandement sur les résultats observés. Nous avons alors dû modéliser toutes les pièces des centrifugeuses en Conception Assistée par Ordinateur (CAO), et maillé suffisamment finement les zones d’intérêt dans le but de pouvoir visualiser les phénomènes tout en ayant une complexité de calcul raisonnable. \n\nPuis, à ces simulations, nous avons dû ajouter des paramètres indispensables pour notre étude : les caractéristiques de la boue. C’est alors que nous avons rencontré notre deuxième verrou technologique. Nous devions simuler numériquement le comportement de la boue et la paramétrer dans nos résultats, sachant qu’il peut y avoir des phénomènes complexes tels que des bulles d’air dans la boue, ce dont il faudra prendre en compte en comparant nos résultats aux mesures physiques de pression d’alimentation. D’autant plus que chaque boue testée se comporte différemment. Les boues présentent en effet d’importantes variations de composition selon leur origine ou leur âge, et les traitements qu’elles ont subis peuvent amplifier ces variations, ce qui influence directement leur comportement mécanique pendant l’étape de déshydratation.\n\nA ces phénomènes complexes, s’est ajoutée notre troisième incertitude, celle-ci concernant la complexité des phénomènes prenant place à l’intérieur de la centrifugeuse de par la rotation du bol mais aussi du tube d’alimentation. Pour réaliser nos études, nous avons étudié dans un premier temps la centrifugeuse en statique, sans rotation et ensuite ajouté la rotation de la géométrie dans le but de simplifier l’analyse des phénomènes, et de pouvoir accéder à la pression dynamique.  \nNous avons aussi fait face à une difficulté d’interprétation : après les premières simulations, nous avons observé des résultats qui ont nécessité de nombreuses études paramétriques dans le but de comprendre s’ils étaient dus à la vitesse de rotation du bol, au débit, où à la géométrie de la centrifugeuse. Nous avons donc dû poser des hypothèses de remplissage des ouvertures dans le but d’avoir un flux continu et de négliger certains effets instables.  \nDe plus, pour étudier les phénomènes sur des zones précises, nous avons effectué les simulations sur ces zones mais il est important de prendre en compte l’impact du reste de la géométrie sur le flux, et pour cela, de paramétriser le débit du flux.  \nEnfin, dans le but de faire nos simulations, nous avons posé des hypothèses que nous avons vérifiées par calcul supplémentaire (par exemple, effet de l’épaisseur de la sortie fluide sur les phénomènes)."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte des travaux (2021)** :\n  - Confrontation à de nombreux verrous technologiques.\n\n- **Verrou principal** :\n  - Difficulté d'interprétation des résultats de performances faibles des boues concentrées suite à la campagne d’essais de l'année précédente.\n  - Nécessité d'expérimentations et de tests pour résoudre cette problématique.\n  - Recherche de phénomènes responsables et détermination des paramètres critiques à mesurer sur la centrifugeuse prototype.\n\n- **Autre verrou technologique** :\n  - Impossibilité de simuler numériquement les phénomènes à l'intérieur de la centrifugeuse (espace fermé).\n  - Observation des phénomènes physiques par capteurs non réalisable sans influencer les résultats.\n  - Réalisation d'expérimentations en arrêtant la machine et en effectuant des coupes pour observer le comportement du solide.\n\n- **Complexité du comportement des boues** :\n  - Présence de phénomènes complexes (ex. : bulles d’air) à prendre en compte lors de la comparaison des résultats avec les mesures physiques de pression d’alimentation.\n  - Variations significatives de composition des boues selon leur origine, leur âge et les traitements subis, influençant leur comportement mécanique durant la déshydratation.\n\n- **Difficulté de prédiction** :\n  - Complexité à prédire les phénomènes dans les centrifugeuses et le comportement des boues.\n  - Impact des paramètres opérationnels (choix du polymère, débit de la boue, vitesse différentielle, force centrifuge) sur l’augmentation de la siccité et l’amélioration des performances difficile à évaluer.\n\n- **Nécessité de R&D** :\n  - Travaux de recherche et développement indispensables pour lever les verrous technologiques.\n  - Objectif : améliorer les performances des centrifugeuses."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDurant nos travaux de 2021, nous nous sommes confrontés à de nombreux verrous technologiques.  \n\nDans un premier temps, le verrou principal qui a orienté tout le projet était la difficulté d'interprétation pour expliquer l'origine des résultats de performances faibles dans le cas des boues concentrées à la suite de la campagne d’essais de l’année dernière. C’est dans le but de résoudre cette problématique que nous avons effectué des expérimentations et des tests.  \nCette problématique a entrainé une difficulté liée à la recherche de phénomènes potentiellement responsables et la détermination des paramètres critiques à mesurer sur notre centrifugeuse prototype pour confirmer ou infirmer nos hypothèses.  \n\nL’autre verrou technologique de notre étude a été l’impossibilité de simuler numériquement les phénomènes se produisant à l’intérieur de la centrifugeuse. En effet, la centrifugeuse étant un espace fermé, l’observation des phénomènes physiques par la mise en place de capteurs n’est pas possible ou, le cas échéant, influera grandement sur les résultats observés. Nous avons alors dû réaliser des expérimentations en arrêtant la machine en cours de fonctionnement et en effectuant des coupes afin de suivre le comportement du solide à l’intérieur de la machine.  \nLe comportement de la boue complique également la compréhension et la prédiction des phénomènes qui se passent dans nos centrifugeuses. En effet, il peut y avoir des phénomènes complexes tels que des bulles d’air dans la boue, ce dont il faudra prendre en compte en comparant nos résultats aux mesures physiques de pression d’alimentation. D’autant plus que chaque boue testée se comporte différemment. Les boues présentent en effet d’importantes variations de composition selon leur origine ou leur âge, et les traitements qu’elles ont subis peuvent amplifier ces variations, ce qui influence directement leur comportement mécanique pendant l’étape de déshydratation.  \nLa complexité de prédire les phénomènes qui se passent à l’intérieur de nos centrifugeuses ainsi que le comportement de la boue rendent difficile de deviner l’impact des différents paramètres opérationnels comme le choix du polymère, le débit de la boue, la vitesse différentielle ou encore la force centrifuge sur l’augmentation de la siccité et l’amélioration de la performance.  \nDes travaux de R&D sont alors nécessaires afin de lever nos verrous technologiques et de répondre à notre objectif d’améliorer les performances de nos centrifugeuses."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Difficultés d'interprétation des performances** :\n  - Faibles performances de déshydratation observées lors de la campagne d'essais précédente sur les boues concentrées.\n  - Recherche d'explications sur les phénomènes responsables et identification des paramètres critiques sur la centrifugeuse pour valider les hypothèses.\n\n- **Incertitudes rencontrées dans les travaux de R&D** :\n  \n  - **Incertitude liée aux simulations numériques** :\n    - Difficulté d'instrumentation d'un rotor tournant à plus de 3000 tr/min, limitant la visibilité sur les processus internes des prototypes.\n    - Incertitudes sur les facteurs limitant la performance (clarification, déshydratation, convoyage de la boue).\n    - Complexité des simulations numériques due à l'hétérogénéité élevée de la boue.\n\n  - **Incertitude liée aux propriétés intrinsèques de la boue** :\n    - Comportement complexe de la boue, incluant la présence potentielle de bulles d'air.\n    - Variations de composition des boues selon leur origine, leur âge et les traitements subis, influençant leur comportement mécanique lors de la déshydratation.\n    - Manque de maîtrise sur l'évolution temporelle des propriétés des boues, introduisant des biais difficiles à ajuster.\n\n  - **Incertitude liée à la prédiction des phénomènes physiques** :\n    - Complexité de prédire les phénomènes internes des centrifugeuses et le comportement de la boue.\n    - Difficulté à prédire l'impact des paramètres opérationnels (choix du polymère, débit de la boue, vitesse différentielle, force centrifuge) sur la siccité et la performance.\n    - Questionnement sur les effets sous-jacents de ces paramètres expliquant la baisse de performance observée pour des concentrations de boues plus élevées dans la gamme DU.\n\n- **Objectif des travaux de R&D** :\n  - Résoudre les problématiques identifiées liées aux aléas et incertitudes dans le processus de déshydratation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons rencontré des difficultés d’interprétation ou d’explication en ce qui concerne l’origine des faibles performances de déshydratation obtenues dans le cadre de la campagne d’essais de l’année précédente portant sur les boues concentrées. Nous avions cherché à comprendre les phénomènes potentiellement responsables de ces observations, et aussi à déterminer les paramètres critiques sur notre centrifugeuse, ceci quant à la validation de nos hypothèses. \n\nNous présentons dans ce qui suit les incertitudes rencontrées cette année, en ce qui concerne nos travaux de R&D. \n\nIncertitude liée aux difficultés de simulations numériques au sein d’un espace fermé\n\nL’absence de visibilité sur les processus internes de nos prototypes (difficulté d’instrumenter un rotor tournant à plus de 3000 tr/min), engendre des incertitudes quant aux facteurs limitant éventuellement la performance (la clarification, la déshydratation et le convoyage de la boue). En effet, en raison de la nature close de la centrifugeuse, il n’est pas envisageable d’observer ces phénomènes physiques par le biais de capteurs, ou dans le cas contraire, cela aurait une incidence significative sur les résultats observés. Les simulations numériques sont également très complexes dans cette configuration, en raison de la très forte hétérogénéité de la boue. \n\nIncertitude liée aux propriétés intrinsèques de la boue\n\nLe comportement de la boue complique également la compréhension et la prédiction des phénomènes qui se passent dans nos centrifugeuses. En effet, il peut y avoir des phénomènes complexes tels que des bulles d’air dans la boue, ce dont il faudra prendre en compte en comparant nos résultats aux mesures physiques de pression d’alimentation. Les boues peuvent également présenter d’importantes variations de composition selon leur origine ou leur âge, et les traitements qu’elles ont subis peuvent amplifier ces variations, ce qui influence directement leur comportement mécanique pendant l’étape de déshydratation. Même si nous avons connaissance de ces phénomènes dans l’état de l’art nous ne maitrisons pas l’évolution dans le temps des propriétés des boues. Cela pourrait introduire un biais qu'il devient difficile d’ajuster ou d'éliminer.\n\nIncertitude liée à la prédiction des phénomènes physiques et leur influence sur les variables paramétriques de la performance\n\nLa complexité de prédire les phénomènes qui se passent à l’intérieur de nos centrifugeuses ainsi que le comportement de la boue rendent difficile la prédiction de l’impact des différents paramètres opérationnels comme le choix du polymère, le débit de la boue, la vitesse différentielle ou encore la force centrifuge sur l’augmentation de la siccité et l’amélioration de la performance. Notamment la question ici est de savoir quelles sont les effets sous-jacents liées à ces paramètres expliquant le recul des performances observé pour des concentrations de boues plus élevées au sein de la gamme DU en tenant compte des contraintes suscités ? \n\nC’est donc dans le but de résoudre ces problématiques que nous avons engagé des travaux de R&D."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Difficultés d’interprétation des performances de déshydratation (2022)**  \n  - Faibles performances observées lors des essais de déshydratation des boues concentrées.\n  - Recherche des phénomènes responsables et des paramètres critiques sur la centrifugeuse pour valider les hypothèses.\n\n- **Incertitudes rencontrées en R&D cette année**  \n  - **Difficultés de simulations numériques**  \n    - Absence de visibilité sur les processus internes des prototypes (rotor à plus de 3000 tr/min).\n    - Incertitudes sur les facteurs limitant la performance (clarification, déshydratation, convoyage de la boue).\n    - Impossibilité d'observer les phénomènes physiques avec des capteurs sans affecter les résultats.\n    - Complexité des simulations numériques en raison de l'hétérogénéité de la boue dans un repère tournant.\n\n  - **Propriétés intrinsèques de la boue**  \n    - Comportement complexe de la boue, incluant la présence potentielle de bulles d’air.\n    - Variations de composition des boues selon leur origine, âge et traitements antérieurs, influençant leur comportement mécanique.\n    - Manque de maîtrise sur l'évolution temporelle des propriétés des boues, introduisant des biais difficiles à ajuster.\n\n  - **Prédiction des phénomènes physiques**  \n    - Difficulté à prédire l’impact des interactions entre la boue et les pièces internes de la centrifugeuse (fut de vis, filets) sur la sédimentation.\n    - Complexité dans l'évaluation de l'impact des paramètres opérationnels (choix du polymère, débit de boue, vitesse différentielle) sur la performance de siccité.\n    - Nécessité de comprendre les effets sous-jacents expliquant la baisse de performances observée pour des concentrations de boues plus élevées.\n\n- **Engagement dans des travaux de R&D**  \n  - Objectif de résoudre les problématiques identifiées liées aux incertitudes et aux verrous technologiques."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons rencontré des difficultés d’interprétation ou d’explication en ce qui concerne l’origine des faibles performances de déshydratation obtenues dans le cadre de la campagne d’essais de l’année dernière (2022) portant sur les boues concentrées. Nous avions cherché à comprendre les phénomènes potentiellement responsables de ces observations, et aussi à déterminer les paramètres critiques sur notre centrifugeuse, ceci quant à la validation de nos hypothèses.\n\nNous présentons dans ce qui suit les incertitudes rencontrées cette année, en ce qui concerne nos travaux de R&D.\nIncertitude liée aux difficultés de simulations numériques au sein d’un espace fermé\nL’absence de visibilité sur les processus internes de nos prototypes (difficulté d’instrumenter un rotor tournant à plus de 3000 tr/min) engendre des incertitudes quant aux facteurs limitant éventuellement la performance (la clarification, la déshydratation et le convoyage de la boue). En effet, en raison de la nature close de la centrifugeuse, il n’est pas envisageable d’observer ces phénomènes physiques par le biais de capteurs, ou dans le cas contraire, cela aurait une incidence significative sur les résultats observés. Les simulations numériques sont également très complexes dans cette configuration. Les solutions de simulation de fluide dans un repère tournant ne donnent pas de bons résultats en raison de l’hétérogénéité de la boue.\nIncertitude liée aux propriétés intrinsèques de la boue\nLe comportement de la boue complique également la compréhension et la prédiction des phénomènes qui se passent dans nos centrifugeuses. En effet, il peut y avoir des phénomènes complexes tels que des bulles d’air dans la boue, ce dont il faudra tenir compte en comparant nos résultats aux mesures physiques de pression d’alimentation. Les boues peuvent également présenter d’importantes variations de composition selon leur origine ou leur âge, et les traitements qu’elles ont subis peuvent amplifier ces variations, ce qui influence directement leur comportement mécanique pendant l’étape de déshydratation. Même si nous avons connaissance de ces phénomènes dans l’état de l’art nous ne maitrisons pas l’évolution dans le temps des propriétés des boues. Cela pourrait introduire un biais qu'il devient difficile d’ajuster ou d'éliminer.\nIncertitude liée à la prédiction des phénomènes physiques et leur influence sur la performance\nL’incertitude liée aux phénomènes qui se passent à l’intérieur de nos centrifugeuses telles que l’interaction entre la boue et les pièces internes (fut de vis, filets) sur le processus de sédimentation (surface non libre) ainsi que le comportement de la boue rendent difficile la prédiction de l’impact des différents paramètres opérationnels comme le choix du polymère, le débit de la boue, ou encore la vitesse différentielle sur l’augmentation de la performance de siccité. La question ici est de savoir quels sont les effets sous-jacents liés à ces paramètres expliquant le recul des performances observé pour des concentrations de boues plus élevées au sein de la gamme DU en tenant compte des contraintes suscitées.\n\nC’est donc dans le but de résoudre ces problématiques que nous avons engagé des travaux de R&D."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Ressources Informatiques**: Élaboration et supervision d'une machine virtuelle pour chaque page web nécessitent des ressources considérables, entraînant des problèmes de performance et de scalabilité.\n\n- **Synchronisation Vidéo**: Optimisation de la synchronisation entre le flux vidéo et les interactions utilisateur est un défi technique majeur pour garantir une expérience fluide et sans retard.\n\n- **Compatibilité et Affichage**: Problèmes d'interopérabilité entre différents systèmes d'exploitation, navigateurs web et environnements 3D, nécessitant des solutions sur mesure pour une expérience homogène et immersive.\n\n- **Normes Ouvertes et API**: Besoin de développer des normes ouvertes et des API pour prendre en charge divers environnements d'exploitation.\n\n- **Algorithmes de Répartition**: Identification d'un verrou concernant l'efficacité des algorithmes de répartition et de gestion des ressources serveur, nécessitant une adaptation à des conditions de charge variables et imprévisibles.\n\n- **Technologies Évolutives**: L'évolution rapide des technologies et standards en réalité virtuelle et 3D crée une incertitude sur l'intégration et la compatibilité avec l'architecture actuelle.\n\n- **Obsolescence Technologique**: Nécessité de mises à jour continues pour éviter l'obsolescence technologique et rester à la pointe des avancées.\n\n- **Défis Globaux**: La technologie de VM instantanée pour l'affichage de pages web en 3D multi-utilisateur pose des défis en performance, compatibilité et sécurité, essentiels pour garantir une expérience utilisateur optimale."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNous avons identifié plusieurs verrous technologiques et incertitudes scientifiques quant à la réalisation de nos travaux de recherche. En premier lieu, élaborer et superviser une machine virtuelle pour chaque page web sollicite des ressources informatiques considérables, ce qui peut engendrer des problèmes de performance et de scalabilité. Par ailleurs, il faut optimiser la synchronisation entre le flux vidéo et les interactions de l'utilisateur pour garantir une expérience fluide et sans retard, ce qui représente un défi technique significatif.\n\nDe plus, la compatibilité et l'affichage posent des problèmes en raison de l'interopérabilité entre divers systèmes d'exploitation, navigateurs web et environnements 3D. Par conséquent, afin de s’assurer que tous les utilisateurs bénéficient d’une expérience homogène et immersive, quel que soit le matériel ou le logiciel, les développeurs doivent élaborer des solutions sur mesure. Ces difficultés nécessitent le développement de normes ouvertes, la création d'API (Application Programming Interfaces) et la prise en charge de divers environnements d'exploitation. \n\nD’autre part, nous avons identifié un autre verrou quant à la capacité des algorithmes de répartition et de gestion des ressources serveur. En effet, l'efficacité des algorithmes utilisés pour la répartition des ressources serveur et la gestion des charges est fondamentale. Il est donc important de les adapter efficacement à des conditions de charge variables et imprévisibles, surtout dans des contextes où les interactions utilisateur sont dynamiques et peuvent varier considérablement entre chaque utilisation.\n\nDe plus,  l’évolution rapide des technologies et des standards dans les domaines de la réalité virtuelle et de la 3D sur le web, peut provoquer une incertitude quant à l'intégration et la compatibilité de ces nouvelles avancées avec notre architecture actuelle.  Par conséquent, afin d’assurer une solution innovante, il est crucial d’intégrer la question de l'obsolescence technologique et de la nécessité de mises à jour continues pour rester à la pointe.\n\nDans l'ensemble, la technologie de VM instantanée pour l'affichage de pages web dans un environnement 3D multi-utilisateur soulève plusieurs défis et incertitudes, qui nécessiteront des avancées significatives en termes de performance, de compatibilité et de sécurité pour garantir une expérience utilisateur optimale."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Identification de verrous technologiques** :\n  - Limitations de puissance CPU et GPU sur serveurs et utilisateurs.\n  - Impact sur le rendu graphique, l’interactivité et la latence des contenus.\n\n- **Contraintes liées aux connexions** :\n  - Nombre limité de connexions/utilisateurs sur serveurs.\n  - Nécessité d’optimiser l’exploitation de la bande passante.\n  - Défis de spatialisation du son pour une expérience immersive.\n\n- **Accessibilité de la plateforme** :\n  - Objectif de rendre la plateforme accessible à un nombre illimité d’utilisateurs.\n  - Problèmes liés à l’accessibilité via différents périphériques (smartphones, tablettes, ordinateurs).\n  - Variabilité de la puissance de calcul entre ces périphériques, affectant l’ergonomie.\n\n- **Architecture réseau** :\n  - Nécessité d'une architecture réseau adaptée aux besoins de scalabilité et de fluidité.\n  - Limites des approches existantes dans la littérature pour un contexte d’expérience participative."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNous avions identifié de nombreux verrous technologiques quant à la réalisation de nos travaux de recherche. En effet, il nous fallait faire face aux limitations de puissance CPU (Central Processing Unit) et GPU (Graphics Processing Unit) en ce qui concerne les serveurs et les utilisateurs, fortement susceptibles d’influencer le rendu graphique, l’interactivité ou la latence des contenus proposés.  \n\nNous devions aussi nous confronter au nombre de connexions ou d’utilisateurs limité sur les serveurs en vue d’une meilleure exploitation de la bande passante, mais aussi à la spatialisation du son dans l’univers virtuel en vue de proposer l’expérience la plus immersive possible aux utilisateurs de notre plateforme. Cet obstacle technologique, comme nous l’avons vu précédemment avec l’état de l’art, s’oppose évidemment à notre volonté de rendre notre plateforme accessible à un nombre illimité de personnes.  \n\nUn verrou supplémentaire découlait également de notre détermination à rendre notre plateforme accessible à partir d’un navigateur web sur n’importe quel périphérique tel qu’un smartphone, une tablette ou encore un ordinateur. La puissance de calcul disponible via ces différents périphériques n’est pas la même, ce qui peut donc se révéler être un problème majeur en termes d’ergonomie pour l’utilisateur.  \n\nFinalement, un verrou technologique majeur consistait à  architecture réseau répondant aux besoins de scalabilité et de fluidité de notre produit. Comme nous l’avons vu précédemment, les approches proposées dans la littérature présentent des limites dans notre contexte d’expérience participative."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Problèmes rencontrés dans l'industrie musicale :**\n  - Difficultés liées à la collecte et à la distribution des redevances pour l'utilisation des œuvres musicales.\n\n- **Manque de visibilité :**\n  - Les créateurs de musique peinent à suivre l'utilisation de leurs œuvres (radio, streaming, publicités, films, émissions de télévision, jeux vidéo).\n  - Cette absence de visibilité complique la connaissance des revenus à percevoir.\n\n- **Dépendance envers des entreprises externalisées :**\n  - La collecte des redevances est souvent confiée à des sociétés de gestion collective (ex : SACEM).\n  - Ces entreprises peuvent être perçues comme lentes, coûteuses et inefficaces.\n\n- **Absence de récupération automatique des droits :**\n  - Contrairement à d'autres secteurs, l'industrie musicale manque de systèmes automatisés pour une récupération rapide et efficace des redevances.\n  - Risque d'erreurs humaines dans le processus de collecte, entraînant des retards ou des erreurs dans les paiements.\n\n- **Besoins des sociétés de gestion collective :**\n  - Nécessité d'aide pour collecter des données précises sur l'utilisation des œuvres musicales.\n  - Le manque de données précises peut mener à une sous-estimation des revenus et à une distribution inexacte des redevances.\n\n- **Marge de redevances non collectées :**\n  - Estimation de 20% à 30% de redevances non collectées par des entités comme la SACEM.\n  - De nombreux revenus générés par l'utilisation de musique ne parviennent pas aux créateurs, impactant leur source de revenu.\n\n- **Défis dans la gestion des droits d'auteur :**\n  - Les auteurs, compositeurs et éditeurs font face à de nombreux défis sans une gestion efficace de leurs droits d'auteur et de la collecte de leurs redevances.\n\n- **Proposition de solutions :**\n  - Nécessité d'une solution innovante pour améliorer la gestion de la collecte des données, des droits et la distribution des revenus.\n  - Objectif : garantir une rémunération équitable pour les créateurs de musique."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes problèmes rencontrés par les auteurs, compositeurs et éditeurs dans l'industrie musicale, en particulier en ce qui concerne la collecte et la distribution des redevances pour l'utilisation de leurs œuvres.  \nPremièrement il y a un manque de visibilité sur l'utilisation de leur musique. En effet, les créateurs de musique peuvent avoir du mal à suivre comment leurs œuvres sont utilisées notamment en ce qui concerne les diffusions à la radio, en streaming, ou des utilisations dans des publicités, des films, des émissions de télévision, des jeux vidéo, etc. Sans visibilité sur ces utilisations, il est difficile pour les musiciens de connaître précisément les revenus qu'ils doivent percevoir.  \nDe plus, une autre incertitude s’impose en ce qui concerne la dépendance envers des entreprises externalisées. En effet, la collecte des redevances est souvent confiée à des sociétés de gestion collective telles que la SACEM. Cependant, ces entreprises peuvent être perçues comme étant lentes, coûteuses et inefficaces dans la collecte et la distribution des revenus aux créateurs. Cela crée une dépendance des auteurs, compositeurs et éditeurs envers ces entités pour percevoir leurs revenus de manière adéquate. D’autre part, l’absence de récupération automatique des droits peuvent causer des incertitudes dans ce secteur. Contrairement à d’autres secteurs ou la gestion des droits peut plus automatisée, dans l’industrie musicale, il n’existe pas toujours de systèmes automatisés permettant une récupération rapide et efficace des redevances pour les créateurs. De plus, il y a une incertitude quant à l’exploitation humaine du système et risque d’erreurs. En effet, le processus de collecte des redevances est souvent géré manuellement, ce qui le rend sujet aux erreurs humaines. Cela peut entrainer des retards dans le paiement des redevances ou même des erreurs dans le montant des paiements.  \nPour surmonter ces limites, les sociétés de gestion collective, telles que la SACEM, ont besoin d’aide pour collecter des données précises sur l’utilisation des œuvres musicales afin de garantir une répartition équitable des revenus aux créateurs. Le manque de données précises peut entraîner une sous-estimation des revenus générés par les utilisations de musique et donc une distribution inexacte des redevances.  \nAinsi, il existe une marge importante, allant de 20% à 30%, de redevances qui ne sont pas collectées par des entités telles que la SACEM. Cela signifie que de nombreux revenus générés par l’utilisation de musique ne parviennent pas aux créateurs, ce qui nuit à leur source de revenu.  \nEnfin, les défis auxquels sont confrontés les auteurs, les compositeurs et les éditeurs sans la gestion de leurs droits d’auteur et la collecte de leurs redevances dans l’industrie sont nombreux. Pour remédier à ces limites, il faut proposer une solution innovante qui améliore la gestion de la collecte des données, la gestion des droits et la distribution des revenus pour garantir une rémunération équitable pour les créateurs de musique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Problèmes dans l'industrie musicale :**\n  - Difficultés rencontrées par auteurs, compositeurs et éditeurs concernant la collecte et la distribution des redevances.\n\n- **Manque de visibilité :**\n  - Les créateurs de musique peinent à suivre l'utilisation de leurs œuvres (radio, streaming, publicités, films, émissions de télévision, cinéma).\n  - Absence de visibilité complique la connaissance des revenus à percevoir.\n\n- **Dépendance envers des entreprises externalisées :**\n  - La collecte des redevances est souvent confiée à des sociétés de gestion collective (ex. : SACEM).\n  - Ces sociétés peuvent être perçues comme lentes, coûteuses et inefficaces.\n  - Dépendance des créateurs pour percevoir leurs revenus de manière adéquate.\n\n- **Absence de récupération automatique des droits :**\n  - Manque de systèmes automatisés pour une récupération rapide et efficace des redevances.\n  - Processus de collecte souvent géré manuellement, sujet aux erreurs humaines.\n\n- **Risques d'erreurs :**\n  - Retards dans le paiement des redevances possibles.\n  - Erreurs dans le montant des paiements dues à la gestion manuelle.\n\n- **Besoins des sociétés de gestion collective :**\n  - Nécessité de collecter des données précises sur l'utilisation des œuvres musicales.\n  - Importance d'une répartition équitable des revenus aux créateurs.\n\n- **Impact du manque de données :**\n  - Sous-estimation des revenus générés par l'utilisation de musique.\n  - Distribution inexacte des redevances.\n\n- **Marge de redevances non collectées :**\n  - Entre 20% et 30% des redevances ne sont pas collectées par des entités comme la SACEM.\n  - De nombreux revenus générés par l'utilisation de musique ne parviennent pas aux créateurs, nuisant à leur source de revenu.\n\n- **Défis dans la gestion des droits d’auteur :**\n  - Auteurs, compositeurs et éditeurs font face à de nombreux défis sans une gestion adéquate de leurs droits et redevances.\n\n- **Proposition de solution :**\n  - Nécessité d'une solution innovante pour améliorer la collecte de données, la gestion des droits et la distribution des revenus.\n  - Objectif : garantir une rémunération équitable pour les créateurs de musique."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\n\nLes problèmes rencontrés par les auteurs, compositeurs et éditeurs dans l'industrie musicale, en particulier en ce qui concerne la collecte et la distribution des redevances pour l'utilisation de leurs œuvres sont nombreux. \nPremièrement il y a un manque de visibilité sur l'utilisation de leur musique. En effet, les créateurs de musique peuvent avoir du mal à suivre comment leurs œuvres sont utilisées notamment en ce qui concerne les diffusions à la radio, en streaming, ou des utilisations dans des publicités, des films, des émissions de télévision, au cinéma etc. Sans visibilité sur ces utilisations, il est difficile pour les musiciens de connaître précisément les revenus qu'ils doivent percevoir.\nDe plus, une autre incertitude s’impose en ce qui concerne la dépendance envers des entreprises externalisées. En effet, la collecte des redevances est souvent confiée à des sociétés de gestion collective telles que la SACEM. Cependant, ces entreprises peuvent être perçues comme étant lentes, coûteuses et inefficaces dans la collecte et la distribution des revenus aux créateurs. Cela crée une dépendance des auteurs, compositeurs et éditeurs envers ces entités pour percevoir leurs revenus de manière adéquate. D’autre part, l’absence de récupération automatique des droits peuvent causer des incertitudes dans ce secteur. Contrairement à d’autres secteurs ou la gestion des droits peut davantage être automatisée, dans l’industrie musicale, il n’existe pas toujours de systèmes automatisés permettant une récupération rapide et efficace des redevances pour les créateurs. De plus, il y a une incertitude quant à l’exploitation humaine du système et un risque d’erreurs. En effet, le processus de collecte des redevances est souvent géré manuellement, ce qui le rend sujet aux erreurs humaines. Cela peut entrainer des retards dans le paiement des redevances ou même des erreurs dans le montant des paiements. \nPour surmonter ces limites, les sociétés de gestion collective, telles que la SACEM, ont besoin d’aide pour collecter des données précises sur l’utilisation des œuvres musicales afin de garantir une répartition équitable des revenus aux créateurs. Le manque de données précises peut entraîner une sous-estimation des revenus générés par les utilisations de musique et donc une distribution inexacte des redevances.\nAinsi, il existe une marge importante, allant de 20% à 30%, de redevances qui ne sont pas collectées par des entités telles que la SACEM. Cela signifie que de nombreux revenus générés par l’utilisation de musique ne parviennent pas aux créateurs, ce qui nuit à leur source de revenu. \nEnfin, les défis auxquels sont confrontés les auteurs, les compositeurs et les éditeurs sans la gestion de leurs droits d’auteur et la collecte de leurs redevances dans l’industries sont nombreux. Pour remédier à ces limites, il faut proposer une solution innovante qui améliore la gestion de la collecte des données, la gestion des droits et la distribution des revenus pour garantir une rémunération équitable pour les créateurs de musique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Incertitudes, Verrous, Aléas\n\n- **Absence de Données Qualifiées :**\n  - Verrou principal lié à l'absence de données qualifiées pour le cas d'usage visé.\n  - Nécessité de jeux de données labellisées.\n  - Recours à des méthodes de recueil de données ouvertes et qualification automatique.\n  - Combinaison de multiples sources de données pour obtenir des jeux de données suffisants pour l'entraînement des modèles.\n  - Performance des algorithmes dépendante de la quantité de données mobilisées.\n\n- **Modélisation du Phénomène :**\n  - Second verrou lié à la modélisation expérimentale nécessaire pour chaque contexte d'étude.\n  - Besoin de détecter des signaux et de reconstruire un modèle en boîte noire pour numériser et simuler le phénomène.\n\n- **Incertitudes Scientifiques et Techniques :**\n  - Présence d'incertitudes spécifiques à chaque étude.\n\n### Détection de la Consommation de Gaz par un Foyer\n\n- **Développement d'un Algorithme :**\n  - Algorithme embarqué dans un capteur optique pour détecter la consommation de gaz.\n  - Optimisation des ressources pour le fonctionnement de l'algorithme dans un capteur à ressources limitées.\n  - Exigence de performances techniques en temps réel.\n\n- **Détection de Signaux Faibles :**\n  - Difficulté à détecter des signaux faibles considérés comme consommation.\n  - Verrou technique lié à la détection de changements rapides et petits dans les signaux.\n  - Risque de manquer des changements de tendance si le pas de mesure est trop élevé, assimilant la consommation à du bruit.\n\n- **Modèle Statistique :**\n  - Modèle basé sur des considérations statistiques intégrant la variabilité observée lors des expérimentations.\n  - Entraînement et tests de l'algorithme auprès du client.\n\n### Désagrégation de la Consommation de Gaz par Usage\n\n- **Objectif :**\n  - Déterminer la contribution individuelle de chaque usage à la consommation globale de gaz.\n\n- **Utilisation du Chauffage :**\n  - Basé sur la consommation de gaz liée au chauffage.\n  - Données disponibles peu précises, nécessitant le développement d'un algorithme alternatif pour collecter des données précises.\n  - Suivi des séquences d'allumage et d'extinction des chauffages insuffisant pour déduire la consommation.\n\n- **Courbe de Consommation :**\n  - Chaque modèle de chauffage a une courbe de consommation unique à approcher par apprentissage pour chaque logement.\n\n### Modélisation du Comportement Thermique d’un Logement\n\n- **Raffinement de la Modélisation :**\n  - Considération de chaque phénomène physique et paramétrique impactant la température intérieure.\n  - Hypothèses sur la capacité du modèle à décrire les phénomènes tout au long de l'année.\n\n- **Planning de Mise en Fonctionnement Optimal :**\n  - Élaboration de trois scénarios de consommation (confortable à économique).\n  - Verrou à adresser : capacité du modèle à intégrer les changements de comportement des utilisateurs (changement de température de consigne).\n  - Correction des prédictions à long terme pour simuler la consommation annuelle en cas de persistance dans le mode de fonctionnement des chauffages."}, {"role": "assistant", "content": "Incertitudes, verrous, aléas  \nUn premier verrou est toujours lié à l’absence de données qualifiées pour le cas d’usage visé et a fortiori dans ce contexte, l’absence de jeux de données labellisées. Nous devions ainsi nous appuyer sur des méthodes de recueil de données ouvertes et leur qualification automatique mais aussi la combinaison de multiples sources de données afin de disposer de jeux de données de taille suffisante pour l’entrainement de nos modèles et leur évaluation. En effet, la performance des algorithmes développés est limitée par la quantité de données mobilisées pour leur entrainement.  \nUn second verrou auquel nous sommes toujours confrontés et qui requiert une démarche expérimentale à dérouler pour chaque contexte d’étude particulier est lié à la modélisation du phénomène à analyser. En effet, nous sommes confrontés au besoin de détecter des signaux et pouvoir reconstruire en boite noire un modèle permettant de numériser le phénomène et le simuler à travers des algorithmes.  \nD’autres incertitudes scientifiques et techniques se sont également posées de manière spécifique pour chacune de nos études.  \nDétection de la consommation de gaz par un foyer  \nCet axe de travail vise le développement d’un algorithme embarqué dans un capteur de mesure optique destiné à la détection de la consommation de gaz d’un foyer. Nous avons été confrontés au besoin d’optimisation des ressources requises pour faire fonctionner l’algorithme au sein d’un capteur où les ressources sont limitées et où les performances techniques en temps réel doivent être satisfaisantes.  \nEn outre, une autre difficulté portait sur le besoin de détecter des signaux faibles et les considérer en tant que consommation. Le principal verrou technique était lie à la détection d’un changement rapide et potentiellement petit dans les signaux détectés, ce qui contraint les algorithmes utilisés et le pas auquel les mesures doivent être prises pour les préparer. En effet, si le pas est trop élevé, le point de changement de tendance dans la consommation dans un signal furtif peut-être raté et cette consommation pourrait être assimilée à du bruit.  \nLe modèle que nous avons développé s’appuie sur des considérations statistiques, permettant d’intégrer dans la modélisation des signaux à caractériser toute la variabilité que nous avons pu relever au cours de nos expérimentations. Nous avons ensuite entrainé notre algorithme et mis en test auprès de notre client.  \nDésagrégation de la consommation de gaz par usage  \nUne fois la consommation de gaz détectée, notre objectif était de pouvoir déterminer la contribution individuelle de chaque usage à cette consommation globale.  \nNous nous sommes appuyés sur la consommation de gaz issue de l’utilisation du chauffage. Les données disponibles étant peu précises, nous avons dû développer un algorithme alternatif afin de collecter des données les plus précises possibles et monitorer l’allumage et l’extinction des chauffages. Toutefois, la connaissance des séquences d’allumage et d’extinction ne suffit pas pour en déduire la consommation de gaz par le chauffage. En effet, selon la marque/modèle de chauffage, celui-ci suit une courbe de consommation unique que nous devrons approcher par apprentissage au niveau de chaque logement.  \nModélisation du comportement thermique d’un logement en vue de l’optimisation du planning de chauffe  \nEn vue de la détermination de la consommation de gaz par un logement, nous avons dû raffiner la modélisation en considérant la contribution de chaque phénomène physique mais aussi paramétrique ayant un impact sur la température intérieure du logement. Nos hypothèses portaient alors sur la capacité de notre modèle à décrire, tout au long de l’année les phénomènes à l’œuvre.  \nNous avons ensuite considéré un planning de mise en fonctionnement optimal des chauffages dans le logement en déclinant en trois scénarios de consommation (du plus confortable au plus économique). Le verrou à adresser portait alors sur la capacité de notre modèle à tenir compte des changements de comportement des utilisateurs (notamment le changement de température de consigne) de manière immédiate et à corriger les prédictions sur le long terme pour simuler la consommation annuelle dans le cas où l’utilisateur persisterait dans ce mode de fonctionnement des chauffages."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Littérature scientifique** : Abondante sur l'optimisation des performances des panneaux photovoltaïques, mais peu d'études sur la limitation des pertes dues à l'encrassement.\n  \n- **Détection de l'encrassement** : Enjeux liés à l'utilisation de moyens optiques pour détecter l'encrassement des panneaux.\n\n- **Évaluation des performances** : Discussions sur la perte globale de performances dans des installations photovoltaïques spécifiques.\n\n- **Approches basées sur les données** : \n  - Évaluent les conditions météorologiques pertinentes pour établir des similitudes avec des données historiques.\n  - Permettent de déduire les performances de production pour des dates spécifiques.\n  \n- **Lacune dans la recherche** : Aucune étude ne combine les données météorologiques et d'encrassement pour expliquer les écarts de performances.\n\n- **Étude unique** : Une seule étude académique (Acharya, Wi & Lee, 2020) aborde la problématique de manière similaire, mais :\n  - Manque de mise en œuvre pratique.\n  - Aucune certitude sur la faisabilité de la solution à grande échelle discutée par les auteurs."}, {"role": "assistant", "content": "Incertitudes, verrous, aléas  \nSi la littérature scientifique sur le sujet de l’optimisation des performances de panneaux photovoltaïques est fournie, peu d’études traitent de solutions pour limiter les pertes de performances en lien avec l’encrassement des panneaux. En effet, des enjeux de détection de l’encrassement par des moyens optiques sont décrits, de même que l’évaluation de la perte globale de performances dans un cas particulier d’installation photovoltaïque sont débattus.  \nLes seules approches centrées sur les données que nous avons relevées visent à évaluer les variables des conditions météorologiques les plus pertinentes pour déterminer la similitude entre un jour donné et les données historiques afin de déduire les performances de production pour cette date donnée. Aucune étude ne croise les données météo et les données d’encrassement pour expliquer un écart de performances.  \nEn outre, nous avons constaté que seule une étude (académique) (Acharya, Wi & Lee (2020).) considère le même angle de résolution de la problématique que nous. Une mise en œuvre en pratique manque et aucune certitude de la faisabilité de la solution à large échelle n’est discutée par les auteurs."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Verrous technologiques rencontrés :**\n  - Nécessité de trouver des modèles de prédiction adaptés aux variations de fréquence du réseau électrique par rapport aux 50 Hz nominaux.\n  - Objectif : Mobiliser des sources complémentaires (batteries) pour compensation.\n  - Modèles de prédiction existants jugés non optimisés et manquant de robustesse.\n  - Contrainte sur la marge d’erreur pour éviter la surestimation ou sous-estimation de la fréquence.\n  - Importance d'une précision suffisante dans la prédiction de la fréquence électrique.\n\n- **Hypothèses sur la typologie des équipements :**\n  - Nécessité de modéliser les équipements dans le jumeau numérique d’un foyer pour détecter les équipements électriques.\n  - Verrou lié à la désagrégation d’une courbe de charge en charges individuelles.\n  - Utilisation de la technologie NILM (Non-Intrusive Load Monitoring) basée sur des méthodes de Machine Learning.\n  - Difficultés dans le choix des hyperparamètres et efficacité des algorithmes selon le type de données.\n  - Problèmes de mesure de signaux bruités pouvant être confondus avec l’activité d’équipements à faible consommation.\n  - Manque d'études approfondies sur cette problématique dans la littérature.\n\n- **Projets de flexibilisation du réseau électrique :**\n  - Développement ou amélioration de systèmes comme les EMS (Energy Management Systems).\n  - Objectif : Optimiser la recharge des véhicules électriques tout en minimisant l’impact carbone et le coût économique.\n  - Ces paramètres sont peu investigués dans la littérature.\n\n- **Incertitudes scientifiques :**\n  - Élaboration d’un modèle non invasif pour évaluer l’état de santé des batteries électriques.\n  - Incertitude sur la possibilité de déduire l’état de santé d’une batterie à partir de l’analyse de ses courbes de charge et de décharge, ainsi que d’une analyse en série temporelle."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNous avons rencontré de nombreux verrous technologiques quant à la réalisation de nos travaux de recherche. En effet, il nous fallait trouver les modèles de prédiction adéquats quant aux variations de fréquence du réseau électrique par rapport aux 50 Hz nominaux, ceci pour éventuellement mobiliser des sources complémentaires (batteries) de compensation. Comme nous l’avons vu avec l’état de l’art, les modèles de prédiction précédemment utilisés ne sont pas suffisamment optimisés et manquent souvent de robustesse quant à la réalisation de cette tâche. Une autre contrainte portait sur la marge d’erreur permise pour limiter ou éviter les erreurs de surestimation ou de sous-estimation de la fréquence. Nous devions ainsi nous assurer que notre algorithme prenne en compte cette contrainte et prédise avec une précision suffisante la fréquence électrique du réseau.\n\nNous avons également été confrontés au besoin de faire des hypothèses sur la typologie des équipements branchés et les modéliser au sein du jumeau numérique d’un foyer, ceci dans le cadre de notre projet de conception d’un outil de détection des équipements électriques branchés au sein du foyer. Le verrou sous-jacent à l’émission de ces hypothèses était la prise en compte de la désagrégation d’une courbe de charge comme la combinaison de charges individuelles de chaque équipement. Comme nous l’avons également vu précédemment, cette désagrégation exploitant notamment une technologie NILM basée sur des méthodes de Machine Learning n’est pas toujours évidente, avec la difficulté du choix des hyperparamètres et de l’efficacité des algorithmes exploités à cet effet vis-à-vis du type de données. En plus des verrous de modélisation, nous avons dû faire face à des difficultés posées par la mesure de signaux bruitées qui pouvaient erronément être considérés comme l’activité d’un équipement à faible consommation. Cette dernière problématique n’a pas été l’objet d’études très approfondies dans la littérature.\n\nDans le cadre de notre projet visant à la flexibilisation du réseau électrique et à la décarbonation des bornes de recharge des véhicules électriques, ou même dans le cadre de la production de l’hydrogène vert, nous avons été confrontés au besoin de développer ou d’améliorer des systèmes comme les EMS, ceci dans le but d’optimiser la recharge des véhicules électriques tout en minimisant l’impact carbone et le coût économique de la recharge, qui sont ici des paramètres relativement peu investigués dans la littérature.\n\nFinalement, nous avons rencontré des incertitudes quant à l’élaboration d’un modèle non invasif d’évaluation de l’état de santé des batteries électriques, représentant l’un de nos projets majeurs de R&D. Notre incertitude scientifique portait sur la possibilité d’une déduction de l’état de santé d’une batterie à partir de l’analyse de ses courbes de charge et de décharge et d’une analyse en série temporelle."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Problématiques combinées :**\n  - Numérisation à grande échelle de chaînes de télévision (25 chaînes).\n  - Transformation des contenus pour ré-exploitation sur appareils mobiles.\n  - Récupération et traitement rapide des éléments de métadonnées.\n  - Synchronisation de l'ensemble des données.\n\n- **Système de référence :**\n  - Voxalead, système expérimental limité à quelques chaînes et journaux d'actualités.\n\n- **Projet CMSE (Consumer Multimedia Search Engine) :**\n  - Lancement de Voxalead permettant la recherche de mots-clés dans des vidéos et programmes de radio, en plusieurs langues.\n  - Intégration de technologies de partenaires, notamment Vocapia pour le « Speech-to-Text ».\n  - Voxalead a reçu plusieurs distinctions internationales et a des clients prestigieux (ex. Université King Saud, Elysée).\n\n- **Utilisation de la reconnaissance vocale :**\n  - Transcription vocale automatique pour moteur de recherche.\n  - Système de transcription de Vocapia, basé sur des travaux du laboratoire d’ORSAY Lipsis et le programme Quaero.\n\n- **Limitations de la reconnaissance vocale :**\n  1. **Temps de traitement :**\n     - Environ 1 fois le temps réel (ex. 30 min de reconnaissance pour 30 min d’émission), entraînant un retard potentiel de 60 minutes.\n  2. **Difficulté de reconnaissance :**\n     - Problèmes avec les noms propres et marques, en raison de dictionnaires de phonèmes prédéfinis.\n\n- **Difficulté de traitement de l’actualité :**\n  - Incertitude sur les éléments à venir (ex. crise alimentaire autour de Findus).\n\n- **Solution proposée :**\n  - Utilisation de sous-titrages malentendants intégrés dans les programmes TV.\n  - En cas d'absence de sous-titrage, recours à la reconnaissance vocale.\n\n- **Réglementation :**\n  - Obligations du CSA concernant le sous-titrage des programmes (Loi n° 2005-102, Loi n° 86-1067, Loi n° 2009-258).\n  - Pourcentage de programmes sous-titrés variant entre 100% et 10% selon la chaîne.\n\n- **Incertitudes techniques :**\n  - Capacité à récupérer et indexer les contenus sous-titrés, et à les resynchroniser avec l'image.\n\n- **Traitement des métadonnées :**\n  - Nécessité de récupérer et traiter les signaux transmis en DVB, combinant technologies du broadcast, des télécommunications et de l'informatique.\n\n- **Complexité du post-traitement :**\n  - Transcodage en H264/AAC.\n  - Transcription audio à partir des sous-titrages.\n  - Enrichissement de la transcription : détection d’entités nommées, géolocalisation, analyse des mots importants.\n  - Limitation du retard entre diffusion et disponibilité dans le moteur de recherche.\n  - Traitement simultané de 25 chaînes dans un environnement sans perte de contenu."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\n\nNous combinons plusieurs problématiques qui sont la numérisation à grande échelle de chaines de TV (25), la transformation de ces contenus pour une ré-exploitation sur des appareils mobiles, la récupération d’éléments de métadata, et leur traitement dans le plus court temps ainsi que la synchronisation de cet ensemble.\n\nLe système le plus proche dans la partie concernée par ce dossier est Voxalead. Il est a noter que ce système est expérimental et est limité à quelques chaines et aux journaux d’actualités.\n\nDans le cadre du projet applicatif CMSE (Consumer Multimedia Search Engine), nous avons pu lancer Voxalead, qui permet de rechercher des mots-clés directement dans des vidéos ou des programmes de radio, et dans plusieurs langues. L’application Voxalead a été rendue possible grâce à l’intégration des technologies développées par nos différents partenaires, comme Vocapia pour le « Speech-to-Text ». Voxalead a non seulement reçu plusieurs distinctions internationales mais a également conquis des clients prestigieux, comme l’Université King Saud ou l’Elysée.\n\nCette expérimentation utilise un système de reconnaissance vocale afin de réaliser de manière automatique de la transcription vocale. Cette transcription étant utilisée dans un moteur de recherche afin de retrouver un sujet télévisuel faisant référence à un ou plusieurs mots clés. Le système de transcription utilisé par Voxalead est celui de la société Vocapia (licenciée de travaux du laboratoire d’ORSAY Lipsis avec également le soutien du programme Quaero).\n\nNous désirons dans la mesure du possible ne pas utiliser de reconnaissance vocale pour deux raisons :\n\n1. Le temps de traitement, en général autour d’une fois le temps réel (30 minutes de reconnaissance pour 30 minutes d’émission, ce qui génère potentiellement un retard de 60 minutes entre la diffusion et la fin de la transcription), contradictoire avec notre objet de quasi temps réel.\n2. La difficulté pour les systèmes de reconnaissance vocale de reconnaitre les noms propres ou les noms de marques, ces dispositifs fonctionnant avec des dictionnaires de phonèmes prédéfinis.\n\nLa plupart du temps dans une logique de traitement de l’actualité, il est difficile de connaitre à l’avance les éléments qui seront cités (exemple avec la crise alimentaire autour de Findus).\n\nPour éviter ces deux problèmes nous nous proposons d’utiliser à la place de la reconnaissance vocale, les sous-titrages malentendants qui sont intégrés dans la plupart des programmes TV actuels. Dans le cas de programmes n’ayant pas de sous-titrage, nous utiliserons de la reconnaissance vocale.\n\nLe CSA (Conseil Supérieur de l’Audiovisuel) oblige les chaines dans un cadre légal (Loi n° 2005-102 du 11 février 2005 pour l’égalité des droits et des chances, la participation et la citoyenneté des personnes handicapées - Loi n° 86-1067 du 30 septembre 1986 modifiée relative à la liberté de communication - Loi n° 2009-258 du 5 mars 2009 relative à la communication audiovisuelle et au nouveau service public de la télévision) de diffuser selon la chaine entre 100% et 10% des programmes sous-titrés malentendants.\n\nNous ne savons pas si nous serons capables de récupérer ces contenus, d’en faire des éléments indexables et resynchronisables avec l’image une fois extraits du contenu diffusé.\n\nLe besoin de traiter ces metadata va nous obliger à récupérer et traiter les signaux tels que transmis en DVB et non les dérivés directement exploitables que sont l’image et le son, donc de combiner des technologies venant du broadcast, de l’univers des opérateurs de télécommunication et de l’informatique.\n\nEnfin on ajoute à ceci la complexité du post traitement, à savoir :\n\n- Transcodage en H264/AAC\n- Transcription audio depuis les sous-titrages\n- Enrichissement de la transcription avec détection d’entités nommées, géolocalisation des lieux cités, analyse des mots importants, etc.\n- Limitation du retard entre la diffusion et la mise à disposition des contenus dans le moteur de recherche\n- 25 chaines à traiter simultanément et ceci dans un environnement zéro perte de contenu"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Verrou Technique Principal :**\n  - Problème d’amplification des exons et des exons padding des gènes majeurs du système HLA par PCR Short Range dans les nano-chambres de l’IFC.\n  - Solution envisagée : Conception de jeux d’amorces HLA pour amplifier les séquences géniques d’intérêt pour le typage HLA.\n\n- **Aléas Rencontrés :**\n  1. **Difficulté de Multiplexage :**\n     - Problèmes de réactions concurrentes ou inhibitrices lors de l’amplification avec les amorces HLA.\n     - Conséquence : Non efficience du procédé d’amplification.\n     - Solutions mises en œuvre :\n       - Amélioration de la formulation des séquences d’amorces HLA.\n       - Optimisation de la qualité du multiplexage.\n       - Réduction des interférences entre les amorces de Classe I et II.\n       - Détermination des concentrations d’amorces pour PCR simplex et multiplexé.\n\n  2. **Co-amplification des Gènes HLA :**\n     - Approche pour résoudre cette difficulté :\n       - Détermination du meilleur positionnement des amorces sur la séquence de référence.\n       - Élaboration de dilutions appropriées des amorces pour cibler certains gènes.\n\n- **Autre Verrou :**\n  - Constitution d’une bibliothèque (library) satisfaisante.\n  - Spécifications :\n    - Atteindre un équilibre molaire entre les régions exoniques (environ 0.5 sur une échelle de 0.3 à 0.7).\n    - Obtenir une bonne profondeur (nombre de reads supérieurs à 100) pour tous les exons sans échec PCR.\n\n- **Autres Aléas :**\n  - Panne indétectable du séquenceur MiSeq durant les travaux."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe principal verrou technique que nous avons rencontré était lié au processus d’amplification des exons et des exons padding des gènes « majeurs » du système HLA par PCR Short Range dans les nano-chambres de l’IFC. Nous avons souhaité adresser ce verrou par la conception de jeux d’amorces HLA permettant d’amplifier les séquences géniques d’intérêt pour le typage HLA. Deux nouveaux aléas ont été rencontrés lors de la réalisation de ces travaux et ont nécessité une démarche itérative de développement de prototypes et de tests pour guider notre conception.\n\nLa première difficulté était liée à des problèmes de multiplexage à l’origine de réactions concurrentes ou inhibitrices lors de l’amplification à partir des amorces HLA conçues résultant en une non efficience du procédé d’amplification. De nombreux travaux d’amélioration portant sur différents aspects de notre conception nous ont permis de nous affranchir de cette difficulté. Ces travaux ont porté à la fois sur la formulation des séquences d’amorces HLA, la qualité du multiplexage, la diminution des interférences entre les amorces de Classe I et II, la détermination des concentrations des amorces pour les PCR simplex et en PCR multiplexé.\n\nLe second aléa rencontré s’est manifesté par la co-amplification des gènes HLA. Nous avons adressé cette difficulté en l’abordant selon deux axes de travail : D’une part, la détermination du meilleur positionnement possible des amorces sur la séquence de référence ; D’autre part, l’élaboration de dilutions appropriées de nos amorces afin de cibler certains gènes.\n\nUn autre verrou était lié à la constitution d’une bibliothèque (library) satisfaisante. En d’autres termes, nos spécifications étaient l’atteinte du meilleur équilibre molaire possible entre les régions exoniques soit environ 0.5 sur une échelle de 0.3 à 0.7 et avec une bonne profondeur (nombre de reads supérieurs à 100) pour tous les exons et sans échec PCR.\n\nPar ailleurs, nous avons rencontré d’autres aléas lors de la réalisation de nos travaux, notamment une panne indétectable de notre séquenceur MiSeq."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\n- **Verrou technique principal** : \n  - Processus d’amplification des exons et exons padding des gènes majeurs du système HLA par PCR Short Range dans les nano-chambres de l’IFC.\n  - Solution envisagée : Conception de jeux d’amorces HLA pour amplifier les séquences géniques d’intérêt pour le typage HLA.\n\n- **Aléas rencontrés** :\n  - **Difficulté 1** : Problèmes de multiplexage entraînant des réactions concurrentes ou inhibitrices.\n    - Conséquence : Non-efficience du procédé d’amplification.\n    - Solutions mises en œuvre :\n      - Amélioration de la formulation des séquences d’amorces HLA.\n      - Optimisation de la qualité du multiplexage.\n      - Réduction des interférences entre les amorces de Classe I et II.\n      - Détermination des concentrations des amorces pour PCR simplex et multiplexé.\n\n  - **Difficulté 2** : Co-amplification des gènes HLA.\n    - Approches pour résoudre ce problème :\n      - Meilleur positionnement des amorces sur la séquence de référence.\n      - Élaboration de dilutions appropriées des amorces pour cibler certains gènes.\n\n- **Verrou supplémentaire** : Constitution d’une bibliothèque (library) satisfaisante.\n  - Objectif : Atteindre un équilibre molaire d’environ 0.5 (échelle 0.3 à 0.7) avec une bonne profondeur (plus de 100 reads) pour tous les exons, sans échec PCR.\n  - Complexité : Difficulté à atteindre les objectifs de performance en raison des codépendances entre les paramètres.\n    - Modifications bénéfiques pour l’équilibre molaire peuvent nuire à la profondeur de typage.\n  - Incertitude majeure : Impact des modifications sur les performances individuelles et globales du kit de typage.\n  - Solution : Développement de prototypes successifs pour évaluer les performances et suivre l’impact des modifications.\n\n- **Difficulté liée au logiciel d’interprétation** :\n  - Défis à relever :\n    - Gestion du volume de données.\n    - Structure des données pour permettre les appels aux bases de données pertinentes.\n    - Gestion des échantillons de l’étude et de leurs références."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**  \nLe principal verrou technique que nous avons rencontré était lié au processus d’amplification des exons et des exons padding des gènes « majeurs » du système HLA par PCR Short Range dans les nano-chambres de l’IFC. Nous avons souhaité adresser ce verrou par la conception de jeux d’amorces HLA permettant d’amplifier les séquences géniques d’intérêt pour le typage HLA. Deux nouveaux aléas ont été rencontrés lors de la réalisation de ces travaux et ont nécessité une démarche itérative de développement de prototypes et de tests pour guider notre conception.\n\nLa première difficulté était liée à des problèmes de multiplexage à l’origine de réactions concurrentes ou inhibitrices lors de l’amplification à partir des amorces HLA conçues résultant en une non-efficience du procédé d’amplification. De nombreux travaux d’amélioration portant sur différents aspects de notre conception nous ont permis de nous affranchir de cette difficulté. Ces travaux ont porté à la fois sur la formulation des séquences d’amorces HLA, la qualité du multiplexage, la diminution des interférences entre les amorces de Classe I et II, la détermination des concentrations des amorces pour les PCR simplex et en PCR multiplexé.\n\nLe second aléa rencontré s’est manifesté par la co-amplification des gènes HLA. Nous avons adressé cette difficulté en l’abordant selon deux axes de travail : D’une part, la détermination du meilleur positionnement possible des amorces sur la séquence de référence ; D’autre part, l’élaboration de dilutions appropriées de nos amorces afin de cibler certains gènes.\n\nUn autre verrou était lié à la constitution d’une bibliothèque (library) satisfaisante. En d’autres termes, nos spécifications étaient l’atteinte du meilleur équilibre molaire possible entre les régions exoniques soit environ 0.5 sur une échelle de 0.3 à 0.7 avec une bonne profondeur (nombre de reads supérieurs à 100) pour tous les exons et sans échec PCR. Toutefois, l’atteinte de nos objectifs de performance, pour tous les paramètres que nous étudions était complexe en raison des codépendances entre ceux-ci. En effet, des modifications pouvant être bénéfiques pour l’équilibre molaire, auront un effet néfaste sur la profondeur de typage. L’incertitude majeure à lever réside ainsi sur l’impact des modifications réalisées sur les différents constituants de notre kit de typage sur les performances individuelles et globales. Notre défi était alors de développer une multitude de prototypes successifs, permettant d’évaluer ces grandeurs et de suivre l’impact des modifications sur les performances.\n\nUne autre difficulté que nous avons eu à surmonter portait sur la conception du logiciel d’interprétation, accompagnant notre kit de typage HLA. Les défis à adresser sont liés au volume de données à gérer, à la structure de celles-ci pour permettre les appels aux bases de données pertinentes, mais aussi la gestion des échantillons de l’étude et de leurs références."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\n- **Verrou technique principal :** Manque de profondeurs pour certains exons, nécessitant une réévaluation des cibles pour typage de 96 échantillons.\n\n- **Difficultés liées au multiplexage :**\n  - Problèmes de réactions concurrentes ou inhibitrices lors de l’amplification avec les amorces HLA.\n  - Non-efficience du procédé d’amplification.\n  - Travaux d'amélioration réalisés sur :\n    - Formulation des séquences d’amorces HLA.\n    - Qualité du multiplexage.\n    - Diminution des interférences entre amorces de Classe I et II.\n    - Détermination des concentrations des amorces pour PCR simplex et multiplexé.\n\n- **Second aléa : Co-amplification des gènes HLA :**\n  - Approche en deux axes :\n    - Meilleur positionnement des amorces sur la séquence de référence.\n    - Élaboration de dilutions appropriées des amorces pour cibler certains gènes.\n  - Amélioration du pipeline pour gérer la détection de co-amplification sur les gènes non classiques et suppression des ambiguïtés.\n\n- **Verrou lié à la constitution d’une bibliothèque :**\n  - Objectif : Équilibre molaire entre régions exoniques (environ 0.5 sur une échelle de 0.3 à 0.7) avec bonne profondeur (nombre de reads supérieurs à 100) pour tous les exons sans échec PCR.\n  - Complexité due aux codépendances entre les paramètres de performance.\n  - Modifications bénéfiques pour l’équilibre molaire peuvent nuire à la profondeur de typage.\n  - Incertitude majeure : Impact des modifications sur les performances individuelles et globales.\n  - Développement de prototypes successifs pour évaluer les impacts des modifications.\n\n- **Difficulté de conception du logiciel d’interprétation :**\n  - Défis liés au volume et à la structure des données à gérer.\n  - Nécessité d’appels aux bases de données pertinentes.\n  - Gestion des échantillons de l’étude et de leurs références.\n\n- **Aléas rencontrés en 2021 :**\n  - Phenomenon de crossmapping : amplification non spécifique d’une région d’intérêt, entraînant perte d’information.\n  - Réamplification : déstabilisation de l’équilibre.\n  - Problèmes de contamination des échantillons.\n  - Incertitudes dans le développement du logiciel d’analyse liées à la détermination de seuils de significativité pour les anomalies, visant à établir des règles statistiques pour la détection d’anomalies sans faux positifs."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**  \nLe principal verrou technique que nous avons rencontré était lié au manque de profondeurs pour certains exons, de ce fait une réévaluation des cibles a été faite afin de typer 96 échantillons.\n\nLa première difficulté était liée à des problèmes de multiplexage à l’origine de réactions concurrentes ou inhibitrices lors de l’amplification à partir des amorces HLA conçues résultant en une non-efficience du procédé d’amplification. De nombreux travaux d’amélioration portant sur différents aspects de notre conception nous ont permis de nous affranchir de cette difficulté. Ces travaux ont porté à la fois sur la formulation des séquences d’amorces HLA, la qualité du multiplexage, la diminution des interférences entre les amorces de Classe I et II, la détermination des concentrations des amorces pour les PCR simplex et en PCR multiplexé.\n\nLe second aléa rencontré s’est manifesté par la co-amplification des gènes HLA. Nous avons adressé cette difficulté en l’abordant selon deux axes de travail : D’une part, la détermination du meilleur positionnement possible des amorces sur la séquence de référence ; D’autre part, l’élaboration de dilutions appropriées de nos amorces afin de cibler certains gènes. Amélioration du pipeline permettant de gérer la détection de la co-amplification sur les gènes non classiques et suppression des ambigüités générées.\n\nUn autre verrou était lié à la constitution d’une bibliothèque (library) satisfaisante. En d’autres termes, nos spécifications étaient l’atteinte du meilleur équilibre molaire possible entre les régions exoniques soit environ 0.5 sur une échelle de 0.3 à 0.7 avec une bonne profondeur (nombre de reads supérieurs à 100) pour tous les exons et sans échec PCR. Toutefois, l’atteinte de nos objectifs de performances, pour tous les paramètres que nous étudions était complexe en raison des codépendances entre ceux-ci. En effet, des modifications pouvant être bénéfiques pour l’équilibre molaire, auront un effet néfaste sur la profondeur de typage. L’incertitude majeure à lever réside ainsi sur l’impact des modifications réalisées sur les différents constituants de notre kit de typage sur les performances individuelles et globales. Notre défi était alors de développer une multitude de prototypes successifs, permettant d’évaluer ces grandeurs et de suivre l’impact des modifications sur les performances.\n\nUne autre difficulté que nous avons eu à surmonter portait sur la conception du logiciel d’interprétation, accompagnant notre kit de typage HLA. Les défis à adresser sont liés au volume de données à gérer, à la structure de celles-ci pour permettre les appels aux bases de données pertinentes, mais aussi la gestion des échantillons de l’étude et de leurs références.\n\nEnfin, en 2021 nous avons été confrontés tout particulièrement à plusieurs aléas comme le phénomène du crossmaping qui est une amplification non spécifique d’une région d’intérêt provoquant ainsi la perte de l’information, la réamplification qui déstabilise l’équilibre ou encore les problèmes de contamination des échantillons. Sur le périmètre du développement de notre logiciel d’analyse, nous avons en outre été confrontés à des incertitudes liées à la détermination de seuils de significativité des anomalies relevées en vue de l’élaboration de règles statistiques pour des détections d’anomalies sans avoir de faux positifs."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Verrou Technique Principal**:\n  - Manque de profondeurs pour certains exons.\n  - Réévaluation des cibles pour typer 96 échantillons.\n\n- **Première Difficulté**:\n  - Problèmes de multiplexage entraînant des réactions concurrentes ou inhibitrices lors de l’amplification avec les amorces HLA.\n  - Non-efficience du procédé d’amplification.\n  - Travaux d'amélioration réalisés sur :\n    - Formulation des séquences d’amorces HLA.\n    - Qualité du multiplexage.\n    - Diminution des interférences entre les amorces de Classe I et II.\n    - Détermination des concentrations d’amorces pour PCR simplex et multiplexé.\n\n- **Second Aléa**:\n  - Co-amplification des gènes HLA.\n  - Deux axes de travail pour résoudre ce problème :\n    - Meilleur positionnement des amorces sur la séquence de référence.\n    - Élaboration de dilutions appropriées des amorces pour cibler certains gènes.\n  - Amélioration du pipeline pour gérer la détection de la co-amplification sur les gènes non classiques et suppression des ambiguïtés.\n\n- **Verrou Lié à la Bibliothèque**:\n  - Constitution d'une bibliothèque satisfaisante avec un équilibre molaire entre régions exoniques (environ 0.5 sur une échelle de 0.3 à 0.7).\n  - Objectif de bonne profondeur (nombre de reads > 100) pour tous les exons sans échec PCR.\n  - Complexité due aux codépendances entre les paramètres de performance.\n  - Impact des modifications sur les performances individuelles et globales à évaluer.\n  - Développement de prototypes successifs pour évaluer ces grandeurs.\n\n- **Difficulté de Conception du Logiciel d’Interprétation**:\n  - Volume de données à gérer et structure requise pour appels aux bases de données pertinentes.\n  - Gestion des échantillons de l’étude et de leurs références.\n\n- **Aléas Confrontés en 2021**:\n  - Phénomène de crossmapping : amplification non spécifique entraînant perte d’information.\n  - Réamplification déstabilisant l’équilibre.\n  - Problèmes de contamination des échantillons.\n  - Incertitudes sur la détermination des seuils de significativité des anomalies pour l’élaboration de règles statistiques, visant à éviter les faux positifs."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe principal verrou technique que nous avons rencontré était lié au manque de profondeurs pour certains exons, de ce fait une réévaluation des cibles a été faite afin de typer 96 échantillons.\n\nLa première difficulté était liée à des problèmes de multiplexage à l’origine de réactions concurrentes ou inhibitrices lors de l’amplification à partir des amorces HLA conçues résultant en une non-efficience du procédé d’amplification. De nombreux travaux d’amélioration portant sur différents aspects de notre conception nous ont permis de nous affranchir de cette difficulté. Ces travaux ont porté à la fois sur la formulation des séquences d’amorces HLA, la qualité du multiplexage, la diminution des interférences entre les amorces de Classe I et II, la détermination des concentrations des amorces pour les PCR simplex et en PCR multiplexé.\n\nLe second aléa rencontré s’est manifesté par la co-amplification des gènes HLA. Nous avons adressé cette difficulté en l’abordant selon deux axes de travail : D’une part, la détermination du meilleur positionnement possible des amorces sur la séquence de référence ; D’autre part, l’élaboration de dilutions appropriées de nos amorces afin de cibler certains gènes. Amélioration du pipeline permettant de gérer la détection de la co-amplification sur les gènes non classiques et suppression des ambigüités générées.\n\nUn autre verrou était lié à la constitution d’une bibliothèque (library) satisfaisante. En d’autres termes, nos spécifications étaient l’atteinte du meilleur équilibre molaire possible entre les régions exoniques soit environ 0.5 sur une échelle de 0.3 à 0.7 avec une bonne profondeur (nombre de reads supérieurs à 100) pour tous les exons et sans échec PCR. Toutefois, l’atteinte de nos objectifs de performances, pour tous les paramètres que nous étudions était complexe en raison des codépendances entre ceux-ci. En effet, des modifications pouvant être bénéfiques pour l’équilibre molaire, auront un effet néfaste sur la profondeur de typage. L’incertitude majeure à lever réside ainsi sur l’impact des modifications réalisées sur les différents constituants de notre kit de typage sur les performances individuelles et globales. Notre défi était alors de développer une multitude de prototypes successifs, permettant d’évaluer ces grandeurs et de suivre l’impact des modifications sur les performances.\n\nUne autre difficulté que nous avons eu à surmonter portait sur la conception du logiciel d’interprétation, accompagnant notre kit de typage HLA. Les défis à adresser sont liés au volume de données à gérer, à la structure de celles-ci pour permettre les appels aux bases de données pertinentes, mais aussi la gestion des échantillons de l’étude et de leurs références.\n\nEnfin, en 2021 nous avons été confrontés tout particulièrement à plusieurs aléas comme le phénomène du crossmaping qui est une amplification non spécifique d’une région d’intérêt provoquant ainsi la perte de l’information, la réamplification qui déstabilise l’équilibre ou encore les problèmes de contamination des échantillons. Sur le périmètre du développement de notre logiciel d’analyse, nous avons en outre été confrontés à des incertitudes liées à la détermination de seuils de significativité des anomalies relevées en vue de l’élaboration de règles statistiques pour des détections d’anomalies sans avoir de faux positifs."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Verrou Technique Principal :**\n  - Manque de profondeurs pour certains exons.\n  - Réévaluation des cibles pour le typage de 96 échantillons.\n\n- **Difficultés de Multiplexage :**\n  - Problèmes de réactions concurrentes ou inhibitrices lors de l’amplification avec les amorces HLA.\n  - Améliorations apportées :\n    - Reformulation des séquences d’amorces HLA.\n    - Amélioration de la qualité du multiplexage.\n    - Réduction des interférences entre amorces de Classe I et II.\n    - Détermination des concentrations d’amorces pour PCR simplex et multiplex.\n\n- **Co-Amplification des Gènes HLA :**\n  - Résolution par deux axes :\n    - Meilleur positionnement des amorces sur la séquence de référence.\n    - Élaboration de dilutions appropriées des amorces pour cibler certains gènes.\n  - Amélioration du pipeline pour gérer la détection de la co-amplification des gènes non classiques et éliminer les ambiguïtés.\n\n- **Constitution d’une Bibliothèque Satisfaisante :**\n  - Objectif : équilibre molaire optimal entre régions exoniques (0,5 sur une échelle de 0,3 à 0,7) avec une profondeur de typage (nombre de reads > 100) pour tous les exons.\n  - Complexité due à l’interdépendance des paramètres : modifications bénéfiques pour l’équilibre molaire peuvent nuire à la profondeur de typage.\n  - Défi : développement de prototypes successifs pour évaluer l'impact des modifications sur les performances.\n\n- **Conception du Logiciel d’Interprétation :**\n  - Défis à surmonter :\n    - Gestion du volume de données.\n    - Organisation des données pour faciliter les requêtes vers les bases de données.\n    - Gestion des échantillons de l’étude et de leurs références.\n\n- **Nouveaux Aléas en R&D :**\n  - Crossmapping : amplification non spécifique entraînant une perte d'information.\n  - Réamplification perturbant l'équilibre molaire.\n  - Problèmes de contamination des échantillons impactant la qualité des résultats.\n  - Poursuite d’efforts en prototypage pour optimiser les performances du kit de typage HLA.\n\n- **Incertitudes dans le Développement du Logiciel d’Analyse :**\n  - Détermination des seuils de significativité des anomalies.\n  - Élaboration de règles statistiques pour détecter les anomalies sans générer de faux positifs.\n\n- **Travaux de 2023 :**\n  - Répétition des difficultés d’équilibre molaire, profondeur de typage et co-amplification.\n  - Nouvelle norme de typage pour les DVMO : typage basé sur 6 loci au lieu de 11.\n  - Nécessité d’un nouveau prototypage des amorces et de l’algorithme d’interprétation.\n  - Incertitudes quant à la compatibilité du kit avec différents types d’échantillons d’ADN (sanguin, salivaire, écouvillon) et sur la précision et performance atteignables."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLe principal verrou technique que nous avons rencontré était lié au manque de profondeurs pour certains exons, ce qui nous a conduits à réévaluer les cibles pour typer 96 échantillons.\n\nLa première difficulté concernait des problèmes de multiplexage à l’origine de réactions concurrentes ou inhibitrices lors de l’amplification à partir des amorces HLA conçues, rendant le procédé d'amplification inefficace. De nombreux travaux d’amélioration portant sur différents aspects de notre conception nous ont permis de nous affranchir de cette difficulté. Ces travaux ont inclus la reformulation des séquences d’amorces HLA, l’amélioration de la qualité du multiplexage, la réduction des interférences entre les amorces de Classe I et II, ainsi que la détermination des concentrations des amorces pour les PCR simplex et multiplex.\n\nLe second aléa rencontré s’est manifesté par la co-amplification des gènes HLA. Nous avons adressé cette difficulté en l’abordant selon deux axes de travail : D’une part, la détermination du meilleur positionnement possible des amorces sur la séquence de référence ; D’autre part, l’élaboration de dilutions appropriées de nos amorces afin de cibler certains gènes. Nous avons également amélioré le pipeline pour gérer la détection de la co-amplification des gènes non classiques et éliminer les ambiguïtés générées.\n\nUn autre verrou technique concernait la constitution d’une bibliothèque (librairie) satisfaisante. En effet, nos spécifications exigeaient d'atteindre un équilibre molaire optimal entre les régions exoniques, soit environ 0,5 sur une échelle de 0,3 à 0,7, avec une bonne profondeur (nombre de reads supérieur à 100) pour tous les exons sans échec de PCR. Toutefois, l’atteinte de nos objectifs de performances, pour tous les paramètres que nous étudions s’est avérée complexe en raison de leur interdépendance. En effet, des modifications pouvant être bénéfiques pour l’équilibre molaire, pourraient avoir un effet néfaste sur la profondeur de typage. L’incertitude majeure à lever réside ainsi sur l’impact des modifications réalisées sur les différents constituants de notre kit de typage sur les performances individuelles et globales. Notre défi était alors de développer une multitude de prototypes successifs, permettant d’évaluer ces grandeurs et de suivre l’impact des modifications sur les performances.\n\nUne autre difficulté que nous avons eu à surmonter concernait la conception du logiciel d’interprétation accompagnant notre kit de typage HLA. Les défis à adresser incluaient la gestion du volume de données, l'organisation de celles-ci pour faciliter les requêtes vers les bases de données pertinentes, ainsi que la gestion des échantillons de l’étude et de leurs références.\n\nAu cours de nos travaux de R&D, nous avons été confrontés à de nouveaux aléas, en particulier le crossmapping, qui est une amplification non spécifique d'une région d'intérêt, entraînant ainsi une perte d'information. De plus, la réamplification a perturbé l'équilibre molaire, tandis que les problèmes de contamination des échantillons ont impacté la qualité des résultats. Pour surmonter ces verrous, nous poursuivons nos efforts en itérant le processus de prototypage pour optimiser les performances du kit de typage HLA.\n\nEn outre, sur le périmètre du développement de notre logiciel d’analyse, nous avons dû faire face à des incertitudes liées à la détermination des seuils de significativité des anomalies relevées, afin d'élaborer des règles statistiques permettant la détection d'anomalies sans générer de faux positifs.\n\nEnfin, dans le cadre de nos travaux de 2023, nous avons été confrontés à nouveau à ces difficultés d’équilibre molaire, de profondeur de typage et de co-amplification dans le cadre de notre nouvelle norme de typage, conformément aux attentes de typages pour les DVMO qui exigent un typage basé sur une évaluation de 6 loci, contre les 11 loci que nous avions initialement. Cette évolution majeure de notre kit de typage nécessite un nouveau prototypage de nos amorces et de notre algorithme d’interprétation. La compatibilité de notre kit avec tout type d’échantillons d’ADN (sanguin, salivaire et par écouvillon) pose également des incertitudes quant à la précision et la performance atteignables."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Aléas rencontrés lors de la conception des outils :**\n  - Création d'une liste de biens standardisés nécessitant un outil de classification.\n  - Recherche de variantes associées à des biens standardisés et caractéristiques descriptives.\n  - Regroupement des biens standardisés en types d’unités, nécessitant renaming.\n\n- **Nettoyage et organisation des données :**\n  - Nettoyage de la liste pour regrouper les biens en types d’unités.\n  - Maintien d'une organisation hiérarchique dans le design des systèmes (Usine/Unité/Matériel1/Sous matériel1).\n  - Dépendance de la valeur d’un bien à son utilité, liée à l’unité dans laquelle il se trouve.\n\n- **Problèmes de données dans la base :**\n  - Seulement 8% des lignes de la base de données ont un champ de capacité renseigné.\n  - Incohérences dans certaines valeurs nécessitant une nouvelle campagne de nettoyage.\n  - Nécessité d'une intervention humaine pour ajouter des capacités.\n\n- **Définition des prédicteurs de valeur :**\n  - Besoin de définir les meilleurs prédicteurs pour la valorisation des unités.\n  - Valorisation en fonction des capacités des biens de classification.\n  - Estimation de la valeur actuelle nette (VAN) d'équipements non listés à partir d'équipements connus.\n\n- **Incertitudes scientifiques sur l'estimation de la VAN :**\n  - Difficulté d'estimer la VAN totale d'un bâtiment et de ses équipements sans estimation individuelle des biens.\n  - Création et exploitation d'une base de données pour rechercher une corrélation entre surface immobilière et estimation des biens.\n  - Réalisation de tests pour identifier et améliorer les facteurs de corrélation selon différents paramètres."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de la conception de nos outils, nous avons été confrontés à de nombreux aléas. Dès le début du projet, la création de cette liste de biens standardisés a impliqué la recherche d’un outil de classification qui contient des variantes associées à des biens standardisés et des caractéristiques descriptives. Elle implique aussi le regroupement des biens standardisés en types d’unités. Nous devions donc renommer des biens standardisés et non standardisés afin de créer les types d’unités (niveau supérieur de la classification). Ensuite, nous avons dû nettoyer la liste et la regrouper en types d’unité. Les systèmes étant composés d’éléments organisés hiérarchiquement, nous avons donc dû conserver cette organisation dans le design des systèmes (Usine/Unité/Matériel1/Sous matériel1), la valeur d’un bien dépendant directement de son utilité, donc de l’unité dans laquelle il est.\n\nDe plus, dans la base de données, seul 8% des lignes ont un champ de capacité renseigné. Nous avons également été confrontés au fait que certaines valeurs sont incohérentes, ce qui nécessite une autre campagne de nettoyage, ainsi qu’idéalement, une intervention humaine pour ajouter des capacités.\n\nNous nous sommes également heurtés au besoin de définir les meilleurs prédicteurs de la valeur de l’unité. Nous devions donc effectuer une valorisation en fonction des capacités des biens de classification. Il nous fallait donc, pour un équipement non listé, trouver un moyen d’estimer la VAN d’un équipement à partir de cellules d’équipements connus.\n\nEnfin, une autre incertitude scientifique portait sur la possibilité d’estimer la VAN totale du bâtiment et de ses équipements, sans disposer d’une estimation individuelle des biens qui s’y trouvent. Pour répondre à cette problématique, nous avions besoin de créer et exploiter notre base de données dans le but de chercher une corrélation entre la surface immobilière et l’estimation des biens le composant. Cette recherche nous a conduit à de nombreux tests dans le but de trouver des facteurs de corrélation suivant différents paramètres, puis de les améliorer dans le but d’avoir le meilleur facteur possible."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Conception de Predict Value** :\n  - Confrontation à de nombreuses incertitudes lors de la conception de l'outil.\n  \n- **Expertise et Neutralité** :\n  - Predict Value doit refléter l'expertise et la neutralité de l'équipe.\n  - Considéré comme un produit d'appel pour promouvoir une expertise humaine en aval.\n  - Questionnement sur la capacité à créer un outil d'aide à la décision validant la capacité prédictive et la fiabilité du modèle de données.\n\n- **Estimation de Valeur** :\n  - Nécessité d'obtenir des hypothèses fiables pour évaluer la valeur de reconstruction d'un site existant.\n  - Interrogation sur la fiabilité des données pour garantir des estimations justes.\n\n- **Pédagogie et Communication** :\n  - L'outil doit être pédagogique pour transmettre l'expertise sur les dommages à d'autres utilisateurs.\n  - Besoin de définir comment assurer cette dimension pédagogique.\n\n- **Adaptabilité et Perspectives Futures** :\n  - Équilibre à maintenir entre la conception d'un outil à court terme pour Sanofi et sa future adaptabilité à d'autres prospects.\n  - Cibles potentielles incluent des clients grands comptes existants et des courtiers en assurance."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de la conception de notre outil Predict Value, nous avons été confrontés à de nombreuses incertitudes. \n\nD’une part, notre outil Predict Value doit servir notre expertise et notre neutralité. Predict Value est vu comme un produit d’appel, visant à faire passer une expertise humaine en aval de Predict Value pour raffiner l’estimation fournie. Pourrions-nous donc sortir un outil d’aide à la décision et créer un cas d’usage visant à valider la capacité prédictive de notre expertise et la fiabilité de notre modèle de données ?\n\nD’autre part, notre outil doit être juste en termes d’estimation de valeur. Nous devons ainsi obtenir des hypothèses fiables pour déterminer la valeur de reconstruction à l’identique d’un site existant. Ainsi, pourrions-nous assurer une fiabilité des données pour des estimations justes ? \n \nPredict Value doit également être pédagogique et didactique pour communiquer auprès d’autres utilisateurs l’expertise du dommage. Comment pourrions-nous assurer ceci ? \n\nEnfin, nous devons réussir à tenir le jeu d’équilibriste de la conception de Predict Value, un outil à court terme pour Sanofi, tout en gardant en tête que demain ce produit pourrait / devrait être proposé et donc adaptable à d’autres prospects de Cabinet Roux (clients grands comptes existants, mais aussi des cibles très différentes qui pourraient être intéressées, telles que les courtiers en assurance)."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Contexte du projet** : Conception de l'outil Predict Value.\n  \n- **Incertitudes rencontrées** :\n  - **Expertise et neutralité** : \n    - Predict Value doit refléter l'expertise et la neutralité de l'équipe.\n    - Produit d’appel pour promouvoir une expertise humaine complémentaire.\n    - Question : Peut-on concevoir un outil d’aide à la décision qui valide la capacité prédictive de l'expertise et la fiabilité du modèle de données ?\n  \n  - **Estimation de valeur** :\n    - Importance d'obtenir des hypothèses fiables pour évaluer la valeur de reconstruction d'un site existant.\n    - Question : Comment garantir la fiabilité des données pour des estimations précises ?\n  \n  - **Pédagogie et didactique** :\n    - Predict Value doit être conçu pour communiquer efficacement l'expertise sur les dommages à d'autres utilisateurs.\n    - Question : Quelles méthodes peuvent être mises en place pour assurer cette pédagogie ?"}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\n\nLors de la conception de notre outil Predict Value, nous avons été confrontés aux incertitudes suivantes :\n\nD’une part, notre outil Predict Value doit servir notre expertise et notre neutralité. Predict Value est vu comme un produit d’appel pour pouvoir faire passer une expertise humaine en aval de Predict Value pour raffiner l’estimation fournie. Pourrions-nous donc concevoir un outil d’aide à la décision et créer un cas d’usage visant à valider la capacité prédictive de notre expertise et la fiabilité de notre modèle de données ?\n\nD’autre part, notre outil doit être juste en termes d’estimation de valeur. Nous devons ainsi obtenir des hypothèses fiables pour déterminer la valeur de reconstruction à l’identique d’un site existant. Ainsi, pourrions-nous assurer une fiabilité des données pour des estimations justes ? \n\nDe plus, Predict Value doit être pédagogique et didactique pour communiquer auprès d’autres utilisateurs l’expertise du dommage. Comment pourrions-nous assurer ceci ?"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Anticipation des Durées de Consultation**  \n  - Nécessité d'anticiper dynamiquement, de manière cohérente et pertinente, les durées de consultation.  \n  - Objectif : réduire la variance tout en maintenant un temps de calcul limité pour proposer des créneaux en temps réel.\n\n- **Organisation des Plannings des Médecins**  \n  - Capacité à optimiser les temps de travail des médecins en temps réel lors de la prise de rendez-vous.\n\n- **Impact de la Personnalisation des Temps de Consultation**  \n  - Évaluation du gain de productivité lié à la personnalisation des temps de consultation.  \n  - Réactions des médecins et patients face à des situations inédites, telles que :  \n    - Consultations courtes calibrées par l'algorithme.  \n    - Enchaînement de protocoles médicaux différents des pratiques traditionnelles.  \n    - Introduction d’un nouveau rôle d’assistant médical.\n\n- **Gestion des Urgences et Imprévus**  \n  - Capacité du système à absorber les urgences et les imprévus.\n\n- **Incertitude sur le Nombre de Patients**  \n  - Nombre de patients potentiels inconnu, nécessitant un système capable de supporter une montée en charge.\n\n- **Problèmes de \"Trous\" dans les Plannings**  \n  - Création de créneaux personnalisés entraînant des \"trous\" trop courts (ex. : 7 ou 10 minutes) pour des consultations.  \n  - Problématiques de gestion de ces \"trous\" impactant la productivité des médecins.\n\n- **Évaluation de l'Impact des Développements**  \n  - Difficultés à évaluer l'impact des développements sur les durées de consultation et l'optimisation du temps de travail des médecins.  \n  - Hypothèses moyennes prises en attendant des tests pour obtenir des retours.\n\n### Verrous Techniques (API)\n\n- **Gestion des Conflits Côté Serveur**  \n  - Problèmes liés à la communication en parallèle et en temps réel entre plusieurs modules et la base de données.\n\n- **Complexités d'Architecture Technique**  \n  - Difficultés dans l'obtention d'un générateur de code systématique et propre."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans le cadre de nos travaux de R&D nous avons fait face à un ensemble d’incertitudes scientifiques et de verrous techniques, que nous résumons dans la suite :  \n\nLa capacité à anticiper de manière dynamique, cohérente et pertinente les durées de consultation, avec une variance réduite, tout en conservant un temps de calcul limité pour que le système propose des créneaux en temps réel.  \nLa capacité du système à organiser les plannings des médecins de façon à optimiser leurs temps de travail en temps réel lors de la prise de rendez-vous.  \nLe gain réel en terme de productivité lié à cette personnalisation des temps de consultation, et la réaction sur le terrain des acteurs (médecin et patient) face à des situations inédites (à titre d’exemple : des consultations calibrées par l’algorithme comme particulièrement courtes dans certains cas, enchaînement de protocoles médicaux différents des protocoles traditionnels, introduction d’un rôle nouveau d’assistant médical).  \nLa capacité du système à absorber les urgences et imprévus.  \nLe nombre de patients potentiels étant inconnu, il nous faut un système supportant la montée en charge en termes du nombre de patients souhaitant prendre rendez-vous.  \nLe fait d’avoir des créneaux personnalisés entraîne des « trous » dans les plannings de médecins. Ces « trous » sont trop courts (par exemple, 7 ou 10 minutes) pour effectuer une consultation. Nous avons rencontré des problématiques pour la gestion de ce problème qui entraîne une baisse de la productivité des médecins.  \nEnfin, nous avons rencontré des difficultés quant à l’évaluation de l’impact des développements que nous avons apportés sur les durées de consultation et donc sur l’optimisation du temps de travail des médecins.  \n\nFace à cette incertitude, nous avons pris des hypothèses moyennes, et attendons de tester pour avoir des retours. Du point de vue purement technique, concernant l’API, les verrous rencontrés sont les suivants :  \n\nLa gestion des conflits côté serveur avec plusieurs modules qui communiquent en parallèle et en temps réel avec la base de données.  \nL’obtention d’un générateur de code systématique et propre présente d’importantes complexités d’architecture technique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Anticipation des Durées de Consultation**  \n  - Nécessité d'une anticipation dynamique, cohérente et pertinente des durées de consultation.  \n  - Objectif : réduire la variance tout en maintenant un temps de calcul limité pour proposer des créneaux en temps réel.\n\n- **Organisation des Plannings des Médecins**  \n  - Capacité à optimiser les temps de travail des médecins en temps réel lors de la prise de rendez-vous.\n\n- **Gain de Productivité et Réactions des Acteurs**  \n  - Évaluation du gain réel de productivité grâce à la personnalisation des temps de consultation.  \n  - Réactions des médecins et patients face à des situations inédites, par exemple :  \n    - Consultations courtes calibrées par l'algorithme.  \n    - Enchaînement de protocoles médicaux différents des pratiques traditionnelles.  \n    - Introduction d’un nouveau rôle d’assistant médical.\n\n- **Gestion des Urgences et Imprévus**  \n  - Capacité du système à intégrer les urgences et les imprévus.\n\n- **Montée en Charge des Patients**  \n  - Inconnu du nombre de patients potentiels nécessitant un système capable de supporter une augmentation du volume de rendez-vous.\n\n- **Problèmes de Planification**  \n  - Création de « trous » dans les plannings des médecins dus à des créneaux personnalisés trop courts (ex. : 7 ou 10 minutes) pour des consultations.  \n  - Impact négatif sur la productivité des médecins.\n\n- **Évaluation de l'Impact des Développements**  \n  - Difficultés à évaluer l'impact des développements sur les durées de consultation et l'optimisation du temps de travail des médecins.  \n  - Hypothèses moyennes prises en attendant des tests pour obtenir des retours.\n\n### Verrous Techniques Relatifs à l'API\n\n- **Gestion des Conflits Côté Serveur**  \n  - Problèmes liés à la communication parallèle et en temps réel de plusieurs modules avec la base de données.\n\n- **Complexité du Générateur de Code**  \n  - Obtention d’un générateur de code systématique et propre présente des complexités d’architecture technique importantes."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans le cadre de nos travaux de R&D nous avons fait face à un ensemble d’incertitudes scientifiques et de verrous techniques, que nous résumons dans la suite :  \n\nLa capacité à anticiper de manière dynamique, cohérente et pertinente les durées de consultation, avec une variance réduite, tout en conservant un temps de calcul limité pour que le système propose des créneaux en temps réel.  \nLa capacité du système à organiser les plannings des médecins de façon à optimiser leurs temps de travail en temps réel lors de la prise de rendez-vous.  \nLe gain réel en termes de productivité lié à cette personnalisation des temps de consultation, et la réaction sur le terrain des acteurs (médecin et patient) face à des situations inédites (à titre d’exemple : des consultations calibrées par l’algorithme comme particulièrement courtes dans certains cas, enchaînement de protocoles médicaux différents des protocoles traditionnels, introduction d’un rôle nouveau d’assistant médical).  \nLa capacité du système à absorber les urgences et imprévus.  \nLe nombre de patients potentiels étant inconnu, il nous faut un système supportant la montée en charge en termes du nombre de patients souhaitant prendre rendez-vous.  \nLe fait d’avoir des créneaux personnalisés entraîne des « trous » dans les plannings de médecins. Ces « trous » sont trop courts (par exemple, 7 ou 10 minutes) pour effectuer une consultation. Nous avons rencontré des problématiques pour la gestion de ce problème qui entraîne une baisse de la productivité des médecins.  \nEnfin, nous avons rencontré des difficultés quant à l’évaluation de l’impact des développements que nous avons apportés sur les durées de consultation et donc sur l’optimisation du temps de travail des médecins.  \n\nFace à cette incertitude, nous avons pris des hypothèses moyennes, et attendons de tester pour avoir des retours. Du point de vue purement technique, concernant l’API, les verrous rencontrés sont les suivants :  \n\nLa gestion des conflits côté serveur avec plusieurs modules qui communiquent en parallèle et en temps réel avec la base de données.  \nL’obtention d’un générateur de code systématique et propre présente d’importantes complexités d’architecture technique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### Composante Juridique\n- L'état actuel de la science juridique permet une analyse exhaustive d'un domaine à un moment donné, mais ne permet pas de modéliser le risque de manière chiffrée.\n- La modélisation du risque juridique est essentielle pour les entreprises, les couples, les salariés et les employeurs.\n- Cette modélisation est cruciale pour le secteur bancaire en lien avec les normes Bâle II, III et IV.\n\n#### Composante TALN (Traitement Automatique du Langage Naturel)\n- **Verrous Technologiques :**\n  - **Analyseur Syntaxique :** \n    - Absence d'un analyseur performant pour le français, moins efficace que pour l'anglais.\n    - Les outils disponibles sont souvent entraînés sur des corpus non juridiques, limitant leur efficacité.\n  - **Extraction d'Entités :**\n    - Nécessite un analyseur syntaxique précis pour la tokenisation, le postaging et la dépendance syntaxique.\n    - Les erreurs de tokenisation affectent la dépendance syntaxique, compromettant l'extraction des critères.\n  - **Problèmes de Tokenisation :**\n    - La tokenisation est difficile en raison de la morphologie complexe des textes juridiques.\n    - Les mots composés et les cas d'ambiguïté (ex. : trait d'union ou apostrophe) compliquent la tâche.\n\n#### Composante AA (Apprentissage Automatique)\n- **Verrous Technologiques :**\n  - **Quantité de Données :**\n    - Faible disponibilité de données pour la plupart des contentieux, sauf en droit social (ex. : indemnités de licenciement).\n    - Les cas typiques contiennent quelques milliers de décisions, avec des cas extrêmes à quelques centaines.\n  - **Prédiction des Décisions :**\n    - Nécessité de prédire des décisions basées sur une seule ligne de données, avec peu de répétitions dans les jeux de données.\n    - Pour chaque ligne, il faut \"deviner\" les décisions potentielles au-delà de celle enregistrée.\n  - **Explicabilité des Décisions :**\n    - La technologie doit fournir des explications sur les décisions prises, et ne peut se limiter à des distributions de probabilités."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nComposante juridique  \nA ce jour l’état de la science juridique permet de faire un point exhaustif dans un domaine particulier, à un moment donné mais aucunement d’en tirer un modèle pour arriver à un chiffrage du risque. Ce chiffrage est nécessaire pour une entreprise principalement, mais aussi pour un couple, un salarié ou un employeur. En sus, la modélisation est également utile pour des plaideurs qui souhaitent privilégier la négociation.  \n\nLa modélisation du risque juridique d’une entreprise, c’est à dire la traduction chiffrée de celui-ci est un événement fondateur ; pour le secteur bancaire cette modélisation est fondamentale dans l’optique des normes Bâle II, III et IV.  \nComposante TALN  \nPour l’extraction automatique des critères à partir de décisions de justice, il existe deux grands verrous technologiques :  \nL’absence d’un analyseur syntaxique performant et robuste pour le français : La plupart des analyseurs et des outils linguistiques gratuits et open source sont moins performants pour le français que, en particulier, pour l’anglais. D’autre part, ces derniers sont généralement entraînés sur des textes qui viennent de Wikipédia ou d’autres corpus en ligne, qui ne contiennent que peu ou pas de textes juridiques.  \nLa tâche d’extraction des entités à partir de textes juridiques nécessite un bon analyseur syntaxique avec une forte précision au niveau de la tokenisation (c’est-à-dire la segmentation de textes en mots), du postaging et de la dépendance syntaxique : si l’analyseur génère des erreurs dans la tokenisation, celles-ci se propagent dans la dépendance syntaxique et par conséquent l'extraction automatique des critères échoue ;  \nLe problème de tokenisation : la tâche d’extraction des critères à partir décisions de justice est basée essentiellement sur la tokenisation ; nous utilisons en effet principalement des approches à base de règles qui sont fondées sur cette technique.  \nLa tokenisation des textes juridiques est une tâche difficile à traiter par la machine, entre-autres à cause de la présence des mots composés et de la morphologie complexe de ces textes.  \nLa tokenisation des mots qui contiennent un trait d'union ou une apostrophe représentent des cas d'ambiguïté pour le tokenizer : parfois un trait d'union fait partie d'un token, comme dans « Saint-Etienne » et parfois ce n'est pas le cas comme dans « parcourt-elle ».  \nComposante AA  \nConcernant l’apprentissage automatique, les verrous technologiques auxquels nous devons faire face sont au nombre de trois :  \nPour la plupart des contentieux que nous étudions, le nombre de données disponibles est faible, voire très faible. À part en droit social, et plus précisément dans le cas des indemnités pour licenciement sans cause réelle et sérieuse, où il est possible de disposer de centaines de milliers de décisions, les valeurs typiques sont de quelques milliers (c’est le cas par exemple pour les prestations compensatoires, le préjudice corporel ou la franchise). Dans plusieurs cas, le nombre de lignes se compte même en centaines : environ trois cent pour le manquement d’initiés, autour de deux cents pour les ententes. Ces quantités très faibles imposent l’utilisation de techniques d’apprentissage robustes et pouvant être déployées avec très peu de données, ou bien d’être capable de mettre en place des techniques d’augmentation de données ;  \nNous devons prédire l’ensemble des décisions qui seraient prises sur un dossier alors qu’en général nous ne disposons que d’une seule ligne dans la base avec un dossier donné : en effet, pour une moyenne de 50 critères et 5000 données, nous ne trouvons que rarement deux lignes identiques et en tout état de cause jamais plus de 10. Pour un jeu de données fixé, nous devons donc, pour chaque ligne, « deviner » les décisions qui pourraient être prises en sus de celle qui a été réellement prise et qui a été enregistrée dans la base ;  \nEnfin, s’agissant d’une technologie destinée à des juristes, nous ne pouvons pas nous contenter de produire la distribution de probabilités des décisions rendues. Il est indispensable de savoir expliquer pourquoi chaque décision a été prise."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques et verrous technologiques\n\n- **Complexité de l'algorithme OCR** :\n  - Difficulté à obtenir des résultats satisfaisants pour des photos prises rapidement.\n  - Dépendance de la reconnaissance de caractères à la qualité des documents :\n    - Mauvais contraste.\n    - Mauvais alignement des lignes de texte.\n    - Caractères tordus ou se touchant (écritures manuscrites).\n    - Fond de page coloré ou tâches.\n  - Possibilité de corrections et retouches pour améliorer la fiabilité de la reconnaissance.\n\n- **Verrou technique : absence de modèle standard de facture** :\n  - Identifiants (EAN, prix, date d’achat) non uniformément positionnés.\n  - Nécessité de développer un système pour :\n    - Déterminer les zones de texte sur le document.\n    - Extraire le texte pertinent.\n    - Ignorer certaines informations (ex. montant de la TVA).\n  - Exemples de confusion dans l'extraction :\n    - 5 chiffres : référence produit ou code postal.\n    - 6 chiffres : référence produit ou élément à ignorer.\n    - Dates : date d'achat ou date de garantie.\n    - 13 chiffres : EAN ou numéro de facture.\n    - Un chiffre : quantité ou numéro de colonne.\n  - Robustesse requise face aux variations de police, taille de police, orientation du document.\n  - Gestion des documents partiellement photographiés.\n\n- **Organisation et traitement des données** :\n  - Création d'un algorithme de reconnaissance et de comparaison de données basé sur une approche « web cognitivement sémantique ».\n  - Traitement intelligent des données pour exclure les informations erronées (ex. prix sur eBay).\n  - Objectif : obtenir une valeur estimée précise.\n\n- **Incertitudes liées à la mise en service du moteur d’indexation** :\n  - Variabilité des structures de données entre les places de marché (eBay, Amazon).\n  - Difficulté à trouver le modèle exact de l’utilisateur et son prix sur ces plateformes.\n  - Besoin de rapidité pour fournir une estimation de la valeur résiduelle.\n\n- **Sensibilité du moteur d’indexation aux variations de prix** :\n  - Doit être insensible aux fluctuations de prix à court terme (soldes, promotions).\n\n- **Complexité du champ d’application** :\n  - Vaste éventail de paramètres à considérer (caractéristiques du bien, vétusté, date d’achat, état, présence de code-barres).\n  - Exploration de nombreuses thématiques récentes (Big-Data, sécurité de l’information, intelligence artificielle).\n\n- **Difficulté dans le calcul de la valeur estimée** :\n  - Nécessité de récupérer un maximum d’informations sur les produits.\n  - Informations parfois disponibles uniquement sur des sites payants ou fermés, nécessitant des partenariats.\n  - Informations fournies par l’utilisateur peuvent être lacunaires (ex. absence de numéro de série).\n  - Capacité à déterminer une valeur résiduelle avec ou sans données des sites de vente.\n  - Gestion de la montée en charge et de la densité d’indexation (objectif : indexer 100 millions de produits par jour).\n\n- **Principale incertitude** :\n  - Fournir des prix résiduels fiables rapidement avec les informations fournies par l’utilisateur."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nL’algorithme d’OCR est le premier frein majeur de notre projet car il reste très complexe à réaliser pour obtenir des résultats satisfaisants pour des photos prises dans un contexte rapide. D’une manière générale, la reconnaissance de caractères est très dépendante de la qualité des documents à traiter. Bien souvent, elle peut être impactée par les éléments suivants :\n\nun mauvais contraste,\nun mauvais alignement des lignes de texte,\ndes caractères tordus ou qui se touchent (cas des écritures manuscrites),\nl’existence d’un fond de page coloré ou de tâches.\n\nToutefois, des corrections et des retouches peuvent rectifier ces défauts pour augmenter la fiabilité de la reconnaissance.\n\nEn outre, nous avons fait face à un verrou technique de taille : l’absence de modèle standard de facture. L’identifiant EAN de l’objet, son prix et sa date d’achat ne sont donc pas toujours au même endroit. Cela pose un réel problème de traitement des factures et de reconnaissance de ces éléments par notre système. Pour cela, nous devions développer un système capable de déterminer les zones du document où se trouve le texte, puis extraire ce texte. Notre système doit également être capable de reconnaître les informations à ignorer (typiquement, le montant de la TVA est à ignorer). En effet :\n\n5 chiffres peuvent être une référence produit (à extraire), ou un code postal (à ignorer) ;\n6 chiffres peuvent être une référence produit (à extraire), ou un élément à ignorer ;\nune date peut être la date d'achat (à extraire), ou la date de garantie (à ignorer dans l’actuelle version de notre système) ;\n13 chiffres peuvent être un EAN (à extraire), ou une référence de numéro de facture (à ignorer) ;\nun chiffre peut être une quantité (à extraire), ou un numéro de colonne (à ignorer) ;\n\nNotre système devait également être robuste aux variations de police, aux variations de taille de police, ainsi qu’aux possibles variations dans l’orientation du document. Enfin, il fallait que notre système soit capable de gérer le cas d’un document partiellement pris en photo (donc avec des informations manquantes).\n\nNotre deuxième verrou concerne l’organisation et le traitement des données issues de l’outil de reconnaissance de données. Nous devrons en effet, créer et mettre en œuvre un algorithme de reconnaissance et de comparaison de données en nous appuyant sur une approche « web cognitivement sémantique ». Le traitement des données doit être rendu intelligent, en effet les informations trouvées sur internet (par exemple, les prix sur eBay) peuvent être fausses, ces informations devront donc être exclues automatiquement de notre base de données. La valeur estimée doit être la plus juste possible.\n\nNous avons donc rencontré des incertitudes liées à la mise en service du moteur d’indexation. En effet, chaque place de marché (eBay, Amazon, etc.) possède sa propre structure de données. Il est donc très difficile de pouvoir trouver le modèle détenu par l’utilisateur, et par extension, il est difficile de trouver le prix pratiqué sur ces plateformes pour le modèle exact de l’utilisateur. En outre, il faut pouvoir trouver ces prix rapidement afin que l’utilisateur ait une idée de la valeur résiduelle de son bien.\n\nPour finir, le moteur d’indexation ne doit pas être sensible à de fortes variations des prix sur une courte période, par exemple lors de soldes ou de promotions en cours.\n\nEnfin, le champ d’application et les paramètres étant tellement vastes (caractéristiques du bien, vétusté, date d’achat, état du bien, présence ou absence de code-barres, etc.), il nous a été difficile d’aboutir à une solution efficace du premier coup. De nombreuses thématiques ont été explorées pour nos projets, dont la plupart sont des thématiques récentes, comme les méthodes liées au Big-Data, la sécurité de l’information, l’intelligence artificielle, etc.\n\nConcernant le calcul de la valeur estimée, la difficulté qui se pose est le fait de récupérer un maximum d’informations liées aux produits et de les sauvegarder. L’information recherchée se trouve parfois sur des sites payants et/ou fermés, ce qui peut nécessiter un partenariat avec les entreprises de commerces électroniques. Parfois, l’information fournie par l’utilisateur peut être lacunaire (pas de numéro de série, pas de détails sur la date et le prix d’achat, etc.). Nous devons tout de même être capables de déterminer une valeur résiduelle, avec ou sans l’aide des informations contenue sur les sites de vente. Notre solution devra donc pouvoir gérer la montée en charge et la densité d’indexation. Nous devrons en effet très rapidement pouvoir indexer près de 100 000 000 de produits par jour (problématique Big-Data). De plus, nous devrons gérer des paramètres qui peuvent influer la valeur d’un bien (son âge, sa compatibilité avec les autres modèles existants, etc.).\n\nLa principale incertitude liée à notre système était donc de fournir des prix résiduels fiables en un minimum de temps avec les informations fournies par l’utilisateur."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Objectif du projet** : Extraire des informations structurées à partir des résultats d'un système de reconnaissance optique de caractères (OCR).\n  \n- **Problèmes liés à l'OCR** :\n  - Erreurs de lecture fréquentes, même sur des images de qualité parfaite.\n  - Types d'erreurs :\n    - Chiffre transformé en un autre chiffre, lettre ou symbole.\n    - Chiffre manquant ou en trop.\n    - EAN faussement coupé en deux.\n    - EAN collé à un autre élément textuel.\n    - EAN complètement absent.\n\n- **Erreurs de détection de contenu** :\n  - Éléments ressemblant à des prix ou EAN, mais représentant d'autres informations (ex : numéro de facture ou client).\n  - Ces éléments peuvent être à l'intérieur ou à l'extérieur de la base de référencement.\n\n- **Développement d'un algorithme de détection** :\n  - Nécessité de définir les éléments pertinents pour détecter les produits.\n  - Choix incorrect d'éléments peut perturber l'algorithme.\n\n- **Perte d'informations contextuelles** :\n  - Informations facilitant la lecture humaine (polices, lignes, codes-barres, couleurs) sont perdues dans le processus actuel.\n\n- **Dimension sémantique et contextuelle** :\n  - Reconnaissance des informations à extraire dépendante du contexte et des variations de layouts.\n  - Lecture de plusieurs exemples et vérifications sur les sites web nécessaires pour déterminer la signification des éléments.\n  - Problématique d'intelligence artificielle forte, difficile à résoudre uniquement par l'intervention humaine.\n\n- **Échantillonnage représentatif** :\n  - Difficulté d'obtenir un échantillon suffisant de factures et tickets de caisse, surtout pour les enseignes plus petites.\n\n- **Lenteur du processus de détection des layouts** :\n  - Problème pour l'expansion vers d'autres marketplaces et l'internationalisation.\n  - Système actuel trop rigide pour être scalable.\n\n- **Évolution vers le Machine Learning** :\n  - Envisagée pour pallier le manque d’adaptabilité.\n  - Problème de la base d’apprentissage non disponible.\n  - Préférence pour développer un système de détection automatique des layouts plutôt que de recourir au Machine Learning."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\nExtraction d’informations structurées à partir d’un OCR  \nNous souhaitons extraire des informations structurées à partir des résultats d’un système de reconnaissance optique de caractère (OCR). Seulement, en cas d’erreur de lecture par l’outil OCR, les données que nous extrayons sont susceptibles d'être erronées ou absentes. Typiquement, pour un EAN* (13 chiffres), les erreurs des systèmes d'OCR (Google Vision, Abbyy, mais aussi Ocropy) sont :\n\nUn chiffre transformé soit en un autre chiffre, soit en une lettre ou autre chose,  \nUn chiffre manquant ou un chiffre en trop,  \nL'EAN a été faussement coupé en deux,  \nL'EAN a été faussement collé à un autre élément textuel,  \nL'EAN manque complètement.  \n\nCes erreurs sont relativement fréquentes et indépendantes de ce qui nous semble être la qualité de l'image d'un point de vue humain. Des erreurs apparaissent sur des images d'une qualité parfaite, et inversement les éléments textuels peuvent être relativement bien reconnus là où l'œil humain y reconnait que difficilement les informations.\n\nEn outre, au-delà des erreurs liées à l’OCR (c'est-à-dire même avec des données d'extractions parfaites ou correctes), il peut y avoir des erreurs de détection de contenu lorsque les éléments de la facture ne correspondent pas au contenu attendu. \n\nSouvent, divers éléments ressemblent à des prix, des références ou des EAN, mais représentent en réalité autre chose : numéro de facture ou de client, par exemple. Ces éléments peuvent apparaitre soit à l'intérieur de la base de référencement, soit à l'extérieur.\n\nPour contourner cette difficulté, il a donc fallu que nous développions un algorithme de détection de produits présents dans la base de référencement. Mais le développement d’un tel algorithme est complexe, notamment car il faut définir quels sont les éléments pertinents pour détecter les produits présents dans la base de référencement. Si l’on choisit les mauvais, ils perturbent l’algorithme.\n\nDe plus, une grande partie des informations qui permettent à un être humain de lire plus facilement une facture est perdue dans les processus actuels (utilisation de différentes polices de caractères, lignes horizontales et verticales, présence de code barre, couleurs, ...).\n\nEnsuite, on ne saurait insister assez sur la dimension très fortement sémantique et contextuelle dans la reconnaissance des informations à extraire (ainsi que des éléments à éviter), et la haute variabilité des layouts* et contenus des documents à analyser. En effet, c'est souvent la lecture de plusieurs exemples, ainsi que des vérifications sur les pages internet des enseignes, qui permet de déterminer la signification ou l'inutilité d'un élément. Nous entrons donc très vite dans une problématique d'intelligence artificielle forte qu'on ne saura que partiellement résoudre par une intervention humaine qui enregistre des informations diverses concernant les layouts des factures (a priori knowledge).  \nDans ces circonstances, un des problèmes très concret consiste aussi, pour une enseigne donnée, de disposer d'un échantillon suffisamment représentatif de factures et tickets de caisse, particulièrement pour des enseignes plus petites.\n\nLa détection des layouts est aujourd’hui un processus lent. Cela est problématique car nous souhaitons nous étendre à davantage de marketplaces et nous internationaliser. Or, le système actuel de détection des layouts rend notre système trop rigide pour être scalable.\n\nAfin de pallier au manque d’adaptabilité de notre système, nous pouvons envisager une évolution vers du Machine Learning. Cependant, nous nous heurtons une fois de plus à la problématique de la base d’apprentissage, dont nous ne disposons pas. Il serait donc davantage favorable de développer un système de détection automatique des layouts plutôt que de passer par du Machine Learning."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Conception de la plateforme\n- Difficultés liées à la conception d'une infrastructure en ligne :\n  - Scalabilité et performance (temps de réponse) face à un grand volume de données.\n  - Données générées par les freelances (compétences et profils) et par les sociétés (missions).\n- Choix de technologies émergentes de Google :\n  - Utilisation du langage GO et des outils App Engine pour la maintenabilité.\n- Problèmes rencontrés :\n  - Manque de maturité des technologies, entraînant des difficultés.\n  - Nécessité d'une démarche expérimentale et de développements spécifiques pour assurer l'interopérabilité avec des briques technologiques tierces.\n\n#### Normalisation des données\n- Problématique principale :\n  - Conception d'une nouvelle structure de données pour analyses automatisées.\n- Défis à relever :\n  - Limiter la perte de données historiques pour conserver un volume suffisant pour le machine learning.\n  - Développer une nouvelle structure pour recueillir des données plus précises sur les freelances.\n- Gestion des données historiques :\n  - Identification des données valides et non valides.\n  - Outil nécessaire pour vérifier en temps réel les données saisies par les freelances lors de leur inscription.\n- Collecte des données des entreprises :\n  - Actuellement non adressée par les outils.\n  - Prévision de développement d'un système automatisé pour recueillir ces informations, jusqu'à présent saisies manuellement par les équipes commerciales.\n\n#### Matching\n- Objectif :\n  - Automatiser et systématiser l'analyse des données structurées et nettoyées, auparavant réalisée manuellement.\n- Besoin d'expérimentation :\n  - Évaluation des critères pertinents et de leur pondération par les équipes internes basées sur leur expertise métier.\n- Problèmes rencontrés :\n  - Faux positifs et faux négatifs dans le matching, nécessitant des solutions pour limiter ces erreurs.\n  - Élargissement du périmètre de recherche plutôt que restriction en cas de besoin.\n- Introduction d'un critère supplémentaire :\n  - Formalisation des critères de priorisation des missions, nécessitant l'automatisation d'une tâche précédemment réalisée par des experts humains."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nConception de la plateforme  \nLes principales difficultés auxquelles nous avons été confrontés étaient liées à la conception d’une infrastructure en ligne scalable et performante (temps de réponse). La contrainte de scalabilité s’est fait ressentir face au grand volume de données généré par les freelances qui doivent renseigner leurs compétences et leurs profils, mais aussi par les sociétés qui peuvent poster de nombreuses missions.  \nDans un souci de maintenabilité de nos développements, nous avons choisi de nous tourner vers des technologies émergentes de Google, le langage GO ainsi que la palette d’outils App Engine.  \nSi à première vue l’utilisation de ces technologies semblait nous permettre de nous affranchir des considérations d’infrastructure, le manque de maturité de ces technologies nous a confrontés à de nombreuses difficultés que nous avons dû adresser par une démarche expérimentale et des développements spécifiques permettant de rendre interopérable la plateforme que nous avons conçue avec des briques technologiques tierces.  \nNormalisation des données  \nLa principale problématique à laquelle nous avons été confrontés était liée à la conception d’une nouvelle structure des données en vue de réaliser, sur la base de ces données, des analyses automatisées.  \nLe défi que nous nous proposions de relever était de limiter la perte de données historiques afin de conserver un volume suffisant de données dans nos jeux de données en vue du développement d’algorithmes de machine learning tout en développant une nouvelle structure des données afin de permettre de recueillir des données plus précises sur les profils des freelances.  \nPar ailleurs, parmi les données historiques, certaines ne sont pas valides et notre outil doit être en mesure de reconnaitre les données pertinentes ou non. Cet outil doit également permettre, pour les nouvelles données renseignées, de proposer des mécanismes de vérification, en temps-réel, des données saisies par les freelances au moment de leur inscription.  \nEnfin, la collecte des données du côté des entreprises reste aujourd’hui non adressée par nos outils mais nous prévoyons de développer un système capable de recueillir de manière automatisée les informations qui sont jusqu’à présent renseignées manuellement par nos équipes sales.  \nMatching  \nSur la base des données structurées et nettoyées, nous avons souhaité automatiser et rendre systématique une analyse qui, jusqu’à présent, était humaine. Un besoin d’expérimentation des différents critères pertinents, de la pondération de chacun a ainsi été mené, avec une évaluation assurée par nos équipes internes qui réalisent, sur la base de leur expertise-métier. En particulier, nous avons été confrontés à des faux positifs et faux négatifs de matching que nous souhaitions adresser et limiter, et dans le pire des cas élargir le périmètre de recherche plutôt que de le restreindre.  \nDe plus, l’introduction d’un critère supplémentaire de formalisation des critères de priorisation des missions à boucler nous a également confrontés au besoin d’automatiser une tâche qui jusque là était réalisée par des humains avec une expertise métier forte."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Conception de l’Architecture de la Plateforme\n- **Objectif** : Surmonter les limites de performance de l'infrastructure existante en développant une solution plus performante et sécurisée.\n- **Technologie choisie** : Adoption de l'infrastructure as code via Terraform.\n- **Verrou identifié** : Multiples environnements nécessitant la duplication de portions de code pour spécifier des données spécifiques à chaque environnement.\n- **Solution envisagée** : Couplage de Terraform avec Terragrunt pour résoudre la duplication de code.\n- **Protocole de duplication** : Conception d'un protocole pour transférer des données d'un environnement source vers un environnement cible, rendant la présence simultanée de deux environnements transparente dans le code.\n\n#### Développement des Mécanismes de Matching\n- **Défi** : Concevoir un algorithme intelligent et performant pour associer les freelances aux offres de missions sur la plateforme.\n- **Problématique des données** : \n  - Normalisation des données des freelances initiée en 2018, mais qualité et quantité insuffisantes pour un matching efficace.\n  - Absence de hiérarchisation des compétences requises par mission et manque d'information sur la maîtrise des compétences par les freelances.\n- **Améliorations souhaitées** : \n  - Introduction d'une composante personnalisée pour la suggestion de missions, prenant en compte non seulement le profil de compétences, mais aussi les interactions des freelances avec les annonces sur CDLC.\n- **Critères de recherche** : \n  - Missions considérées comme « difficiles » nécessitant un assouplissement des critères de recherche pour obtenir un nombre suffisant de résultats de profils de freelances.\n  - Compromis à déterminer dynamiquement pour chaque offre de mission.\n- **Technologies utilisées** : Développements réalisés sur un nouvel outil en raison des limitations de l'univers Google (notamment l'impossibilité d'utiliser des librairies externes) et incompatibilité avec les développements futurs prévus (comme l'introduction du machine learning)."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nConception de l’architecture de la plateforme  \nL’objectif de ces travaux était de surmonter les limites de performances de l’infrastructure qui avait été progressivement développée lors de nos précédents travaux, en nous orientant vers une solution plus performante et sécurisée.  \nNous nous sommes orientés vers une solution d’infrastructure as code via la technologie Terraform. Nous avons toutefois été confrontés à un verrou lors d de plusieurs environnements qui nous conduisait à dupliquer de nombreuses portions de codes et à intervenir sur celles-ci afin de spécifier les données liées à chaque environnement. Nous avons ainsi étudié l’intérêt de coupler la technologie Terraform avec une seconde technologie, Terragrunt afin d’adresser cette difficulté, tout en concevant en parallèle un protocole de duplication d’un environnement source vers un environnement cible qui permette de rendre transparente dans le code la présence simultanée de deux environnements.  \nDéveloppement des mécanismes de matching  \nLe défi que nos travaux visent à relever porte sur la conception d’un algorithme intelligent, performant, capable de réaliser une association pertinente entre les freelances et les offres de missions disponibles sur notre plateforme.  \nLa problématique à laquelle nous avons été confrontés est la nature des données des freelances qu’exploite notre algorithme. Un premier travail de normalisation des données a été initié en 2018, toutefois la qualité et la quantité des données ne suffisait pas pour assurer un matching performant pour chaque mission.  \nL’une des limites de notre fonctionnement historique était l’absence de hiérarchisation entre les compétences requises par une mission et l’absence de renseignement de la maitrise des différentes compétences par les freelances. Nous souhaitions également introduire une composante personnalisée pour la suggestion des missions qui ne dépende pas seulement du profil de compétences mais aussi des interactions des freelances avec les annonces sur CDLC.  \nDe plus, selon les critères requis pour une mission, celle-ci peut être considérée comme « difficile », et il est alors nécessaire d’assouplir les critères de recherche afin de disposer d’un nombre suffisant de résultats de profils de freelances. Ce compromis doit être déterminée à la volée pour chacune des offres de missions proposées sur notre plateforme.  \nEnfin, tous ces développements ont dû être menés sur un outil s’appuyant sur de nouvelles technologies en raison des limites posées par l’univers Google que nous utilisions auparavant (notamment l’impossibilité d’utiliser des librairies externes) et l’incompatibilité de ces technologies avec les développements futurs que nous prévoyons sur la plateforme (notamment l’introduction du machine learning)."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Contraintes techniques initiales** :\n  - Acquisition, extraction, normalisation et modélisation des données.\n  - Difficulté accrue due à l'hétérogénéité et à la mauvaise qualité des données.\n\n- **Exécution de requêtes complexes** :\n  - Nécessité d'obtenir des résultats rapides sur l'intégralité du graphe des transitions de poste.\n  - Contrainte technique majeure pour atteindre les performances visées.\n\n- **Évaluation de la qualité des suggestions** :\n  - Mise en place de critères objectifs pour évaluer la pertinence des suggestions des algorithmes.\n  - Essentiel pour atteindre un niveau de pertinence satisfaisant.\n\n- **Pertinence de l'algorithme de suggestion** :\n  - Importance d'évaluer la pertinence des suggestions pour garantir l'intérêt des modifications algorithmiques.\n  - Nécessité d'itérer en fonction des spécificités des clients.\n\n- **Solutions envisagées à court terme** :\n  - Création manuelle d'un jeu de tests incluant profils, offres et matchings.\n  - Objectif : aligner les matchings suggérés avec ceux d'un opérateur humain.\n  - Utilisation des données pour quantifier la capacité des algorithmes à prédire des parcours de carrière réussis.\n\n- **Différences entre systèmes de recommandations** :\n  - Distinction entre systèmes classiques et systèmes de recommandations d'emplois.\n  - Contraintes temporelles et réciprocité (matching talent/offre et offre/talent).\n  - Compétitivité entre candidats pour les offres.\n  - Longs cycles d'évaluation avec peu de retours.\n  - Importance d'une quantité significative d'informations contextuelles."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes premières contraintes techniques que nous avons dues surmonter sont liées à l'acquisition, l'extraction, la normalisation et la modélisation des données qui sont autant d'étapes déterminantes pour leur bonne exploitation dans nos algorithmes. Ces tâches sont d'autant plus difficiles que les données sont hétérogènes et souvent mal renseignées.\n\nLa nécessité de pouvoir exécuter des requêtes très complexes et obtenir des résultats dans des temps très courts sur l'intégralité du graphe des transitions de poste fut également une contrainte technique forte à lever pour atteindre les performances visées.\n\nLa mise en place de critères objectifs d'évaluation de la qualité des suggestions produites par nos algorithmes est également centrale pour atteindre un niveau de pertinence satisfaisant.\n\nL'un des techniques rencontrés a trait à la pertinence de l'algorithme de suggestion de matchings. Il est fondamental de pouvoir évaluer la pertinence des suggestions afin de s'assurer que les modifications apportées à l'algorithme sont intéressantes et de pouvoir itérer dans la bonne direction en s'adaptant aux spécificités de nos clients. A court terme nous avons envisagé quelques pistes pour résoudre ces difficultés: création manuelle d'un jeu de tests de profils, offres et matchings. L'objectif est alors de tendre vers les matchings suggérés manuellement par un opérateur humain. utilisation de l'ensemble de nos données afin de quantifier à quel point nos algorithmes parviennent à prédire des sauts de carrière existants et ayant donné lieu à un parcours que l'on peut qualifier de \"réussis\".\n\nIl est de réaliser les différences entre systèmes de recommandations classiques et systèmes de recommandations d'emplois: contraintes temporelles, réciprocité (matching talent/offre et offre/talent), compétitivité (candidats en concurrence pour des offres), cycle d'évaluation (longs cycles temporels et peu de retour), quantité importante d'informations contextuelles."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous techniques et incertitudes scientifiques rencontrés :**\n  \n  - **Cloisonnement des plateformes :**\n    - Nécessité de cloisonner les plateformes pour assurer la sécurité des utilisateurs.\n    - Objectif de fournir les mêmes fonctionnalités et contenus à différents utilisateurs sans duplication des serveurs.\n    - Développement d'un système algorithmique pour offrir les mêmes services sans utiliser les mêmes espaces mémoires.\n\n  - **Traduction des spécifications pédagogiques :**\n    - Incertitude sur la conversion des spécifications des chercheurs en pédagogie en termes informatiques.\n    - Défi de passer du monde pédagogique au monde informatique.\n\n  - **Mise à jour des cours :**\n    - Problématique de mettre à jour un cours sans perturber les apprenants déjà engagés.\n    - Nécessité d'éviter la mise en maintenance de la plateforme lors des mises à jour.\n\n  - **Gestion des données :**\n    - Exigence de gérer de gros volumes de données et d'assurer une montée en charge rapide.\n    - Besoin de structurer des données brutes qui ne sont pas initialement organisées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNous présentons ici les différents verrous techniques et les incertitudes scientifiques que nous avons rencontrés au cours du développement du projet.  \n\nTout d’abord, afin d’assurer la sécurité des plateformes de nos utilisateurs, elles devaient toutes être cloisonnées les unes par rapport aux autres. Cependant, nous voulions être capables de fournir les mêmes fonctionnalités, voire les mêmes contenus, à différents utilisateurs, sans effectuer de copie d’un serveur à l’autre.  \nNous avons donc dû trouver comment concevoir un système algorithmique permettant de fournir les mêmes fonctionnalités et les mêmes contenus à nos clients sans passer par les mêmes espaces mémoires.  \n\nUne autre incertitude scientifique à laquelle nous avons été confrontés était le passage du monde pédagogique au monde informatique. Comment pouvions-nous alors traduire en termes informatiques les spécifications de nos chercheurs en pédagogie ?  \n\nUne troisième incertitude scientifique à laquelle nous avons été confrontés portait sur la mise à jour des cours. En effet, comment pouvions-nous mettre à jour un cours sans perturber les apprenants qui l’ont déjà entamé, et sans mettre la plateforme momentanément en maintenance ?  \n\nEnfin, notre système devait être capable de gérer de gros volumes de données et une montée en charge rapide des volumes. Concernant les données que nous gérons, nous avons dû trouver une solution pour structurer nos données (qui ne sont pas structurées quand elles sont brutes)."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### Intelligence artificielle pour la recommandation de cours\n- **Verrou technique principal** : Déterminer comment recommander des cours qui se complètent, correspondent aux intérêts et besoins de l’apprenant, et présentent une difficulté graduelle.\n- **Approche choisie** : Classification des apprenants selon leurs comportements et goûts via un algorithme de machine learning.\n- **Incertitudes scientifiques** :\n  - Critères pertinents pour classifier les apprenants.\n  - Détermination de la granularité de la classification.\n\n#### Mail hooks\n- **Difficulté rencontrée** : Incapacité à stopper le décrochage d'un certain nombre d'apprenants.\n- **Solution envisagée** : Envoi de mails de relance sans paraître insistants.\n- **Incertitudes scientifiques** :\n  - Calcul de la probabilité de décrochage d’un apprenant basé sur son comportement et comparaison avec d'autres apprenants.\n  - Complexité de la prédiction du taux d'abandon due à :\n    - Données non qualifiées.\n    - Présence d'apprenants ayant peu ou pas d'expérience sur la plateforme.\n  - Classification des apprenants en décrochage pour des relances personnalisées.\n  - Détermination du nombre optimal de clusters à qualifier.\n  - Identification du contenu le plus pertinent à envoyer selon l'appartenance à un cluster.\n\n#### App Totem\n- **Nouveau mode de jeu** : Conception d'un jeu en équipe et en live pour l'application « Totem ».\n- **Incertitudes scientifiques** :\n  - Gestion du temps réel.\n  - Gestion des équipes.\n  - Modèle d’unification des données."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\n\nIntelligence artificielle pour la recommandation de cours\nLe principal verrou technique que nous avons rencontré au cours de nos travaux sur la recommandation de cours portait sur la façon dont nous allions recommander les cours. L’objectif n’était pas de proposer des cours similaires, mais des cours qui se complétaient, qui correspondaient aux intérêts et aux besoins de l’apprenant et qui soient de difficulté graduelle.\n\nNous avons choisi de classifier les apprenants selon leurs comportements et leurs goûts afin de faciliter la recommandation à travers un algorithme de machine learning. Cependant, nous nous sommes retrouvés face à une nouvelle incertitude scientifique : comment classifier les apprenants (quels étaient les critères les plus pertinents ?) et selon quelle granularité ? \nMail hooks\nNous faisions face à une difficulté qui était que nous ne parvenions pas à stopper le décrochage d’un certain nombre de nos apprenants. Nous avions l’idée d’envoyer des mails de relance et de reprise de contact aux apprenants, mais nous ne voulions pas paraître insistants et ainsi détourner les utilisateurs de notre plateforme.\n\nLa première incertitude scientifique que nous avons rencontrée dans le cadre de ce développement concernait donc le calcul de la probabilité de décrochage d’un apprenant. Nous souhaitions prévoir si un apprenant reviendrait sur notre plateforme sous dix jours en nous basant sur son comportement et en le comparant aux comportements des autres apprenants de notre base de données. \nLa prédiction du taux d'abandon était complexe pour deux raisons : les données n’étaient pas qualifiées et il y avait beaucoup d’apprenants qui ne s’étaient connectés qu’une seule fois ou ayant très peu d’expérience sur notre plateforme.\n\nEnsuite, nous souhaitions classifier les apprenants en décrochage afin de leur envoyer un mail de relance le plus personnalisé possible. L’incertitude scientifique était ici de déterminer le bon nombre de cluster à qualifier. Enfin, la dernière incertitude concernait le courrier le plus pertinent à envoyer en fonction de l’appartenance de l’apprenant à un cluster. \nApp Totem\nPour notre application « Totem », nous avons conçu un nouveau mode de jeu joué par les apprenants en équipe et en live.\n\nLes incertitudes scientifiques étaient ici nombreuses : \nComment gérer le temps réel ? \nComment gérer les équipes ? \nComment gérer le modèle d’unification des données ?"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Contexte du projet (2019)** : Soutien technologique aux enseignants et étudiants pour optimiser le processus d'enseignement-apprentissage.\n  \n- **Incertitudes rencontrées** :\n  - **Gestion des LMS** : Difficulté à intégrer les LMS des clients qui gèrent leur propre référentiel utilisateur sans nuire au couplage et aux performances du MOOC Corpacademy.\n  \n  - **Envoi de données vers LRS** : Problème d'envoi des données dans la base LRS en respectant le format xAPI, ce qui soulève des questions sur la remontée en temps réel et la mise en place rapide via les backoffices web.\n  \n  - **Résistance à la panne et idempotence** : Nécessité d'assurer la résistance à la panne et l'idempotence des opérations en raison du stockage externe et des exigences en temps réel. \n    - **Solution développée** : Création d'un algorithme de différence pour traiter simultanément une multitude de progressions."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLors de nos travaux de l’année 2019, nous souhaitions apporter un soutien technologique aux enseignants et aux étudiants pour optimiser les différentes phases du processus d'enseignement-apprentissage. Dans ce cadre, notre équipe R&D a été confrontée à plusieurs incertitudes.  \n\nLa première incertitude scientifique que nous avons rencontrée dans le cadre de ce développement concernait les LMS de nos clients qui géraient leur propre référentiel utilisateur. Mais, comment gérer ceci sans limiter le couplage et l’impact sur les performances de notre MOOC Corpacademy ?  \n\nEnsuite, nous faisions face à une difficulté qui était que ne parvenions pas à envoyer nos données dans la base LRS en respectant le format xAPI et les caractéristiques du format et de la base LRS.  Dès lors, comment assurer une remontée en temps réel et une mise en place rapide via les backoffice web ?  \n\nEnfin, une autre difficulté était liée à la résistance à la panne et l’idempotence des opérations qui étaient primordiales étant donné que le stockage est externe et répond à un besoin en temps réel. Dans ce but, nous avons développé un algorithme de différence permettant de traiter une multitude de progression en même temps."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Objectif du projet (2020)** : Soutenir technologiquement les enseignants et managers pour optimiser le processus d'enseignement-apprentissage.\n  \n- **Intégration LMS (Cornerstone/SAP/Workday)** :\n  - **Problème principal** : Pas d'update possible via l'API CSOD pour la gestion du contenu et des complétions.\n  - **API CSOD** : REST API sans possibilité d'update, rendant la mise à jour de contenu problématique.\n  - **Conséquences** :\n    - Contenu mis à jour par l'équipe de création ne se synchronise pas avec le LMS.\n    - Nécessité de supprimer et re-synchroniser le contenu manuellement.\n  - **Progression** :\n    - Les progressions ne changent pas mais doivent être recalculées pour réappliquer les récompenses.\n    - Ignorance de la fonction de \"rejouer\" les progressions côté LMS, rendant l'intégration défaillante.\n  - **Comparaison avec d'autres LMS** : Intégrations avec SAP, Workday et XAPI permettent une mise à jour automatique des contenus, ce qui n'est pas le cas avec CSOD.\n  - **Solution développée** : Identification de solutions techniques pour contourner les limitations de l'API CSOD.\n  - **API complétion CSOD** : Non destinée à une utilisation externe, incompatible avec le fonctionnement de Coorpacademy.\n\n- **Contenu Externe / Intégration de Catalogue Externe** :\n  - **Difficultés techniques** : Gestion de ressources volumineuses (plusieurs centaines de Mo) entraînant des timeouts de l'API.\n  - **Solution adoptée** : Extraction des SCORM lors de l'upload, permettant de consommer des SCORM de plusieurs Go.\n\n- **H5P Intégration – Série Suspect** :\n  - **Développement de scénarios** : Utilisation de H5P pour des scénarios à embranchement multiples.\n  - **Difficultés techniques** :\n    - **Personnalisation du feedback** : Nécessité de créer une API pour rediriger vers la plateforme et afficher le bon feedback.\n    - **Restrictions H5P** : Pas d'appels d'URL externes et passage de paramètres dynamiques non autorisés.\n  - **Solution mise en place** : Emballage de tout le contenu H5P dans un package SCORM, hébergé sur un sous-domaine de Coorpacademy pour contourner les restrictions."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de nos travaux de l’année 2020, nous souhaitions apporter un soutien technologique aux enseignants et aux managers pour optimiser les différentes phases du processus d'enseignement-apprentissage. Dans ce cadre, notre équipe R&D a été confrontée à plusieurs incertitudes. \n\nIntégration LMS (cornestone/ SAP/ Workday)\nLors du processus d’analyse le premier point de blocage est très rapidement apparu : il n’y a pas d’update possible exposée par l’API CSOD pour la gestion du contenu et des complétions. Leur API est une API REST qui n’expose donc pas d’update. Cela va donc à l’encontre de notre système interne de gestion de contenu et progression, effectivement :\nNotre contenu est très régulièrement mis à jour par notre équipe de création de contenu (erreur rencontrée lors de la validation du cours par un utilisateur ou même mise à jour de contenu pour rester à jour avec le temps). \nLe compte rendu est donc très handicapant de ce côté-là car un contenu mis à jour ne l'est pas côté LMS. La seule manière de mettre à jour le contenu est donc de le supprimer côté LMS pour pouvoir le re-synchroniser ou bien d’apporter les modifications à la main côté LMS admin interface.\n\nNos completions (appelé progression en interne) ne sont pas amenées à être modifiées mais de nombreux processus nécessitent de “rejouer” ces progressions pour réappliquer le calcul de progression ou étoiles (récompense de fin de validation de cours) sur notre écosystème. Rejouer les progressions est une fonction qui peut être ignorée côté LMS, mais nous ne pouvons compter sur les passages successifs d’un utilisateur sur un même contenu, ceci rend le caractère isométrique de l’intégration LMS vis à vis de Coorpacademy défaillant. \n\nPlus encore, nos différentes intégrations à des LMS externes (SAP, Workday ou même XAPI) ont tous bénéficié d’une architecture permettant au système d'être automatiquement alimenté avec les créations, mis à jours et délétion de contenu et d’impacter la partie LMS correspondante de ces modifications.\nLe principal problème technique à palier est dû au fait que comme CSOD n’expose pas de route d’update, il ne permet pas de récupérer la présence d’un mapping déjà existant (hormis au moment de la création en exploitant le retour de requête POST et en sauvegardant l’id CSOD).\nGrâce à nos développement, nous avons trouvé des solutions techniques et/ou fonctionnelles pour y remédier. \n\nPar ailleurs, lors de l’analyse de l’API complétion de CSOD, nous nous sommes rapidement rendu compte que leur API n'était pas destinée à une utilisation externe. Leur LMS (front end) fait des requêtes vers cette api pour générer des complétions lors de la complétion par l’utilisateur de contenu détenu par CSOD ou de SCOM externe relié à CSOD.\nCe n’est pas compatible avec le fonctionnement du MOOC Coorpacademy où les cours sont la propriété intellectuelle de Coorpacademy (sauf ceux créés par nos partenaires) et sont un format très customisé pour être lisible sur le player de notre plateforme MOOC.\nSuite à une analyse plus poussée, test sur leur API et conversation avec CSOD, nous avons trouvé un format alternatif compatible décrit dans la section suivante). \n\nContenu externe / intégration de catalogue externe \nLes difficultés techniques rencontrées étaient liées au fait de servir des ressources volumineuses. En effet, avec la première solution mise en place, on ne pouvait pas consommer les ressources qui pesaient plusieurs centaines de Mo. L’API était en timeout. Il lui fallait plus de 30 sec pour répondre à notre demande. Le temps d’extraction de la ressource dans le zip pouvait prendre plusieurs minutes.\n\nPour remédier à ce problème nous avons décidé d’extraire le SCORM lors de l’upload. Grâce à nos travaux, nous arrivons aujourd'hui à consommer des SCORM de plusieurs Go.\n\nH5P Intégration – Série suspect \nLors du développement de la série pédagogique et interactive, nous avons utilisé H5P pour définir des scénarios à embranchement multiples. Pour ce faire, nous avons intégré ce nouveau format au sein de la plateforme. Nous voulions, en fin de l'épisode, présenter un feedback personnalisé et créditer un nombre d'étoiles en fonction du parcours de l’apprenant.\n\nLa première difficulté technique était liée au fait que H5P ne permet pas la personnalisation de cela, nous avons donc dû créer une API pour rediriger vers la plateforme et afficher le bon feedback et créditer le bon nombre d'étoiles à l'apprenant. \n\nLa seconde difficulté technique était liée au fait que H5P n’autorise pas l’appel d’url externes (qui ne sont pas sur le même domaine) et le passage de paramètres dynamiques. Pour contourner ce problème nous avons dû embarquer tout le contenu H5P dans un package SCORM. Ce qui nous a permis de le placer dans un des sous domaines de Coorpacademy pour remédier à cette restriction."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Contexte du projet (2020)** : Automatisation de la diffusion des missions sur la plateforme CDLC vers des freelances pertinents.\n  \n- **Problèmes rencontrés** :\n  - **Limites des technologies Cloud** : Difficultés liées aux performances techniques des services d'Amazon Web Services (AWS).\n  - **Recours à des services externes** : Complexité accrue lors de l'intégration de solutions externes.\n\n- **Architecture technique** :\n  - **Base de données** : Utilisation d'une base de données Postgres pour le matching.\n  - **Module Pipedrive** : Connexion d'un module Pipedrive pour le traitement des données et la récupération des résultats.\n\n- **Difficultés techniques** :\n  - **Hétérogénéité technologique** : Incompatibilité entre les différentes technologies utilisées.\n  - **Echanges entre services** : Problèmes d'intégration entre les services externes et l'infrastructure Cloud sur AWS, entraînant de nombreux développements en 2020."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes travaux menés en 2020 qui visaient l’automatisation de la diffusion des missions ajoutées sur la plateforme CDLC vers les profils de freelances pertinents. Nous avons principalement été confrontés aux limites des technologies Cloud d’Amazon Web Services en termes de performances techniques mais aussi lorsque nous envisageons le recours à des services externes. En effet, nous avons initialement conçu notre solution de matching en nous appuyant sur une base de données Postgres à laquelle nous avons connecté un module Pipedrive pour permettre à nos différentes équipes de réaliser les traitements des données et récupérer les résultats de requêtes. Nous avons toutefois été confrontés à des difficultés posées par l’hétérogénéité de ces différentes technologies et leur incompatibilité à fonctionner de manière optimale. En particulier, les échanges entre des services externes et notre infrastructure Cloud hébergée sur AWS ont été à l’origine de nombreux développements réalisés en 2020."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Objectif des travaux (2021)**: Automatisation de la diffusion des missions sur la plateforme CDLC vers les freelances pertinents.\n  - **Approche**: Développement d'une infrastructure centralisée et amélioration de l'algorithme de matching.\n\n- **Limitations rencontrées**:\n  - **Technologies Cloud**: Problèmes de performances techniques avec Amazon Web Services (AWS).\n  - **Services externes**: Difficultés lors de l'intégration de services externes dans notre solution.\n\n- **Conception initiale**:\n  - Utilisation d'une base de données Postgres connectée à un module Pipedrive.\n  - Objectif: Permettre aux équipes de traiter les données et récupérer les résultats de requêtes.\n\n- **Problèmes d'hétérogénéité**:\n  - Incompatibilité entre différentes technologies utilisées.\n  - Difficultés dans les échanges entre services externes et l'infrastructure Cloud sur AWS.\n\n- **Défis liés au matching**:\n  - Nécessité de collecter des données provenant de différentes équipes et outils de travail.\n  - Objectif: Ciblage précis des talents en croisant :\n    - Données de compétences métiers.\n    - Données d'activité (en mission, en pause, etc.).\n    - Données sectorielles pour valider le matching entre talents et entreprises."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes travaux que nous avons réalisés en 2021 visaient l’automatisation de la diffusion des missions ajoutées sur la plateforme CDLC vers les profils de freelances pertinents grâce à un effort double : le développement d’une infrastructure centralisée et l’amélioration de notre algorithme de matching.  \nNous avons principalement été confrontés aux limites des technologies Cloud d’Amazon Web Services en termes de performances techniques mais aussi lorsque nous envisageons le recours à des services externes. En effet, nous avons initialement conçu notre solution de matching en nous appuyant sur une base de données Postgres à laquelle nous avons connecté un module Pipedrive pour permettre à nos différentes équipes de réaliser les traitements des données et récupérer les résultats de requêtes. Nous avons toutefois été confrontés à des difficultés posées par l’hétérogénéité de ces différentes technologies et leur incompatibilité à fonctionner de manière optimale. En particulier, les échanges entre des services externes et notre infrastructure Cloud hébergée sur AWS ont été à l’origine de nombreux développements.  \nConcernant le matching, nous avons été confrontés au besoin de collecter des données recueillies par différentes équipes sur leurs outils de travail distincts en vue d’un ciblage plus précis des talents. En effet, nous devions à la fois croiser des données de compétences métiers, d’activité (en mission, en pause, etc.) ainsi que des données sectorielles pour confirmer le matching entre un talent et une entreprise."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Objectif des travaux 2022** : Automatisation de la diffusion des missions sur la plateforme CDLC vers les freelances pertinents.\n  - Efforts déployés : Développement d'une infrastructure centralisée et amélioration de l'algorithme de matching.\n\n- **Problèmes rencontrés** :\n  - **Limitations des technologies Cloud** : Constatation de performances techniques insuffisantes avec Amazon Web Services (AWS).\n  - **Incompatibilité technologique** : Difficultés dues à l'hétérogénéité des technologies utilisées (Postgres, Pipedrive) qui ont entravé un fonctionnement optimal.\n  - **Développements nécessaires** : Les échanges entre services externes et l'infrastructure Cloud sur AWS ont nécessité de nombreux développements.\n\n- **Solution proposée** :\n  - Mise en place d'une architecture expérimentale centralisée, permettant un contrôle total sur les données.\n\n- **Difficultés spécifiques au matching** :\n  - Problème de non mise à jour des profils des freelances après la réalisation de missions.\n  - Masquage de certaines compétences des freelances, nécessitant une approche de matching plus approfondie pour inclure ces caractéristiques."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes travaux que nous avons réalisés en 2022 visaient l’automatisation de la diffusion des missions ajoutées sur la plateforme CDLC vers les profils de freelances pertinents grâce à un effort double : le développement d’une infrastructure centralisée et l’amélioration de notre algorithme de matching.  \nNous avons principalement été confrontés aux limites des technologies Cloud d’Amazon Web Services en termes de performances techniques mais aussi lorsque nous envisagions le recours à des services externes. En effet, nous avons initialement conçu notre solution de matching en nous appuyant sur une base de données Postgres à laquelle nous avons connecté un module Pipedrive pour permettre à nos différentes équipes de réaliser les traitements des données et récupérer les résultats de requêtes. Nous avons toutefois été confrontés à des difficultés posées par l’hétérogénéité de ces différentes technologies et leur incompatibilité à fonctionner de manière optimale. En particulier, les échanges entre des services externes et notre infrastructure Cloud hébergée sur AWS ont été à l’origine de nombreux développements. Nous avons ainsi proposé une architecture expérimentale centralisant l’ensemble de nos données sur une infrastructure propre sur laquelle nous avons pleinement le contrôle.  \nConcernant le matching, nous avons été confrontés à la difficulté de non mise à jour des profils des freelances suite à la réalisation de missions. En effet, un certain nombre de compétences est ainsi masqué et nous avons souhaité tout de même inclure ces caractéristiques dans un matching plus approfondi."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Méthode actuelle de détection des failles cryptographiques :**\n  - Dépendance à une équipe d'experts pour effectuer des tests.\n  - Mise en œuvre lourde due à :\n    - Temps nécessaire pour les tests.\n    - Coût élevé des interventions.\n    - Clauses de confidentialité requises.\n\n- **Verrou technologique :**\n  - Impossibilité d'automatiser la détection des vulnérabilités d'une interface cryptographique.\n  - Solution proposée : logiciel effectuant des attaques en chaîne sur le modèle formel.\n  - Performances atteintes : jamais vues auparavant.\n\n- **Délai de test pour 1000 séquences d'attaques :**\n  - Tests manuels par des experts : **10 jours**.\n  - Tests automatiques par Cryptosense Analyzer : **< 1 minute**.\n\n- **Création du modèle de l’outil :**\n  - Phase 1 : Tests avec développement d'un algorithme interne.\n  - Phase 2 : Inférence du modèle, utilisant une technologie brevetée développée à l'INRIA.\n\n- **Fonctionnalités à ajouter et défis technologiques :**\n  - **Langage de définition d’interface T-IDL :**\n    - Doit être expressif pour un maximum d'interfaces.\n    - Doit rester simple pour faciliter l'ingénierie inverse et la recherche d'attaques.\n    - Nécessité de compromis dans les choix de conception impactant les versions spécialisées de Cryptosense Analyzer.\n    - Équipe compétente en vérification de programmes et modélisation de protocoles cryptographiques.\n\n  - **Module \"Cryptosense doctor\" :**\n    - Défi technique : identification des heuristiques pour résoudre des vulnérabilités variées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAujourd’hui la seule solution pour détecter des failles cryptographique consiste à faire appel à une équipe d’experts qui effectue une série de tests. Mais cela est souvent lourd à mettre en œuvre, notamment à cause du temps nécessaire, le coût que cela engendre, les clauses de confidentialité nécessaires, etc…\nL’impossibilité d’automatiser la détection des vulnérabilités d’une interface cryptographique présentait un véritable verrou technologique. Notre logiciel, en effectuant des attaques en chaine sur le model formel, a permis d’obtenir des performances jamais atteintes auparavant.\nDélai nécessaire pour tester 1000 séquences d’attaques :\n• Tests manuels par des experts : 10 jours.\n• Tests automatiques par Cryptosense Analyzer : < 1 minute.\nLa création du modèle de l’outil se fait en 2 phases. D’abord une phase de tests, pour laquelle nous avons dû mettre au point un algorithme in-house. Puis la phase d’inférence du modèle. Cette seconde phase, qui implique l’apprentissage du modèle, s’appuie sur une technologie que nous avions développée précédemment à l’INRIA, et qui fait l’objet d’un brevet.\n\nParmi les fonctionnalités que nous allons ajouter à l’outil existant, on peut noter deux principales difficultés technologiques à surmonter, décrite ci-après.\nLe langage de définition d’interface T-IDL :\nIl doit être à la fois expressif, pour que l’on puisse l’utiliser pour un maximum d’interfaces, et simple, pour que le processus d’ingénierie inverse ainsi que la recherche d’attaques restent possibles. Il y a donc des compromis à trouver et les choix de conception que nous ferons auront un impact important sur notre capacité à fournir des versions de Cryptosense Analyzer spécialisées. Notre équipe combine une grande expertise en vérification de programmes et en modélisation de protocoles cryptographiques : elle a donc les compétences idéales pour la conception de ce langage de définition d’interface.\n«Cryptosense doctor» :\nCe module présente lui aussi un défi technique. En effet, il va nous falloir identifier les heuristiques qui permettront de trouver des solutions à des vulnérabilités très variables."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques et verrous technologiques\n\n- **Incertitudes générales :**\n  - Confrontation à des incertitudes scientifiques et des verrous technologiques concernant :\n    - Performances et sécurité de l'outil.\n    - Capacité à répondre aux besoins des utilisateurs.\n\n- **Exploration de données volumineuses :**\n  - **Incertitudes :**\n    - Problématiques de performances de la plateforme DSS.\n    - Nécessité d'une stratégie de sampling complexe pour l'interfaçage visuel en temps réel.\n    - Formatage particulier requis pour le stockage des données HDFS.\n    - Développement d'un mécanisme pour garantir l'accès et la stabilité de la plateforme sans connaître à l'avance l'usage.\n    - Outil DSS doit rester fonctionnel en cas de surcharge mémoire.\n  - **Verrou technique :**\n    - Expérience utilisateur de l'interface pour des opérations à distance, nécessitant conservation des performances et sécurité.\n\n- **Création d’API de prédiction temps réel :**\n  - **Difficultés fonctionnelles :**\n    - Passage vers un système externe des opérations temps réel (scoring) sous DSS.\n  - **Difficultés techniques :**\n    - Amélioration des performances des moteurs d'exécution via génération automatique de code optimisé (Python, Java, SQL).\n    - Visualisation de l'orchestration pour exécuter des modules sur infrastructures distantes.\n\n- **Frameworks de mise en production de workflow :**\n  - **Incertitudes :**\n    - Proposer une solution répondant aux besoins des équipes techniques et design dans un même outil.\n    - Définition de processus clairs entre développement et production.\n    - Développement d'une solution d'aide à la formulation de problématiques avec différentes architectures.\n    - Monitoring et qualification des résultats : définition d'un framework générique et efficace pour tous types d'utilisateurs.\n\n- **Développement de framework visuel de préparation de données :**\n  - **Difficultés techniques :**\n    - Spécificité des bases de données.\n  - **Incertitudes :**\n    - Formulation de requêtes isomorphes à SQL avec accès visuel.\n    - Exécution des requêtes sur différentes bases de données.\n    - Séparation de l'aspect visuel de l'exécution.\n    - Optimisation de l'outil pour d'importants volumes de données, en ne calculant que les éléments pertinents.\n    - Passage simple d'un environnement de faible volumétrie à un environnement Big Data, avec ajustement automatique des paramètres.\n\n- **Aspects sécuritaires dans les systèmes d’orchestration de calcul Big Data :**\n  - **Incertitude :**\n    - Définition d'un framework de sécurité :\n      - Configurable par un administrateur système de niveau standard.\n      - Renforcement de la sécurité dans les couches basses selon leur modèle.\n      - Adapté aux usages de partage de données entre groupes.\n      - Interopérable avec les systèmes de sécurité standard des entreprises."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours de nos travaux, nous avons été confrontés à un certain nombre d’incertitudes scientifiques et de verrous technologiques portant sur les performances et la sécurité de notre outil, ainsi que sur sa capacité à répondre de façon pertinente et satisfaisante aux besoins de ses utilisateurs. \n\nTout d’abord, concernant l’exploration des ensembles de données volumineux, la principale incertitude résidait dans les problématiques de performances de la plateforme DSS. En effet, l'interfaçage visuel des données doit être fait en temps réel implique de définir une stratégie de sampling complexe à coder puisqu'elle doit être assez générique pour répondre aux contraintes et à la sémantique de chacun des engines. En outre, la visualisation des données doit être réalisée sur la mémoire vive de la plateforme, ce qui implique donc un formatage particulier du stockage des données HDFS (Hadoop Distributed File System). De plus, nous devions trouver une manière de garantir l'accès et la stabilité à la plateforme en ne connaissant pas l'usage qui en sera fait à l'avance, nous devions donc développer un mécanisme afin de se prémunir de la perte de données (en mémoire vive) en cas de surcharge. Cela engendre une autre incertitude, qui est que notre outil DSS doit rester fonctionnel, même en cas de surcharge sur les ressources mémoires, sans connaître par avance le type d'usage des utilisateurs (calcul effectué dans une infrastructure vs exploration interactive).\nPour finir sur les incertitudes liées à l’exploration d’importants volumes de données, un verrou technique concernait l’expérience utilisateur de notre interface. En effet, nous voulions réaliser des opérations à distance pour des opérations normalement réalisées sur un environnement unique. La difficulté résidait dans la conservation des performances et dans les contraintes de sécurité engendrées par une telle architecture.\n\nConcernant la création d’API de prédiction temps réel, la difficulté du point de vue fonctionnel était le passage vers un système externe des opérations temps réel (scoring) réalisées sous DSS. Techniquement, il s’agissait d'améliorer les performances des moteurs d'exécution en générant automatiquement à la volée du code source le plus optimisé possible en fonction des types d'opérations (Python, Java, SQL). Une autre incertitude scientifique résidait dans la visualisation de l'orchestration en elle-même pour exécuter des modules sur des infrastructures distantes en simplifiant le passage d’environnements de développements à des environnements de production. \n\nConcernant les frameworks de mise en production de workflow, une des incertitudes scientifiques résidait dans le fait de pouvoir proposer une solution répondant à la fois aux besoins des équipes techniques et aux besoins des équipes design, au sein d’un même outil. Il fallait notamment que nous soyons en mesure de proposer des processus clairs et définis entre les environnements de développement et de production. Nous devions également développer une solution permettant l'aide à la formulation de cette problématique en proposant différentes architectures et configurations en fonction des besoins des différentes équipes. \nUne autre incertitude concernait le monitoring et la qualification des résultats. Dans ce cas, comment définir un framework qui ait du sens, et qui soit générique et efficace pour n'importe quel type d'utilisateur ?\n\nConcernant le développement de framework visuel de préparation de données et d’orchestration automatique, la difficulté technique résidait dans la spécificité de chacune des bases de données. Voici les quelques incertitudes scientifiques auxquelles nous avons fait face : \nComment formuler les requêtes, en étant isomorphes à SQL, tout en permettant un accès uniquement visuel ?\nComment réussir à exécuter les requêtes quelques soient les bases de données ? Comment séparer l'aspect visuel de l'exécution ?\nComment optimiser notre outil pour le rendre utilisable avec d’importants volumes de données ? Nous avons pensé à ne mettre à jour que la part des éléments nécessaires. Ainsi la question qui était posée était comment ne calculer que les éléments pertinents ? Quelle méthode devions-nous utiliser pour faire la part entre les éléments pertinents ou non ?\nComment faire en sorte qu'un projet (multi-user, multi-environnements, etc.) puisse passer de manière simple d'un environnement de faible volumétrie à un environnement à plus forte volumétrie (voire Big Data) ? Nous voulions être en mesure de modifier automatiquement les paramètres en fonction des infrastructures tout en les gardant dans un état fonctionnel, de manière transparente pour l'utilisateur. \n\nEnfin, concernant les aspects sécuritaires dans les systèmes d’orchestration de calcul Big Data, nous avons fait face à une incertitude qui était liée à la définition d’un framework commun de définition de sécurité qui :\nSoit configurable de manière par un administrateur système d’un niveau standard ;\nRenforce la sécurité dans les couches basses selon leur modèle de sécurité ;\nConvienne aux usages de partage de données entre groupe de personnes ;\nSoit interopérable par les systèmes de sécurité standard des entreprises."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Incertitudes Principales :**\n  - Interopérabilité des systèmes avec ceux des hôpitaux (ex. HYDMedia) : défis d'intégration et de format de données.\n  \n- **Traçabilité et Transmission des Données :**\n  - Nécessité d'un système simple à mettre en place et difficile à contrefaire.\n  - Élimination de l'option QR Code au domicile des patients en raison de sa vulnérabilité (facilité de contrefaçon).\n  - Exploration de nouvelles solutions pour garantir la sécurité et l'efficacité.\n\n- **Robotique d'Assistance :**\n  - **Connectivité :**\n    - Intégration bi-directionnelle du robot à la plateforme Domicalis pour l'écosystème de la personne dépendante.\n    - Problèmes d'interopérabilité entre le robot et les systèmes d'information hospitaliers.\n  - **Données Machine to Machine :**\n    - Récupération des données via le réseau SIGFOX et sécurisation pour respecter les contraintes réglementaires.\n  - **Interaction Humain-Robot :**\n    - Mise en place d'un mode d'interaction simple entre intervenants humains et le robot.\n  - **Analyse Sémantique :**\n    - Isolement des données pertinentes dans un flot d'informations variées pour le moteur d'analyse sémantique.\n\n- **Sous-Projet MORPHEUS :**\n  - Développement d'un outil de santé mobile validé scientifiquement et cliniquement, accessible au grand public.\n  - Objectif de réduire le coût à environ 7 fois moins cher que les actimètres professionnels.\n  - Défi de créer un algorithme performant pour la détection du sommeil, en tenant compte :\n    - Volume de données à transmettre (40 Mo pour 24 heures à 20 Hz).\n    - Limitations des technologies existantes (Smart Bluetooth, GSM, Wifi).\n\n- **Facteurs de Risque :**\n  - Complexité des prises en charge impliquant divers professionnels (médecins, infirmiers, kinésithérapeutes, etc.).\n  - Difficulté de réunir des utilisateurs avec des intérêts divergents autour d'un dossier médical partagé.\n  \n- **Hypothèse d'Appropriation :**\n  - L'appropriation du système d'information (SI) dépend de la richesse et de l'actualisation du contenu.\n  - Plus le contenu est riche, plus les professionnels de santé s'y connecteront, devenant ainsi producteurs de données.\n  - Importance de concevoir un système facile à utiliser pour les usagers, même ceux avec peu de compétences informatiques."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours de nos travaux de développement, nous avons fait face à un ensemble d’incertitudes scientifiques et verrous techniques que nous exposons ici. \n\nLa principale incertitude que nous avons eue au cours du projet était relative à l’interopérabilité de nos systèmes avec ceux qui équipent déjà les hôpitaux (notamment HYDMedia), en termes d’intégration, de format de données, etc. \n\nAfin d’assurer la traçabilité des soignants et d’assurer la transmission des données, nous avons dû trouver un système simple à mettre en place et difficile à contrefaire. Nous avons ainsi dû éliminer une solution qui consistait à assigner un QR Code au domicile des patients, car il était facile de prendre ce QR Code en photo et de le scanner sans même être sur place. Nous avons donc exploré de nouvelles pistes afin de proposer une solution simple et efficace à cette incertitude. \n\nEn ce qui concerne la robotique d’assistance, les principales incertitudes étaient les suivantes : \n\nComment pouvions-nous connecter bi-directionnellement le robot à la plateforme Domicalis pour l’intégrer à l’écosystème de la personne dépendante ? Nous voulions que le robot ait les mêmes droits que les autres acteurs déjà existants dans ce « réseau social » construit autour de la personne prise en charge à domicile. A ce niveau nous avons rencontré une problématique d’interopérabilité entre les systèmes de gestion du robot et les différents systèmes d’information hospitaliers ; \nComment pouvions-nous récupérer les données « Machine to Machine » du robot via le réseau SIGFOX ? Comment sécuriser ces données afin de respecter les contraintes règlementaires fortes du système de soins ? \nComment pouvions-nous mettre en place un mode d’interaction simple entre les intervenants humains et le robot ? \nComment pouvions-nous isoler, dans le flot d’informations plus ou moins bien structurées provenant du domicile, les données d’entrée à fournir au moteur d’analyse sémantique chargé de l’élaboration du modèle de prévention personnalisé ? \n\nDans le cadre du sous-projet MORPHEUS, le challenge technique était de pouvoir développer un outil de santé mobile validé scientifiquement et cliniquement qui pourra être utilisé en médecine, en soin à domicile et par le grand public avec un prix beaucoup plus abordable que les actimètres « professionnels » validés médicalement (de l’ordre de 7 fois moins chers). Un des verrous sous-jacents est de créer un algorithme de détection du sommeil très performant étant donné le volume conséquent des données à transmettre (40Mo pour 24 heures avec un échantillonnage à 20Hz), les technologies existantes pas assez performantes (Smart Bluetooth ou GSM) ou ayant une configuration complexe (Wifi).\n\nDe façon plus générale, le projet était confronté à un fort facteur de risque dû au fait que les prises en charge complexes mettent en action différents corps de métiers, tels que les médecins traitants, les infirmiers libéraux, les kinésithérapeutes, les psychologues, les assistants sociaux, les mandataires de justice ou bien encore les structures d’accompagnement telles que les CCAS ou les équipes mobiles gérontologiques. La complexité de réunir autour d’un même système des utilisateurs ayant des intérêts différents dans l’usage d’un dossier médical partagé est un facteur de risque dans l’appropriation de l’outil. \n\nCe facteur de risque avait bien évidemment été identifié et l’hypothèse envisagée fut la suivante : l’appropriation du SI doit venir de la richesse de son contenu. Plus riche et actualisé est le contenu, plus les professionnels de santé s’y connecteront. Dès lors, les consommateurs de la donnée deviendront à leur tour des producteurs de la donnée. Il est nécessaire ici de veiller à concevoir un système facilement utilisable par les usagers ne possédant pas forcément une maitrise totale des outils informatique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Anticipation Statistique des Durées de Consultation :**\n  - Nécessité d'anticiper les durées de consultation de manière cohérente et pertinente.\n  - Importance de réduire la variance tout en maintenant un temps de calcul limité.\n  - Objectif : proposer des créneaux en temps réel.\n\n- **Efficience de la Personnalisation :**\n  - Évaluation du gain réel d'efficience grâce à la personnalisation des temps de consultation.\n  - Réactions des acteurs (médecins et patients) face à des situations inédites :\n    - Consultations jugées particulièrement courtes par l'algorithme.\n    - Enchaînement de protocoles médicaux différents des protocoles traditionnels.\n    - Introduction d’un rôle nouveau d’assistant médical.\n\n- **Gestion des Urgences et Imprévus :**\n  - Capacité du système à absorber les urgences et les imprévus.\n\n- **Hypothèses et Tests :**\n  - Adoption d'hypothèses moyennes face aux incertitudes.\n  - Attente de retours suite aux tests effectués.\n\n- **Verrous Techniques de l'API :**\n  - Gestion des conflits côté serveur avec plusieurs modules en communication parallèle et en temps réel avec la base de données.\n  - Complexités d'architecture technique pour l'obtention d'un générateur de code systématique et propre.\n\n- **État d'Avancement :**\n  - Après 6 mois de développement, une première version de l'API a été mise en place.\n  - Cette version est actuellement en cours de test et de maturation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans le cadre de nos travaux de R&D nous avons fait face à un ensemble d’incertitudes scientifiques et de verrous techniques, que nous résumons dans la suite :  \n\nLa capacité à anticiper de manière statistique cohérente et pertinente les durées de consultation, avec une variance réduite, tout en conservant un temps de calcul limité pour que le système propose des créneaux en temps réel.  \nLe gain réel en terme d’efficience lié à cette personnalisation des temps de consultation, et la réaction sur le terrain des acteurs (médecin et patient) face à des situations inédites (à titre d’exemple : des consultations calibrées par l’algorithme comme particulièrement courtes dans certains cas, enchaînement de protocoles médicaux différents des protocoles traditionnels, introduction d’un rôle nouveau d’assistant médical).  \nLa capacité du système à absorber les urgences et imprévus.  \n\nFace à cette incertitude, nous avons pris des hypothèses moyennes, et attendons de tester pour avoir des retours. Du point de vue purement technique, concernant l’API, les verrous rencontrés sont les suivants :  \n\nLa gestion des conflits côté serveur avec plusieurs modules qui communiquent en parallèle et en temps réel avec la base de données.  \nL’obtention d’un générateur de code systématique et propre présente d’importantes complexités d’architecture technique.  \n\nAprès 6 mois de développement, nous avons mis en place une première version de l’API, mais celle-ci est en cours de test et de maturation."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Recherche d'algorithme** : Développement d'un algorithme pour modéliser les risques et associer des contrats d'assurances adaptés.\n  \n- **Indicateurs et Profils d'Assurance** : \n  - Nécessité de définir des indicateurs et des profils d'assurance en raison de l'absence de bases de données accessibles d'assureurs.\n  - Manque de littérature sur les critères numériques réels utilisés pour établir des contrats d'assurance.\n  - Réalisation d'un travail de recherche pour définir ces indicateurs à partir de données de contrats d'assurances papier et de connaissances métier.\n\n- **Difficultés rencontrées** : \n  - Identification des critères nécessaires pour cadrer un profil de risque.\n\n- **Association Questionnaire-Profils** : \n  - Deuxième phase des travaux : associer un questionnaire rempli par un utilisateur aux profils d'assureurs.\n  - Objectifs :\n    - Définir le profil du client à l'aide des indicateurs établis.\n    - Matcher le profil client avec les profils d'assureurs.\n    - Déduire les différents risques, leur intensité et leur probabilité d'occurrence.\n\n- **Évolutivité de l'algorithme** : \n  - L'algorithme de prédiction de risques doit être évolutif et capable de réévaluer régulièrement les risques en fonction des changements de situations.\n\n- **Développement d'une API** : \n  - Création d'une API pour faciliter l'accès aux travaux sur la plateforme."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques et verrous technologiques\nLa recherche d’un algorithme modélisant les risques et y associant des contrats d’assurances adaptés nous a amené à définir, dans un premier temps, des indicateurs et des profils d’assurance. En effet, actuellement il n’existe pas de bases de données accessibles d’assureurs, et très peu de littérature concernant les critères numériques réels pris en compte pour l’établissement de contrats. Ainsi, nous avons dû effectuer un travail de recherche pour définir ces indicateurs et, à partir de données de contrats d’assurances papier et de notre connaissance métier, nous avons établi des profils de contrats. Les difficultés de ces travaux résidaient donc dans la recherche de critères pour cadrer un profil de risque.  \nLa deuxième partie des travaux consistait à associer un questionnaire rempli par un utilisateur à ces profils assureurs. Pour cela, nos objectifs étaient de définir, à l’aide des indicateurs mis en place précédemment, le profil du client, de le matcher avec nos profils, et d’en déduire les différents risques, l’intensité de ces risques et leur probabilité d’occurrence. Nous avons de même voulu que cet algorithme de prédiction de risques soit évolutif et réévalue régulièrement les risques encourus selon les changements de situations. \nEnfin, nous avons développé une API dans le but de pouvoir rendre ces travaux simples d’accès pour notre plateforme."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Objectif du projet** : Mettre en place un questionnaire dynamique pour évaluer la connaissance des utilisateurs sur leur couverture d'assurance.\n  \n- **Caractéristiques du questionnaire** :\n  - Adaptation dynamique aux réponses des utilisateurs.\n  - Ordre et contenu des questions non prédéfinis.\n  - Nombre élevé de combinaisons de questions, entraînant des défis en termes de :\n    - Stockage des possibilités.\n    - Performance (temps de réponse).\n\n- **Difficultés rencontrées** :\n  - Choix des informations à fournir, dépendant du secteur d'activité de l'utilisateur.\n  - Absence de classification des métiers, nécessitant le développement d'un référentiel pour classifier les métiers des utilisateurs.\n\n- **Incertitudes principales** :\n  - Identification des informations pertinentes pour regrouper des métiers ayant des noms différents mais exerçant la même activité.\n\n- **Problématique d'accès aux pages** :\n  - Limitation des utilisateurs dans la navigation entre les pages du site due à l'architecture web parent/enfant.\n  - Nécessité de remonter l'arborescence pour accéder à d'autres branches, réduisant le nombre de pages visitées.\n\n- **Solution envisagée** :\n  - Développement d'une nouvelle architecture permettant une navigation rapide entre les différentes branches du site."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans l’optique de mieux appréhender le niveau de connaissance de nos utilisateurs vis-à-vis de leur couverture d’assurance et leur proposer une couverture d’assurance depuis notre plateforme, nous avons souhaité mettre en place un questionnaire dynamique. Nous souhaitions que ce questionnaire s’adapte de manière dynamique aux réponses fournies par l’utilisateur, ce qui implique que l’ordre ni même le contenu des questions posées ne sont pas connus à l’avance. Ainsi, il existe un nombre de combinaisons de questions extrêmement important, ce qui pose un problème en termes de stockage des possibilités et en termes de performance (i.e. temps de réponse) du questionnaire. Aussi, nous avons rencontré des difficultés lors du choix des informations à fournir à tel ou tel utilisateur. En effet, ces informations dépendent fortement du secteur d’activité dans lequel il exerce, mais la classification des métiers est un sujet qui n’a pas été traité jusqu’alors. L’un de nos axes de travail consistait ainsi à développer un référentiel permettant de classifier les métiers de nos utilisateurs. Les principales incertitudes rencontrées concernaient le choix des informations à considérer pour rassembler les métiers dont le nom diffère mais constituant une même activité.  \n\nNous nous sommes dans un premier temps confronté à la problématiques de l’accès des différentes pages de notre site à un utilisateur présent sur le site. Les architectures web existantes fonctionnant sous la forme de branches parent/enfant(s) demandent à un utilisateur de remonter l’entièreté de la branche sur laquelle il se trouve afin de pouvoir accéder à une branche voisine. Cela limite grandement le nombre de pages visitées par un utilisateur présent sur notre site. Nous avons donc souhaité résoudre ce problème en développant notre propre architecture permettant à un utilisateur de passer rapidement d’une branche à une autre du site."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Incertitudes techniques, verrous technologiques et problèmes à résoudre\n\n#### Réduction du temps d'intégration d'une sonde de supervision\n- **Problématique** : Le temps d’intégration des sondes logicielles existantes est principalement dû à la rédaction et à la maintenance de scripts pour récupérer les informations nécessaires.\n- **Objectif** : Créer une sonde qui ne nécessite pas de scripts complexes.\n- **Approche** : Développer une sonde capable de lire divers types de sources d’information via un fichier descriptif des données à récupérer.\n\n#### Difficultés et verrous technologiques :\n- **Variété des sources** : Les nombreuses sources d’informations (fichiers, bases de données, flux réseaux, capteurs) rendent incertaine la capacité de la sonde à être suffisamment générique pour s’interfacer avec toutes.\n- **Compatibilité système** : Incertitude quant à la possibilité de créer un logiciel compatible avec tous les systèmes d’exploitation (Linux, FreeBSD, Unix, Solaris, Windows, etc.).\n- **Système de description** : Nécessité de créer un système de description des informations à récupérer, qui doit être généré automatiquement pour une utilisation facile.\n\n#### Amélioration des performances et de la fiabilité du système de détection\n- **Constat** : Les systèmes de supervision existants présentent des limites, nécessitant une solution plus performante pour répondre aux exigences croissantes.\n  \n#### Difficultés et verrous technologiques :\n- **Objectifs de performance** : Les performances visées sont supérieures à celles des concurrents, ce qui soulève des doutes sur leur atteignabilité.\n- **Chiffrement** : Le chiffrement souhaité pour une sécurité maximale pourrait nuire aux performances, impactant l'atteinte des objectifs.\n- **Espace disque** : L’objectif est de minimiser l’espace occupé par la sonde, mais l’incertitude demeure quant à la faisabilité par rapport à la taille des sondes concurrentes.\n- **Configuration minimale** : La sonde doit être embarquée sur des configurations minimales sans affecter les performances, ce qui reste incertain face aux performances des sondes concurrentes.\n\n#### Amélioration du temps de traitement des réponses et de la sécurité\n- **Urgence d'alerte** : En cas d’incident critique, le délai entre la récupération de l’information et l’envoi de l’alerte doit être réduit au minimum.\n- **Confiance** : Les concurrents ont développé des sondes capables de récupérer des informations en moins d’une seconde, mais le traitement subséquent prend souvent plusieurs minutes.\n\n#### Difficultés et verrous technologiques :\n- **Délai d’alerte** : Réduire le délai d’alerte à moins de deux minutes.\n- **Critères de sécurité** : Assurer le respect de tous les critères de sécurité définis.\n\n#### Récupération d'indicateurs techniques sur des objets connectés\n- **Innovation R&D** : Répondre à des verrous technologiques non résolus, notamment :\n  - Articulation entre objets physiques et applications.\n  - Passage à l'échelle d'une architecture hybride (systèmes embarqués / clouds).\n  - Auto-description sémantique des différentes couches du système.\n- **Intervention** : Focalisation sur le passage à l'échelle d'une architecture hybride.\n\n#### Automatisation de la construction des messages d'alerte\n- **Nouveau domaine** : L’automatisation de la construction de messages d’alerte n’a jamais été abordée auparavant.\n- **État du traitement du langage** : Le traitement du langage est encore à ses débuts.\n- **Collaboration** : Prévision de collaborations avec d'autres entreprises (ex : Apicultur, Espagne) et des laboratoires sur ce sujet."}, {"role": "assistant", "content": "Incertitudes techniques, verrous technologiques et  Problèmes à résoudre\nRéduction du temps d'intégration d'une sonde de supervision dans un système d'information\nLe temps d’intégration des sondes logicielles existantes était en grande partie dû au temps passé à écrire des scripts pour aller récupérer toutes les informations nécessaires et à la maintenance de ces derniers. L’objectif que nous nous sommes fixé était donc de créer une sonde qui ne souffrirait pas de ce défaut.\n\nNous sommes partis de l’idée qu’il était vraisemblablement possible de créer une sonde capable de lire tout type de source d’information pour laquelle il suffirait de passer un fichier descriptif des informations à aller récupérer.\n\nDifficultés et verrous technologiques :\nLes sources d’informations existantes étant nombreuses et de natures différentes (fichiers, bases de données, flux réseaux, capteurs en tout genre, etc.) nous n’étions pas sûrs de pouvoir rendre la sonde assez générique pour qu’elle puisse s’interfacer avec l’ensemble des sources possibles.\nLa sonde doit pouvoir s’installer sur n’importe quel système, quel que soit son système d’exploitation, nous ne sommes pas sûrs de pouvoir réaliser un logiciel compatible avec l’ensemble des systèmes (linux et toutes ses distributions, FreeBSD, Unix, Solaris, Windows, etc.)\nLe système de description des informations à aller récupérer était à créer entièrement et il devait pouvoir être généré automatiquement afin qu’il soit utilisable facilement.\n\nAmélioration des performances et de la fiabilité du système de détection des sondes de supervision\nConstatant les limites existant dans les systèmes de supervision (cf. Etat de l'art existant) nous voulions proposer quelque chose de plus performant afin de répondre aux exigences toujours plus importantes du milieu de la supervision.\n\nDifficultés et verrous technologiques :\nLes objectifs que nous nous sommes fixés en matière de performances (cf. 4.2) étant meilleurs que ce que nous avons pu constater chez nos concurrents, nous ignorions s’il était possible de les atteindre.\nLe chiffrement auquel nous tenions afin de pouvoir proposer une sécurité maximale est un gros handicap pour les performances, nous ne savions pas dans quelles mesures cela nous empêcherait d’atteindre nos objectifs.\nNous souhaitions que l’espace occupé par la sonde sur le disque soit le plus faible possible et là encore nous ignorions s’il était possible d’y arriver vu la taille des sondes concurrentes.\nLa sonde devait pouvoir être embarquée sur la plus petite configuration possible sans en impacter les performances et là encore, au vu des performances des sondes concurrentes qui n’étaient pas faites pour ça, nous ignorions si c’était possible.\n\nAmélioration du temps de traitement des réponses et de la sécurité des événements de supervision\nDans le cas d’incident critique, le temps entre la récupération de l’information et l’envoi de l’alerte doit être le plus court possible. Nos concurrents étant parvenus à créer une sonde qui récupérait les informations dans la seconde, nous étions confiants sur cet aspect-là des choses. En revanche la suite du traitement est plus aléatoire et on observe chez l’ensemble de nos concurrents un délai de l’ordre de quelques minutes.\n\nDifficultés et verrous technologiques : \nFaire tomber le délai d’alerte sous la barre des deux minutes ;\nL’ensemble des critères de sécurité (cf. 4.3) doivent être respectés.\n\nRécupération d'indicateurs techniques sur des objets connectés\nDans le cadre de ce projet, l'innovation R&D prévoit d'apporter des réponses à des verrous technologiques encore non élucidés : \nArticulation objet physique (entité) et les applications ;\nPassage à l'échelle d'une architecture hybride (systèmes embarqués / clouds) ;\nAuto description sémantique de l'ensemble des couches du système.\n\nNous intervenons sur le deuxième point.\n\nAutomatisation de la construction des messages d'alerte\nL’automatisation de la construction de messages d’alerte est un domaine qui à notre connaissance n’a jamais été abordé. Le traitement du langage d’une manière générale en est encore à ses débuts et nous prévoyons de travailler avec d’autres entreprises (ex : Apicultur, Espagne) et des laboratoires sur ce sujet."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Projet SaT5G\n- **Incertitudes scientifiques** : Intégration des satellites dans le réseau 5G.\n- **Solutions à définir** :\n  - Backhaul et déchargement du trafic par satellite.\n  - Architecture de sécurité pour la connectivité de backhaul vers petites cellules (véhicules, zones isolées).\n- **Gestion des clés** : Modèle efficace pour réduire le temps de latence utilisateur dans les scénarios avec trafic satellite.\n- **Défis de latence** : Latence accrue et délais de sécurité à optimiser pour protéger le trafic sans passer par le réseau principal.\n- **Développements réalisés** :\n  - Scission entre contrôle et plan utilisateur.\n  - Correspondance avec différentes technologies physiques.\n  - Nouvelles méthodes d'échange de clés à faible latence pour Satcom.\n- **Verrou technique** : Protection de la sécurité dans des environnements partagés, où plusieurs opérateurs utilisent la même infrastructure NFVI.\n\n#### Projet 5Genesis\n- **Responsabilité d'Ekinops** : Mise en œuvre des prototypes multi-lien de SaT5G sur la plateforme OpenStack de Limassol.\n- **Verrou technologique principal** : Portage des technologies dans une infrastructure OpenStack, complexité d'intégration des machines virtuelles (VM) dans des domaines virtuels (DM).\n- **Autres verrous technologiques** :\n  - Interopérabilité des différentes technologies intégrées.\n  - Incertitudes sur la scalabilité et la capacité de montée en charge rapide.\n  - Optimisation de l'architecture pour un déploiement mondial massif.\n- **Normes 5G** : En cours d'élaboration, ajoutant une difficulté à la flexibilité des développements.\n\n#### Projet UNICORE\n- **Objectif** : Concevoir une solution pour compiler une application dans un environnement virtuel léger sans système d'exploitation traditionnel.\n- **Verrou principal** : Faire fonctionner un logiciel classique avec des ressources matérielles réduites tout en garantissant une haute performance.\n- **Questions à résoudre** :\n  - Réduction du nombre de librairies pour alléger l'environnement.\n  - Optimisation du temps de mise en œuvre de cette optimisation.\n- **Spécialisation et personnalisation** : Augmenter le taux de réutilisation du code pour éviter la construction manuelle des unikernels.\n- **Limitations techniques** : Migration des librairies et des bases de codes dans un temps limité.\n- **Automatisation** : Nécessité d'automatiser le processus d'optimisation pour garantir la meilleure configuration.\n- **Unikernels multi-plateformes** : Permettre la construction d'un unikernel pour plusieurs plateformes sans changement d'application, réduisant le temps de construction.\n- **Sécurité** : Minimiser l'exposition aux cyberattaques et défauts de production non corrigeables.\n- **Transfert de valeurs automatisé** : Garantir l'exécution sécurisée de contrats intelligents, neutraliser les contrats potentiellement défectueux.\n- **Barrières à l'utilisation des Unikernels** : Recherche de méthodes pour lever les obstacles à leur adoption, positionnant l'Europe pour exploiter cette technologie prometteuse."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\nSaT5G\n\nDans le cadre du projet SaT5G, nous avons fait face à des incertitudes scientifiques liés à l’intégration des satellites dans le réseau 5G. Nous devions définir des solutions optimales de backhaul et de déchargement du trafic par satellite. Nous devions définir une architecture de sécurité de la liaison par satellite utilisée pour la connectivité de backhaul vers de petites cellules, telles que celles montées sur des véhicules (avions, trains, bus) et pour une utilisation dans des zones isolées (zones non couvertes et mal desservies) ; Nous devions proposer un modèle de gestion efficace des clés pour réduire le temps de latence de l’utilisateur final dans les scénarios où une partie du trafic transite par l’infrastructure par satellite. Pour ces cas d'utilisation, une latence accrue et les délais de sécurité associés constituent en effet un défi. Par conséquent, la sécurité doit être optimisée de manière que le trafic puisse être protégé sans passer par le réseau principal. Nos développements ont couvert la scission entre le contrôle et le plan utilisateur et leur mise en correspondance avec différentes technologies physiques, ainsi que de nouvelles méthodes d'échange de clés à faible temps de latence pour Satcom ; Enfin, le dernier verrou technique consistait à assurer la protection de la sécurité des environnements partagés. En effet, de nombreux opérateurs exploitent leurs réseaux sur la même infrastructure, ce qui peut conduire à des situations dans lesquelles une faille de sécurité dans l’une des infrastructures de l’opérateur pourrait affecter les autres opérateurs utilisant le même NFVI. Nous devions donc trouver un moyen de palier à cette problématique. \n\n5Genesis\n\nDans le cadre du projet 5Genesis, Ekinops a été responsable de la mise en œuvre les prototypes multi-lien issus du projet SaT5G sur la plateforme OpenStack de Limassol. \n\nLe principal verrou technologique concerne le portage de ces technologies dans une infrastructure Openstack où nous devions faire face à des niveaux de complexité dus à l’intégration des machines virtuelles (VM) dans des domaines virtuels (DM), ce qui pose des problèmes spécifiques d’instanciation, d’adressage etc...\n\nNous avons également fait face à d’autres verrous technologiques concernant l’interopérabilité des différentes technologies que nous avons intégrées dans le cadre de ce projet. En outre, il restait encore de nombreuses incertitudes sur leur scalabilité ainsi que sur leur capacité à monter rapidement en charge. De plus, nous ne savions pas si l’architecture à laquelle nous allions aboutir était optimisée pour un déploiement massif de la technologie au niveau mondial.\n\nUn dernier verrou technologique concernait le fait que les normes de la 5G sont encore en cours d’élaboration. Elles sont donc très mouvantes. Cela ajoutait une difficulté supplémentaire à nos développements, qui devaient rester suffisamment flexibles pour être adaptés à une éventuelle modification des normes. Cependant, notre rôle de pionnier dans le développement de la 5G nous permet de contribuer à l’établissement des normes, ce qui sécurise notre projet. \n\nUNICORE\n\nComme évoqué dans la partie 1.1, l’objectif du projet UNICORE est de concevoir une solution qui permet de compiler une application dans un environnement virtuel très léger où il n'y a pas de système d'exploitation traditionnel, ce qui nous permettra d’apporter une réponse à notre verrou principal qui est de réussir à faire fonctionner un logiciel (machine virtuelle classique) avec des ressources matérielles moins lourdes tout en garantissant un haut niveau de performance. Comment pourrons-nous donc concevoir cet environnement très contraint ? Devrons-nous réduire au minimum le nombre de librairies afin alléger cet environnement ? Quelle est la démarche à suivre dans ce cas ? Comment optimiser le temps nécessaire à la mise en œuvre de cette optimisation ?\n\nEn effet, ce nouvel environnement a pour but d’assurer à nos utilisateurs un très haut niveau de spécialisation et de personnalisation, et dans ce cadre, nous devions trouver le moyen d’augmenter considérablement le taux de réutilisation du code pour éviter la construction manuelle des unikernels, nous avons également été confrontés aux limitations techniques liées à la migration des librairies et des bases de codes en une durée de temps limitée.\n\nPar ailleurs, assurer un haut niveau de spécialisation de notre solution requiert un temps considérable de recherche tout au long de son cycle de vie afin de garantir un haut niveau de performance. Nous devons donc trouver le moyen d’automatiser le processus d’optimisation pour garantir la meilleure configuration possible à tout instant.\n\nD’autre part, nous souhaitions offrir à nos utilisateurs la possibilité de construire un même unikernel pour plusieurs plateformes sans avoir à changer d’application, ce qui permettra de réduire le temps de construction d’unikernels de plusieurs mois à quelques heures voir à quelques minutes, sans avoir besoin de développement pour supporter plusieurs plateformes.\n\nNous devions également garantir un haut niveau de sécurité des systèmes déployés, dans ce cadre, nous devions trouver le moyen le plus efficace de minimiser l’exposition aux cyberattaques et les défauts de production des logiciels qui ne sont pas corrigeables.\n\nD’autre part, nous souhaitions introduire un système de transfert de valeurs automatisé fondé sur des conditions mutuellement convenues. L’objectif est d’empêcher une plateforme déterministe et sécurisée d’exécuter un contrat intelligent. En effet, nous devions trouver le moyen de garantir une exécution sans danger des contrats intelligents non sécurisés ou potentiellement défectueux, mais également de neutraliser les contrats intelligents expressifs et s’assurer que leur exécution se déroule d’une façon déterministe.\n\nDe plus, malgré le fait que les Unikernels soient considérés comme une technologie ayant un grand potentiel, ils ne sont en réalité utilisés que dans un nombre limité de produits et de domaines soit parce qu’ils sont considérés peu matures, soit parce qu’ils demandent une quantité de travail importante pour les développer. Notre objectif est donc de rechercher une méthode pour enlever ses barrières et ainsi offrir à l’Europe une position privilégiée pour exploiter cette technologie prometteuse."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques dans le Projet EFFLAM\n\n- **Modification de l'architecture de l'amplificateur :**\n  - Transition d'une architecture à pompage direct de la fibre active à une architecture double gaine.\n  - Source de verrous techniques liés aux composants de base, notamment les fibres dopées amplificatrices et passives.\n\n- **Travaux antérieurs :**\n  - Des recherches sur des fibres à 6 cœurs ont été réalisées, utilisant du pompage double gaine avec des fibres dopées uniquement en Er3+.\n\n- **Évolutions à anticiper :**\n  - Augmentation du nombre de cœurs à 12.\n  - Optimisation du positionnement des cœurs et de la géométrie de la fibre active pour un couplage efficace avec les fibres passives.\n    - Définition nécessaire de :\n      - La géométrie de la fibre et la position des cœurs.\n      - La nature du pompage.\n      - Le niveau de dopage des cœurs (concentration en erbium et ytterbium).\n\n- **Incertitudes scientifiques :**\n  - Niveaux de diaphonie entre fibres à minimiser.\n  - Simulations requises pour optimiser :\n    - Taille des cœurs.\n    - Ouverture numérique.\n    - Recouvrement modal.\n  - Outils de simulation à utiliser : Monte-Carlo, Beam Propagation Method (BPM).\n\n- **Compatibilité avec les composants passifs :**\n  - Incertitude sur la compatibilité dès la conception des fibres actives pour la fabrication des autres composants (combineurs de pompe, éclateurs).\n\n- **Couplage optique entre MCF :**\n  - Incertitude sur la reproductibilité industrielle tout en maintenant de bonnes performances (faibles pertes, faibles réflexions, faible diaphonie).\n\n- **Mutualisation des amplificateurs :**\n  - Impact potentiel sur les conditions d'exploitation du réseau, établissant des relations entre fonctions indépendantes.\n  - Technologie de pompage par la gaine choisie pour des raisons de coût et d’efficacité énergétique, induisant une corrélation entre les gains des cœurs.\n  - Incertitude sur l'impact sur les règles de conception des réseaux de transport optiques WDM, notamment pour les architectures des nœuds de brassage.\n\n- **Uniformité des gains :**\n  - Les gains des cœurs de l’amplificateur seront identiques, nécessitant que les pertes à compenser soient également identiques.\n  - Prise en compte de ce paramètre dans la conception des nœuds de brassage pour uniformiser les pertes des chemins optiques.\n\n- **Pertes d'insertion des composants internes des ROADM C/D/C :**\n  - Nécessité d'une amplification importante pour compenser les pertes d'insertion.\n  - Verrou technologique : identification et quantification des pertes d’insertion de cette technologie complexe et peu déployée.\n  - Études engagées pour spécifier l’amplificateur multicœurs à développer dans un nœud ROADM C/D/C.\n\n- **Définition de l'architecture :**\n  - Difficulté à concevoir une architecture utilisant des fonctions d’amplification unitaires avec le même gain pour exploiter l’amplificateur multicœurs."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nDans le cadre de notre projet EFFLAM, nous modifions fortement l’architecture de l’amplificateur par rapport aux amplificateurs double gaine existants qui n’utilisent qu’un seul cœur. Le passage d’une architecture d’amplificateur dite « à pompage direct de la fibre active par le cœur » à une architecture « double gaine » a été la source de verrous techniques liés aux composants de base et en particulier des fibres dopées amplificatrices et passives nécessaires à la fabrication des autres composants.\n\nDes travaux en fibre 6 cœurs ont déjà été menés comme le présente la Erreur ! Source du renvoi introuvable. ci-dessous. Il s’agissait d’une architecture d’amplificateur utilisant du pompage double gaine à base de fibres dopées uniquement Er3+.\n\nL’architecture présentée ci-dessus à la Erreur ! Source du renvoi introuvable. permet d’identifier les évolutions auxquelles le projet devra faire face :\nAugmentation du nombre de cœurs, avec un passage à 12 cœurs\nOptimisation du positionnement des cœurs et de la géométrie globale de la fibre active qui devra pouvoir être raccordée aux fibres passives, et assurer un couplage efficace de la puissance de pompe vers les cœurs dopés ; il faudra pour cela définir :\nLa géométrie de la fibre et la position des cœurs,\nLa nature du pompage,\nLe niveau de dopage des cœurs (concentration en erbium et ytterbium) de façon à optimiser l’efficacité de pompage.\n\nDe plus, nous avons rencontré une incertitude scientifique concernant les niveaux de diaphonie entre fibres, qui devront être minimisés. En outre, des simulations devront notamment porter sur l’optimisation de la taille des cœurs, de l’ouverture numérique, du recouvrement modal. Ces simulations nécessiteront l’utilisation d’outils de simulation de type Monte-Carlo ou Beam Propagation Method (BPM).\n\nD’autre part, nous avons pris en compte le fait qu’il subsistait une incertitude sur la compatibilité avec les composants passifs dès le début de la conception des fibres actives de façon à pouvoir réaliser les autres composants nécessaires à la fabrication de l’amplificateur que sont les combineurs de pompe et les éclateurs.\n\nNous avons également rencontré une incertitude scientifique pour assurer le couplage optique entre MCF avec un procédé industriel reproductible tout en gardant de bonnes performances (faibles pertes, faibles niveaux de réflexion, faible diaphonie, etc.).\n\nLa mutualisation des amplificateurs est susceptible de modifier les conditions d’exploitation du réseau en fixant des relations entre des fonctions auparavant indépendantes. En particulier, la technologie de pompage par la gaine retenue dans EFFLAM pour des raisons de coût et d’efficacité énergétique induit une corrélation entre les gains des différents cœurs. Cela engendre une incertitude scientifique sur l’impact de cette technologie sur les règles de conception des réseaux de transport optiques WDM, notamment via les choix qui seront faits pour les architectures des nœuds de brassage.\n\nLes gains des différents cœurs de l’amplificateur seront identiques de par l’architecture système de l’amplificateur. Cela implique que les pertes à compenser par les différents cœurs soient identiques. Ce paramètre doit être pris en compte dans la conception des nœuds de brassage, afin d’uniformiser au sein du nœud les pertes des différents chemins optiques à amplifier.\n\nLes composants internes des ROADM C/D/C nécessitent une grande quantité d'amplification pour compenser leurs pertes d'insertion. Le verrou technologique réside ici dans l’identification et la quantification des pertes d’insertion de cette technologie complexe et encore peu déployée. Nous avons engagé des études pour réaliser les spécifications de l’amplificateur multicœurs à développer dans un nœud ROADM C/D/C.\n\nLa difficulté a consisté à définir une architecture qui permet d’utiliser des fonctions d’amplification unitaires ayant toutes le même gain, afin de tirer profit de cette caractéristique de l’amplificateur multi cœur, qui présente le même gain sur tous les cœurs."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques dans le Projet EFFLAM\n\n- **Modification de l'architecture de l'amplificateur** :\n  - Transition d'une architecture à pompage direct de la fibre active à une architecture double gaine.\n  - Source de verrous techniques liés aux composants de base, notamment les fibres dopées amplificatrices et passives.\n\n- **Travaux antérieurs** :\n  - Réalisations sur des fibres à 6 cœurs avec pompage double gaine utilisant uniquement des fibres dopées Er3+.\n\n- **Évolutions à anticiper dans le projet** :\n  - Augmentation du nombre de cœurs à 12.\n  - Optimisation nécessaire du positionnement des cœurs et de la géométrie de la fibre active pour :\n    - Raccordement aux fibres passives.\n    - Couplage efficace de la puissance de pompe.\n  - Définition des éléments suivants :\n    - Géométrie de la fibre et position des cœurs.\n    - Nature du pompage.\n    - Niveau de dopage des cœurs (concentration en erbium et ytterbium).\n\n- **Incertitudes scientifiques rencontrées** :\n  - Niveaux de diaphonie entre fibres à minimiser.\n  - Simulations requises pour optimiser :\n    - Taille des cœurs.\n    - Ouverture numérique.\n    - Recouvrement modal.\n  - Outils de simulation nécessaires : Monte-Carlo, Beam Propagation Method (BPM).\n\n- **Compatibilité avec les composants passifs** :\n  - Incertitude sur la compatibilité dès la conception des fibres actives pour réaliser les composants nécessaires (combineurs de pompe, éclateurs).\n\n- **Couplage optique entre MCF** :\n  - Incertitude sur la reproductibilité industrielle tout en maintenant de bonnes performances (faibles pertes, faibles niveaux de réflexion, faible diaphonie).\n\n- **Mutualisation des amplificateurs** :\n  - Modifications potentielles des conditions d'exploitation du réseau.\n  - Corrélation entre les gains des différents cœurs due à la technologie de pompage choisie, impactant les règles de conception des réseaux de transport optiques WDM.\n\n- **Conception des nœuds de brassage** :\n  - Les gains identiques des cœurs impliquent que les pertes à compenser doivent également être identiques.\n  - Nécessité d'uniformiser les pertes des différents chemins optiques au sein des nœuds.\n\n- **Composants internes des ROADM C/D/C** :\n  - Besoin d'une amplification importante pour compenser les pertes d'insertion.\n  - Verrou technologique : identification et quantification des pertes d'insertion dans cette technologie complexe.\n\n- **Architecture d'amplification** :\n  - Définition d'une architecture permettant d'utiliser des fonctions d’amplification unitaires avec gain identique pour profiter de l’amplificateur multicœurs.\n\n- **Difficultés rencontrées en 2020** :\n  - Non-obtention de la puissance moyenne de sortie attendue.\n  - Besoin de recherche et développement supplémentaires pour identifier et corriger la source de la perte."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nDans le cadre de notre projet EFFLAM, nous modifions fortement l’architecture de l’amplificateur par rapport aux amplificateurs double gaine existants qui n’utilisent qu’un seul cœur. Le passage d’une architecture d’amplificateur dite « à pompage direct de la fibre active par le cœur » à une architecture « double gaine » a été la source de verrous techniques liés aux composants de base et en particulier des fibres dopées amplificatrices et passives nécessaires à la fabrication des autres composants.\n\nDes travaux en fibre 6 cœurs ont déjà été menés comme le présente la Figure 1.8 ci-dessous. Il s’agissait d’une architecture d’amplificateur utilisant du pompage double gaine à base de fibres dopées uniquement Er3+.\n\nL’architecture présentée ci-dessus à la Figure 1.9 permet d’identifier les évolutions auxquelles le projet devra faire face :\nAugmentation du nombre de cœurs, avec un passage à 12 cœurs\nOptimisation du positionnement des cœurs et de la géométrie globale de la fibre active qui devra pouvoir être raccordée aux fibres passives, et assurer un couplage efficace de la puissance de pompe vers les cœurs dopés ; il faudra pour cela définir :\nLa géométrie de la fibre et la position des cœurs,\nLa nature du pompage,\nLe niveau de dopage des cœurs (concentration en erbium et ytterbium) de façon à optimiser l’efficacité de pompage.\n\nDe plus, nous avons rencontré une incertitude scientifique concernant les niveaux de diaphonie entre fibres, qui devront être minimisés. En outre, des simulations devront notamment porter sur l’optimisation de la taille des cœurs, de l’ouverture numérique, du recouvrement modal. Ces simulations nécessiteront l’utilisation d’outils de simulation de type Monte-Carlo ou Beam Propagation Method (BPM).\n\nD’autre part, nous avons pris en compte le fait qu’il subsistait une incertitude sur la compatibilité avec les composants passifs dès le début de la conception des fibres actives de façon à pouvoir réaliser les autres composants nécessaires à la fabrication de l’amplificateur que sont les combineurs de pompe et les éclateurs.\n\nNous avons également rencontré une incertitude scientifique pour assurer le couplage optique entre MCF avec un procédé industriel reproductible tout en gardant de bonnes performances (faibles pertes, faibles niveaux de réflexion, faible diaphonie, etc.).\n\nLa mutualisation des amplificateurs est susceptible de modifier les conditions d’exploitation du réseau en fixant des relations entre des fonctions auparavant indépendantes. En particulier, la technologie de pompage par la gaine retenue dans EFFLAM pour des raisons de coût et d’efficacité énergétique induit une corrélation entre les gains des différents cœurs. Cela engendre une incertitude scientifique sur l’impact de cette technologie sur les règles de conception des réseaux de transport optiques WDM, notamment via les choix qui seront faits pour les architectures des nœuds de brassage.\n\nLes gains des différents cœurs de l’amplificateur seront identiques de par l’architecture système de l’amplificateur. Cela implique que les pertes à compenser par les différents cœurs soient identiques. Ce paramètre doit être pris en compte dans la conception des nœuds de brassage, afin d’uniformiser au sein du nœud les pertes des différents chemins optiques à amplifier.\n\nLes composants internes des ROADM C/D/C nécessitent une grande quantité d'amplification pour compenser leurs pertes d'insertion. Le verrou technologique réside ici dans l’identification et la quantification des pertes d’insertion de cette technologie complexe et encore peu déployée. Nous avons engagé des études pour réaliser les spécifications de l’amplificateur multicœurs à développer dans un nœud ROADM C/D/C.\n\nLa difficulté a consisté à définir une architecture qui permet d’utiliser des fonctions d’amplification unitaires ayant toutes le même gain, afin de tirer profit de cette caractéristique de l’amplificateur multi cœur, qui présente le même gain sur tous les cœurs.\n\nEnfin, au cours des travaux de l’année 2020, nous avons rencontré une difficulté supplémentaire, n’obtenant pas la puissance moyenne de sortie attendue. Cette difficulté nous a demandé des travaux de recherche et développement supplémentaire dans le but de trouver l’origine de la perte et de la corriger."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Modification de l'architecture de l'amplificateur** :\n  - Transition d'une architecture à pompage direct de la fibre active par un cœur à une architecture double gaine.\n  - Source de verrous techniques liés aux composants de base, notamment les fibres dopées amplificatrices et passives.\n\n- **Travaux antérieurs sur les fibres** :\n  - Réalisations sur des fibres à 6 cœurs avec pompage double gaine utilisant uniquement des fibres dopées Er3+.\n\n- **Évolutions à anticiper dans le projet** :\n  - Augmentation du nombre de cœurs à 12.\n  - Optimisation du positionnement des cœurs et de la géométrie de la fibre active pour un couplage efficace avec les fibres passives.\n  - Définition nécessaire de :\n    - La géométrie de la fibre et la position des cœurs.\n    - La nature du pompage.\n    - Le niveau de dopage des cœurs (concentration en erbium et ytterbium) pour optimiser l’efficacité de pompage.\n\n- **Incertitudes scientifiques identifiées** :\n  - Minimisation des niveaux de diaphonie entre fibres.\n  - Simulations nécessaires pour optimiser :\n    - Taille des cœurs.\n    - Ouverture numérique.\n    - Recouvrement modal.\n  - Outils de simulation requis : Monte-Carlo et Beam Propagation Method (BPM).\n\n- **Compatibilité avec les composants passifs** :\n  - Incertitude sur la compatibilité dès la conception des fibres actives pour la fabrication des combineurs de pompe et éclateurs.\n\n- **Couplage optique entre MCF** :\n  - Incertitude sur la reproductibilité industrielle du couplage optique tout en maintenant de bonnes performances (faibles pertes, faibles réflexions, faible diaphonie).\n\n- **Mutualisation des amplificateurs** :\n  - Impact sur les conditions d’exploitation du réseau, corrélation entre les gains des cœurs due à la technologie de pompage choisie pour des raisons de coût et d’efficacité énergétique.\n  - Incertitude sur les règles de conception des réseaux de transport optiques WDM, influencées par l'architecture des nœuds de brassage.\n\n- **Uniformisation des pertes dans les nœuds de brassage** :\n  - Les gains identiques des cœurs impliquent que les pertes à compenser soient également identiques, nécessitant une conception adéquate des nœuds.\n\n- **Pertes d'insertion dans les composants internes des ROADM C/D/C** :\n  - Identification et quantification des pertes d’insertion comme verrou technologique.\n  - Études engagées pour spécifier l’amplificateur multicœurs pour les nœuds ROADM C/D/C.\n\n- **Définition d'une architecture d'amplification unitaire** :\n  - Difficulté à concevoir une architecture permettant d’utiliser des fonctions d’amplification unitaires avec le même gain sur tous les cœurs.\n\n- **Difficultés rencontrées en 2020** :\n  - Problème de puissance moyenne de sortie non atteinte, nécessitant des travaux supplémentaires de recherche et développement pour identifier et corriger la source de la perte."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nDans le cadre de notre projet EFFLAM, nous modifions fortement l’architecture de l’amplificateur par rapport aux amplificateurs double gaine existants qui n’utilisent qu’un seul cœur. Le passage d’une architecture d’amplificateur dite « à pompage direct de la fibre active par le cœur » à une architecture « double gaine » a été la source de verrous techniques liés aux composants de base et en particulier des fibres dopées amplificatrices et passives nécessaires à la fabrication des autres composants.\n\nDes travaux en fibre 6 cœurs ont déjà été menés comme le présente la Figure 1.8 ci-dessous. Il s’agissait d’une architecture d’amplificateur utilisant du pompage double gaine à base de fibres dopées uniquement Er3+.\n\nL’architecture présentée ci-dessus à la Figure 1.9 permet d’identifier les évolutions auxquelles le projet devra faire face :\nAugmentation du nombre de cœurs, avec un passage à 12 cœurs\nOptimisation du positionnement des cœurs et de la géométrie globale de la fibre active qui devra pouvoir être raccordée aux fibres passives, et assurer un couplage efficace de la puissance de pompe vers les cœurs dopés ; il faudra pour cela définir :\nLa géométrie de la fibre et la position des cœurs,\nLa nature du pompage,\nLe niveau de dopage des cœurs (concentration en erbium et ytterbium) de façon à optimiser l’efficacité de pompage.\n\nDe plus, nous avons rencontré une incertitude scientifique concernant les niveaux de diaphonie entre fibres, qui devront être minimisés. En outre, des simulations devront notamment porter sur l’optimisation de la taille des cœurs, de l’ouverture numérique, du recouvrement modal. Ces simulations nécessiteront l’utilisation d’outils de simulation de type Monte-Carlo ou Beam Propagation Method (BPM).\n\nD’autre part, nous avons pris en compte le fait qu’il subsistait une incertitude sur la compatibilité avec les composants passifs dès le début de la conception des fibres actives de façon à pouvoir réaliser les autres composants nécessaires à la fabrication de l’amplificateur que sont les combineurs de pompe et les éclateurs.\n\nNous avons également rencontré une incertitude scientifique pour assurer le couplage optique entre MCF avec un procédé industriel reproductible tout en gardant de bonnes performances (faibles pertes, faibles niveaux de réflexion, faible diaphonie, etc.).\n\nLa mutualisation des amplificateurs est susceptible de modifier les conditions d’exploitation du réseau en fixant des relations entre des fonctions auparavant indépendantes. En particulier, la technologie de pompage par la gaine retenue dans EFFLAM pour des raisons de coût et d’efficacité énergétique induit une corrélation entre les gains des différents cœurs. Cela engendre une incertitude scientifique sur l’impact de cette technologie sur les règles de conception des réseaux de transport optiques WDM, notamment via les choix qui seront faits pour les architectures des nœuds de brassage.\n\nLes gains des différents cœurs de l’amplificateur seront identiques de par l’architecture système de l’amplificateur. Cela implique que les pertes à compenser par les différents cœurs soient identiques. Ce paramètre doit être pris en compte dans la conception des nœuds de brassage, afin d’uniformiser au sein du nœud les pertes des différents chemins optiques à amplifier.\n\nLes composants internes des ROADM C/D/C nécessitent une grande quantité d'amplification pour compenser leurs pertes d'insertion. Le verrou technologique réside ici dans l’identification et la quantification des pertes d’insertion de cette technologie complexe et encore peu déployée. Nous avons engagé des études pour réaliser les spécifications de l’amplificateur multicœurs à développer dans un nœud ROADM C/D/C.\n\nLa difficulté a consisté à définir une architecture qui permet d’utiliser des fonctions d’amplification unitaires ayant toutes le même gain, afin de tirer profit de cette caractéristique de l’amplificateur multi cœur, qui présente le même gain sur tous les cœurs.\n\nEnfin, au cours des travaux de l’année 2020, nous avons rencontré une difficulté supplémentaire, n’obtenant pas la puissance moyenne de sortie attendue. Cette difficulté nous a demandé des travaux de recherche et développement supplémentaire dans le but de trouver l’origine de la perte et de la corriger."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### One526 Shrink\n- Développement d'un produit compact pour le transport de lignes analogiques sur réseau IP.\n- Basé sur le produit précédent One526, mais plus grand que prévu (24 FXS, 8 BRI, 1 PRI).\n- Manque de support pour :\n  - Interfaces M.2 et mPCIe pour LTE.\n  - Entrée d'alimentation large gamme (actuellement 12 V fixe).\n  - Contacts E/S.\n- Choix de routage sur 4 couches dans un boîtier plus petit (210x120 mm) pour réduire les coûts.\n- Défis rencontrés :\n  - Intégration de M2 et mPCIe avec espace limité.\n  - Densification du routage sur PCB 4 couches.\n\n#### DDR RAM Issue Hynix\n- Anomalies détectées sur produits One25xx lors des tests de conformité.\n- Problèmes aléatoires de détection des composants RAM par le processeur, résolus par ressoudage.\n- Investigation R&D sur :\n  - Composant mémoire RAM.\n  - Contrôleur de mémoire intégré.\n  - Problèmes de soudure en production.\n\n#### OneOs2540 QuadPhy rev2 - Problème de Production\n- 25% des appareils avec la nouvelle version Ethernet PHY VSC8504 ne fonctionnaient pas correctement.\n- Validation initiale en R&D sans problème apparent.\n- Défi : identifier la cause des dysfonctionnements.\n\n#### 10G EAD\n- Implémentation de fonctionnalités dans un switch Marvell avec contraintes hardware.\n- Questions sur le nombre de files d'attente, access lists, moteurs de QoS.\n- Nécessité de développer une solution pour mapper le logiciel sur le matériel.\n\n#### Mempool Debug Application\n- Séparation du plan contrôle et management du plan de données dans OneOs6.\n- Utilisation de POS pour ARM et PPC, allocation mémoire Linux pour X86.\n- Outils de débogage intégrés dans TDRE, défis techniques pour assurer fonctionnalités avec commandes CLI « expert ».\n\n#### PKI Arbiter & Certificate Management\n- Protection des informations sensibles et garantie de provenance/intégrité.\n- Services à fournir :\n  - Authentification de clé publique.\n  - Création, remise, publication, vérification et révocation de certificats.\n\n#### OneOs 3540 - 64 bits Data Path\n- Difficultés techniques liées à des problèmes conceptuels en environnement 32 bits.\n- Identification et résolution de cas d'utilisation des pointeurs 32 bits.\n- Incertitude sur l'utilisation des memory rings pour la messagerie.\n\n#### Nouvelle Version pour la Distribution Yocto\n- Développement d'un SDK pour environnement mixte 32/64 bits.\n- Nécessité d'adapter Yocto et mise à jour de bibliothèques pour cohérence.\n\n#### Gestion Dynamique des Crypto Map pour VPN IPSEC\n- Configuration avec crypto map dynamique pour milliers de clients, recherche SA inefficace.\n\n#### LBB250\n- Problèmes de déploiement de la solution SD-WAN, intégration de G-SHDSL.\n- Études pour réduire coûts, validation des solutions optimales.\n- Développement d'un circuit FPGA pour connecter G-SHDSL au processeur.\n- Synchronisation des horloges pour le code FPGA.\n\n#### Quad Core ARM\n- Problèmes de hardware design sur plateforme Quad Core ARM (mémoire cache, SSD, module 4G).\n\n#### Software Disaggregation & Software Factory\n- Fiabilisation des données produites, documentation interne incomplète et erronée.\n- Identification des règles de documentation externe pour exploitation automatique.\n- Interprétation des données des outils d'analyse de codes.\n\n#### License Backoffice\n- Hétérogénéité des systèmes d'information et processus manuels dans le générateur de clés de licence.\n- Paramétrage du générateur selon cas d'utilisation (matériel ou logiciel).\n\n#### Incremental Update & Secure Boot\n- Conception d'une plateforme pour gérer des attaques futures inconnues.\n- Mise à jour non-invasive et adaptation aux normes futures.\n- Détermination du moment pertinent pour les vérifications d'authentification.\n\n#### Développement de Modules Logiciels pour Communications Voix\n- Analyse de l'impact des appels vocaux sur le trafic et qualité de la voix.\n- Performances initiales faibles par rapport aux objectifs.\n- Incertitude sur l'architecture logicielle pour améliorer les performances.\n- Implémentation du protocole STUN dans le data path du routeur pour performance élevée."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nOne526 Shrink \nNous souhaitions développer un produit compact prenant en charge le transport de lignes analogiques louées sur un réseau IP. Pour ce faire, nos développements se sont basés sur un produit précédent (One526) qui contient certains des éléments. Cependant, le produit est beaucoup plus grand que prévu car il prend en charge jusqu'à 24 FXS, 8 BRI et 1 PRI. De plus, il ne prend pas en charge : \nPour LTE, la prise en charge les interfaces M.2 et mPCIe.\nEntrée d'alimentation large gamme. Dans tous nos produits actuels, nous ne prenons en charge qu'une entrée d'alimentation fixe de 12 V\nContacts E / S\n\nAlors que le One526 est routé sur une six couches, nous avons décidé d’acheminer le One526 Shrink sur quatre couches dans un boîtier plus petit (210x120 mm) afin de réduire les coûts.\n\nDans le passé, nous implémentions déjà des E/S contacts, mais à la différence le même PIN peut être utilisée comme entrée ou sortie et que l'entrée doit avoir une résistance élevée. \n\nNous avions donc été confrontés aux défis suivants :\n Mettre M2 et mPCIe avec un minimum d'espace\nDensifier le routage sur un PCB 4 couches\nDDR RAM Issue Hynix \nSur la chaîne de production des équipements de la gamme One25xx, un certain nombre de produits ont relevé des anomalies lors des tests de conformité. Après investigation par la R&D, il est apparu des problèmes de détections des composants mémoires de type RAM par le processeur.  Ce problème était aléatoire et disparaissait si on ressoudait une nouvelle mémoire RAM sur l’équipement.  Nos travaux de R&D ont donc porté sur l’analyse de l’origine du problème :\nProblème sur le composant de mémoire RAM\nProblème lié au contrôleur de mémoire intégré dans le processeur\nProblème de soudure des composants sur la chaine de production\n\n\nOneOs2540 QuadPhy rev2- Production issue\nAprès que Microsemi était venu avec une nouvelle version de l’Ethernet PHY VSC8504, 25% des appareils produits ne fonctionnaient pas correctement.\n\nAu départ, nous avions validé cette révision en R&D et n'y voyions pas de problème.\nLe défi était donc de trouver la cause de ces problèmes. \n10G EAD\nNous avons implémenté les fonctionnalités d’un plan de données dans un switch de type Marvell. Un switch hardware (HW) a des limitations en termes de ressources nettement plus contraignantes que sur un logiciel. De plus, nous avons un certain nombre de questions concernant le nombre de files d’attentes à utiliser, le nombre d’access lists, le nombre de moteurs de QoS, …\n\nNous devons donc développer une solution pour mapper le software sur du hardware. Comment allons-nous donc effectuer les configurations ?\nMempool Debug application\nDans l’architecture de OneOs6, nous avons séparé le plan contrôle et management du plan de données.\nSur nos produits à base d’architecture ARM et PPC, le plan de données repose sur une système d’opération propriétaire appelé POS. La gestion de la mémoire allouée aux applications du plan de donnée et à la mémoire partagée avec le plan contrôle est gérée par une librairie issue du produit TDRE.\nDans OneOS6, nous l’utilisons sur nos produits basés sur l’architecure X86, nous utilisons l'allocation de mémoire Linux.\n\nDans TDRE, un certain nombre d'outils de débogage de la mémoire étaient intégrés, accessibles dans l'onglet « Débogage » de TMA. La principale difficulté technique a été de pouvoir assurer ces fonctionnalités avec les commandes CLI « expert ». \nNous devons ainsi détecter les fuites de mémoire, détecter la mémoire corrompue, …\n\nPKI Arbiter & Cerficate Management\nPour satisfaire l’obligation de sécurité des données, nos travaux ont porté sur la nécessité de protéger des informations sensibles. Il peut s’agir d’informations confidentielles qui ne doivent être connues que par certaines personnes. Mais aussi, il y a ce besoin de garantir la provenance et l’intégrité d’une information.\n\nLa PKI (Public Key Infrastructure) ou IGC en français (Infrastructure de Gestion de Clés) ou encore ICP (Infrastructure à Clés Publiques), a rendu le chiffrement, la signature et l’authentification transparentes pour les utilisateurs. Elle permet la gestion et la protection de certificats numériques. Nos incertitudes portaient donc à fournir les services suivants :\nAuthentification de clé publique\nCréation de certificats\nRemise du certificat au porteur\nPublication de certificat sur un annuaire\nVérification des certificats\nRévocation des certificats\nOneOs 3540\n64 bits Data Path\nLes principales difficultés techniques que nous avons rencontrées étaient liées aux nombreux problèmes conceptuels que nous avons dus traiter.\n\nDans un environnement 32 bits, les pointeurs sont placés dans des entiers 32 bits. Cette hypothèse a été explicitement utilisée dans un certain nombre de cas. Ces cas devront ainsi être identifiés et résolus.\n\nPar ailleurs, une autre incertitude technique était liée à l’utilisation des memory rings pour la messagerie : actuellement, les champs d'un memory ring sont des entiers de 32 bits. La majorité des pointeurs sont basés sur cela. Par exemple, tous les threads de communication gérant les différentes fonctionnalités du Data Path (dp_deployment), gestionnaire de time-out, messagerie Intercore dans le 2540, QOS shaper Voice queuing. Nous devons ainsi répondre à cette faiblesse. \nNouvelle version pour la distribution Yocto\nAu cours des développements concernant l’exécution dans un environnement mixe, nous avons développé notre SDK qui permet de combiner 32 bits control plane et 64 bits data plane. Le verrou technique résidait dans le fait que nous n’avions pas de référence pour connaître les bonnes pratiques. \n\nNous avons notamment rencontré une incertitude scientifique lors de nos travaux de développement. La solution fournie dans l’environement Yocto nous semblait une solution potentielle pour répondre aux besoins concernant nos attentes.\nCependant, nous avons pu constater que le développement de notre SDK nécessitait la transformation et l’adaptation de Yocto. Nous avons donc développé et mis à niveau certaines bibliothèques afin d’assurer une cohérence globale dans un environnement mixe.\nGestion dynamique des Crypto map pour des connections VPN IPSEC\nDans le cadre du développement de nouvelles solutions pour l’amélioration du processus de recherche SA. \n\nLa principale difficulté technique était liée à une configuration avec crypto map dynamique où des milliers de clients peuvent se connecter avec leur propre ACL, la recherche SA pour le chiffrement peut être très consommatrice. Actuellement, nous recherchons les SA en boucle sur des listes et / ou sous-listes d'ACL, ce qui n'est pas très efficace lorsqu'un grand nombre de SA et d'ACL sont impliqués.\n\nLBB250\nLes travaux sur ce sujet sont partis du constat que l’architecture existante de notre solution présentait des problèmes de déploiement de la solution SD-WAN. La principale incertitude scientifique à laquelle nous avons fait face dans le cadre de ces développements a donc été d’intégrer la technologie G-SHDSL. \n\nCette architecture étant manifestement très coûteuse, nous avons envisagé de mener des études afin de réduire le coût. Cependant, nous n’étions pas certains de pouvoir déployer la solution optimale. Nous avons donc dû la valider et remédier à ses limitations. La première incertitude était due à l’absence d’un bus parallèle pour connecter le port G-SHDSL au processeur. Afin d’y remédier, nous avons développé un circuit FPGA permettant de translater un bus série en un bus parallèle. La seconde incertitude consistait à déterminer le nombre de couches de routage pour assurer un compromis entre l’efficacité du routage et le prix.\n\nUne autre difficulté technique était liée au code FPGA. Pour rappel ce dernier permet principalement d’effectuer la conversion des messages envoyés via le bus SPI en messages pour le bus Intel Demux de la puce SHDSL (Lantiq Socrates). L’horloge du bus SPI étant indépendante de l’horloge interne du FPGA qui est utilisée pour contrôler le bus Demux certaines précautions ont dû être prises pour synchroniser les bus. Un problème typique est la lecture d’un bit (effectuée via un horloge A) alors que celui-ci est dans un état instable car en cours d’écriture (via une autre horloge B).\n\nQuad Core ARM\nLors du développement de notre plateforme Quad Core ARM, nous avons rencontré des problèmes sur différents aspects (mémoire cash, SSD, module 4G, …). Notre principale difficulté technique a donc été de définir le hardware design. Nous avons donc été amenés à détailler le schéma fonctionnel de tous les composants, les interfaces et la conception au niveau des composants du produit. \nSoftware Disaggregation & Software Factory\nVisibility Database\nLa principale difficulté technique que nous avons rencontrée était liée à la nécessité de fiabiliser les données produites. \n\nEn effet, la documentation interne n’était pas forcément dédiée à y extraire des données, elle n’était ni complète, ni à jour et parfois erronée. Elle a nécessité le développement et l’exploitation d’outils permettant d’en faire une analyse précise et de pouvoir y identifier les erreurs, les manques, ainsi qu’un processus de correction pour stabiliser l’information utile.\n\nDe plus, la documentation externe n’est pas entièrement cohérente. Il nous a donc fallu identifier les différentes règles qui la géraient pour pouvoir l’exploiter par des outils automatiques et ainsi retrouver les données nécessaires. Ces règles ne sont pas documentées et cela a nécessité une analyse complexe et itérative pour les identifier.\n\nQuant aux outils d’analyse de codes, ils ne sont pas sans fautes. Il nous a donc fallu interpréter les données produites dans le but de les fiabiliser.\nL’interprétation des interactions entre les licences logicielles, les obligations et interdictions imposées par ces dernières est un domaine complexe qui laisse place à beaucoup d’interprétation et de désaccord. \nLicense Backoffice\nLors du développement de notre générateur de clés de licence, la principale difficulté technique que nous avons rencontrée était liée à l’hétérogénéité des systèmes d’information avec des processus très manuels.   \n\nNos incertitudes ont également porté sur le paramétrage du générateur et son intégration en fonction du cas d’utilisation, la clé de license pouvant être liée à équipement matériel, ou à un service logiciel.  \n\nIncremental update & Secure Boot\nLa principale incertitude à laquelle nous avons fait face lors du développement de notre Trusted Platform Module pour nos Appliance et notre White Box était de concevoir une plateforme capable de gérer de futurs types d’attaques inconnus ou inexistants au moment de nos développements. En effet, une fois que notre élément de sécurité sera intégré à nos équipements, nous devrons être en mesure de le mettre à jour de manière non-invasive.\nDe même, notre module doit pouvoir être facilement mis à jour pour répondre aux futures normes. \n\nDe plus, une seconde incertitude concernait le processus d’authentification du logiciel chargé sur un équipement. Nous avons ainsi dû déterminer à quel moment de ce processus il était le plus pertinent de déclencher des actions de vérification. \n\nDéveloppement de modules logiciels pour le traitement des communications de type voix\nAu cours de nos développements sur l’amélioration de la performance et la sécurité de nos équipements, nous voulons comprendre comment la qualité de la voix évolue pour un nombre croissant d’appels. De plus, nous voulons comprendre l’impact des appels vocaux sur le trafic et sur la qualité de la voix.\n\nLes tests initiaux ont révélé des performances (nombre d’appels par seconde) très faibles par rapport à nos objectifs. \n\nLa principale incertitude scientifique à laquelle nous avons fait face dans le cadre de ces développements a donc été de déterminer quel type d’architecture logicielle permettrait de remédier à ces insuffisances. \n\nCependant, même si nous étions certains d’améliorer les performances en optimisant certaines parties du logiciel, nous n’étions pas certains que l’architecture du logiciel existant permettrait d’atteindre l’objectif fixé, nous avons donc dû les valider afin d’assurer les performances attendues lors de la montée en puissance de notre solution. \n\nConcernant le support du mode Media-Bypass, il a fallu trouver une solution permettant de traiter des requêtes STUN (protocole ICE) avec un niveau de performance élevée (chaque communication VoIP peut nécessiter une ou plusieurs requêtes STUN par seconde). La gestion du protocole STUN a donc été implémentée dans la partie « data path » du routeur (qui gère le transfert de données IP / RTP) au lieu de la partie « control plane » qui gère les protocoles, en particulier le SIP."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### Whitebox\n- Développement d'une Whitebox pour assurer l'interopérabilité entre transpondeurs 100G, 200G, 400G et un DSP de 100W.\n- Refonte de l'architecture de transmission des données pour extraire et supprimer les doublons.\n- Résultat : solution à coûts réduits et mise en œuvre simplifiée.\n\n#### Incertitudes scientifiques\n- **Gestion de la température :**\n  - Problème majeur lié à la surchauffe dans un espace restreint, augmentant le risque d'incendie.\n  - Composants de la Whitebox développés spécifiquement pour le châssis, entraînant des inconnues sur leur comportement thermique.\n  - Développement d'un système de ventilation pour gérer la température.\n\n- **Compatibilité technologique :**\n  - Difficulté à assurer l'interopérabilité entre les technologies NetConf et Yang.\n  - Nécessité de cette interopérabilité non précisée dans le texte.\n\n- **Tests et waveplan :**\n  - Incertitude durant la phase de tests : impossibilité d'obtenir un waveplan lors de la restauration.\n\n#### Verrous technologiques\n- **Composants haut-débit 400G :**\n  - Nouveaux composants, incertitude sur leur intégration dans le châssis de la Whitebox.\n  - Développements supplémentaires nécessaires en raison de pertes d'insertion constatées avec ces composants."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nWhitebox\nAu cours de nos travaux, nous avons développé une Whitebox qui est chargée d’assurer l’interopérabilité entre un transpondeur 100G, 200G, et 400G et un DSP (Digital Signal Processor) de 100W. Dans le cadre de ce développement, nous avons réalisé une refonte de l’architecture de notre fonction de transmission des données afin d’en extraire et supprimer les doublons. Nous avons ainsi obtenu une solution aux coûts plus faibles et à la mise en œuvre plus simple. \n\nLa principale incertitude scientifique que nous avons rencontrée était liée à la gestion de la température à l’intérieur du châssis. En effet, la Whitebox opère les mêmes fonctions de transmission que les autres boîtiers, mais dans un espace beaucoup plus restreint. Cela occasionne d’importants risques de surchauffe, et donc d’incendie, de l’équipement. Comme les différentes cartes composant l’architecture de notre Whitebox ont été développées spécifiquement pour ce châssis, il existait de nombreuses inconnues sur leur comportement thermique, et nous avons dû développer un système de ventilation permettant d’y pallier. \n\nUne autre difficulté à laquelle nous avons fait face a été d’assurer la compatibilité entre les technologies NetConf et Yang. Pourriez-vous nous préciser pour quelle raison il fallait assurer cette interopérabilité ? \n\nUn autre verrou technologique auquel nous avons été confrontés concernait les composants dédiés au haut-débit 400G. Ces composants étant tout à fait nouveaux, nous ne pouvions pas prévoir s’ils allaient s’encastrer correctement dans le châssis de la whitebox. Ainsi, nous avons dû mener des développements supplémentaires car nous avons constaté des pertes d’insertion avec ces composants. \n\nEnfin, la dernière incertitude scientifique que nous avons rencontrée s’est déclarée lors de la phase de tests car nous n’arrivions pas à obtenir de waveplan dans la restauration."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Objectif des travaux** : Recueillir de nouveaux signaux pour analyser les grandeurs du véhicule équipé du boîtier XeeConnect.\n\n- **Verrous identifiés** :\n  - Absence de méthodologie adaptée pour le temps réel et tous types de véhicules.\n  - Manque de documentation sur les bus-CAN par les constructeurs.\n  - Aucune spécification sur la logique ou le principe de codification des informations par les constructeurs.\n\n- **Signaux étudiés en 2017** :\n  - **Géolocalisation** :\n    - Difficultés rencontrées dues aux limites du GPS en zone urbaine.\n    - Problème d'estimation de la position en cas d'indisponibilité du réseau.\n    - Défi : Concevoir une solution utilisant de nouveaux composants pour déduire la position du véhicule en cas de perte de connexion, en se basant sur les dernières informations valides et les déplacements réalisés.\n\n  - **Consommation instantanée de carburant** :\n    - Hypothèse : Existence d'une donnée sur le réseau CAN liée aux injections de carburant dans la chambre de combustion pour obtenir le volume réel de carburant consommé en temps réel.\n    - Enjeux :\n      - Confirmer la circulation de cette information sur le bus CAN.\n      - Décoder cette donnée pour déterminer la consommation instantanée de carburant.\n    - Démarche : Identifier les variations de grandeurs dynamiques sous sollicitation lors d'accélérations.\n    - Complexité : Grand nombre d'ECUs sur les véhicules récents, chaque calculateur codé sur un nombre variable d’octets.\n    - Nécessité de réaliser une cartographie des adresses CAN pertinentes pour l'étude."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAnalyse des signaux\nL’objectif de ces travaux était de recueillir de nouveaux signaux pertinents pour réaliser des analyses sur les grandeurs du véhicule équipé du boîtier XeeConnect.\nLe principal verrou qui s’est posé dans ces travaux était lié à l’absence de méthodologie adaptée à notre besoin (temps réel et adapté à tout type de véhicule) et l’absence de documentation au sujet des bus-CAN par les différents constructeurs. Ainsi, aucune spécification ne précise la logique ou le principe adopté pour la codification d’une information par les constructeurs. \n\nEn 2017, notre étude a porté sur deux signaux en particulier :\nLa géolocalisation\nLa consommation instantanée de carburant ;\n\nL’étude de la géolocalisation a été à l’origine de difficultés.\nEn effet, le GPS atteint ses limites en zone urbaine et nous souhaitions dépasser ces limitations de performances. De plus, en cas d’indisponibilité du réseau, le GPS est incapable de fournir une estimation de la position du véhicule. Notre défi consistait alors à concevoir une solution, s’appuyant sur de nouveaux composants, permettant de déduire la position du véhicule même en cas de perte de connexion, à partir des dernières informations valides et les déplacements réalisés.\n\nDans le cas du calcul de la consommation instantanée de carburant, notre hypothèse principale portait sur l’existence d’une donnée sur le réseau CAN liée aux injections de carburant dans la chambre de combustion afin de disposer en temps réel, du volume réel de carburant consommé.\nLes enjeux de nos travaux était d’une part de confirmer que cette information circule sur le bus CAN et d’autre part de parvenir à décoder cette donnée pour déterminer la consommation instantanée de carburant.\nNotre démarche a alors consisté à identifier les variations de grandeurs dynamiques sous sollicitation en cas d’accélération. Ce travail s’est avéré très complexe en raison du grand nombre d’ECUs présents sur les véhicules récents. Chaque calculateur étant codé sur un nombre variable d’octets, nous avons dû réaliser une cartographie des adresses CAN qui présentent un intérêt pour notre étude."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Injection de fautes laser sur smartphone**  \n  - L'implémentation de l'attaque par injection de fautes optiques sur smartphones est inexplorée.\n  - Présence de nombreux challenges au niveau logiciel et matériel.\n\n- **Défis au niveau logiciel**  \n  - Recherche de l'état de l'art révèle peu d'attaques ciblant la séquence « secure boot ».\n  - Verrous identifiés : \n    - Accès au programme de démarrage.\n    - Interprétation du programme, en supposant un accès possible.\n    - Interférence avec l'exécution du programme.\n  - Problématique : Identifier un point d'entrée précoce dans l'exécution pour compromettre la sécurité.\n\n- **Défis au niveau matériel**  \n  - Difficulté d'accès aux zones d'intérêt pour l'attaque.\n  - Problèmes liés à l'architecture package-on-package :\n    - Zone cible située sous les mémoires DRAM.\n    - Défi d'écarter les composants sans les endommager ni perturber le fonctionnement du système."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\n\nInjection de fautes laser sur smartphone\nL’implémentation d’attaque par injection de fautes optiques reste inexplorée sur les smartphones et comporte de nombreux challenges, tant au niveau logiciel qu’au niveau matériel.\n\nAu niveau logiciel, nos recherches de l’état de l’art nous ont permis de constater que très peu d’attaques visaient la séquence « secure boot » de démarrage du système. Une telle attaque comporte en effet de nombreux verrous : l’accès à ce programme, son interprétation en supposant qu’on puisse y accéder et la manière d’interférer avec son exécution sont autant de défis que nous avons dû relever. Notre problématique consistait ainsi à identifier un point d’entrée, suffisamment tôt dans l’exécution du programme sur le système pour permettre d’attaquer cette séquence et compromettre toute la sécurité qui reposait dessus.\n\nDu point de vue matériel, nous avons été confrontés à une difficulté d’accès aux zones d’intérêt pour notre attaque. En effet, l’architecture package-on-package nous a posé des problèmes d’accessibilité à la zone cible qui se trouvait en dessous des zones mémoires DRAM. Le défi était alors de réussir à écarter ces composants sans les détériorer et sans perturber le fonctionnement du système."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### Injection de fautes laser sur smartphone\n- **État de l'art** : L'attaque par injection de fautes optiques sur smartphones est encore inexplorée.\n- **Challenges** :\n  - **Logiciel** :\n    - Peu d'attaques ciblent la séquence « secure boot » du système.\n    - Verrous identifiés :\n      - Accès au programme de démarrage.\n      - Interprétation du programme une fois l'accès obtenu.\n      - Interférence avec l'exécution du programme.\n    - Objectif : Identifier un point d'entrée précoce dans l'exécution du programme pour compromettre la sécurité.\n  \n  - **Matériel** :\n    - Difficulté d'accès aux zones d'intérêt pour l'attaque.\n    - Problèmes liés à l'architecture package-on-package :\n      - Zone cible située sous les mémoires DRAM.\n      - Défi : Écarter les composants sans les endommager ni perturber le fonctionnement du système."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nInjection de fautes laser sur smartphone\nL’implémentation d’attaque par injection de fautes optiques reste inexplorée sur les smartphones et comporte de nombreux challenges, tant au niveau logiciel qu’au niveau matériel.\n\nAu niveau logiciel, nos recherches de l’état de l’art nous ont permis de constater que très peu d’attaques visaient la séquence « secure boot » de démarrage du système. Une telle attaque comporte en effet de nombreux verrous : l’accès à ce programme, son interprétation en supposant qu’on puisse y accéder et la manière d’interférer avec son exécution sont autant de défis que nous avons dû relever. Notre problématique consistait ainsi à identifier un point d’entrée, suffisamment tôt dans l’exécution du programme sur le système pour permettre d’attaquer cette séquence et compromettre toute la sécurité qui reposait dessus.\n\nDu point de vue matériel, nous avons été confrontés à une difficulté d’accès aux zones d’intérêt pour notre attaque. En effet, l’architecture package-on-package nous a posé des problèmes d’accessibilité à la zone cible qui se trouvait en dessous des zones mémoires DRAM. Le défi était alors de réussir à écarter ces composants sans les détériorer et sans perturber le fonctionnement du système."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous techniques et incertitudes scientifiques** : Identification de plusieurs défis dans différents axes de travail lors des recherches.\n  \n- **Plan mécanique** : \n  - Nécessité de garantir la compacité de la solution pour intégration dans l'équipement roulant.\n\n- **Plan électronique** : \n  - Respect des normes ferroviaires en matière d'électromagnétisme.\n  - Assurance de l'acquisition de données et de la communication interne/externe de manière fiable.\n\n- **Conception énergétique** : \n  - Mise en œuvre de stratégies pour optimiser la consommation des systèmes, en accord avec les objectifs d'optimisation.\n\n- **Conception logicielle** : \n  - Établissement d'une communication bidirectionnelle entre le matériel roulant et les systèmes externes.\n  - Garantie de la sécurité des données.\n  - Développement des fonctionnalités en surcouche, mettant en avant :\n    - Modularité (conception carte mère - cartes filles spécialisées).\n    - Mises à jour à la volée.\n    - Sécurité.\n    - Maintenance prédictive.\n\n- **Performance du réseau** : \n  - Optimisation de l'utilisation de la bande de fréquence disponible pour la diffusion d'informations critiques dans des conditions environnementales difficiles.\n  - Développement d'algorithmes sophistiqués pour :\n    - Réduire les interférences.\n    - Maximiser l'efficacité du spectre.\n  - Importance de la détection et gestion des canaux libres pour un fonctionnement optimal du réseau.\n\n- **Traitement des données** : \n  - Défis liés au stockage sécurisé, à la transmission efficace et à l'analyse approfondie des données.\n  - Objectifs : résoudre les problèmes et améliorer la performance globale du système."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLors de la réalisation de nos travaux de recherche, nous avons été confronté à plusieurs verrous techniques et incertitudes scientifique quant à différents axes de travail. \n\nTout d'abord, sur le plan mécanique, nous devons garantir la compacité de la solution pour qu'elle puisse être intégrée facilement dans l'équipement roulant. Ensuite, sur le plan électronique, nous devons respecter les normes ferroviaires en matière d'électromagnétisme afin d'assurer l'acquisition des données et la communication interne et externe du système de manière fiable. Du côté de la conception énergétique, nous devons mettre en œuvre des stratégies pour optimiser la consommation des systèmes, conformément à nos objectifs d'optimisation.\n\nParallèlement, la conception logicielle représente un défi majeur, avec la nécessité d'établir une communication bidirectionnelle entre le matériel roulant et les systèmes externes, tout en garantissant la sécurité des données. De plus, toutes les fonctionnalités doivent être développées en surcouche, en mettant en avant notre proposition de valeur unique : la modularité (via la conception carte mère - cartes filles spécialisées), les mises à jour à la volée, la sécurité, et la maintenance prédictive, entre autres.  \nDe plus, pour améliorer la performance du réseau, il est essentiel d'optimiser l'utilisation de la bande de fréquence disponible, qui est essentielle pour la diffusion des informations critiques dans des conditions environnementales difficiles. Cela nécessite le développement d'algorithmes sophistiqués visant à réduire les interférences et à maximiser l'efficacité du spectre. La détection et la gestion des canaux libres revêtent également une importance capitale pour garantir un fonctionnement optimal du réseau. En outre, le traitement des données représente un autre défi majeur, impliquant le stockage sécurisé, la transmission efficace et l'analyse approfondie des données afin de résoudre les problèmes et d'améliorer la performance globale du système."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Source de données** : Utilisation d'un dataset privé de radiographies avec compte-rendu d'un radiologue.\n- **Annotations** : \n  - Les examens fournissent des observations et des radiographies associées.\n  - Les fractures sont annotées par des zones rectangulaires, ce qui peut entraîner une perte de précision.\n  - La majorité de la littérature utilise cette convention d'annotation.\n- **Impact de la perte de précision** : \n  - Généralement faible sur la détection des fractures, car celles-ci n'ont pas de contours nets.\n- **Visibilité des fractures** : \n  - Chaque zone annotée reçoit un qualificatif de visibilité (facilement visible ou non).\n  - Certaines fractures ne sont visibles que sur des coupes spécifiques.\n- **Entraînement des modèles** : \n  - Limité aux fractures visibles et peu visibles.\n  - Les fractures très peu visibles (zones noires) peuvent nuire à l'apprentissage en introduisant des incohérences.\n- **Difficultés anatomiques** : \n  - Présence d'ombres dues à la superposition de tissus mous.\n  - Objets ressemblant à des fractures peuvent compliquer la détection.\n- **Dataset utilisé** : \n  - Comprend 2514 examens et 7593 radiographies.\n  - Total de 8597 zones de détection, majoritairement visibles, mais avec une part significative de zones noires.\n- **Limitation des données** : \n  - Le nombre d'images est limité pour les techniques de deep learning.\n  - Nécessité de développer des modèles performants et convergents rapidement avec des jeux de données restreints.\n- **Agréger des données** : \n  - Objectif d'atteindre jusqu'à 60 000 images annotées issues de 20 000 examens.\n- **Architectures de réseaux** : \n  - Basées sur des modèles efficaces pour des tâches similaires, mais non identiques.\n  - Nécessité d'adapter ces architectures à la problématique spécifique de détection de fractures.\n  - Besoin de concevoir de nouvelles architectures optimisées pour la nature des images traitées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nL'entraînement et l'évaluation des réseaux de neurones que nous concevons sont faits sur un dataset privé de radiographies accompagnées d’un compte-rendu complet réalisé par un radiologue. Un examen permet d’obtenir une liste d’observations issues du compte-rendu ainsi qu’une liste de radiographies associées aux localisations de ces observations. Le compte-rendu permet ainsi d’annoter les images en spécifiant par exemple où se situe une fracture. La Figure 1-8 illustre un tel examen, avec en haut les observations, et en bas les clichés les illustrant.\n\nLes anomalies repérées à l’examen sont ainsi repérées par des zones rectangulaires. Cette contrainte peut impliquer une perte de précision mais elle convient généralement bien et la majorité de la littérature s’appuie sur cette convention. La perte de précision a en effet, en général, peu d’impact sur la détection d’une fracture, puisqu’il n’y a pas de contours nets et délimités.\n\nOn associe également à chaque zone un qualificatif de visibilité, précisant si la fracture est facilement visible sans indication du compte-rendu ou autre incidence. Il est important de noter qu’une fracture ne peut être visible que sur certaines coupes (vues).\n\nLa Figure 1-8 illustre les différentes vues d’une même fracture, sur lesquelles elle est plus ou moins visible.\n\nL’entraînement de nos modèles prédictifs est restreint aux fractures visibles et peu visibles. Les zones noires, correspondant aux fractures très peu visibles, peuvent freiner l’apprentissage en introduisant une annotation incohérente.\n\nÀ ces problèmes de visibilité s’ajoutent des difficultés liées à l’anatomie : par exemple, des ombres peuvent apparaître par superposition de parties molles, et de façon générale des objets ressemblant à des fractures peuvent être présents, comme illustré en Figure 1-10.\n\nPour entraîner nos modèles à la détection de fractures au niveau des pieds et des chevilles, nous avons utilisé un dataset comportant 2514 examens, soit 7593 radiographies. La distribution du nombre de radiographies par examen est représentée en Figure 1-10. Ces radiographies rassemblent au total 8597 zones de détection, dont la majorité sont visibles, ainsi qu’une part importante de zones noires. Ce nombre d’images reste toutefois limité pour utiliser les techniques de deeplearning. Nous devrons donc développer des modèles suffisamment performants, et convergeant suffisamment rapidement pour être efficace avec un entraînement réalisé sur des jeux de données limités. En parallèle, nous agrégeons toujours plus de données, pour obtenir des datasets plus complets allant jusqu’à 60 000 images annotées, récoltées au cours de 20 000 examens.\n\nPar ailleurs, les architectures de réseaux sur lesquelles nous nous basons sont issues de modèles qui se sont révélés efficaces sur des tâches similaires mais non-identique à celle qui nous intéresse (tâche de classification ou bien jeux de données d’une nature différente), et que nous adaptons pour notre problématique. Ce ne sont cependant pas des architectures optimales pour résoudre notre problème précis. Il nous faudra donc concevoir de nouvelles architectures adaptées à notre problématique et à la nature des images sur lesquelles nous travaillons."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Nature des images traitées :**\n  - Utilisation de radiographies pour l'entraînement et l'évaluation des réseaux de neurones.\n  - Dataset privé contenant des radiographies annotées par des radiologues, incluant des observations et des localisations de fractures.\n\n- **Problèmes d'annotation :**\n  - Les fractures sont annotées par des zones rectangulaires, ce qui peut entraîner une perte de précision car la majorité de la surface encadrée peut ne pas correspondre à une fracture.\n  - Alternatives comme des boîtes orientées ou des heatmaps ne sont pas applicables en pratique en raison de la difficulté à définir les contours des fractures.\n\n- **Visibilité des fractures :**\n  - La visibilité d'une fracture dépend de sa localisation et de l'angle de prise de vue.\n  - Nécessité d'associer un qualificatif de visibilité à chaque zone pour indiquer si la fracture est facilement identifiable.\n\n- **Restrictions d'entraînement :**\n  - Les modèles sont entraînés uniquement sur des fractures visibles et peu visibles pour éviter les incohérences dues aux fractures très peu visibles.\n\n- **Caractéristiques anatomiques du patient :**\n  - Superpositions de parties molles peuvent créer des ombres trompeuses.\n  - Présence de lignes de physe chez les enfants pouvant être interprétées comme des fractures.\n  - Besoin d'intégrer l'âge osseux du patient dans le modèle, nécessitant une estimation à partir des données morphologiques des radiographies.\n\n- **Difficultés d'apprentissage :**\n  - L'âge osseux n'est mentionné que pour la moitié des radiographies disponibles.\n  - L'âge réel peut différer de l'âge osseux, ce qui peut fausser l'apprentissage.\n  - La solution doit déduire l'âge osseux à partir de radiographies de différentes parties du corps, pas seulement des mains et poignets.\n\n- **Conception d'un outil unique :**\n  - Objectif de créer un outil applicable à l'ensemble du squelette humain pour augmenter le volume de données d'entraînement.\n  - Complexité accrue pour l'algorithme en raison des différences d'identification des fractures selon la localisation (hanche, genou, main).\n  - Difficulté à inférer la partie du corps à partir de la radiographie en raison de l'absence d'annotations standardisées.\n\n- **Architectures de réseaux :**\n  - Utilisation de modèles inspirés de tâches similaires, mais non optimaux pour le problème spécifique.\n  - Développement de nouvelles architectures adaptées à la nature des images traitées.\n  - Problème de consommation de ressources avec les techniques de type NAS, abordé par l'approche de weight sharing, mais avec des incertitudes sur son impact sur l'optimisation des paramètres.\n\n- **Évaluation des architectures :**\n  - Difficulté d'évaluer la qualité des corrélations entre évaluations indépendantes et celles en weight sharing.\n  - Utilisation de bases de données comme NASBench, contenant plus de 423 000 architectures uniques évaluées, pour guider le développement."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nUne partie des difficultés que nous avons rencontrées dans le cadre de ce projet réside dans la nature même des images traitées, c’est-à-dire les radiographies. L'entraînement et l'évaluation des réseaux de neurones que nous concevons sont faits sur un dataset privé de radiographies accompagnées d’un compte-rendu complet réalisé par un radiologue. Un examen permet d’obtenir une liste d’observations issues du compte-rendu ainsi qu’une liste de radiographies associées aux localisations de ces observations. Le compte-rendu permet ainsi d’annoter les images en spécifiant par exemple où se situe une fracture. La Figure 6 illustre un tel examen, avec en haut les observations, et en bas les clichés les illustrant.\n\nComme on peut l’observer, les anomalies repérées à l’examen sont mises en évidence par des zones rectangulaires. Cette contrainte convient généralement bien et la majorité de la littérature s’appuie sur cette convention, puisque la fracture n’a, a priori, pas de contours nets et délimités. Néanmoins, il est évident qu’elle peut impliquer une perte de précision car la grande majorité de la surface encadrée correspond à de la « non-fracture ». Si d’autres options sont facilement imaginables : des boîtes orientées, des heatmaps de détection, une segmentation au pixel près, etc. ; aucune d’entre elles n’est applicable en pratique car il est souvent impossible de définir précisément les délimitations de l’objet fracture.\n\nUn deuxième aléa réside dans la visibilité qu’offre une radiographie donnée d’une lésion, en fonction de sa localisation. En effet, une fracture peut n’être visible que sur certaines coupes, ou sous certains angles de prise de vue. Nous devons donc associer à chaque zone un qualificatif de visibilité, précisant si la fracture est facilement visible sans indication du compte-rendu ou autre incidence. La Figure 7 illustre les différentes vues d’une même fracture, sur lesquelles elle est plus ou moins visible.\n\nL’entraînement de nos modèles prédictifs est restreint aux fractures visibles et peu visibles car les zones noires, correspondant aux fractures très peu visibles, peuvent freiner l’apprentissage en introduisant une annotation incohérente.\n\nÀ ces premières difficultés s’ajoutent des aléas liés aux caractéristiques anatomiques du patient. A titre d’exemple, des ombres peuvent apparaître par superposition de parties molles. De façon générale, des objets ressemblant à des fractures peuvent être présents, sans pour autant témoigner de lésions. C’est notamment le cas des lignes de physe, qui s’intercalent entre les différentes parties d’un même os chez l’enfant. Ils ne sont pas mis en valeur par radiographie et apparaissent donc comme une rupture de la continuité des os que l’algorithme est susceptible d’interpréter comme une fracture. La Figure 8 illustre ce cas de figure. Face à ce problème, nous avons identifié le besoin d’intégrer l’âge osseux du patient à notre modèle. Une intégration qui requiert d’estimer cet âge à partir des données morphologiques qu’offre la radiographie étudiée. Pour permettre à notre modèle d’apprendre à effectuer cette inférence, nous nous basons sur les radiographies de notre base d’entraînement qui mentionnent l’âge du patient. Néanmoins, la phase d’apprentissage de notre modèle soulève plusieurs difficultés. En plus de n’être mentionné que pour la moitié des radios à notre disposition, l’âge réel de l’individu n’est pas forcément identique à son âge osseux (la différence pouvant s’élever à plusieurs années), ce qui est susceptible de fausser l’apprentissage de notre algorithme. Par ailleurs, notre problématique diverge de celle traitée dans la majorité de l’état de l’art du bone age assessment sur deux aspects principaux. En effet, notre solution devra apprendre à déduire l’âge osseux d’un patient à partir d’incidences beaucoup plus variées que simplement les mains et les poignets. Elle devra être capable d’inférer l’âge du patient à partir d’une radiographie de n’importe quelle partie de son corps, dont la majorité ne présente pas des caractéristiques morphologiques aussi explicites que les mains et les poignets. Enfin, notre objectif est légèrement différent. Nous ne cherchons pas à minimiser la différence globale entre âge osseux estimé et réel, mais plutôt à obtenir des résultats cohérents dans l’ensemble, mais beaucoup plus précis pour les enfants et les personnes âgées, qui figurent au centre de notre problématique.\n\nEnsuite, nous souhaitons concevoir un outil unique, applicable à l’ensemble du squelette humain. Cette volonté est à double tranchant. D’un côté, cela permet d’agréger les bases de données dont on dispose, peu importe la partie du corps qui figure sur la radiographie, et donc d’augmenter le volume de données d’entraînement. De l’autre, cela complique significativement la tâche de l’algorithme car les éléments permettant d’identifier une fracture ne sont pas les mêmes selon si elle se situe au niveau de la hanche, du genou ou de la main. Une solution a priori pertinente pour permettre à un modèle unique d’analyser une radiographie, peu importe la partie du squelette à laquelle elle s’apparente, est d’informer l’algorithme de quelle partie du corps il s’agit, avant qu’il ne l’analyse. Néanmoins, cela implique qu’un outil soit capable de l’inférer avec précision, ce qui pose un problème majeur. En effet, il n’existe pas d’unique annotation correspondant à la bonne information. L’identification de la partie du corps qui figure sur une radio est sujette à interprétation et dépend du radiologue qui l’annote. A titre d’exemple, il est difficile de faire la différence entre un radio de hanche, de pelvis ou de fémur. Cela est d’autant plus vrai que les frontières entre parties du corps sont elles-mêmes difficiles à établir. Par ailleurs, il n’existe pas d’ontologie définie et utilisée par tous : une même zone d’intérêt peut donc être qualifiée de nombreux termes différents.\n\nEnfin, les architectures de réseaux sur lesquelles nous nous basons sont inspirées de modèles qui se sont révélés efficaces sur des tâches similaires mais non-identiques à celle qui nous intéresse (il s’agissait souvent de tâches de classification ou bien de jeux de données de nature différente). Ce ne sont cependant pas des architectures optimales pour résoudre notre problème précis. C’est pourquoi nous travaillons également à la conception nouvelles architectures, sans précédent dans l’état de l’art, adaptées à notre problématique et à la nature des images sur lesquelles nous travaillons. Si le verrou principal associé à l’utilisation de techniques de type NAS, à savoir leur consommation très importante en termes de ressources, semble pouvoir être adressé par l’approche du weight sharing, cette dernière soulève elle-même un écueil important. De fait, les effets de cette approche sur l’optimisation des paramètres du réseau global demeurent obscurs. Plus particulièrement, la corrélation entre les évaluations des architectures, dans le cadre du weight sharing d’une part, indépendante d’autre part, n’est pas claire. En effet, si un modèle dont l’évaluation indépendante montre des performances faibles a été surévalué dans le cadre du weight sharing, son utilisation dans l’optimisation des paramètres sera néfaste pour le résultat final. Or il est en pratique très difficile d’estimer précisément la qualité de la corrélation susmentionnée car il est inenvisageable d’évaluer l’ensemble des architectures neuronales possibles de façon indépendante. Heureusement, des catalogues de telles évaluations ont été produits par des laboratoires spécialisés et très bien équipés pour la réalisation de telles tâches. Un exemple pertinent pour notre utilisation est la base de données NASBench, contenant plus de 423000 architectures uniques entraînées sur le jeu d’images CIFAR-10 et évaluées à l’aide de diverses métriques."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Mise en place d'un framework logiciel et algorithmique :**\n  - Objectif : Accélérer le développement de modèles d'intelligence artificielle.\n  - Complexité : Le framework sera plus lourd que ceux utilisés pour des applications moins avancées que le deep learning.\n  - Problématique : Gestion d'un grand nombre de types d'objets (images, rapports, modèles d'IA, jeux de données, labels).\n  - Impératifs : Assurer fluidité et intégrité des bases de données pour ne pas ralentir l'utilisation des outils.\n  - Interopérabilité : Garantir l'agencement facile des différentes briques d'intelligence artificielle.\n\n- **Intégration des outils sur les plateformes partenaires :**\n  - Problème : Chaque éditeur de PACS et de réseau informatique a ses propres protocoles.\n  - Solution recherchée : Intégration universelle sans nécessiter d'adaptations pour chaque client.\n  - Impact : Les adaptations sont chronophages et coûteuses, constituant un verrou pour l'intégration des produits.\n\n- **Évolution des solutions d'intelligence artificielle (BoneView et ChestView) :**\n  - Problème : Mauvaise classification des pathologies (faux positifs et faux négatifs).\n  - Objectif : Classifier la gravité des lésions.\n  - Action : Optimisation des algorithmes et développement de nouvelles versions des solutions.\n\n- **Détection de points d'intérêts sur images médicales :**\n  - Objectif : Détecter des pathologies via des mesures entre points d'intérêt sur radiographies du squelette de la main.\n  - Verrous à lever : Diminuer le nombre de paramètres et le temps de calcul des algorithmes.\n  - Action : Recherche approfondie pour identifier les algorithmes les plus pertinents.\n\n- **Gestion des données personnelles des patients :**\n  - Mission : Traiter des images radiographiques tout en respectant la confidentialité des données.\n  - Procédure d'anonymisation : Écarter la possibilité de réidentification des données pour se conformer à la réglementation RGPD.\n  - Verrous rencontrés : Accès illégitime aux données, modifications non désirées, disparition des données.\n\n- **Modèles NAS pour améliorer les performances :**\n  - Objectif : Comparer la performance des modèles NAS avec les modèles classiques pour classifier les fractures osseuses.\n  - Problématiques : Réduction du temps de calcul et de la quantité de données nécessaires pour l'entraînement des modèles."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous souhaitons dans un premier temps mettre en place un framework logiciel et algorithmique qui nous permettra d’accélérer la mise au point de nos modèles d’intelligence artificielle. Le framework envisagé sera en revanche beaucoup plus lourd et complexe que les framework classiquement utilisés pour des applications moins poussées que le deep learning. Cette problématique s’explique par le grand nombre de types d’objets à gérer : images, rapports d’analyse, comptes-rendus radiologiques, modèles d’intelligence artificielle, jeux de données, labels, etc. Nous sommes donc confrontés à des impératifs de fluidité et d’intégrité, car la multiplicité et la lourdeur des bases de données ne doit pas ralentir l’utilisation des outils. Aussi, puisque nous ambitionnons d’utiliser plusieurs briques d’intelligence artificielle, il nous est primordial de garantir l’interopérabilité des différentes briques, tout en pouvant facilement les agencer.\n\nLes outils que nous développons sont intégrés sur les plateformes de nos partenaires de santé. En revanche, chaque éditeur de PACS (pour Picture Archive and Communication System) et de réseau informatique fonctionne selon ses propres protocoles. Nous devions donc trouver un moyen d’intégration universelle aux outils des praticiens, qui ne nécessite pas une nouvelle adaptation à chaque nouveau client. Ces étapes sont en effet chronophages et coûteuses et représentent un réel verrou lors de l’intégration de nos produits chez les partenaires de santé.\n\nLes solutions d’intelligence artificielle que nous utilisons pour nos outils BoneView et ChestView sont également en constante évolution. Un problème récurrent auquel nous sommes confrontés est la mauvaise classification des pathologies détectées. Il convient en effet de limiter au maximum le nombre de faux positif ou faux négatif. De la même manière, il nous fallait être en mesure de classifier la gravité de la lésion. En ce sens, il nous a fallu optimiser les algorithmes d’intelligence artificielle en développant de nouvelles version de nos solutions BoneView et ChestView.\n\nLa détection de points d’intérêts permet, à partir d’images médicales, de détecter des pathologies à travers les mesures effectuées entre les points d’intérêt. Nous avons alors souhaité développer un modèle de détection des points d’intérêts sur des images de radiographies du squelette de la main. Les verrous principaux à lever étaient la diminution du nombre de paramètres et du temps de calcul des algorithmes. En ce sens, un profond travail de recherche a été effectué pour balayer l’ensemble des algorithmes sur lesquels nous pouvions nous appuyer afin de déterminer le plus pertinent pour notre analyse.\n\nGleamer a pour mission de traiter des images radiographiques issues d’examen médicaux. Les données récoltées par les cliniciens contiennent alors un lot d’informations relatives aux données personnelles des patients. Ces données doivent néanmoins rester confidentielles et ne pas quitter les serveurs des établissements de santé sans avoir subi une phase d’anonymisation. Nous souhaitions alors mettre en place une procédure d’anonymisation nous garantissant de pouvoir écarter la possibilité de réidentification des données une fois le processus appliqué. Ce processus d’anonymisation nous permettra alors de nous soustraire à la réglementation RGPD puisqu’aucune information personnelle des patients ne sera rattachée aux images que nous possédons. Les principaux verrous que nous avons rencontrés concernaient l’accès illégitime à des données personnelles, des modifications non désirées des données, ou encore la disparition des données.\n\nEnfin, les modèles NAS étant prometteur dans l’optique d’améliorer les performances des modèles d’intelligence artificielle, nous avons souhaité déterminer et comparer la performance de ces modèles par rapport aux modèles classiquement utilisés pour classifier les fractures osseuses. Nous étions alors confrontés à des problématiques de réduction du temps de calcul et de quantité de données nécessaires afin d’entrainer nos modèles."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Aléas et incertitudes scientifiques** :\n  - De nombreuses sources d'incertitudes persistent concernant le projet.\n  \n- **Potentiel de production d'hydrogène** :\n  - La détermination du potentiel de production d'hydrogène des mâchefers (matières premières) est incertaine.\n  - Trois mâchefers ont été testés lors de la thèse d'A. Boulanger.\n  - Nécessité de généraliser les observations établies à d'autres mâchefers.\n\n- **Reproductivité des résultats** :\n  - Les résultats doivent être fiables et réitérables pour établir la répétabilité des phénomènes.\n\n- **Transfert d'échelle** :\n  - Phase délicate nécessitant une recherche préalable importante (pilote et modélisation).\n\n- **Essais de production d'hydrogène** :\n  - Des essais ont été réalisés avec succès lors de la thèse d'Anthony Boulanger (2011).\n  - Les mécanismes de production d'hydrogène doivent encore être confirmés.\n\n- **Objectifs de l'équipe de recherche** :\n  - Reproduire les résultats novateurs obtenus.\n  - Comprendre et identifier les mécanismes de production d'hydrogène.\n  - Améliorer les rendements de production.\n  - Passer du laboratoire au prototype en vue d'une potentielle commercialisation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nPour atteindre les performances énoncées précédemment, de nombreuses sources d’incertitudes demeurent à ce jour et concernent principalement :\n\nLa détermination du potentiel de production d’hydrogène des différents mâchefers (matières premières). En effet, lors de la thèse (A. BOULANGER), 3 mâchefers ont pu être testés. Il est donc nécessaire de généraliser les observations établies.\nLa reproductivité des résultats qui doit être fiable et réitérable afin d’établir la répétabilité des phénomènes.\nEnfin, le transfert d’échelle, phase délicate et nécessitant une recherche en amont importante (pilote et modélisation).\n\nDes essais de production d’hydrogène à partir de mâchefers ont étés réalisés et se sont révélés concluant lors de la thèse d’Anthony BOULANGER soutenue en 2011. Les mécanismes à l’origine de cette production sont à confirmer.\n\nNotre équipe de recherche doit donc réussir à reproduire ces résultats novateurs, comprendre et identifier les mécanismes qui en sont à l’origine, améliorer les rendements, et aller vers un changement d’échelle pour passer du laboratoire au prototype en vue d’une potentielle commercialisation."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Contexte des travaux** : Recherche et développement réalisés en 2016 et 2017, rencontre de verrous technologiques.\n\n- **Importance de la gestion des ressources humaines** : \n  - Essentielle pour le bon fonctionnement d'une entreprise.\n  - Nécessité d'une bonne gestion du personnel et d'une planification efficace des affectations et du temps de travail.\n\n- **Limites de la littérature existante** : \n  - Ne définit pas tous les paramètres et contraintes à considérer.\n  - Manque de directives sur la modélisation des contraintes légales et des besoins des employés concernant leur environnement de travail.\n\n- **Travaux sur la planification et l’optimisation des temps de travail** : \n  - De nombreux travaux réalisés, mais modèles très spécifiques à des domaines particuliers.\n  - Difficulté de généralisation des solutions existantes, insuffisantes pour intégrer des contraintes complexes comme les préférences des employés.\n\n- **Objectifs des travaux** : \n  - Évolution des algorithmes usuels pour les généraliser.\n  - Répondre aux problèmes liés aux préférences des employés dans la planification.\n\n- **Étude de l'annualisation du temps de travail** : \n  - Peu d'études sur ce sujet, malgré son potentiel pour la planification.\n  - Nécessité d'intégrer de nouveaux paramètres et contraintes pour optimiser les plannings.\n\n- **Cas limites dans la planification** : \n  - Les accords d'annualisation, en plus de la législation classique, génèrent de nombreux cas limites à exploiter.\n  \n- **Développement d'un nouvel algorithme** : \n  - Objectif de créer un algorithme capable d'optimiser les contraintes habituelles tout en gérant les cas limites rencontrés."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de nos travaux de recherche et développement des années 2016 et 2017, nous avons rencontré un certain nombre de verrous technologiques. \t\n\nLa littérature souligne l’importance de la recherche en gestion des ressources humaines. Une bonne gestion du personnel, et une bonne planification des affectations et temps de travail sont essentiels au bon fonctionnement d’une entreprise. Toutefois, ces publications ne définissent pas précisément tous les paramètres et toutes les contraintes à prendre en compte, ni comment les modéliser. En particulier, il est complexe de modéliser mathématiquement les contraintes légales ainsi que les souhaits et besoins des employés vis-à-vis de leur environnement de travail. \n\nBeaucoup de travaux ont tout de même été menés sur la planification et l’optimisation des temps de travail. Les modèles proposés sont cependant très spécifiques à des domaines particuliers, chacun avec ses contraintes et ses paramètres. Les solutions existantes sont donc difficiles à généraliser et ne sont pas suffisantes pour tenir compte de contraintes avancées et complexes comme les préférences des employés. Nos travaux devront donc permettre de faire évoluer les algorithmes usuels pour les généraliser et permettre de répondre à ces problèmes. \n\nEnfin, l’annualisation du temps de travail a été assez peu étudiée. De tels accords d’entreprise apportent pourtant de nouvelles possibilités à la planification et requiert également de nombreux paramètres et contraintes en plus pour optimiser les plannings. Les accords d’annualisation, en plus de la législation classique sur le travail, créent de nombreux cas limites dans la planification que les entreprises souhaitent exploiter au maximum. Nos travaux doivent donc nous permettre de développer un algorithme optimisant les contraintes habituelles mais également capable de gérer tous les cas limites que l’on peut rencontrer."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous technologiques rencontrés en 2019 :**\n  - Impossibilité d'absorber une augmentation de volume de données clients.\n  - La solution Timesquare ne peut être lancée que sur une seule instance, limitant la scalabilité.\n  - Nécessité d'une refonte totale de Timesquare pour répondre au besoin de scalabilité.\n\n- **Difficultés lors de la refonte en Cloud Native :**\n  - Verrous liés à l'architecture interne du programme.\n  - Problèmes de sécurisation des processus du programme.\n  - Intégration des moyens de sécurité dans le développement de l'application Cloud Native.\n\n- **Aléas rencontrés :**\n  - Complexité liée à la multitude de modes de connexions à sécuriser.\n\n- **Impact sur l'équipe :**\n  - Mobilisation d'une grande partie de l'équipe d'ingénieurs.\n  - Nécessité d'une phase de recherche et d'état de l'art préliminaire.\n  - Mise en place de protocoles de tests et de maquettage pour avancer dans le projet."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nAu cours de nos travaux R&D de l’année 2019, nous avons rencontré de nombreux verrous technologiques. Un premier verrou, à l’origine de ces nouveaux travaux a été l’impossibilité d’absorption d’une augmentation de volume de données clients. En effet, notre solution Timesquare ne pouvant que se lancer que sur une instance, il n’était pas possible de multiplier le nombre de machines dans le but d’effectuer les calculs en parallèle. C’est donc ce besoin de scalabilité qui a impliqué une refonte totale de Timesquare. \n\nLors de notre démarche de refonte en Cloud Native, nous avons rencontré de nombreux verrous liés à l’architecture interne de notre programme. En effet, un autre verrou au cours de nos travaux fût lié à la sécurisation des différents processus du programme et à l’intégration des moyens de la sécurité dans le développement de notre application en Cloud Native. \n\nAussi, nous avons fait face à des aléas liés à la multitude des modes de connexions à sécuriser.\n\nAu cours de l’année, ces verrous technologiques ont mobilisé une grande partie de notre équipe d’ingénieurs et impliqué une phase de recherche et d’état de l’art préliminaire, ainsi que la mise en place de protocoles de tests et de maquettage. Les travaux sont détaillés ci-dessous."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous technologiques rencontrés en 2020 :**\n  - Impossibilité d'absorption d'une augmentation de volume de données clients.\n  - Solution Timesquare limitée à une seule instance, empêchant le calcul parallèle sur plusieurs machines.\n  - Besoin de scalabilité entraînant une refonte totale de Timesquare.\n\n- **Difficultés lors de la refonte en Cloud Native :**\n  - Problèmes liés à l'architecture interne du programme.\n  - Verrou sur la sécurisation des processus du programme.\n  - Intégration des moyens de sécurité dans le développement de l'application Cloud Native.\n\n- **Aléas rencontrés :**\n  - Multiplicité des modes de connexions à sécuriser.\n\n- **Impact sur l'équipe :**\n  - Mobilisation d'une grande partie de l'équipe d'ingénieurs.\n  - Nécessité d'une phase de recherche et d'état de l'art préliminaire.\n  - Mise en place de protocoles de tests et de maquettage."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours de nos travaux R&D de l’année 2020, nous avons rencontré de nombreux verrous technologiques. \nUn premier verrou, à l’origine de ces nouveaux travaux a été l’impossibilité d’absorption d’une augmentation de volume de données clients. En effet, notre solution Timesquare ne pouvant que se lancer que sur une instance, il n’était pas possible de multiplier le nombre de machines dans le but d’effectuer les calculs en parallèle. C’est donc ce besoin de scalabilité qui a impliqué une refonte totale de Timesquare. \n\nLors de notre démarche de refonte en Cloud Native, nous avons rencontré de nombreux verrous liés à l’architecture interne de notre programme.\nEn effet, un autre verrou au cours de nos travaux fût lié à la sécurisation des différents processus du programme et à l’intégration des moyens de la sécurité dans le développement de notre application en Cloud Native. \n\nAussi, nous avons fait face à des aléas liés à la multitude des modes de connexions à sécuriser.\n\nAu cours de l’année, ces verrous technologiques ont mobilisé une grande partie de notre équipe d’ingénieurs et impliqué une phase de recherche et d’état de l’art préliminaire, ainsi que la mise en place de protocoles de tests et de maquettage. Les travaux sont détaillés ci-dessous."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous rencontrés :**\n  - Niveau élevé de décentralisation visé par la blockchain.\n    - Nécessité d'un protocole de consensus compatible avec la décentralisation.\n    - Objectif d'assurer un temps d'exécution rapide et des transactions peu coûteuses.\n\n- **Ouverture de la blockchain via une gateway :**\n  - Sécurisation des clés cryptographiques.\n  - Gestion des transactions pour maintenir la cohérence du réseau.\n  - Monitoring de l'ajout de nouveaux blocs.\n  - Conversion des LUM en devises FIAT intégrée dans la solution.\n\n- **Mise en expérimentation :**\n  - Cas d'usage spécifique : récompense de créateurs de contenu (avis clients).\n  - Identification de nouvelles contraintes :\n    - Nécessité d'une blockchain publique pour une utilisation ouverte et transparente."}, {"role": "assistant", "content": "Les verrous auxquels nous avons été confrontés étaient nombreux :\nEn effet, un premier est lié au niveau élevé de décentralisation visé par notre blockchain. Nous devions ainsi nous assurer d’instaurer un protocole de consensus compatible avec cette contrainte et permettant d’atteindre une véritable décentralisation. Nous souhaitions en outre assurer un temps d’exécution très rapide et avoir des transactions peu coûteuses, allant dans le sens de la décentralisation visée.\n\nConcernant l’ouverture de notre blockchain à travers une gateway, des verrous spécifiques nécessitaient d’être levés. En particulier, la sécurisation des clefs cryptographiques, la gestion des transactions pour assurer la cohérence de l’ensemble dans le réseau et enfin le monitoring de l’ajout de nouveaux blocs. La conversion des LUM en devises FIAT devait également être assurée directement au sein de notre solution.\n\nEnfin, la mise en expérimentation de notre blockchain dans un cas d’usage spécifique de récompense de créateurs de contenu de type avis clients nous a permis d’identifier de nouvelles contraintes. En effet, pour envisager une utilisation ouverte et transparente, la blockchain sous-jacente doit nécessairement être publique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Système de Représentation 3D**:\n  - Utilisation de photographies 2D soumises par l'utilisateur pour créer une représentation 3D.\n  - Aléas liés à la qualité des photos : \n    - Présence d'objets (meubles).\n    - Mauvais éclairage.\n    - Photographies floues.\n\n- **Incertitudes Scientifiques**:\n  - Développement d'un système robuste pour une visualisation 3D réaliste des mesures d'espace.\n  - Complexité d'établir une représentation 3D prenant en compte :\n    - Dimensions.\n    - Orientation.\n  - Dépendance à la qualité des images 2D prises par les utilisateurs avec leurs Smartphones.\n\n- **Technologie de Réalité Augmentée**:\n  - Intégration d'une technologie de réalité augmentée pour le positionnement d'objets.\n  - Verrous technologiques :\n    - Gestion du positionnement d'objets par superposition dans la pièce.\n    - Prise en compte de la taille des objets et des perspectives pour un positionnement optimal.\n  - Technologies existantes (ARKIT, ARCORE, Hololens) ne permettent pas de proposer un design complet de la pièce.\n  - Développement d'une technologie de virtualisation et de positionnement des meubles, intégrant un moteur de Machine Learning basé sur ARKIT et ARCORE.\n\n- **Moteur Sémantique pour la Décoration**:\n  - Conception d'un moteur sémantique pour optimiser la recherche de décoration et de mobiliers d’intérieur.\n  - Incertitude sur le développement d'une sémantique adaptée à la décoration, en fonction des mots clés utilisés par l'utilisateur.\n\n- **Identification Automatisée du Mobilier**:\n  - Développement d'un système d'identification automatique du mobilier sur des images via Deep Learning.\n  - Difficulté de créer un outil de reconnaissance d'image capable de :\n    - Reconnaître un mobilier enregistré sur la plateforme.\n    - Tagger automatiquement le mobilier lors du traitement d'image par le serveur d’Hubstairs.\n\n- **Service d'Affichage d'Images et de Produits**:\n  - Incertitude sur le développement d'un service pour :\n    - Afficher toutes les images web présentant un mobilier donné.\n    - Accéder aux sites des partenaires vendeurs.\n    - Proposer des éléments similaires ou alternatifs.\n  - Nécessité de crawler automatiquement les marketplaces et sites de tendances pour identifier des produits.\n  - Besoin d'un algorithme de Deep Learning puissant, actuellement indisponible."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNotre système propose à l’utilisateur de soumettre des photographies 2D de son logement pour en créer une représentation 3D. Seulement, nous avons dû tenir compte des aléas rencontrés par ce genre de fonctionnement, liés principalement à la qualité ou même à la présence d’objets sur les photos, i.e. des meubles, mauvais éclairage, photographies floues, etc.  \n\nUne des incertitudes scientifiques de notre projet était donc d’avoir un système robuste fournissant la visualisation 3D la plus proche de la réalité possible concernant les mesures d’espace. Plus précisément, la complexité ici est d’établir une représentation prenant en compte de l’ensemble des caractéristiques de l’espace (dimensions, orientation, etc.), et ce, à partir d’images 2D dont la qualité n’est pas assurée (puisque ce sont des images prises par nos utilisateurs avec leurs Smartphones).  \n\nEn plus du moteur de génération 3D, nous prévoyons d’implanter une technologie de réalité augmentée. Les principaux verrous technologiques liés à ces travaux consistent à gérer le positionnement d’objet dans la pièce par superposition. Cette étape est particulièrement complexe puisque le positionnement doit tenir compte de la taille des objets et des perspectives pour être positionné de façon optimale dans la pièce. Il existe aujourd’hui des technologies de réalité augmentée : ARKIT, ARCORE et des lunettes telles que les lunettes Hololens de Microsoft, mais nous souhaitons développer notre propre technologie de virtualisation et de positionnement des meubles.  \n\nLes technologies existantes permettent, dans une pièce vide, de placer un mobilier sélectionné par l’utilisateur. Cependant, elles ne permettent en aucun cas de proposer un design complet de la pièce (avec association de produits). C’est la raison pour laquelle nous développons notre propre technologie de réalité augmentée, possédant son propre moteur de Machine Learning, sur la base des technologies ARKIT et ARCORE.  \n\nNous souhaitions également concevoir un moteur sémantique adapté à la recherche de décoration et de mobiliers d’intérieur. Un tel moteur permettrait d’optimiser le temps de l'utilisateur sur Hubstairs en le dirigeant plus efficacement vers ce qu'il recherche.  \n\nL’incertitude était alors relative au développement d’une sémantique spécifiquement adaptée à la décoration, en songeant à ce que l’utilisateur recherche lorsqu’il entre des mots clés précis dans le moteur.  \n\nEnfin, une dernière incertitude portait sur le développement d'un système permettant d’identifier de manière automatisée le mobilier sur des images grâce au Deep-Learning. Le but était d’identifier automatiquement des produits sur les images.  \n\nUne première difficulté portait sur le développement de l’outil de reconnaissance automatique d’image basé sur le Deep Learning. En effet, cet outil devait être capable de reconnaître un élément de mobilier dans une photo si cet élément est enregistré parmi les mobiliers disponibles sur notre plateforme. Cela signifie que notre outil doit être capable de tagger automatiquement le mobilier, au moment du traitement de l’image par le serveur d’Hubstairs, en enregistrant sa présence et sa localisation. Cette fonctionnalité n’est pas une problématique classique en traitement de l’image de nos jours.  \n\nLe développement du service permettant d'afficher toutes les images présentes sur le web présentant un mobilier donné, d'accéder directement au site du partenaire vendeur de ce mobilier, ainsi que de présenter des éléments similaires ou alternatifs constitue une incertitude non négligeable. En effet, il fallait être capable de crawler automatiquement les marketplaces et les sites de tendances afin d’identifier automatiquement un produit dans les images de ces sites. Cela requiert un algorithme de Deep Learning très puissant qui n’est pas à disposition aujourd’hui."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### Question Answering\n\n- **Question Answering adversarial** :\n  - État de l'art avancé en anglais grâce au dataset SQuAD2.0.\n  - Modèles entraînés sur SQuAD2.0 atteignent un F1 score > 90%, surpassant parfois la performance humaine.\n  - Peu d'études sur le Question Answering adversarial dans d'autres langues que l'anglais.\n  - Absence de dataset de Question Answering adversarial en dehors de SQuAD2.0.\n\n- **Incertitudes concernant d'autres langues** :\n  - Aucune certitude sur la possibilité de mener à bien la tâche en langues autres que l'anglais.\n  - Pas de ressources ou datasets disponibles pour confirmer la faisabilité.\n\n- **Volume de questions sans réponse** :\n  - Incertitude sur le nombre de questions adversariales à annoter pour des modèles performants.\n  - Estimation initiale : annotation de 10 à 20 000 questions sans réponse en 2021.\n  - Si le volume nécessaire dépasse cette estimation, redéfinition du plan d'action et des objectifs sera nécessaire.\n\n- **Seuil de performance** :\n  - Zone de flou sur le seuil de performance à atteindre pour considérer les modèles comme utilisables.\n  - Questions sur le F1 score requis : 70%, 80% ou 90% ?\n\n- **Temps d'inférence des modèles** :\n  - Importance d'avoir des modèles avec des ressources raisonnables en mémoire et processeur.\n  - Nécessité d'un temps d'utilisation court pour des prédictions en temps réel.\n  - Modèles compacts (LePetit, CamemBERT-base) montrent des performances plus faibles que des modèles plus grands (CamemBERT-large).\n  - Écarts de performance potentiellement plus importants pour la tâche de Question Answering adversariale.\n\n- **Méthodes de compression de modèles** :\n  - Nécessité d'utiliser des méthodes de compression à l'état de l'art pour atteindre les objectifs.\n\n- **Utilisation de modèles multilingues** :\n  - Incertitude sur l'intérêt d'utiliser des modèles multilingues (XLM-RoBERTa, BERT multilingue).\n  - Question : L'utilisation de modèles multilingues, entraînés sur SQuAD2.0 et FQuAD2.0, permet-elle de surpasser les performances d'un modèle francophone entraîné uniquement sur FQuAD2.0 ?"}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\n**Question Answering**\n\n**Question answering adversarial**\n\nLe Question Answering adversarial est une tâche où l'état de l'art est assez avancé en anglais, notamment grâce au dataset SQuAD2.0 (https://arxiv.org/abs/1806.03822). Les modèles entrainés à l'aide de ce dataset atteignent un F1 score supérieur à 90%, dépassant la performance humaine pour certains. Toutefois, ce paradigme n'a été que très peu étudié pour d'autres langues que l'anglais. À notre connaissance, aucun dataset de Question Answering adversarial n'existe en dehors de SQuAD2.0.\n\nNous n'avons donc pas de certitudes sur le fait que la tâche de Question Answering adversariale puisse être menée à bien dans d'autres langues que l'anglais, notamment car aucune ressource ou dataset n'existe jusqu'à maintenant dans aucune langue autre que l'anglais pour confirmer la faisabilité de ce défi.\n\nDans l'hypothèse où l'apprentissage de cette tâche serait possible pour un modèle de langage francophone, une incertitude existe également sur le volume de questions sans réponse (donc adversariales) nécessaire à annoter pour obtenir des modèles performants et utilisables dans des cas d'usages industriels comme ceux rencontrés par ILLUIN. En effet, annoter des datasets volumineux est à la fois coûteux et chronophage. Nous estimons possible au début de ces travaux d'annoter entre 10 et 20 000 questions sans réponse en 2021, mais si la limite critique se situe au-delà de ce volume, nous serions dans l'obligation de redéfinir notre plan d'action et nos objectifs pour ces travaux de recherche.\n\nEnfin, une zone de flou existe même aussi sur la position de ce seuil de performance à atteindre pour que nous puissions considérer les modèles obtenus comme utilisables dans des cas d'usages métier : Est-ce à partir de 70% de F1 score ? ou plutôt 80% ? ou 90% ?\n\nUn autre axe d'incertitude autour de ces travaux réside dans le temps d'inférence des modèles obtenus à l'issue de ces recherches. Il est crucial pour des applications dans des systèmes en production d'avoir des modèles nécessitant à la fois des ressources raisonnables en termes de mémoire et de processeur, mais aussi que leur temps d'utilisation pour effectuer une prédiction soit court pour qu'ils puissent être utilisés en temps réel. Or, nous savons, comme cela a pu être montré dans https://arxiv.org/pdf/2002.06071.pdf ou https://arxiv.org/pdf/2010.03813.pdf que des modèles de language plus compacts comme LePetit ou CamemBERT-base ont aussi des performances plus faibles que de plus gros modèles comme CamemBERT-large, notamment pour des tâches de Question Answering. La tâche de Question Answering adversariale, potentiellement encore plus difficile, pourra voir ces écarts se creuser.\n\nIl sera donc probablement nécessaire d'utiliser des méthodes de compression de modèles à l'état de l'art pour atteindre nos objectifs afférents à cette problématique.\n\nDernière incertitude : l'intérêt de l'utilisation de modèles de language multilingues tels que XLM-RoBERTa (https://arxiv.org/pdf/1911.02116.pdf) ou BERT multilingue (https://arxiv.org/pdf/1906.01502.pdf), pour compenser une quantité de données plus faible en français par rapport à l'anglais : Est-ce que l'utilisation d'un tel modèle, entraîné à la fois sur SQuAD2.0 et FQuAD2.0, permet de dépasser les performances d'un modèle uniquement francophone entraîné seulement sur FQuAD2.0 ?"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte du Projet**: Développement d'une solution alternative à l'utilisation d'avions pour réaliser des inventaires forestiers via des drones.\n\n- **Première Incertitude**: \n  - Qualité des données collectées par drones pour la prédiction de la biomasse.\n  - Drones ayant une puissance d'impulsions moindre que les avions, nécessitant une vérification de l'impact sur la qualité des données.\n  - Possibilité de densifier le nombre de mesures au mètre-carré, tout en respectant un critère économique.\n\n- **Deuxième Difficulté**: \n  - Combinaison des données LIDAR des drones avec des données préexistantes.\n  - Objectif d'éviter un maillage trop fin ou un excès de mesures au mètre-carré.\n  - Développement d'une méthode innovante pour combiner ces données avec les données terrain préexistantes.\n  - Élaboration d'un protocole de relevés pour atteindre un compromis entre coût et qualité.\n\n- **Absence de Ressources**:\n  - Manque de ressources scientifiques et techniques disponibles sur le sujet.\n  - Nécessité de développer un prototype avancé pour tester et faire évoluer les modèles.\n\n- **Verrou Technologique**: \n  - Structuration de la base de données à concevoir.\n  - Incertitudes sur les attributs des données LIDAR et des données terrain.\n  - Travail approfondi sur l'aspect évolutif et la structuration de la base de données.\n\n- **Incertitudes Supplémentaires**:\n  - Problèmes liés à la géo-position relative des placettes de calibrage entre la mesure terrain et l'acquisition aérienne.\n  - Prévision de tests de plusieurs méthodes de calage.\n\n- **Adaptation des Modèles Existants**:\n  - Les différences de mesure peuvent affecter la qualité des modèles.\n  - Adoption d'une approche par type de peuplement pour l'adaptation des modèles."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu regard de notre solution imaginée, alternative à l’utilisation d’avions, pour réaliser des inventaires forestiers, nous sommes confrontés à certains verrous technologiques.\n\nLa première incertitude à lever concernant l’utilisation des drones consiste à nous assurer que la qualité des données soit suffisante afin de mener nos travaux de prédiction de la biomasse. En effet, le drone étant de plus faible ampleur qu’un avion, il est naturel que la puissance de ses impulsions soit moindre : nous devons ainsi vérifier que cette composante n’altère pas la qualité des données, quitte à prévoir une densification du nombre de mesures au mètre-carré en gardant à l’esprit le critère économique que nous nous sommes fixé.\n\nCette incertitude est directement liée à une seconde difficulté, qui est la combinaison de données issues de relevés à l’aide du dispositif LIDAR embarqué sur les drones, avec les données préexistantes. En effet, si nous souhaitons nous affranchir d’un maillage trop fin ou d’un nombre trop important de mesures au mètre-carré, nous avons pensé à une méthode innovante de combinaison de ces données avec les données terrain préexistantes. Pour cela, nous développerons un protocole de relevés pour arriver à un compromis coût-qualité.\n\nL’absence de ressources scientifiques et techniques à ce sujet nous contraint à être inventifs et à mettre au point un prototype à un stade avancé afin de le tester et faire évoluer nos modèles. Cette difficulté se traduit en un verrou en termes de structuration de la base de données à concevoir. Ne disposant pas d’informations au sujet des attributs qu’auront conjointement les données LIDAR et les données terrain, nous mènerons un travail de fond sur l’aspect évolutif de la base de données ainsi que sur sa structuration. Enfin, le recueil des données fait naître des incertitudes supplémentaires ayant trait à la géo-position relative des placettes de calibrage entre la phase de mesure terrain et l’acquisition aérienne : nous testerons ainsi plusieurs méthodes de calage. Concernant l’adaptation des modèles existants, les différences liées à la mesure sont susceptibles de modifier la qualité de ceux-ci. Une approche par type de peuplement va ici être adoptée."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Verrou de synchronisation des pompes :**\n  - Difficulté à synchroniser les deux pompes du système de pompage pour le piégeage des cellules tumorales.\n  - Nécessité d'éviter le point nul (moment où les pompes se déplacent en phase opposée), entraînant des problèmes de synchronisation.\n  - Mauvaise synchronisation peut causer un reflux, entraînant un retour des cellules piégées et une mauvaise filtration.\n\n- **Maitrise des paramètres des pompes :**\n  - Complexité dans le contrôle des paramètres d'accélération et de décélération des pompes.\n  - Incertitude sur le choix de la typologie de pompe (pompe seringue, pompe pression, pompes statiques).\n  - Impact du choix de matériel sur la qualité du piégeage indéterminé.\n\n- **Problématique de fluidité du circuit :**\n  - Certaines cellules s'abîment lorsque le sang circule pendant 20 minutes.\n\n- **Gestion du processus utilisateur :**\n  - Nécessité de permettre à l'utilisateur de modifier le processus (générer et exporter le code pour mise en fonctionnement).\n  - Objectifs contradictoires : créer des briques précises et détaillées sans complexifier l'utilisation, tout en évitant de simplifier au point de compromettre la qualité de la programmation.\n\n- **Développement de la carte électronique :**\n  - Difficulté liée à l'utilisation dans le domaine médical.\n  - Observations de fortes émissions radiofréquence lors des tests.\n  - Nécessité de réduire ces émissions sans affecter le fonctionnement de la carte électronique.\n  - Réalisation de plusieurs itérations en modifiant composants, plans d'alimentation et filtres hardware."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe verrou de cette opération de R&D est lié à la synchronisation des pompes utilisées pour le piégeage des cellules tumorales. En effet, la synchronisation des deux pompes du système de pompage était difficile puisqu’il était nécessaire d’éviter le point nul. Le point nul est le moment où les pompes se trouvent dans une phase de mouvement opposées, ce qui entraine des problèmes lors de leur synchronisation. La difficulté principale était liée à la synchronisation de ce système de pompage alternatif. Une mauvaise synchronisation causait un reflux ce qui pouvait engendrer un retour des cellules piégées et donc une mauvaise filtration des cellules tumorales.  \nLa maitrise des paramètres d’accélération et décélération des pompes était complexe. De plus, le choix de la typologie de la pompe à utiliser n’était pas connu. Il était nécessaire d’étudier trois types de pompes (pompe seringue, pompe pression et pompes statiques). L’impact du choix de matériel de la pompe sur la qualité du piégeage était également indéterminé.  \nUne autre problématique majeure a concerné la fluidité du circuit. Certaines cellules s’abimaient quand le sang circulait pendant 20 minutes.  \nUne autre problématique a concerné la gestion du processus qui devait donner l’option à l’utilisateur de faire des modifications (générer le code et l’exporter pour le mettre en fonctionnement). Le verrou était lié aux objectifs contradictoires de cet axe, qui consistaient à créer des briques à la fois précises et détaillées sans les rendre trop complexes pour l'utilisateur, tout en évitant de simplifier les modèles au point de compromettre la qualité de la programmation du processus de piégeage.  \nLa dernière problématique de ce projet a concerné le développement de la carte électronique du système de piégeage. La difficulté principale était liée à l’utilisation du système dans le domaine médical. En effet, suite aux tests réalisés, nous avons remarqué des émissions radiofréquence très importantes. La difficulté était de diminuer ces émissions radiofréquence sans impacter le fonctionnement de la carte électronique. Nous avons donc réalisé plusieurs itérations en changeant les composants, les plans d’alimentation et les filtres hardware…"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Gestion des Gaz**:\n  - Défi principal : circulation efficace des gaz (CO2 et O2) entre cuves.\n  - Technologie d'onde stationnaire : prometteuse pour la séparation et la culture cellulaire sans contact direct, mais complique la diffusion homogène des gaz.\n  - Problème de propagation du CO2 : impacte la diffusion rapide et uniforme, essentielle pour maintenir un pH stable (7 à 7,4) pour la croissance cellulaire optimale.\n  - Régulation précise du pH : cruciale pour l'intégrité et la viabilité des cellules.\n\n- **Perspectives de l'Article de C. Jacquin (2018)**:\n  - Sujet : BioRéacteurs à Membranes (BàM).\n  - Utilisation de la spectroscopie de fluorescence tridimensionnelle (3DEEM) pour surveiller la matière organique dissoute (DOM).\n  - Rôle de la DOM dans le colmatage des membranes.\n  - Adaptabilité des techniques pour surveiller certaines substances dans le bioreacteur : nécessite des calibrations spécifiques et une infrastructure analytique complexe et coûteuse.\n  - Verrou technologique significatif pour l'application directe dans le projet.\n\n- **Étude de S. Fiset (2000)**:\n  - Sujet : cinétique de croissance des cellules végétales en fonction des niveaux de O2 et CO2.\n  - Importance du contrôle des gaz dissous.\n  - Différences de réponse des cellules humaines aux niveaux de gaz dissous par rapport aux cellules végétales : incertitudes scientifiques dans la transposition des modèles.\n  - Mise en place de contrôleurs adaptatifs pour réguler les niveaux de gaz : nécessite une infrastructure technique sophistiquée.\n  - Comprend des capteurs précis et des systèmes de contrôle en temps réel, ajoutant complexité et coût."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL'un des défis principaux réside dans la gestion efficace de la circulation des gaz (CO2 et O2) entre les différentes cuves. La technologie d'onde stationnaire, bien que prometteuse pour séparer et cultiver les cellules sans contact direct, complique la diffusion homogène des gaz nécessaires à la régulation du pH et de la température. Le temps de propagation du CO2 est particulièrement problématique. La diffusion rapide et uniforme du CO2 est essentielle pour maintenir un pH stable entre 7 et 7,4, condition indispensable pour la croissance cellulaire optimale. Ce problème de diffusion impacte directement notre capacité à maintenir un environnement contrôlé, car une régulation précise du pH est cruciale pour l'intégrité et la viabilité des cellules.\n\nL'article de C. Jacquin (2018) sur les BioRéacteurs à Membranes (BàM) nous offre des perspectives intéressantes, notamment l'utilisation de la spectroscopie de fluorescence tridimensionnelle (3DEEM) pour surveiller la matière organique dissoute (DOM) et comprendre son rôle dans le colmatage des membranes. Bien que ces techniques puissent être adaptées pour surveiller la concentration de certaines substances dans notre bioreacteur, elles nécessitent des calibrations spécifiques et une infrastructure analytique complexe et coûteuse, ce qui constitue un verrou technologique significatif pour une application directe dans notre projet.\n\nD'autre part, l'étude de S. Fiset (2000) sur la cinétique de croissance des cellules végétales en fonction des niveaux du O2 et du CO2 met en évidence l'importance du contrôle des gaz dissous. Cependant, les cellules humaines ont des réponses différentes aux niveaux de gaz dissous, et la transposition directe des modèles de croissance de cellules végétales aux cellules humaines comporte des incertitudes scientifiques. De plus, la mise en place de contrôleurs adaptatifs pour réguler ces niveaux de gaz nécessite une infrastructure technique sophistiquée, incluant des capteurs précis et des systèmes de contrôle en temps réel, ajoutant une couche de complexité et de coût."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif du projet :**\n  - Développer un algorithme pour proposer des créneaux de rendez-vous médicaux optimisés.\n  - Intégrer cet algorithme dans un système d’information flexible pour la gestion des rendez-vous et de la patientèle.\n\n- **Problématique actuelle :**\n  - Les médecins utilisent des créneaux réguliers sans tenir compte des spécificités des patients.\n  - Cette approche entraîne une uniformisation des protocoles de soin, des vides dans les agendas et des retards.\n  - Difficulté à équilibrer le temps d’intervention du personnel soignant et la minimisation de l’attente des patients.\n\n- **Solutions envisagées :**\n  - Développement de méthodes pour anticiper les protocoles et la durée des consultations spécifiques.\n  - Création d'algorithmes pour remplir l’agenda et réduire les temps d’attente.\n  - Introduction d’un assistant médical travaillant en binôme avec le praticien.\n\n- **Approche statistique :**\n  - Évaluation des durées de consultation basée sur le profil du patient (type et motif de consultation, historique, antécédents).\n  - Mise en place d’un protocole de prise de rendez-vous demandant aux patients de préciser leurs motifs de visite.\n\n- **Développement de l'algorithme :**\n  - Algorithme de découpage et de remplissage de l’agenda pour maximiser les créneaux proposés et minimiser les creux.\n\n- **Solution logicielle :**\n  - Prévision de la durée des rendez-vous et proposition de créneaux en temps réel.\n  - Absence d'offres similaires sur le marché.\n\n- **Regroupement des professionnels de santé :**\n  - Les professionnels se regroupent pour améliorer les services, mais manquent de logiciels adaptés.\n  - Nécessité d’un environnement logiciel adapté pour faciliter la transition vers des centres de santé regroupés.\n\n- **Objectif de la solution logicielle :**\n  - Développer une plateforme SaaS accessible sur tous les terminaux.\n  - Intégration de plusieurs modules informatiques (logiciel médical, administratif, portail web).\n  - Développement d’une API pour accéder à une Base de Données unique, essentielle pour le fonctionnement du cabinet.\n\n- **Enjeux de l'API :**\n  - Importance d’une API bien conçue pour éviter des problèmes futurs.\n  - Difficulté à trouver une solution globale pour la construction et la maintenance de l’API.\n  - Évolution fréquente de la Base de Données nécessitant des mises à jour des applications.\n\n- **Objectif de l'API :**\n  - Approche globale pour faciliter l’implémentation serveur et client.\n  - Outil pour générer automatiquement le code client dans différents langages.\n  - Signalement des changements de structure des données lors de la compilation.\n\n- **Incertitudes :**\n  - Capacité à anticiper statistiquement les durées de consultation avec une variance réduite.\n  - Réactions des médecins et patients face à des consultations personnalisées et à l’introduction d’un assistant médical.\n  - Capacité du système à gérer les urgences et imprévus.\n\n- **Hypothèses et tests :**\n  - Hypothèses moyennes prises en compte, tests à venir pour obtenir des retours.\n\n- **Verrous techniques concernant l'API :**\n  - Gestion des conflits côté serveur avec plusieurs modules en communication parallèle.\n  - Complexités d’architecture pour obtenir un générateur de code systématique et propre.\n\n- **État d'avancement :**\n  - Version beta de l’API développée après 6 mois, mais nécessite plusieurs mois de développement supplémentaires pour maturation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nUn objectif du projet était de construire un algorithme qui permette de proposer des créneaux de rendez-vous médical pour les patients, avec un horaire et une durée permettant d'optimiser l'occupation du personnel médical en minimisant l'attente des patients. Puis d’intégrer cet algorithme dans un système d’information complet et souple pour faciliter la prise de rendez-vous et la gestion de la patientèle des médecins.\n\nUn médecin traditionnellement propose des créneaux réguliers dans son agenda (par exemple toutes les 15 ou 20 minutes), sans prendre en compte la spécificité des patients, faute d’organisation et de systèmes d’information permettant une autre approche. Ces rendez-vous recouvrent pourtant des motifs de consultation, des pathologies et un environnement médico-social plus ou moins complexes, et nécessitent des protocoles de prise en charge très variés. Le résultat de cette approche est non seulement une tendance à l’uniformisation des protocoles de prise en charge au détriment de la qualité, mais également de nombreux vides et retards dans l’agenda des praticiens. L’équilibre entre temps d’intervention du personnel soignant et minimisation de l’attente des patients est particulièrement compliqué à trouver, et représente une source significative d’optimisation du fonctionnement de la prise en charge médicale en soin primaire.\n\nNotre réflexion porte à la fois sur le développement de méthodes d’anticipation des protocoles et de la durée de consultation spécifique à chaque patient, sur des algorithmes de remplissage de l’agenda pour limiter les « trous » et l’attente inutile, et sur la prise en compte de protocoles d’organisation innovants et spécifiques (notamment l’introduction d’un assistant médical qui travaille en binôme avec le praticien).\n\nAu lieu d’assigner une durée fixe de consultation, on se fonde sur une approche statistique tenant compte du profil du patient (type de consultation, motif de consultation, historique de consultations, antécédents, etc.) pour évaluer un temps de consultation individualisé et probable. Cela est rendu possible notamment par un protocole de prise de rendez-vous spécifique demandant au patient de préciser dans un certain nombre de cas les motifs de sa visite.\n\nEnsuite, nous avons mis au point un algorithme de découpage et de remplissage de l’agenda visant à tenir compte de la disponibilité à un moment donné du médecin et de son assistant, à minimiser les creux dans l’agenda tout en maximisant le nombre de créneaux proposés au patient.\n\nLa solution logicielle que nous développons sera capable de prévoir la durée du rendez-vous pour le patient, de proposer une liste de créneaux de rendez-vous médicaux possibles au patient en temps réel, et de remplir l'agenda du médecin. Aucune offre logicielle sur le marché ne propose aujourd’hui une telle approche.\n\nD’une manière plus générale les professionnels de santé tendent à se regrouper pour mettre en commun leurs savoirs et offrir de meilleurs services à leurs patients. Or il n’existe pas de logiciel adapté à de telles structures, et la transition des médecins vers ce type de centres est donc très difficile.\n\nCette transition n’est possible que dans la mesure où les centres de ce type mettent à disposition des professionnels un environnement logiciel adapté. Notre objectif est donc de proposer une solution logicielle dans l’air du temps (Cloud Computing), présentant les avantages du SaaS et offrant aux médecins de multiples fonctionnalités allant de la prise de rendez-vous en ligne à l’aide au diagnostic.\n\nCette plateforme logicielle devra être accessible depuis tous les terminaux connectés, et regroupera plusieurs modules informatiques (logiciel médical sur tablettes, logiciel administratif, portail web …etc.). Pour assurer le fonctionnement d’un cabinet médical Ipso, ces diverses plateformes, développées dans des langages informatiques différents, doivent toutes être en mesure d’accéder à une seule et même Base de Données. Cette contrainte nécessite le développement d’une API de connexion à la BD, qui, par son rôle centrale dans le système d’information de cabinet, représente un enjeu majeur pour la réussite de notre solution.\n\nL’importance de cet enjeu est d’autant plus vraie qu’une API mal conçue va poser de nombreux problèmes par la suite. Il existe une multitude de briques techniques existantes (protocoles, normes, frameworks, ...) pour faciliter la standardisation et la construction de ces API. Il est en revanche difficile de trouver une solution qui propose une approche globale pour mettre en place une API client/serveur et assurer sa maintenance. Au-delà des difficultés techniques qui leur incombent, le développement de ces API demande d'effectuer de nombreuses tâches répétitives et sans valeur métier. La BD évolue de manière significative au cours de nos développements, chaque changement de structure de celle-ci nécessite de mettre à jour les applications en conséquence. Ces changements peuvent concerner des personnes différentes, ce qui demande une bonne organisation. De plus, selon les choix techniques, il n’est pas toujours possible d’identifier a priori tous les impacts car les erreurs peuvent apparaitre uniquement à l’exécution des modules, et non lors de la compilation, ce qui rend très lourd et complexe la maintenance.\n\nFace à cette problématique, nous nous sommes fixés l’objectif suivant : avoir une approche globale de la construction d’une API, qui facilite à la fois l’implémentation de la partie serveur et de la partie client ; et qui se mette à jour facilement lorsque la structure de la BD évolue. Pour cela, il faut un outil capable de générer automatiquement le code utilisé par le client, et ce dans différents langages afin de pouvoir être utilisé sur différents types de terminaux. De plus, ce client permet de signaler lors de la compilation de chaque module informatique, les changements de structure des données pour faciliter la maintenance.\n\nPlusieurs incertitudes pèsent sur nos travaux :\n\nLa capacité à anticiper de manière statistique cohérente et pertinente les durées de consultation, avec une variance réduite, tout en conservant un temps de calcul limité pour que le système propose des créneaux en temps réel.  \nLe gain réel en terme d’efficience lié à cette personnalisation des temps de consultation, et la réaction sur le terrain des acteurs (médecin et patient) face à des situations inédites (à titre d’exemple : des consultations calibrées par l’algorithme comme particulièrement courtes dans certains cas, enchaînement de protocoles médicaux différents des protocoles traditionnels, introduction d’un rôle nouveau d’assistant médical).  \nLa capacité du système à absorber les urgences et imprévus.\n\nFace à cette incertitude, nous avons pris des hypothèses moyennes, et attendons de tester pour avoir des retours.\n\nDu point de vue purement technique, concernant l’API, les verrous rencontrés sont les suivants :  \nLa gestion des conflits côté serveur avec plusieurs modules qui communiquent en parallèle et en temps réel avec la base de données.  \nL’obtention d’un générateur de code systématique et propre présente d’importantes complexités d’architecture technique.\n\nAprès 6 mois de développement, nous avons une abouti à une version beta de l’API, mais celle-ci n’est pas encore mature et nécessite plusieurs mois de développement supplémentaires."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Contexte** : Recherche et développement en 2017 sur les outils BPMN+, incluant la conception d'un nouveau moteur d’exécution et d’un générateur d’IHM.\n  \n- **Incertitudes scientifiques et verrous technologiques rencontrés** :\n  - **Profiler pour le moteur d’exécution BPMN** :\n    - Objectif : Développer un traceur de performances pour identifier et éviter les goulots d’étranglement dans le programme.\n  \n  - **Assistant à la modélisation de processus BPMN** :\n    - Détection automatique d’erreurs : Possible.\n    - Problème ouvert : Correction et complétion automatique des erreurs.\n  \n  - **Base de données pour documents** :\n    - Nécessité de définir une nouvelle hiérarchisation des documents.\n    - Objectif : Mettre en place un système de typage et d’héritage pour filtrer et mieux manipuler les données.\n  \n  - **Amélioration du générateur IHM** :\n    - Objectif : Permettre l’édition en temps réel du code des applications générées.\n    - Défi : Prévisualisation des modifications tout en évitant les conflits entre diverses modifications automatiques du code."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de nos travaux de recherche et développement de l’année 2017 concernant les outils BPMN+ et notamment lors de la conception du nouveau moteur d’exécution et du générateur d’IHM, nous avons rencontré plusieurs incertitudes scientifiques et verrous technologiques : \nPouvons-nous développer un profiler adapté spécifiquement à notre moteur d’exécution BPMN ? Ce traceur de performances doit nous permettre d’identifier les goulots d’étranglement dans le programme et de les éviter ;\nEst-il possible de concevoir un assistant à la modélisation de processus BPMN ? Nous avons vu que la détection automatique d’erreurs était possible, toutefois la correction et complétion automatique semblent constituer un problème ouvert ;\nNous devons développer une base de données adaptée aux documents que nous manipulons. Pouvons-nous définir une nouvelle hiérarchisation des documents, via un système de typage et d’héritage, qui nous permette de filtrer les données afin de mieux les manipuler ?\nEnfin, nous devons améliorer notre générateur IHM. Pouvons-nous rendre possible l’édition en temps réel du code des applications générées tout en les prévisualisant et en évitant les conflits entre les diverses modifications automatiques du code ?"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Capture Omnicanale (2016-2017)**:\n  - Architecture initiale non satisfaisante : IHM communiquait uniquement avec un agent local via un ID unique.\n  - Développement d'une dynamique de relai-routage pour la communication entre l'IHM et les agents.\n  - Protocole HTTPS a empêché le lien direct entre le serveur et l'infrastructure locale.\n  - Nécessité de couper la communication entre l'IHM et l'agent local, entraînant des effets de bord.\n\n- **Verrou Technologique**:\n  - Incapacité à identifier précisément si un agent est local sur l'IHM.\n  - Complexité de l’identification des agents dans la nouvelle architecture, dépendante du système d’information de l’utilisateur.\n\n- **Recherche sur l'État de l'Art**:\n  - Identification des postes d'exécution des applications web impossible sans créer une faille de sécurité.\n  - Solution adoptée : injection de l'identifiant de l'agent dans les données locales du navigateur lors du premier téléchargement.\n  - Limitation : perte de l'identifiant si les données sont effacées ou si un autre navigateur est utilisé.\n\n- **Diversité des Documents**:\n  - Incertitude scientifique sur le traitement de documents variés (papier, numériques, réseaux sociaux, audio, vidéo).\n  - Développement d'une méthode pour extraire des informations satellites afin de structurer les données.\n  - Absence de méthode connue pour extraire des métadonnées de documents disparates.\n  - Exemples de métadonnées à extraire :\n    - Emails : identification de l’émetteur ou du récepteur.\n    - Fichiers .zip : association des documents à leurs fichiers sources.\n    - Scans : détermination de la couleur et différenciation des pages.\n\n- **Outil de Détection de Fraude Documentaire**:\n  - Incertitude sur la capacité de l'outil à détecter toutes les fraudes.\n  - Nécessité d'adaptation à des menaces polymorphes et diverses formes d'attaque.\n  - Questions sur l'efficacité de l'outil face à toutes les techniques de fraude et rapidité d'adaptation.\n  - Importance de la robustesse de l'outil et de sa capacité à supporter une montée en charge rapide.\n\n- **Interopérabilité des Technologies (Inflow et Vidéocodage)**:\n  - Problématique de communication entre technologies legacy (outil FreeMind Enterprise - FME) et technologies modernes.\n  - Incertitude sur l'interopérabilité et la communication entre flux entrants modernes et l'outil FME.\n  - Problématiques de performances : temps d’exécution, fiabilité, robustesse, montée en charge.\n  - FME ne pouvant pas être remodélisé ; solution choisie : intégration avec le moteur de workflow.\n  - Avantage de cette solution : réduction des appels HTTP, diminuant ainsi les temps de latence."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nConcernant la capture omnicanale, l’architecture que nous avions développée en 2016 n’était pas satisfaisante puisque l’IHM n’était en communication directe qu’avec un seul agent local. Chaque agent était identifié par un ID unique. Nous avions donc développé une dynamique de relai-routage pour communiquer entre l’IHM et les agents. Or ce lien concomitant entre le serveur et l’infrastructure locale n’était pas permis par le protocole HTTPS. Nous avons été contraints de couper la communication entre l’IHM et l’agent local et de développer une nouvelle architecture en 2017. La coupure du lien entre l’IHM et l’agent local a entraîné des effets de bord. Cela a introduit un verrou technologique puisque l’on n’est désormais plus en mesure de savoir précisément si un agent est en local sur l’IHM. Ainsi nous avons fait face à une incertitude scientifique relative à l’identification des postes au sein de la nouvelle architecture. L’identification des agents au sein de la nouvelle architecture est complexe et très dépendante de l’architecture du système d’information de l’utilisateur. \n\nNos recherches sur l’état de l’art nous ont montrées que dans les technologies web, il n’est pas possible d’identifier avec exactitude le poste sur lequel s’exécute l’application web et du coup d’être capable d’identifier l’agent local. Le fait d’identifier un poste dans une application web constitue une faille de sécurité. \nNous avons contourné ce problème en injectant lors du premier téléchargement de l’agent, l’identifiant de l’agent dans le système de données locales du navigateur. L’inconvénient de cette méthode est que si l’on efface ces données, ou si on utilise un autre navigateur, on perd l’identifiant de l’agent local. Cela reste un point à améliorer dans les prochaines versions. \n\nUne autre incertitude scientifique à laquelle nous avons fait face était liée à la diversité des documents que nous voulions traiter au sein de notre solution. En effet, nous souhaitions trouver un modèle de traitement commun à tous les canaux (documents papier, documents numériques, flux de réseaux sociaux, documents audios ou vidéos) pour modéliser, extraire et structurer les métadonnées. Nous avons dû développer une méthode pour extraire l’information satellite pour mieux structurer et comprendre les données. Malheureusement, au moment de nos développements, il n’y avait pas de méthode connue pour extraire des métadonnées de documents aussi disparates.\nLes métadonnées peuvent être :\nDans le cas d’un email, déterminer l’émetteur ou le récepteur ;\nDans le cas de documents issus de fichiers .zip, savoir quel document vient de quel fichier .zip ;\nDans le cas d’un scan, déterminer la couleur, différencier les pages.\n\nConcernant notre outil de détection de fraude documentaire, l’incertitude scientifique concernait la capacité de notre outil à détecter l’ensemble des fraudes. Cela nécessite de s’adapter à une menace polymorphe et à de nombreuses formes d’attaque. Nous avions donc une forte incertitude scientifique concernant la pertinence de nos développements : notre outil serait-il efficace contre l’ensemble des techniques de fraude ? Si non, pouvions l’adapter rapidement ? En outre, il fallait que notre outil soit robuste et supporte une rapide montée en charge. \n\nEnfin, dans le cadre de nos développements sur l’inflow et le vidéocodage, nous avons été confrontés à une problématique particulière. Nous voulions faire communiquer des technologies legacy (notamment notre outil FreeMind Enterprise – FME) et des technologies modernes. L’incertitude scientifique résidait donc dans le fait que nous ne savions pas si ces technologies étaient interopérables et si elles pouvaient communiquer entre elles. Plus précisément, comment pouvions-nous faire communiquer les flux entrants issus de technologies modernes et notre outil FME, basé sur des technologies plus anciennes ? Cela posait des problématiques de performances en termes de temps d’exécution, de fiabilité, de robustesse et de montée en charge. \nEn effet, l’outil FME ne pouvait pas être remodélisé. Une première solution eut été de le découper en micro-services, mais nous avons préféré le plugger sur le moteur de workflow pour des raisons de performances. Cette solution présentait l’avantage d’avoir recours à moins d’appels http, ce qui diminue les temps de latence."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Incompatibilité avec les serveurs classiques d'envoi de mail :**\n  - Les besoins du projet nécessitent l'envoi de dizaines à centaines de mails en réponse à une KPI.\n  - Les serveurs récepteurs classifient ces envois comme spam.\n  - Développement de quotas et spécifications pour réguler les envois (ex. : limitation du nombre de mails par minute pour éviter le blacklistage).\n\n- **Incertitude scientifique :**\n  - Détermination des mails les plus pertinents à envoyer.\n  - Priorisation des envois tout en évitant les doublons.\n\n- **Verrou technique :**\n  - Choix des KPI nécessaires pour obtenir des remontées d'information pertinentes.\n\n- **Gestion des pannes et des reprises :**\n  - Évitement des doublons lors des pannes.\n  - Détermination du mail le plus à jour dans ces situations.\n\n- **Incertitude scientifique sur la scalabilité :**\n  - Nécessité de développer une solution scalable pour une montée en puissance rapide.\n  - Flexibilité requise pour une solution hautement customisable, notamment en termes de volumétrie (volumétrie connue uniquement après intégration de la solution).\n  - Problématique accentuée par le fait que la solution est disponible sur cloud.\n\n- **Solutions apportées :**\n  - Développement et intégration de mécaniques incrémentales.\n  - La solution ne stocke pas tous les documents, mais plutôt les tendances et le nombre de documents transités."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nAu cours des développements qui ont porté sur le projet Pilotage & Supervision, nous avons rencontré de nombreuses incertitudes scientifiques et verrous techniques.\n\nTout d’abord, nos besoins n’étaient pas compatibles avec les limitations imposées par les serveurs classiques d’envoi de mail. Le verrou posé était que notre système envoie quelques dizaines à quelques centaines de mails en réponse à une KPI, or les serveurs récepteurs considèrent ces envois comme du spam. Nous avons donc dû développer des quotas et des spécifications pour réguler ces mails (par exemple, limiter le nombre de mails envoyés par minute afin de ne pas être blacklisté).\n\nNous avons ainsi fait face à une incertitude scientifique : comment déterminer quels sont les mails les plus pertinents, et comment prioriser les envois de mail, tout en évitant les doublons ?  \nEn outre, un verrou technique portait sur le choix des KPI que nous devions utiliser pour obtenir des remontées d’information pertinentes.  \nDe plus, nous avons fait face à la problématique de la gestion des pannes et des reprises : comment pouvions-nous éviter les doublons dans ces cas précis, et surtout, comment pouvions-nous déterminer quel était le mail le plus à jour ?\n\nEnfin, nous avons également fait face à une incertitude scientifique posée par la nécessité de développer une solution scalable, capable de faire face à une rapide montée en puissance, tout en étant extrêmement flexible. En effet, nous souhaitions développer une solution hautement customisable, notamment en termes de volumétrie (qui ne sont connues qu’une fois notre solution intégrée). La problématique posée par la volumétrie est d’autant plus mise en exergue par le fait que notre solution est disponible sur cloud.\n\nPour résoudre ces complexités, nous avons développé et intégré des mécaniques incrémentales. Ainsi, notre solution ne stocke pas tous les documents mais plutôt les tendances et le nombre de documents transités."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Objectif du projet (2018)** : Offrir une solution unique et centralisée pour tous les utilisateurs et produits.\n  \n- **Complexité technique à gérer** :\n  - Multi-déclarations des utilisateurs.\n  - Solutions hétérogènes sur les plans technique et fonctionnel.\n  - Approche principalement technique.\n\n- **Exigences de la solution** :\n  - Vue 360° de l’utilisateur.\n  - Gestion des interfaces fonctionnelles, multi-tenant et multi-sociétés.\n\n- **Fonctions principales à assurer** :\n  1. Gestion des utilisateurs.\n  2. Description des organisations.\n  3. Gestion des habilitations.\n  4. Gestion des accès.\n\n### Contraintes à 4 Niveaux\n\n1. **Gestion des utilisateurs** :\n   - Ajout/édition de propriétés.\n   - Suppression logique/désactivation : conservation indéfinie des utilisateurs supprimés du référentiel.\n   - Synchronisation avec des référentiels annuaires : options de réplication (filtrable) en mode batch ou import sélectif.\n   - Gestion d’utilisateurs externes non synchronisés.\n   - Support pour grandes organisations (plusieurs milliers d’utilisateurs).\n\n2. **Description des organisations** :\n   - Gestion des unités hiérarchiques et transverses.\n   - Création logique et activation.\n   - Ajout/édition de propriétés.\n   - Suppression logique/désactivation : conservation indéfinie des unités.\n   - Synchronisation avec des référentiels annuaires.\n   - Rattachement/détachement d’utilisateurs.\n   - Conservation de l’historique des changements (début-fin).\n   - Gestion des absences/délégations.\n   - Adaptabilité à des organisations variées.\n\n3. **Gestion des habilitations** :\n   - Droits sur fonctions et objets (documents, dossiers, processus, etc.).\n   - Attribution/retrait de droits à utilisateurs et organisations.\n   - Héritage des droits selon l’appartenance à une organisation.\n   - Droits d’utilisation et droits en lecture/écriture sur les données.\n   - Droits descendants et montants.\n   - Soumission des droits à des règles (ex. création de DA > 1000€).\n   - Capacités et domaines dépendants des produits (extensibles dynamiquement).\n   - Problématique de granularité des droits et performance requise.\n\n4. **Gestion des accès** :\n   - Conformité aux protocoles standards (OpenID Connect, SAML, OAuth).\n   - Login social (LinkedIn, Facebook, Google).\n   - Login unique SSO.\n   - Double authentification.\n   - Règles de sécurisation (format des mots de passe, mot de passe oublié, non stockage des mots de passe).\n\n### Solutions Utilisées\n\n- **Solutions existantes** :\n  - Gestionnaire des utilisateurs BPMN+.\n  - Fédérateur d’identité Keycloak.\n\n- **État des solutions** :\n  - Multiplicité des solutions utilisées (presque autant que de produits).\n  - Difficulté à défendre ces solutions auprès des utilisateurs.\n\n### Avantages et Limites des Socles\n\n1. **Socle BPMN+** :\n   - **Avantages** :\n     - Gestion des utilisateurs, groupes, staffs, privilèges.\n     - Moteur requis pour différents produits.\n     - Maîtrise complète et administration technique existante.\n     - Synchronisation LDAP.\n   - **Limites** :\n     - Problèmes de fédération d’identités et standards.\n     - Désincarcération moteur.\n\n2. **Socle Keycloak** :\n   - **Avantages** :\n     - Fédération d’identité.\n     - Support des standards.\n     - Produit documenté.\n     - Synchronisation LDAP.\n   - **Limites** :\n     - Solution technique générique nécessitant adaptation.\n     - Intégration BPMN+.\n\n- **Conclusion** : Aucun des deux socles ne répond complètement aux besoins fonctionnels."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de nos travaux de l’année 2018, nous souhaitons offrir à nos utilisateurs une solution unique centralisée, commune à tous les produits.\n\nNous devions nous adapter à la complexité technique globale de mise en œuvre de nos solutions au regard des :\nMulti-déclarations des utilisateurs\nSolutions techniquement et fonctionnellement hétérogènes\nVisions principalement techniques\n\nEn effet, cette solution doit offrir une vue 360° de l’utilisateur et gérer nos besoins d’interface de gestion fonctionnelle, de multi-tenant et multi-sociétés\n\nElle doit par ailleurs assurer quatre grandes fonctions :\nGestion des utilisateurs\nDescription des organisations\nGestion des habilitations\nGestion des accès\n\nNous devions donc gérer des contraintes à 4 niveaux :\n\nAu niveau de la gestion des utilisateurs, \nAjout/ Edition de propriétés ;\nSuppression logique / Désactivation : utilisateurs conservés indéfiniment devant pouvoir subsister à la suppression du référentiel société ;\nSynchronisation avec des référentiels annuaires, deux options : réplication (filtrable) en mode batch, import sélectif lors de l’activation ;\nGestion d’utilisateurs externes à l’organisation (pas nécessairement synchronisés) ;\nBesoin de supporter des organisations de grande taille (plusieurs milliers d’utilisateurs).\n\nAu niveau de la description des organisations :\nUnités hiérarchiques (Staff) et transverses (Groupes) ;\nCréation logique / Activation ;\nAjout / Edition de propriétés ;\nSuppression logique / désactivation : unités conservées indéfiniment ;\nSynchronisation avec des référentiels annuaires ;\nRattachement/détachement d’utilisateurs ;\nConservation de l’historique des changements (début-fin) ;\nGestion des absences / délégations ;\nPouvoir s’adapter à des organisations variées.\n\nAu niveau de la gestion des habilitations :\nDroits sur des fonctions et sur des objets (ex : document, dossier, processus, classe de doc, …) ;\nAttribution / Retrait de droits à des utilisateurs et organisations ;\nHéritages de droits selon appartenance à une organisation ;\nDroits d’utilisation sur une capacité ;\nDroits en lecture / écriture sur des données ;\nDroits descendants et montants ;\nDroits pouvant être soumis des règles (ex. droit de création d’une DA de plus de 1000€) ;\nCapacités et domaines dépendants des produits (extensibles dynamiquement, publication) ;\nProblématique importante sur la granularité des droits et besoin de bonnes performances car point focal beaucoup sollicité.\n\nAu niveau de la gestion des accès :\nConformité aux protocoles standards OpenID Connect, SAML, Oauth ;\nLogin social (LinkedIn, Facebook, Google) ;\nLogin unique SSO ;\nDouble authentification ;\nRègles de sécurisation (format des mots de passe, mot de passe oublié, pas de stockage des Mots de passe).\n\nEn effet, nos solutions utilisaient soit le gestionnaire des utilisateurs BPMN+, soit le fédérateur d’identité Keycloack.\n\nLe tableau suivant récapitule le type de la solution de gestion des utilisateurs utilisé pour chacune de nos applications.\n\nNous avions donc presque autant de solutions que de produits, néanmoins, aucune solution n’est satisfaisante, ce qui représente pour nous une difficulté à les défendre auprès de nos utilisateurs.\n\nEn effet, les solutions basées sur le socle BPMN+ offrent les avantages suivants :\nGestion de users, groups, staffs, privilèges\nMoteur déjà requis pour différents produits\nMaîtrise complète\nAdministration technique existante\nSynchronisation LDAP\n\nToutefois, les limites de ce socle se manifestent dans les aspects suivants :\nFédération d’identités et standards\nDésincarcération moteur\n\nLes solutions basées sur Solution basée sur Keycloak offrent les avantages suivants :\nFédération d’identité\nSupport des standards\nProduit documenté\nSynchronisation LDAP\n\nToutefois, les limites de ce socle se manifestent dans les aspects suivants :\nSolution technique générique qu’il faut adapter\nIntégration BPMN+\n\nEn l’état, aucuns des deux socles ne répond complètement à notre besoin fonctionnel."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif du projet (2019)** : Offrir une solution plus performante et ergonomique aux utilisateurs d'ITESOFT avec un processus d’authentification unique via le User Manager.\n\n#### Évolutions du Token Provider\n- **Conception** : Interface entre un fournisseur de service et un fournisseur d’identité pour transformer les informations d’authentification en token JWT de session.\n- **Problème principal** : Taille excessive du token JWT (dépassement de 2ko), ce qui pose des défis :\n  - **Limite de taille** : Difficulté à demander aux clients de modifier la configuration de leurs serveurs pour accepter des headers HTTP supérieurs à 2ko.\n  - **Encombrement réseau** : Le token doit être transmis à chaque appel API, augmentant le trafic réseau.\n- **Solution recherchée** : Réduire la taille du token sans perte de fonctionnalité, en trouvant une approche générique et standard.\n- **Difficulté supplémentaire** : Définir et gérer des custom claims (attributs clients) de manière centralisée et spécifique au tenant de connexion.\n\n#### Mécanisme de Notification\n- **Envoi de mail** : Identification des actions nécessitant un envoi de courriel (création de compte, réinitialisation de mot de passe, changement de mot de passe).\n- **Première solution** : Enrichir le code des actions pour inclure l’envoi de courriel, avec des incertitudes :\n  - **Validation de l’action** : Risque d’envoi de courriel avant la validation de l’enregistrement en base de données.\n  - **Maintenabilité du code** : Multiplication du code d’envoi de courriel pour chaque API createUser, augmentant le risque de bugs.\n  - **Performance** : L’envoi de courriel peut ralentir le User Manager, surtout en cas de saturation du serveur de courriels.\n  \n- **Historisation** : Les actions d’historisation sont des copies des actions réelles, soulevant des questions similaires de maintenabilité, validation et performance.\n\n#### Génération de l’API Miroir\n- **Difficulté principale** : Écriture de fichiers Template pour la génération précise des notifiants, réalisée par reverse engineering.\n- **Envoi de courriels asynchrone** : Mise en place d’un mécanisme pour enregistrer et envoyer les courriels après validation des données en base. Limite actuelle : pas de mécanisme de rejeu en cas d’échec d’envoi.\n\n#### Stockage de l’Évolution des Données\n- **Incertitude scientifique** : Trouver une solution non invasive et maintenable pour stocker les actions et évolutions des valeurs dans l’historique.\n- **Implémentation** : Utilisation d’un AuditTrailConsumer pour intercepter les notifications de création, modification et suppression, avec des requêtes d’insertion uniformisées selon le type d’élément.\n- **Gestion des modifications** : Stockage uniquement des propriétés modifiées lors des mises à jour."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\nLors de nos travaux de l’année 2019, nous souhaitions offrir à nos utilisateurs une solution plus performante et plus ergonomique.\n\nEn effet, nous souhaitions proposer à nos utilisateurs un processus d’authentification unique pour toutes les applications ITESOFT utilisant le User Manager. Dans ce cadre, notre équipe R&D a été confrontée à plusieurs incertitudes. \n\nEvolutions du Token Provider :\n\nNous avons conçu le Token Provider pour pouvoir s’interfacer avec un fournisseur de service qui lui-même communiquera avec un fournisseur d’identité. Le rôle du Token Provider est donc de réceptionner les informations d’authentification envoyées par le fournisseur de service et de les transformer en token JWT de session. La question qui s’est posée est donc : comment faire lorsque l’on ne souhaite pas déléguer l’authentification à un fournisseur d’identité externe mais que l’on souhaite se connecter directement sur le serveur du User Manager ? Par ailleurs, le token JWT dans sa première version contient les informations suivantes :\nL’identifiant de la session\nLe nom de la session\nLe nom de l’émetteur du Token\nLa session sérialisée sous forme de Principal (objet Java)\nLa session sérialisée sous forme d’objet JSON\nLe problème auquel nous avons été confrontés était lié à la taille du token. Etant donnée la quantité d’informations portées, le token JWT dépasse la taille de 2ko qui est habituellement la taille maximale autorisée par défaut par les serveurs Web et d’applications pour les headers HTTP (un token JWT étant porté par le header HTTP des requêtes le concernant). Cette limite de taille peut être augmentée sur les serveurs Web et d’applications mais il est difficilement concevable de demander à tous nos clients de modifier la configuration de leurs serveurs pour se plier aux exigences hors norme du User Manager (un header HTTP de plus de 2ko n’est pas commun et peut augmenter les risques liés à la sécurité des réseaux). La deuxième raison pour laquelle un token si lourd n’est pas acceptable est lié à l’encombrement réseau. En effet, le token de session est une information qu’il est nécessaire de passer au User Manager à chaque appel API et par extension entre toutes les applications liées au User Manager. Autrement dit, le token JWT fourni par le User Manager transitera énormément sur le réseau. Or une donnée de taille imposante qui transite de nombreuses fois sur le réseau encombre celui-ci. Nous avons donc décidé de chercher une solution pour réduire la taille de ce token sans pour autant perdre en fonctionnalité. La difficulté a résidé dans le fait de trouver une solution à la fois générique et standard afin de répondre aux attentes des applications clientes connues et non connues à ce jour. \n\nUne autre difficulté rencontrée s’est concentrée sur le fait de trouver une solution pour définir de façon simple et centralisée les custom claims (attributs clients) que l’on souhaite attacher au token JWT et de pouvoir spécifier des custom claims différents en fonction du tenant de connexion. Comment procéder ?\n\nMécanisme de notification : \n\nEnvoi de mail \nNous avons commencé nos réflexions autour de l’envoi de mail par chercher à déterminer quelles actions nécessiteraient un envoi de courriel. Les premiers résultats ont été l’envoi de courriel à la création d’un compte utilisateur avec un lien lui permettant de créer son mot de passe, l’envoi d’un courriel lors de la réinitialisation d’un mot de passe et l’envoi d’un courriel lorsqu’après une période donnée un utilisateur doit changer son mot de passe qui arrive à expiration. L’envoi de ces courriels découle directement d’une action sur l’API serveur du User Manager.\n\nNotre première solution consistait à enrichir le code des actions souhaitées pour y ajouter du code dédié à l’envoi de courriel. Cette solution était intéressante mais présentait plusieurs incertitudes scientifiques. La première est la validation de finalisation de l’action, c’est-à-dire comment s’assurer que le courriel part bien une fois l’action terminée et les données enregistrées et validées en base de données. En effet, tant que ceci n’est pas fait il y a toujours la possibilité d’une annulation de l’action (en cas d’erreur notamment). Il faut savoir que la validation de l’enregistrement des données en base, qui rend réellement les données persistantes est la toute dernière opération effectuée par le serveur User Manager. Ainsi si l’on pose de façon séquentielle la solution ici pensée, nous aurons :\nDémarrage de l’action ;\nEnregistrement des données (mais pas encore validées) ;\nEnvoi du courriel ;\nValidation ou annulation de l’enregistrement en base de données.\nNous remarquons que dans ce cas de figure séquentiel le courriel est envoyé même si l’enregistrement en base de données est annulé. Un utilisateur pourrait donc se retrouver avec un courriel lui indiquant de créer son mot de passe alors que son compte n’a finalement pas été créé. \n\nLa seconde incertitude était liée la maintenabilité du code. En effet, si l’on continue sur l’exemple de l’action de création d’utilisateur, le code du User Manager contient plusieurs API createUser avec des signatures différentes qui permettent de créer un utilisateur de différentes manières et avec différentes propriétés. Cependant, dans tous les cas le résultat de ces API est le même, un utilisateur est créé. Il faut donc envoyer un courriel à l’utilisateur créé. Avec la solution d’enrichir le code de création d’utilisateur, il faut donc multiplier ce code autant de fois qu’il y a d’API createUser et surtout penser à chaque fois que l’on ajoute une nouvelle API de ce type à rajouter le code qui permet l’envoi de courriel. Toutefois, la duplication du code n’est jamais une bonne idée en programmation informatique car elle multiplie d’autant les risques de bug et la chance d’oublier d’ajouter le code envoyant les courriels se révèle non négligeable si une telle méthode est rajoutée plusieurs mois voire plusieurs années plus tard, qui plus est si l’équipe de développement a changé entre temps. \n\nEnfin la question des performances entre en ligne de compte. Avec l’algorithme séquentiel présenté ci-dessus, l’envoi de courriel entre dans le temps total nécessaire à effectuer l’action. Le User Manager subissant de fortes charges de requêtes, il se doit d’être performant et rapide et il n’est pas acceptable qu’un envoi de courriel ralentisse son déroulement. L’envoi de courriel pourrait peser lourd dans le temps d’exécution d’une action car nous sommes face à des aléas tels que la disponibilité du serveur de courriel. Si l’action couramment effectuée doit se mettre en attente parce que le serveur d’envoi de courriels est saturé, le temps d’exécution de l’action risque d’exploser.\nHistorisation\nEn parallèle les réflexions autour de l’historisation des actions et des éléments ont fait apparaître que les actions d’historisation se trouvaient être une copie des actions réelles. C’est-à-dire qu’une création d’utilisateur devait mener à une création de cet utilisateur dans la base de données d’historique, une modification d’utilisateur devait mener à une nouvelle création de cet utilisateur dans la base de données historique avec les nouvelles propriétés de l’utilisateur, etc. Chaque action menée se traduit par la même action dans l’historique, seule la façon de stocker l’information diffère.\nBien que les fonctionnalités soient différentes, nous nous retrouvons ici face aux mêmes questions et difficultés qu’exposées ci-dessus pour l’envoi de courriel : Nous pourrions enrichir manuellement chaque API existante pour stocker les informations souhaitées dans la base d’historique, ce qui amène aux mêmes difficultés de maintenabilité, de validation des données et de performance.\n\nGénération de l’API miroir\nLors de la génération de l’API miroir, la principale difficulté a été l’écriture des fichiers Template permettant la génération exacte du contenu des notifiants. Afin de rendre le travail plus précis et efficace nous avons défini ces templates par reverse engineering, c’est-à-dire que nous avons commencé par écrire le résultat attendu pour un intercepteur puis écrit le template générique à partir de ce code. Ainsi par comparaison entre le code généré et le code attendu nous pouvions plus simplement faire les ajustements nécessaires.\n\nLa seconde difficulté a été de réussir l’envoi de courriels de façon asynchrone. Étant donné la façon dont le mécanisme de notification fonctionne, l’envoi et la consommation de notification sont faits de façon synchrone. Afin de réussir l’envoi de courriel une fois les données enregistrées et validées en base de données, nous avons permis l’enregistrement du courriel à envoyer depuis le consommateur dédié sur le module de gestion des transactions SGBD du serveur User Manager. Une fois les données enregistrées et validées dans la base de données, ce module parcourt tous les courriels enregistrés et les envoie. Aujourd’hui ce mécanisme ne permet pas le rejeu, c’est-à-dire que si le courriel n’est pas correctement envoyé du premier coup, il ne sera jamais envoyé. C’est une amélioration qui donnera lieu à de futurs travaux.\n\nStockage de l’évolution des données : \n\nL’incertitude scientifique à laquelle nous avons été confrontés était liée au fait de trouver une solution non invasive (qui ne pollue pas le code) et facilement maintenable dans le temps pour stocker les différentes actions et évolutions de la valeur des éléments dans l’historique. On rejoint ici le projet sur le mécanisme de notification. En effet, nous nous appuyons sur ce mécanisme pour intercepter les différentes actions sur chacun des éléments que l’on souhaite historiser (création/modification/suppression d’utilisateur/profil/unité, etc.).\nPour cela, nous avons implémenté un AuditTrailConsumer qui se charge de recevoir les notifications de création, de modification et de suppression et qui adapte automatiquement la requête d’insertion en fonction du type d’élément concerné. Etant donnée le schéma mis en place, les requêtes d’insertion sont les mêmes quel que soit le type d’élément, seuls les attributs de ces requêtes changent. Lors d’une modification, nous savons exactement quelles propriétés ont été modifiées, ainsi seulement celle-ci seront stockées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Outil de reconnaissance automatique de documents :**\n  - Incertitude sur l'efficacité du Deep Learning pour la reconnaissance des zones (texte, image, tableau).\n  - Résultats initiaux prometteurs, mais performance finale insatisfaisante : 80% de reconnaissance sur 1000 documents.\n  - Problèmes d'identification, notamment pour les documents à plusieurs colonnes, souvent mal classés comme tableaux.\n\n- **Capture omnicanal :**\n  - Développement de la structuration automatique en temps réel confronté à la diversité des documents (papier, numériques, réseaux sociaux, audio, vidéo).\n  - Recherche d'un modèle de traitement commun pour extraire et structurer les métadonnées.\n  - Nécessité de développer une méthode pour extraire des informations satellites, sans méthode connue à l'époque.\n  - Exemples de métadonnées à extraire :\n    - Email : émetteur et récepteur.\n    - Fichiers .zip : association document-fichier.\n    - Scans : couleur et différenciation des pages.\n\n- **Développements sur Inflow et vidéocodage :**\n  - Problèmes de stabilité et de performances liés à l'assemblage Angular et Redux Store.\n  - Centralisation des objets trop complexes non bénéfique, nécessitant une refonte technique pour améliorer performance, évolutivité et maintenance.\n\n- **Gestion d'erreurs de structuration de documents :**\n  - Deux cas de gestion d'erreurs :\n    - Document entrant assemblant plusieurs types (ex. : CNI, RIB).\n    - Fichiers distincts pour différentes pages d'un même document.\n  - Développement de la fonctionnalité drag & drop pour réordonner les pages dans Inflow, une fonctionnalité absente des composants existants.\n  - Problèmes d'effets de bord liés à la combinaison de drag & drop et sélection de pages multiples.\n\n- **Affichage des messages d'erreur :**\n  - Messages d'erreur sous chaque champ peu lisibles et imprécis.\n  - Développement d'un panneau d'erreur Inflow pour un affichage centralisé et complet des erreurs.\n  - Remplacement du mécanisme natif d'affichage des erreurs par une solution plus ergonomique, permettant d'afficher différents niveaux d'erreurs.\n\n- **Demande croissante des utilisateurs :**\n  - Nécessité d'une refonte technique de l'architecture logique de l'application en 2019 pour répondre à l'augmentation du volume d'utilisateurs et améliorer les performances.\n\n- **Module Production Manager :**\n  - Supervision en temps réel limitée aux tâches manuelles et étapes fonctionnelles.\n  - Possibilité de créer des KPI pour alertes, mais non visualisables en temps réel sur le dashboard.\n  - Proposition d'une solution pour afficher les KPI directement sur l'écran d'accueil (dashboard de production) pour les superviseurs."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nConcernant notre outil de reconnaissance automatique de documents, l’incertitude scientifique concernait la pertinence du Deep learning pour la reconnaissance des zones des documents : texte, image et tableau. Cela s’est avéré rapidement prometteur en démonstration sur les premiers tests mais au final les résultats ont été non satisfaisants. En effet, les reconnaissances des zones sur un jeu de 1000 documents étaient à hauteur de 80% seulement. De plus, nous avons rencontré des erreurs d’identification avec des seuils élevés par exemple, des documents avec plusieurs colonnes souvent sont regroupés comme un tableau à tort.\n\nConcernant la capture omnicanal, lors de nos développements sur la structuration automatique et en temps réel des éléments capturés, nous avons fait face à la diversité des documents que nous voulions traiter au sein de notre solution. En effet, nous souhaitions trouver un modèle de traitement commun à tous les canaux (documents papier, documents numériques, flux de réseaux sociaux, documents audios ou vidéos) pour modéliser, extraire et structurer les métadonnées. Nous avons dû développer une méthode pour extraire l’information satellite pour mieux structurer et comprendre les données. Malheureusement, au moment de nos développements, il n’y avait pas de méthode connue pour extraire des métadonnées de documents aussi disparates. Les métadonnées peuvent être : Dans le cas d’un email, déterminer l’émetteur ou le récepteur ; Dans le cas de documents issus de fichiers .zip, savoir quel document vient de quel fichier .zip ; Dans le cas d’un scan, déterminer la couleur, différencier les pages.\n\nUne autre incertitude scientifique à laquelle nous avons fait face était liée à nos développements sur l’Inflow et le vidéocodage où nous avons rencontré de gros problèmes de stabilité et de performances. En effet, nous nous sommes rendus compte que notre choix technologique sur l’assemblage de la technologie Angular et Redux Store n’était pas approprié à la manière dont l’outil devrait évoluer. L’assemblage de ces deux technologies étant particulier, nous avons au final constaté que la centralisation des objets trop nombreux et complexes n’était pas bénéfique dans notre cas. À cet effet, une refonte technique de l’outil était plus que nécessaire afin de remédier aux constats négatifs : performance, évolutivité et maintenance du code de plus en plus complexe avec l’ajout de fonctionnalité.\n\nNous avons également été confrontés à des problèmes de gestion d’erreurs de structuration de documents. Nous distinguons deux types de cas : Un document entrant (par exemple PDF) étant un assemblage de documents différents : plusieurs CNI, plusieurs RIB, plusieurs bulletin de salaires, voire un RIB, une CNI, un justificatif de domicile ; Des fichiers distincts représentant les différentes pages d’un même document (par exemple : recto CNI, verso CNI, avis d’impôt de 4 pages). Pour remédier à cette faiblesse, nous avons développé la fonctionnalité drag & drop permettant de réordonner les pages dans Inflow de façon simple et intuitive. Il est à noter qu’aucun des composants (représentation arborescente d’un dossier) existant n’offrait cette fonctionnalité de drag & drop nativement, d’où la nécessité de la développer. Toutefois, la combinaison de drag & drop, qui plus est avec une sélection de pages multiples est grande, nous a engendré des problèmes d’effets de bord.\n\nDe plus, nous avons remarqué que le message d’erreur positionné sous chacun des champs n’est pas suffisamment lisible lorsque plusieurs messages d’erreurs doivent être affichés. L’erreur affichée n’est pas suffisamment précise. Pour cela, nous avons développé un panneau d’erreur Inflow pour un affichage plus complet des erreurs de contrôle. Nous avons donc remplacé le mécanisme natif de gestion et l’affichage des erreur angulat/ form.io, pour le centraliser et en déporter l’affichage. Au final, nous pouvons même choisir d’afficher l’un, ou l’autre, ou les deux en même temps, ce qui est un gros avantage ergonomique.\n\nUn autre enjeu consistait à répondre à une demande croissante en volume des utilisateurs de la solution, tout en améliorant les performances. Il a donc été nécessaire en 2019 de faire une refonte technique complète de l‘architecture logique de l’application.\n\nEnfin, concernant le module Production Manager, la supervision temps réel se limite au suivi des tâches manuelles et des étapes fonctionnelles. En effet, le PM offre la possibilité au projet (ou client) de créer des indicateurs plus pertinents (les KPI) qui peuvent être utilisés pour lever des alertes (mail ou écran), mais ne peuvent être visualisés en temps réel par le superviseur (non visible par le dashboard). Afin de remédier à ces contraintes/limites, nous avons proposé une solution permettant aux superviseurs d’exploiter leurs KPI directement sur l’écran d’accueil (dashboard de production)."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Connecteur ERP-SAP** :\n  - Objectif : Assurer la communication entre différentes applications au sein de l'entreprise.\n  - Incertitude : Interopérabilité des technologies utilisées et capacité de communication entre elles.\n  - Barrières rencontrées : Différences de langages de programmation, nécessitant la création d'un connecteur sans connaissance préalable des langages et formats de données.\n\n- **Défis liés au cloud** :\n  - Interaction synchrone souhaitée pour la consommation immédiate des messages après leur mise en file.\n  - Garantir l'intégrité des messages en cas d'absence d'un maillon de la chaîne (exemples : fuseau horaire différent, plantage de plateforme).\n\n- **Transfert d'images à l'ERP-SAP** :\n  - Contrairement à l'ERP-Oracle, nécessité d'émettre l'image de la facture à l'ERP-SAP.\n  - Complexité du transfert d'images : \n    - Nécessité d'une transmission fiable des données.\n    - Stockage d'images dans des files d'attente pour permettre une communication asynchrone.\n    - Le destinataire peut récupérer l'image à son rythme, sans obligation d'accusé de réception immédiat.\n\n- **Choix du protocole** :\n  - Problématique dans le choix du protocole de communication.\n  - Étude approfondie des protocoles existants menée pour déterminer la meilleure option.\n  - Adoption du protocole AMQP, permettant une compatibilité totale entre n'importe quel broker et application client utilisant ce standard.\n  - Clarification : RabbitMQ est utilisé uniquement pour la communication SAP vers SLFI.\n  - Difficulté rencontrée lors de l'envoi d'images à SAP : choix de la méthode HTTP à utiliser.\n    - Options considérées : \n      - MULTIPART POST (envoi de l’URL)\n      - MULTIPART FORMPARAM (envoi de l’URL et de l’image)\n      - POST (envoi direct)"}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\n\nConnecteur ERP-SAP :\nConcernant la communication entre différents services, nous voulions faire communiquer différentes applications. L’incertitude scientifique résidait donc dans le fait que nous ne savions pas si ces technologies étaient interopérables et si elles pouvaient communiquer entre elles. En effet, nous avons été confrontés dans le domaine informatique à des barrières linguistiques (entre les différents langages de programmation). Nous avons donc eu le besoin de créer un connecteur permettant de faire communiquer deux applications dont nous ne connaissons pas le langage et le format de données.\n\nDe plus, le fait qu’on soit en cloud a engendré plus de difficulté car nous souhaitions interagir de manière synchrone quand c’est possible (ie consommation du message directement après mise en place dans la file) et garantir l’intégrité des messages si un des maillons de la chaîne est absent (fuseau horaire différent, plantage de plateforme, …).\n\nUne autre difficulté était liée au fait que nous devons émettre à l’ERP-SAP l’image de la facture, contrairement à l’ERP-Oracle où nous avions un lien vers l’image. Le transfert d’images est plus compliqué car nous devons garantir d’une part une transmission des données fiable et, d’autre part, de permettre de stocker des images dans des files d’attente, permettant ainsi une communication asynchrone où l’émetteur et le destinataire n’ont pas à agir au même rythme. Le destinataire (consommateur) de l’image n’est pas obligé d’accepter directement l’information, de la traiter et d’en accuser réception auprès de l’émetteur (producteur). À la place, il va récupérer l’image dans la file d’attente lorsqu’il en a la capacité. Le producteur peut ainsi continuer à travailler sans créer de période d’inactivité.\n\nLe choix du protocole était problématique. Nous avons donc été amenés à faire une étude approfondie sur les protocoles existants. Grâce au choix de l'implémentation du protocole AMQP et à nos travaux, RabbitMQ nous a permis d’assurer une totale compatibilité entre n'importe quel broker et application client implémentant ce standard. \nIci il y a eu incompréhension, RabbitMQ n’est utilisé que dans le sens SAP => SLFI. Lorsque nous envoyons l’image a SAP la difficulté a été de savoir quelle méthode http utiliser.\nMULTIPART POST (envoie de l’url) / MULTIPART FORMPARAM (envoi de l’url et de l’image) / POST"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Optimisation et Amélioration des Performances :**\n  - **Incertitude sur les requêtes dupliquées :**\n    - Problème lié aux requêtes dupliquées provenant du frontend.\n    - Objectif : mise en cache sans introduire de bugs.\n    - Mise en cache peut améliorer significativement les performances en réduisant la charge sur le frontend et le backend.\n    - Nécessité d'étudier les requêtes pour déterminer leur éligibilité à la mise en cache et identifier des cas particuliers.\n\n- **Difficultés liées aux multiples requêtes :**\n  - Observation de 15 appels en moins d'une seconde pour récupérer/mettre à jour une facture en production (CIRCET).\n  - Question de la possibilité de reproduire ce comportement ailleurs que chez CIRCET et d'apporter des améliorations.\n\n- **Problème de remontée de dataEntities :**\n  - Surplus de dataEntries utilisées pour la mise à jour des données de la facture.\n  - Risque de devoir créer des nœuds vides pour alimenter les statusContext si les dataEntries InvoiceData ne sont plus remontées.\n  - Incertitude sur l'impact potentiel : suppression des données de la facture.\n\n- **Optimisation de la relance automatique de facture :**\n  - La version actuelle du système présente une relance automatique de facture non optimale.\n  - Grand nombre de tâches automatiques jugées non pertinentes, entraînant une consommation inutile des performances.\n  - Nécessité de trouver une solution pour remédier à ces faiblesses.\n\n- **Verrou technologique lié à la compatibilité :**\n  - La solution SLFI n'est pas compatible avec SaaS, rendant son exploitation sur le cloud impossible.\n  - Nécessité d'une refonte technologique pour moderniser le système et améliorer les performances.\n  - Difficultés techniques : \n    - Amélioration de la consommation mémoire et optimisation de la consommation CPU via tuning JVM.\n    - Validation/invalidation de l'encapsulation des applications dans une application Springboot pure.\n\n- **Problèmes rencontrés avec Springboot :**\n  - Utilisation de librairies Linux tomcat_native pour éviter le serveur Tomcat, censées offrir de meilleures performances.\n  - Résultats non concluants : \n    - Consommation mémoire accrue.\n    - Temps de traitement prolongés.\n  - Nécessité de trouver une solution pour remédier à ces faiblesses."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\n\nOptimisation et amélioration des performances\nLa première incertitude scientifique était liée aux requêtes dupliquées partant du frontend. L’optimisation consistait à pouvoir faire de la mise en cache en veillant à ne pas introduire de bugs. Certes cette mise en cache peut améliorer les performances très significativement, grâce à l’allègement de la charge du frontend et par conséquent du backend en évitant de solliciter la base de données, mais elle peut aussi avoir un impact significatif. Dès lors, il fallait d’abord étudier nos requêtes pour savoir s’il était possible de les mettre toutes dans le cache ou s’il y a des cas particuliers ? \n\nUne autre difficulté était liée aux multiples requêtes envoyées par l’engine. Effectivement, nous avons remarqué que sur des laps de temps très courts (inférieur à 1s), nous pouvons avoir 15 appels pour récupérer/mettre à jour la facture.  Nous avons constaté ce comportement en production (chez CIRCET), serions-nous donc en mesure de le reproduire autre part que chez CIRCET et apporter une amélioration de ce côté ? \n\nNous avons également été confrontés à une autre difficulté liée à la remontée de dataEntities en surplus. Les dataEntries sont utilisées pour mettre à jour les données de la facture. Si on ne remonte plus les dataEntries InvoiceData, il sera nécessaire de créer des nœuds vides pour alimenter les statusContext. Quel sera l’impact ? Cela ne va-t-il pas supprimer toutes les données de la facture ?\n\nConcernant la version actuelle du système, nous avons constaté que la relance automatique de facture n’est pas optimale. Effectivement, la solution actuelle engendre un grand nombre de tâches automatiques qui ne sont pas pertinentes et consomment inutilement les performances du système. Nous devions donc trouver une solution pour remédier à ces faiblesses.\n\nEnfin, un autre verrou technologique était lié au fait que notre solution SLFI n’est pas compatible avec SaaS ce qui la rend inexploitable sur le cloud. Nous devions ainsi réaliser une refonte technologique afin de moderniser notre système et d’améliorer les performances. La principale difficulté technique consistait donc à améliorer la consommation mémoire et optimiser la consommation CPU de nos applications, en passant par du tuning JVM et valider/invalider l’encapsulation de nos applications dans une application Springboot pure. Toutefois, malgré l’utilisation des librairies linux tomcat_native permettant de ne pas passer par l’intermédiaire un serveur tomcat et étant sensée être beaucoup plus performante, le passage sur Springboot n’est pas concluant. \nLes applications consomment plus de mémoire et les temps de traitement sont plus long. Nous étions donc amenés à trouver une solution afin de remédier à toutes ces faiblesses."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Objectif du projet (2020)** : Offrir une solution plus performante et ergonomique pour les utilisateurs d'ITESOFT.\n- **Processus d’authentification** : Mise en place d’un processus d’authentification unique pour toutes les applications utilisant le User Manager.\n\n#### Difficultés Techniques Rencontrées\n\n1. **Verrouillage de l’écriture lors des imports** :\n   - **Problème** : Assurer que le verrouillage d’un type d’éléments soit effectif uniquement sur le tenant concerné.\n   - **Impact** : Éviter le blocage des opérations d’import ou d’écriture sur d’autres tenants.\n\n2. **Centralisation du mécanisme de verrouillage** :\n   - **Challenge** : Centraliser les algorithmes de verrouillage pour simplifier la maintenabilité du code.\n   - **Solution** : Mise en place d’une méthode dédiée pour chaque opération d’écriture, incluant un appel avec deux paramètres pour gérer les verrous.\n\n3. **Import global** :\n   - **Nécessité** : Parcours du fichier d’import avant l’appel à la méthode de verrouillage pour identifier les types d’éléments concernés.\n   - **Développement** : Création d’un algorithme de parcours partiel pour minimiser l’impact sur les temps d’import.\n\n4. **Gestion des erreurs** :\n   - **Criticité** : Établir un mécanisme de gestion des erreurs robuste pour éviter que des verrous ne restent posés indéfiniment.\n   - **Approche** : Verrouillage et déverrouillage réalisés sur des transactions distinctes avec un mécanisme de timeout pour le déverrouillage automatique.\n\n5. **Prévention des deadlocks** :\n   - **Stratégie** : Mise en place d’un algorithme de vérification d’existence de verrou basé sur le thread d’exécution et le tenant.\n   - **Objectif** : Éviter le blocage par réentrance en vérifiant si un verrou a déjà été posé pour un thread et un tenant donnés avant la vérification et la pose de nouveau verrou.\n   - **Résultat** : Utilisation d’informations enregistrées en mémoire pour prévenir les deadlocks."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de nos travaux de l’année 2020, nous souhaitions offrir à nos utilisateurs une solution plus performante et plus ergonomique.\n\nEn effet, nous souhaitions proposer à nos utilisateurs un processus d’authentification unique pour toutes les applications ITESOFT utilisant le User Manager. Dans ce cadre, notre équipe R&D a été confrontée à plusieurs incertitudes.\n\nVerrouillage de l’écriture lors des imports\n\nLa première difficulté technique consistait à faire en sorte que le verrouillage d’un type d’éléments ne soit effectif que sur le tenant concerné afin de ne pas bloquer les opérations d’import ou d’écriture sur les autres tenant.\n\nUne autre difficulté était liée à la centralisation du mécanisme de verrouillage et de vérification des verrous. Effectivement, les APIs d’import et d’écriture se comptent par dizaine dans le code du User Manager. Il a fallu mettre en place ces algorithmes de verrouillage de façon centralisée afin de ne pas avoir à remanier le code et simplifier la maintenabilité (si un bug apparaît, il ne faut avoir à le corriger qu’à un seul endroit plutôt qu’à des dizaines d’endroits). Chaque opération d’écriture (import comprise) appelle une méthode dédiée indiquant quels types d’éléments sont concernés par l’opération et si un verrou doit être posé ou simplement vérifié. Nous avons donc ajouté un appel à la même méthode avec deux paramètres sur chaque API d’écriture pour pouvoir exploiter l’algorithme de verrouillage.\n\nEn outre, dans le cas de l’import global, il a fallu ajouter un parcours du fichier d’import en amont de l’appel à la méthode de verrouillage afin de déterminer quels types d’éléments sont concernés par l’import. Nous avons donc été amenés à développer un algorithme de parcours partiel afin de déterminer cette information de façon performante (on ne veut pas que cette lecture du fichier d’import qui ne vise qu’à déterminer quels éléments sont concerné ait un impact négatif sur les temps d’import).\n\nDe plus, la gestion des cas d’erreurs a été particulièrement critique. Il a fallu mettre en place un mécanisme de gestion des erreurs sans faille afin d’éviter qu’un verrou ne soit posé sans jamais être supprimé et ainsi éviter les blocages. C’est pour cette raison que le verrouillage et déverrouillage sont fait sur des transactions distinctes et que le mécanisme de timeout pour déverrouillage automatique a été mis en place.\n\nEnfin une dernière difficulté technique consistait à éviter les deadlock. Pour ce faire, nous avions mis en place un algorithme de vérification d’existence de verrou dépendant du thread d’exécution et du tenant afin d’éviter le blocage par réentrance. En effet, l’algorithme de pose de verrou peut être appelé de façon récursive (si une opération d’écriture appelle elle-même un opération d’écriture par exemple). Il a donc été nécessaire de vérifier si pour un thread et un tenant donné un verrou n’avait pas déjà été posé avant d’effectuer la phase de vérification et de pose de verrou. L’exploitation de ces informations, enregistrées en mémoire, empêche les deadlocks."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Capture Omnicanal :**\n  - Complexité des arborescences de définitions des permissions lors de la gestion centralisée des habilitations avec le mode User Manager.\n  - Solution proposée : Génération de règles génériques d’affectation de droits pour couvrir tous les cas d'arborescences complexes.\n\n- **Mode de Distribution :**\n  - Difficulté à faire correspondre le mode de distribution basé sur les corbeilles aux habilitations du mode User Manager.\n  - Nécessité d'une refonte technique de l'outil pour s'adapter aux structures organisationnelles complexes du client.\n\n- **Détection des Pages Blanches :**\n  - Aléa lié à la détection des pages blanches dans les documents.\n  - Problèmes rencontrés : \n    - Pages non parfaitement blanches, taches ou bords noirs non visibles à l'œil nu.\n    - Pages en couleurs nécessitant un traitement particulier.\n\n- **Diversité des Documents :**\n  - Incertitude scientifique concernant le traitement d'une variété de documents (papier, numériques, réseaux sociaux, audio, vidéo).\n  - Objectif : Trouver un modèle de traitement commun pour modéliser, extraire et structurer les métadonnées.\n  - Problème : Absence de méthode connue pour extraire des métadonnées de documents disparates.\n  - Exemples de métadonnées à extraire :\n    - Emails : Identifier l’émetteur et le récepteur.\n    - Fichiers .zip : Identifier quel document provient de quel fichier.\n    - Scans : Déterminer la couleur et différencier les pages.\n  - Solution : Développement de nouveaux algorithmes pour traiter tous les cas possibles.\n\n- **Outil Inflow :**\n  - Difficulté majeure : Concevoir un formulaire dynamique et paramétrable.\n  - Exigences : \n    - Adaptation aux différents formats de documents reconnus par l’OCR.\n    - Capacité à contenir jusqu'à 500 éléments tout en restant ergonomique et compréhensible."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nConcernant la Capture omnicanal, lors de nos développements sur la gestion centralisée des habilitations des utilisateurs avec le mode User Manager, nous avons fait face à la complexité des arborescences de définitions des permissions. Pour remédier à ce problème, générde règles génériques d’affectation de droits pouvant couvrir tous les cas de figures possibles d’arborescences complexes.\n\nUne autre difficulté consistait à faire correspondre notre mode de distribution basé sur la notion des corbeilles au notions d’habilitations du mode User Manager. Celles-ci correspondaient à des structures d’organisation assez complexes chez le client qu’une refonte technique de notre outil afin de remédier à ce problème.\n\nLe aléa concerne la détection des pages blanches afin de séparer les pages dans les documents et résidait dans l’existence des cas particuliers où les pages ne sont pas parfaitement blanches ou bien contiennent des taches ou des bord noirs. Ces défauts ne sont pas forcément visibles à l’œil nu, cependant ils peuvent nuire au traitement adéquat des pages. Un autre cas possible, réside dans l’existence de pages en couleurs dans les documents nécessitant un traitement particulier.\n\nUne autre incertitude scientifique à laquelle nous avons fait face était liée à la diversité des documents que nous voulions traiter au sein de notre solution. En effet, nous souhaitions trouver un modèle de traitement commun à tous les canaux (documents papier, documents numériques, flux de réseaux sociaux, documents audios ou vidéos) pour modéliser, extraire et structurer les métadonnées. Malheureusement, au moment de nos développements, il n’y avait pas de méthode connue pour extraire des métadonnées de documents aussi disparates. Les métadonnées peuvent être :\nDans le cas d’un email, déterminer l’émetteur ou le récepteur ;\nDans le cas de documents issus de fichiers .zip, savoir quel document vient de quel fichier .zip ;\nDans le cas d’un scan, déterminer la couleur, différencier les pages.\n\nAfin de remédier à ces problèmes, nous avons développé des nouveaux algorithmes capables de traiter tous les cas possibles.\n\nConcernant l’outil Inflow, la difficulté majeure était de pouvoir concevoir un formulaire suffisamment dynamique et paramétrable pouvant s’adapter aux différents formats des documents reconnus par l’OCR et pouvant contenir jusqu’au 500 éléments tout en gardant un aspect ergonomique et compréhensible."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Objectif de l'outil** : Récupérer les pièces jointes d'emails pour analyse par d'autres solutions développées.\n  \n- **Verrous d'hétérogénéité** :\n  - Problèmes d'encodage des pièces jointes selon les fournisseurs de messagerie.\n  - Nécessité d'interagir avec divers protocoles de communication.\n\n- **Outils de recensement** :\n  - Existence de solutions comme Chorus Pro pour le recensement des documents administratifs.\n  - Problématique d'accès efficace à ces documents.\n\n- **Mises à jour de l'outil** :\n  - L'outil de capture nécessite des mises à jour régulières durant lesquelles la capture est suspendue.\n  - Problèmes d'architecture des services pour permettre la mise en veille de l'outil sans impacter les autres solutions associées.\n\n- **Identification des documents** :\n  - Difficultés rencontrées dans l'extraction et l'identification des informations des documents.\n  - Cas d'usage signalés par les utilisateurs où les informations extraites étaient inexactes, nécessitant une révision manuelle.\n\n- **Personnalisation des solutions** :\n  - Variété des cas d'usage d'un client à l'autre engendrant un besoin de personnalisation importante.\n  - Développement d'un système adapté aux besoins spécifiques de chaque client."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNotre outil de capture de mail a pour but de récupérer les pièces présentes au sein d’un email afin qu’elles puissent être analysées par les autres solutions que nous développons. Nous avons été confrontés à des verrous d’hétérogénéité d’encodage des pièces jointes en fonction des fournisseurs de messagerie et donc des protocoles de communication avec lesquels nous devions interagir.  \n\nDes outils de recensement des documents administratifs sont mis en place par certaines institutions (i.e. Chorus Pro par exemple). La problématique principale concernant ces solutions repose sur notre capacité à accéder à ces documents de manière efficace.  \n\nAussi, notre outil de capture de document doit subir des mises à jour régulières durant lesquelles la capture même ne peut être effectuée. Nous avons alors fait face à des problématiques d’architecture de nos différents services afin que nous puissions mettre en veille notre outil de capture pendant ces étapes de mise à jour, sans pour autant que cela n’impacte les autres solutions qui lui étaient reliées.  \n\nAu-delà de la capture des documents, nous avons dû faire face à des problématiques d’identification des documents et des informations qui en ont été extraites. En effet, certains cas d’usage nous ont été remontés de la part de nos utilisateurs au cours desquels les informations identifiées n’étaient pas exactes et nécessitaient d’être revues manuellement. Le verrou majeur constituait la multitude de cas différents d’un client à un autre nécessitant un travail de personnalisation majeur. Il nous a alors fallu développer un système répondant aux besoins spécifiques de chaque client."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Temps de réponse des requêtes SQL :**\n  - Augmentations signalées par certains clients lors de l'extraction de documents de la base de données.\n  - Incertitude sur l'impact des opérateurs dans les requêtes.\n  - Sensibilité à la casse des caractères non établie comme facteur influent sur les délais d'extraction.\n\n- **Problématiques d'exploitation des applications :**\n  - Applications énergivores en mémoire et CPU.\n  - Validation de la capacité à encapsuler des microservices dans une application Spring Boot pour réduire la consommation de ressources.\n\n- **Difficultés de stockage des documents :**\n  - Obligation légale de stocker chaque document reçu au format PDF.\n  - De nombreux documents convertis dépassent le seuil de 300 Ko, entraînant des problèmes de stockage et des ralentissements lors des transferts.\n  - Impossibilité de sauvegarder une première instance du fichier PDF avant compression, car elle sert de document légal non modifiable.\n\n- **Optimisation de la relance automatique de factures :**\n  - Système actuel non optimisé avec de nombreuses tâches automatisées inutiles, consommant des ressources.\n  - Besoin de développer un système pour réduire le nombre d'opérations à effectuer.\n\n- **Problème de traitement des mails :**\n  - Analyse des mails trop lente au cours de la journée.\n  - Nécessité de différencier les types de contenus pour éviter le blocage lors du traitement de fichiers provenant de scanners.\n\n- **Conception d'une nouvelle architecture de User Manager :**\n  - Remplacement des utilisateurs par des agents techniques s'authentifiant via certificats au lieu de login/mot de passe.\n  - Identification de problématiques de gestion des sessions lors de la refonte de l'architecture."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nCertains de nos clients nous ont rapporté des augmentations des temps de réponse des requêtes SQL effectuées afin d’extraire certains documents de notre base de données. Une incertitude existait sur l’impact des opérateurs utilisés au sein des requêtes afin d’extraire ces documents. Plus précisément, il n’était pas clairement établi si la sensibilité à la casse des caractères était un facteur influençant de manière significative les délais d’extraction des documents.  \n\nNous avons dans un second temps identifié des problématiques d’exploitations de nos applications. En effet, ces dernières sont très énergivores en termes de consommation de mémoire et de CPU. Nous souhaitions valider ou invalider notre capacité à encapsuler nos différents microservices au sein d’application Spring Boot afin de limiter la consommation de mémoire et de CPU de nos applications, deux éléments en constante augmentation.  \n\nPar la suite, nous avons rencontré des difficultés de stockage des documents que nous recevions de la part de nos clients. Nous sommes dans l’obligation légale de stocker chacun des documents reçus sous le format PDF. Néanmoins, de nombreux documents, une fois convertis au format PDF, dépassaient le seuil de 300 ko que nous avions fixé. Ce dépassement posait par la suite des problèmes de stockage et des ralentissements lors des transferts et des requêtes des fichiers. Nous n’étions pas non plus en mesure de sauvegarder une première instance du fichier sous format PDF avant de le compresser par la suite puisque c’est cette première instance qui fait office de document légal, que nous ne pouvions donc par la suite pas modifier.  \n\nNotre solution actuelle de relance automatique de facture n’est également pas optimisée. Un trop grand nombre de tâches automatisées sont en effet lancées sans pour autant s’avérées utiles, ce qui demande des ressources inutilement au système. Nous avons donc souhaité mettre au point un système permettant d’alléger le nombre d’opérations à effectuer.  \n\nNous avons également identifié un problème de traitement des mails reçus. Nous n’étions en effet pas en mesure d’analyser les mails suffisamment rapidement au cours d’une journée. Il nous a fallu trouver un moyen de différencier les différents types de contenus à analyser puisque l’analyse des mails pouvaient être bloquée lorsque des fichiers provenant par exemple de scanner devaient être traités.  \n\nEnfin, afin de remplacer les utilisateurs par des agents techniques s’authentifiant via des certificats et non pas via une combinaison login/mot de passe, nous avons initié la conception d’une nouvelle architecture de User Manager. Des problématiques au niveau de la gestion des sessions ont néanmoins été identifiées en retravaillant l’architecture de notre solution."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### User Manager\n- **Limitation des sessions utilisateur** : \n  - Objectif : Restreindre à une seule session active par utilisateur pour des raisons de sécurité.\n  - Solution : Développement d'un mécanisme pour fermer automatiquement une session ouverte lors d'une nouvelle connexion.\n\n- **Sécurisation du processus de connexion** :\n  - Axes de développement :\n    1. Protection contre les attaques par force brute.\n    2. Salage des mots de passe.\n  - Problèmes rencontrés :\n    - Limitation de la charge sur les pages de connexion tout en empêchant les tentatives répétées d'attaques.\n    - Assurer que les mécanismes de sécurité ne pénalisent pas les utilisateurs légitimes.\n    - Concordance entre mots de passe renseignés et mots de passe salés nécessitant un mécanisme de stockage et de vérification.\n\n- **Synchronisation des répertoires Azure AD** :\n  - Problème : Absence de mécanisme de synchronisation automatique, nécessitant des fichiers d'importation manuels.\n  - Solution : Développement d'une solution d'automatisation de la synchronisation via les API Microsoft.\n\n- **Gestion des tenants** :\n  - Objectif : Créer des affectations entre un tenant global et les tenants clients.\n  - Problématique : Attribution des droits au tenant global sans compromettre la sécurité des accès entre différents tenants clients.\n\n#### Document Manager\n- **Serveurs de connexion** :\n  - Développement pour permettre aux utilisateurs de se connecter à différents serveurs.\n  - Incertitude : Capacité à répartir les utilisateurs sur les serveurs de manière transparente.\n\n- **Gestion des documents liés** :\n  - Objectif : Fournir un accès direct aux documents dérivés liés à un document source.\n  - Développement nécessaire pour gérer le stockage et définir les liens entre documents.\n\n- **Habilitations spécifiques** :\n  - Problème : Profils utilisateurs trop génériques pour une attribution adéquate des habilitations.\n  - Conséquence : Nécessité de modifications spécifiques sur le frontend de Document Manager pour intégrer des habilitations spécifiques.\n\n#### BPMN+ Engine\n- **Déploiement en microservices** :\n  - BPMN+ Engine : Serveur autonome pour gérer le cycle de vie des processus d'entreprise.\n  - Problème : Mauvaise adaptation aux architectures cloud de type microservices.\n  - Solution : Déploiement de BPMN+ Engine en microservices pour faciliter son utilisation sur le cloud.\n\n- **Problèmes de duplication et synchronisation** :\n  - Problématique : Duplication des données entraînant des problèmes de synchronisation entre BPMN+ Engine et User Manager.\n  - Solution : Simplification des services proposés pour éviter de complexifier l'utilisation de BPMN+ Engine dans un environnement cloud."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nUser Manager\n\nCertains de nos clients souhaitaient limiter le nombre de sessions ouvertes par utilisateur. Plus spécifiquement, ils souhaitaient que chaque utilisateur ne puisse avoir, au maximum, qu’une seule session active. Cette volonté s’appuie sur des considérations de sécurité informatique. Il nous a alors fallu trouver une solution permettant de fermer automatiquement une session ouverte si un utilisateur tentait de se connecter sur un second poste. \n\nIl nous a également fallu sécuriser tout le processus de connexion à nos services. Plus spécifiquement nous avons réalisé des développements sur deux axes principaux : la protection contre les attaques brutes de force, et le salage des mots de passe. En ce qui concerne le premier axe, le verrou que nous avons souhaité lever consistait à limiter la charge des pages de connexion tout en empêchant les tentatives répétées de connexion d’un potentiel attaquant. Dans le même temps, nous devions nous assurer que les mécanismes mis en place ne pénalisent pas les tentatives de connexion provenant d’un humain. Notre volonté de « saler » les mots de passe de nos utilisateurs nous a confronté à des problèmes de concordance entre les mots de passe renseignés par nos utilisateurs et les mots de passe « salés ». Nous avons donc dû intégrer un mécanisme de stockage et de vérification des mots de passe. \n\nOpenID Connect…..\n\nNous souhaitions aussi synchroniser les répertoires Azure AD de nos clients avec notre répertoire User Manager. Or, il n’existe pas de mécanisme de synchronisation automatique et nos clients étaient dans l’obligation de stocker les informations de leurs utilisateurs dans des fichiers importés par la suite. Il nous a alors fallu trouver une solution d’automatisation de la synchronisation, en s’appuyant notamment sur les API proposées par Microsoft.\n\nAu sein d’User Manager, nous souhaitions aussi créer des affectations entre un tenant global et les autres tenants clients. Ce tenant global joue alors un rôle d’administration afin de gérer, créer ou supprimer des tenants existants. La principale problématique que nous avons rencontrée était d’attribuer les droits nécessaires à ce tenant global sans compromettre la perméabilité des accès entre différents tenants clients. \nDocument Manager\nAu sein de Document Manager, nous avons réalisé des développements dans l’objectif de proposer aux utilisateurs différents serveurs auxquels ils pourront se connecter. La principale incertitude concernait notre capacité à répartir les utilisateurs sur les différents serveurs de manière invisible pour eux. \n\nLes documents répertoriés au sein de Document Manager peuvent avoir des liens de dépendance avec un document source. Il s’agissait ici de fournir aux utilisateurs un accès direct aux documents dérivés liés à un document source. Des développements ont donc été entrepris afin de gérer le stockage des documents afin de définir les liens existants entre eux. \n\nNous souhaitions enfin mettre en place des habilitations spécifiques en fonction du profil utilisateur. Les profils existants étaient néanmoins trop génériques pour pouvoir affiner de manière adéquate les différentes habilitations des utilisateurs. L’attribution d’une habilitation spécifique induisait également des modifications spécifiques sur le frontend Document Manager que nous avons dû prendre en compte. \nBPMN+ Engine\nBPMN+ Engine est un serveur autonome permettant de gérer tout le cycle de vie des processus d’entreprise et leur exécution. Néanmoins, il s’adapte mal aux architectures cloud de type microservices. Dans la continuité des travaux des années précédentes, nous avons souhaité déployer l’architecture BPMN+ Engine en une série de microservices, ce qui faciliterai alors son déploiement sur le cloud. Nous avons rencontré des problématiques de duplication des données, engendrant elles-mêmes des problèmes de synchronisation de BPMN+ Engine avec User Manager. Nous avons alors dû réaliser un travail de simplification des services proposés, afin de ne pas complexifier l’utilisation sous le format cloud de BPMN+ Engine."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Verrous, aléas, incertitudes\n\n- **Création d'une série synthétique continue avec saisonnalité**  \n  - Absence de méthode standard pour créer une série de prix synthétique continue avec un profil de saisonnalité.\n  - Observations sur l'inefficience de certaines approches à capturer la saisonnalité, même en présence de celle-ci (exemples : analyse des prix du gaz naturel vs. pétrole dans l'article de Ewald & al., 2021).\n  - Présence d'approches personnalisées selon le sous-jacent, risquant un curve-fitting au lieu d'une méthode généralisable.\n\n- **Détection fiable et robuste de la saisonnalité**  \n  - La saisonnalité annuelle limite le jeu de données à quelques dizaines de périodes.\n  - Défi principal : concevoir une méthode robuste pour une détection fiable de la saisonnalité.\n  - Risque de curve-fitting en raison du faible volume de données, entraînant une modélisation non généralisable dans le temps ou à travers les sous-jacents.\n  - Validation des résultats souhaitée à travers plusieurs approches concurrentes.\n\n- **État de la littérature**  \n  - Résultats mitigés dans les travaux précédents concernant la capture de la saisonnalité, souvent assimilée à un effet aléatoire.\n  - Certaines études fournissent des observations empiriques validées statistiquement a posteriori.\n  - Absence d'approche ad-hoc pour capturer les périodes notables de l'effet saisonnier à appréhender.\n  - Verrou à adresser : définition d'une méthodologie pour la détection de la saisonnalité et cadre d'évaluation associé."}, {"role": "assistant", "content": "**Verrous, aléas, incertitudes**\n\nCréation d’une série synthétique continue exhibant un profil de saisonnalité quel que soit le sous-jacent  \nNotre analyse de l’état de l’art n’a fait apparaitre aucune méthode « standard » permettant de créer une série synthétique de prix qui soit continue et qui exhibe un profil de saisonnalité. Au contraire, parmi les articles que nous avons pu étudier, nous avons pu tirer des observations quant à l’inefficience de certaines approches à exhiber le profil de saisonnalité, même lorsque celui-ci est bien présent (cf. analyse des séries de prix du gaz naturel versus celles du pétrole dans l’article de Ewald & al., 2021). En outre, la littérature a fait apparaitre plusieurs approches personnalisées de modélisation selon le sous-jacent considéré risquant un curve-fitting, plutôt qu’une méthode généralisable.  \nDétection fiable/robuste de la saisonnalité  \nConsidérant une saisonnalité annuelle, notre jeu de données se restreint et ne contient ainsi plus que quelques dizaines de périodes. Tout l’enjeu de nos travaux réside ainsi dans la conception d’une méthode robuste permettant une détection fiable de la saisonnalité. Là encore, étant donné le faible volume de données, nous nous exposons à un risque de curve-fitting qui serait alors une modélisation non généralisable dans le temps ni à travers les sous-jacents. Pour assurer la fiabilité de cette détection de tendance, nous souhaitons pouvoir valider les résultats à travers plusieurs approches concurrentes.  \nComme décrit dans la littérature, les précédents travaux font état de résultats mitigés quant à la capture de saisonnalité, assimilable à un effet aléatoire. D’autres travaux témoignent quant à eux d’observations empiriques validées statistiquement a posteriori. Toutefois, aucune approche ad-hoc n’a été proposée dans la littérature pour capturer les périodes notables dans l’effet saisonnier que nous souhaitons appréhender : le verrou que nous souhaitons adresser vise ainsi la définition d’une telle méthodologie ainsi que le cadre d’évaluation de celle-ci."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### API de recherche multimodale\n- **Augmentation des volumes de données** : Passer de 12 millions à 56 millions de points de trajets référencés (augmentation par un facteur de 4,7).\n- **Complexité algorithmique** : Multiplication par un facteur de 5 de la complexité algorithmique.\n- **Nombre de recherches quotidiennes** : Augmentation de 250 000 à 950 000 recherches quotidiennes (multiplication par 3,8) grâce à l'application RATP.\n- **Premier verrou** : Augmentation significative de la charge à laquelle le système devra faire face.\n- **Niveau de service requis** : Maintenir un temps de réponse inférieur à 500 ms pour 95% des requêtes et un coût d'hébergement limité à 2 fois le montant précédent.\n- **Estimation des performances** : Les performances de l'API VIA doivent être augmentées d'un facteur 45 (calcul : 4,7 x 5 x 3,8 / 2).\n- **Deuxième verrou** : Nécessité d'améliorer les performances à charge égale tout en supportant la montée en charge.\n\n#### Hubs\n- **Confort des utilisateurs** : Prendre en compte le confort des conducteurs et des passagers dans la création de hubs urbains de covoiturage.\n  - Étude préliminaire : Conducteurs prêts à dévier légèrement leur trajet et passagers prêts à marcher un peu pour le compromis.\n- **Aspect humain** : Importance de satisfaire aux exigences de chaque utilisateur dans le covoiturage.\n- **Échelle de la solution** : Viser l'ensemble de la zone urbaine du territoire français.\n- **Second verrou** : Volume de données à traiter pour rendre chaque zone accessible par les hubs urbains.\n  - **Accessibilité** : Définie comme étant située à moins de 5 minutes de marche d'au moins un hub.\n  - **Calcul théorique** : Minimum de 390 000 hubs nécessaires.\n- **Connectivité en temps réel** : Les hubs doivent être reliés par des lignes virtuelles correspondant aux besoins des chauffeurs et passagers.\n- **Source des données** : Extraction des données à partir de bases open data, notamment OpenStreetMap.\n  - **Limites de la base OpenStreetMap** : \n    - Manques de précision géométrique.\n    - Incohérences sémantiques et topologiques.\n    - Manque d’exhaustivité."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAPI de recherche multimodale\nLes évolutions fonctionnelles visant à améliorer notre API VIA impliquent une augmentation très importante des volumes de données à rechercher, de la complexité algorithmique et du nombre de calculs quotidiens d'itinéraires.\nNous passons de 12 à 56 millions de points de trajets référencés, soit une augmentation du volume de données d’un facteur 4,7. Par ailleurs, nous multiplions la complexité algorithmique par un facteur 5. Enfin, l’exposition supplémentaire offerte par l’application RATP a permis de multiplier par 3,8 le nombre de recherches quotidiennes, passant ainsi de 250 000 à 950 000. L’augmentation en charge à laquelle notre système devra faire face constitue le premier verrou de ce projet.\n\nDans le même temps, nous souhaitons conserver le même niveau de service, à savoir un temps de réponse inférieur à 500ms pour 95% des requêtes, et un coût d'hébergement restreint. Nous avons décidé de limiter ce dernier à 2 fois le montant de l'hébergement avant évolutions. En cumulant l'ensemble de ces facteurs, on peut estimer que les performances de l'API VIA doivent être augmentées d'un facteur 45 (4,7 x 5 x 3,8 / 2) par rapport à la première version. Notre solution, en plus de supporter la montée en charge, devra donc largement améliorer ses performances à charge égale. L’envergure de l’amélioration que nous visons constitue le deuxième verrou de ce projet.\nHubs\nUn premier verrou dans le cadre de la création de hubs urbains de covoiturage est la nécessité de prendre en compte le confort, à la fois des conducteurs et des passagers. Bien qu’une étude préliminaire que nous avons menée nous ait permis de mettre en évidence que les uns comme les autres sont prêts à faire certains efforts pour arriver à un compromis (les chauffeurs peuvent dévier un peu de leur trajet pour prendre des passagers, ces derniers sont prêts à marcher un petit peu pour limiter ladite déviation), le covoiturage reste un moyen de transport personnalisé qui doit satisfaire aux exigences de chacun de ses utilisateurs. Il nous faut donc considérer l’aspect « humain » dans notre solution.\n\nL’échelle que nous souhaitons donner à notre solution, à savoir l’ensemble de la zone urbaine du territoire français, implique un second verrou qui est le volume de données à traiter. Il s’agit donc de faire en sorte que cette zone soit intégralement accessible par le réseau de hubs urbains identifiés. Par « accessible » nous entendons située à moins d’une certaine durée de marche à pied d’au moins un hub. Si nous considérons cette durée de l’ordre de 5 minutes, un premier calcul (détaillé dans les travaux), nous permet d’obtenir un minimum théorique de 390 000 hubs. Ces hubs doivent ensuite pouvoir être reliés en temps réel par des lignes virtuelles correspondant aux besoins d’un chauffeur et de ses passagers.\n\nUn dernier verrou réside dans la source de nos données pour l’identification des hubs. En effet, nos contraintes nous imposent d’extraire nos données de bases open data, et en particulier de la base OpenStreetMap. S’il s’agit de la plus grande base de données géographiques du monde, issue de la contribution de plus d’un million de personnes, elle présente plusieurs limites. En effet, des études, confirmées par nos travaux, ont mis en évidence des manques de précision géométrique des éléments renseignés, certaines incohérences sémantiques et topologiques et un manque d’exhaustivité."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Verrou lié au confort des utilisateurs :**\n  - Importance de prendre en compte le confort des conducteurs et des passagers dans la création de hubs urbains de covoiturage.\n  - Étude préliminaire : conducteurs et passagers prêts à faire des compromis (conducteurs peuvent dévier légèrement leur trajet, passagers prêts à marcher un peu).\n  - Nécessité d'une solution qui respecte les exigences de tous les utilisateurs.\n  - Objectif : créer un maillage de hubs « sans trous » pour minimiser les déplacements des passagers et éviter le recouvrement des zones pour réduire la dispersion des demandes.\n\n- **Variabilité de l'offre de covoiturage :**\n  - Comparaison avec les transports publics : ces derniers ont des horaires fixes et une localisation stable, contrairement à l'offre de covoiturage qui peut varier.\n  - Facteurs d'impact sur l'offre de covoiturage : acceptation/annulation de trajets, indisponibilité de conducteurs, changements d'itinéraires, jours et horaires.\n  - Volume de modifications dans le réseau de covoiturage est beaucoup plus élevé que dans les transports en commun.\n  - Nombre théorique de hubs : supérieur à 390 000.\n  - Croissance du nombre d'utilisateurs du service.\n  - Volumétrie des données à traiter constitue un verrou majeur pour le projet R&D.\n  - Exigences de rapidité : modifications de trajets doivent être intégrées en moins d'une minute (idéalement 30 secondes) et temps de réponse pour recherche d'itinéraire doit être inférieur à 500 ms pour 95% des requêtes (idéalement 100 ms).\n\n- **Convergence des données :**\n  - Nécessité de représenter les données du service dans un format compatible avec les réseaux de transport en commun.\n  - Complexité de la transposition due à des différences fondamentales entre les données.\n  - Risques d'effets négatifs :\n    - Directs : génération de volumes de calcul incompatibles avec les contraintes.\n    - Indirects : apparition d'incohérences métier pouvant nuire à la qualité du service proposé par Klaxit."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nUn premier verrou dans le cadre de la création de hubs urbains de covoiturage est la nécessité de prendre en compte le confort, à la fois des conducteurs et des passagers. Bien qu’une étude préliminaire que nous avons menée nous ait permis de mettre en évidence que les uns comme les autres sont prêts à faire certains efforts pour arriver à un compromis (les chauffeurs peuvent dévier un peu de leur trajet pour prendre des passagers, ces derniers sont prêts à marcher un petit peu pour limiter ladite déviation), le covoiturage reste un moyen de transport personnalisé qui doit satisfaire aux exigences de chacun de ses utilisateurs. Il nous faut donc considérer l’aspect « humain » dans notre solution. Plus spécifiquement, cela implique de constituer un maillage de hubs « sans trous », pour que les passagers n’aient pas à se déplacer trop loin de leur point d’origine, mais également sans recouvrement, pour éviter de trop disperser les demandes des passagers dans une même zone géographique et ainsi diminuer la charge des chauffeurs.\n\nUn autre élément problématique apparaissant dans le cadre de ce projet est la relative variabilité de la disponibilité de l’offre de covoiturage par rapport aux transports publics. En effet, si ces derniers sont stables dans le temps, avec des horaires de passage par station établis à l’avance et la liste des stations ainsi que leur localisation fixés, l’offre de covoiturage peut potentiellement être impactée par de nombreux facteurs : l’acceptation ou l’annulation d'un trajet par un passager ou un conducteur, la mise en indisponibilité d’un conducteur, le changement d'itinéraire, de jours et/ou d’horaires de passage d’un conducteur, etc. En conséquence, le volume des modifications applicables au réseau sur une journée sera beaucoup plus grand que les modifications appliquées à un réseau de transport en commun. A cela s’ajoutent le nombre de hubs théoriquement supérieur à 390 000 ou encore le nombre important et croissant d’utilisateurs de notre service. La volumétrie des données à traiter constitue donc le second verrou de notre projet R&D. D’autant plus que les modifications appliquées aux trajets doivent être prises en compte par le moteur de routage en moins d'une minute (idéalement 30 secondes) et que les temps de réponse pour une recherche d'itinéraire ont pour objectif d'être inférieurs à 500ms pour 95% des requêtes (idéalement 100 ms).\n\nUn dernier verrou réside dans la convergence de nos données avec celles exploitées par les outils sur lesquels nous appuierons nos développements. En effet, nous allons devoir représenter les données issues de notre service dans un format spécifique à la représentation des réseaux de transport en commun. Des différences fondamentales rendent particulièrement complexe cette transposition. Les effets négatifs peuvent être directs, avec la génération de volumes à calculer incompatibles avec nos contraintes ou indirects avec l’apparition d’incohérences métier susceptibles de léser la qualité du service proposé par Klaxit."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Aléas et incertitudes scientifiques :**\n  - Dérive des jauges de contrainte au fil du temps.\n  - Nécessité de techniques de correction du fluage complexes.\n  - Calibration fréquente requise pour maintenir la précision.\n  - Utilisation de la thermogravimétrie pour améliorer la précision des mesures, mais nécessite des conditions de température contrôlées.\n  - Difficulté à maintenir des conditions environnementales stables dans des environnements industriels variables.\n\n- **Verrous technologiques :**\n  - Intervention humaine régulière nécessaire pour l'étalonnage, limitant l'automatisation complète.\n  - Risques d'erreurs humaines liés à l'étalonnage manuel.\n  - Complexité des calculs pour la détermination des incertitudes en temps réel, posant des défis de performance et de ressources de calcul.\n  - Intégration de nouveaux matériaux piézorésistifs requérant des recherches supplémentaires pour assurer leur fiabilité et durabilité en milieu industriel.\n\n- **Objectifs à surmonter :**\n  - Développer des systèmes autonomes.\n  - Créer des systèmes peu énergivores et robustes.\n  - Assurer un fonctionnement efficace dans des conditions variées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLe projet de développement d’une balance connectée pour la gestion des stocks de consommables et de bouteilles de vin rencontre plusieurs aléas et incertitudes scientifiques. L'une des principales incertitudes réside dans la dérive des jauges de contrainte au fil du temps, nécessitant des techniques de correction du fluage complexes et une calibration fréquente. De plus, les méthodes telles que la thermogravimétrie, bien que prometteuses pour améliorer la précision des mesures, requièrent des conditions de température contrôlées difficiles à maintenir dans des environnements industriels variables. Cette dépendance à des conditions environnementales stables constitue un défi majeur pour l'application pratique de ces techniques.\n\nLes verrous technologiques incluent également la nécessité d'une intervention humaine régulière pour l'étalonnage, ce qui limite l'automatisation complète du système et introduit des risques d'erreurs humaines. La complexité des calculs nécessaires pour la détermination des incertitudes en temps réel pose des défis en termes de performance et de ressources de calcul. Enfin, l'intégration de nouveaux matériaux piézorésistifs, qui offrent des avantages en termes de précision et de stabilité, nécessite encore des recherches supplémentaires pour garantir leur fiabilité et durabilité dans des environnements industriels. Ces défis doivent être surmontés pour développer des systèmes autonomes, peu énergivores et robustes, capables de fonctionner efficacement dans des conditions variées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Verrou sur la détermination d'un accident :**\n  - Difficulté principale : localisation du capteur au niveau de la tête.\n  - Complexité : mesure des basculements de la tête, incluant les inclinaisons volontaires du porteur.\n\n- **Confirmation d'accident :**\n  - Basée sur plusieurs éléments traités par un algorithme développé.\n  - Utilisation de multiples mesures en temps réduit lors de la détection d'anomalies.\n  - Confrontation des mesures à une grandeur d'accélération acquise indépendamment, provenant du smartphone de l'utilisateur.\n\n- **Verrou sur la conception d'un système compact :**\n  - Objectif : compatibilité avec un large éventail de casques commerciaux.\n  - Différentes architectures testées pour intégrer tous les composants tout en réduisant l'encombrement final.\n\n- **Difficultés en géolocalisation :**\n  - Problématiques rencontrées, notamment en temps réel.\n  - Développement d'un algorithme pour extraire les intersections de l'itinéraire suggéré pour activer les clignotants.\n  - Complexité de la détection des écarts à l'itinéraire, nécessitant une détection en temps réel de la position GPS avec une précision adéquate."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’un des verrous que notre travail visait à lever portait sur la détermination d’un accident lors d’un trajet. La principale difficulté, non adressée dans l’état de l’art résidait dans la localisation du capteur, au niveau de la tête. La complexité de la mesure des basculements de la tête doit prendre en compte les situations où le porteur du dispositif penche volontairement la tête.\n\nNous avons alors décidé de baser la confirmation d’accident sur différents éléments qui sont traités par l’algorithme que nous avons développé : une multitude de mesures dans un temps réduit suivant la détection d’une anomalie et une confrontation de ces mesures à une grandeur d’accélération acquise indépendamment du dispositif, issue du smartphone de l’utilisateur.\n\nUn autre verrou consistait à concevoir un système compact et compatible avec un grand nombre de casques du commerce : différentes architectures ont été testées pour intégrer l’ensemble des composants nécessaires en réduisant l’encombrement final.  \nNous avons par ailleurs rencontré de nombreuses difficultés sur les problématiques de géolocalisation, notamment en temps réel. En effet, nous souhaitons développer un algorithme capable d’extraire de l’itinéraire suggéré l’ensemble des intersections auxquelles le cycliste va tourner pour actionner les clignotants. La complexité de nos travaux est également dans la détection des écarts à cet itinéraire, puisqu’il faut détecter en temps réel la position GPS avec une précision suffisante."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### Développement d’algorithmes de détection de freinage et de chute\n- **Travaux antérieurs** : De nombreux travaux ont été réalisés, mais aucun ne traite spécifiquement de la pollution des mesures due aux vibrations internes et externes.\n- **Verrou principal** : Caractère aléatoire et chaotique des vibrations, rendant difficile l'établissement d'un modèle théorique représentatif.\n- **Mouvements du pilote** : Bien que moins aléatoires, les mouvements du pilote (rotation de la tête, translation sur la selle, etc.) peuvent se superposer aux mouvements du véhicule, faussant ainsi les mesures.\n- **Contexte spécifique** : Les verrous sont accentués pour un vélo, où les vitesses et les décélérations sont plus faibles.\n- **Absence de références** : Pas de référence dans l'état de l'art pour un deux-roues sur routes de qualité variable, entraînant des incertitudes sur les méthodes de filtrage et les seuils à utiliser.\n\n#### Développement d’un protocole de communication distante entre plusieurs devices\n- **Limitation de la technologie BLE** : Incapacité à connecter un système à plusieurs périphériques.\n- **Diversité de scénarios** : Nécessité de considérer des scénarios de communication avec 2 à 4 systèmes, en réponse à la révision du cahier des charges.\n- **Rôle de la télécommande** :\n  - En présence d'un smartphone : configuration en réception d’informations, servant de relais pour les Cosmo Bike.\n  - En absence de smartphone : la télécommande pilote les Cosmo Bike et leur allumage.\n- **Consommation de batterie** : Considérations sur la consommation de batterie limitant le choix des technologies de communication.\n- **Limitation de mémoire RAM** : Les composants choisis initialement ne permettent pas la cohabitation de deux modes d’échanges via le protocole BLE.\n\n#### Développement d’un prototype aux performances lumineuses améliorées\n- **Objectif du prototype** : Créer un dispositif compact et économique capable de générer une lumière puissante et bien répartie pour un feu stop et deux clignotants, avec une puissance ajustée à la luminosité ambiante.\n- **Verrous technologiques** :\n  - Technologie chip-on-board : Bien que potentiellement adéquate, les dispositifs disponibles sur le marché ont des formes et tailles standardisées incompatibles avec le produit.\n  - Nécessité de concevoir une architecture propre en utilisant cette technologie.\n- **Incertitudes critiques** : Choix du substrat, densité et répartition des LEDs, caractéristiques de la couche de protection.\n- **Problème d’auto-éclairage** : Les dispositifs d’éclairage à puissance adaptative existants ne rencontrent pas de pollution lumineuse, contrairement à notre prototype de petite taille, qui pose un problème d’auto-éclairage du capteur de luminosité.\n- **Fixité du capteur** : L'emplacement fixe du capteur sur le circuit imprimé empêche son déplacement dans une zone non éclairée par le prototype."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDéveloppement d’algorithmes de détection de freinage et de chute  \nLe développement d’algorithmes de détection de freinage et de chute ont fait l’objet de nombreux travaux dans le passé. Aucun d’eux, cependant, n’aborde spécifiquement la pollution des mesures causée par des vibrations (pouvant être internes aussi bien qu’externes) ou des mouvements parasites dans le cas où le(s) capteurs se situent sur un individu.  \n\nDans ce contexte, le premier verrou réside dans le caractère aléatoire et chaotique des vibrations. Leurs sources sont si nombreuses qu’il est difficile de mettre en place un modèle théorique représentatif d’un certain type de vibrations. De plus, si les mouvements du pilote sont moins aléatoires et plus facilement identifiables (tourner la tête à gauche ou à droite, s’avancer ou se reculer sur la selle, se pencher dans un virage, se baisser ou lever la tête sont tous des mouvements correspondant à des translations ou rotations simples), ils peuvent facilement se superposer aux mouvements du véhicule et fausser les mesures. Ces verrous sont d’autant plus marqués dans le cadre de l’utilisation d’un vélo, où les vitesses, et donc les décélérations ainsi que la violence des chutes, sont plus faibles.  \n\nLe fait qu’il n’existe pas de référence dans l’état de l’art pour notre contexte, à savoir un deux roues, motorisé ou non, évoluant sur des routes de qualité variable dans divers environnements, génère des incertitudes supplémentaires : quelle méthode de filtrage utiliser ? avec quel seuil ?  \nDéveloppement d’un protocole de communication distante entre plusieurs devices  \nLa limitation fondamentale de la technologie BLE que nous souhaitions adresser était l’incapacité d’un système à se connecter à plusieurs périphériques.  \nUne difficulté qui s’est également posée lors de notre étude était liée au besoin de considérer une grande diversité de scénarios de communication avec deux, trois ou quatre systèmes. Cette difficulté a été posée par la révision du cahier des charges en vue de satisfaire l’ambition de notre partenaire de proposer de nouveaux modes d’utilisation des systèmes Cosmo Bike conçus.  \nPar ailleurs, l’un des systèmes pouvant être impliqué dans la communication, la télécommande, a un rôle double qui doit être examiné dans les scénarios de communication :  \nEn présence du smartphone, la télécommande doit être configurée en réception d’informations et doit servir de relais pour la communication aux Cosmo Bike ;  \nEn l’absence du smartphone, c’est la télécommande qui pilote les Cosmo Bike et leur allumage.  \nEnfin, à toutes ces difficultés à adresser se sont ajoutées des considérations de consommation de batterie restreignant notre choix de technologies de communication.  \nUne autre problématique était liée à la limitation en mémoire RAM au sein des composants que nous avions initialement choisis, qui rendait impossible la cohabitation de deux modes d’échanges via le protocole BLE.  \nDéveloppement d’un prototype aux performances lumineuses améliorées  \nLe développement d’un prototype, de taille et de coût réduits, capable de générer une lumière suffisamment puissante, bien répartie pour représenter précisément un feu stop et deux clignotants, avec une puissance dépendant de la luminosité ambiante, soulève également certains verrous.  \n\nEn effet, si la technologie du chip-on-board, présentée dans l’état de l’art, pourrait à première vue remplir les critères en termes de puissance et de répartition de la lumière de notre prototype, les dispositifs équipés de cette technologie disponibles sur le marché ont une forme et des tailles standardisées incompatibles avec notre produit. Il nous faut donc concevoir puis développer notre propre architecture en s’appuyant sur cette technologie. Comme mis en avant dans l’état de l’art, le choix du substrat, la densité et la répartition des LEDs ainsi que les caractéristiques de la couche de protection constituent des incertitudes cruciales dans le développement du prototype.  \n\nPar ailleurs, si des dispositifs d’éclairage à puissance adaptative à la luminosité extérieure ont été développés dans le passé, en particulier dans le domaine automobile, la taille des produits sur lesquels ceux-ci étaient implémentés (typiquement une voiture) impliquait qu’il n’y avait pas de pollution lumineuse de la part dudit dispositif vis-à-vis de son propre capteur. Or, dans le cas de notre prototype, sa taille réduite induit un problème d’auto-éclairage du capteur de luminosité.  \nDe plus, l’emplacement sur le circuit imprimé du capteur de luminosité étant fixe, il était exclu de pouvoir déplacer le capteur dans une zone du prototype que n’atteint pas la lumière émise."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Conception de la solution de détection :**\n  - Difficultés rencontrées lors de la conception d'une solution de détection intégrée dans un système complexe.\n  - Développement d'algorithmes nécessaires pour interpréter les mesures des capteurs.\n\n- **Choix de la technologie des capteurs :**\n  - Aucun capteur disponible ne répondait aux besoins et contraintes initiales.\n  - Adoption de la technologie capacitive pour les deux projets.\n\n- **Développement des algorithmes :**\n  - Nécessité de créer des algorithmes adaptés à chaque application des projets.\n  - Limitations de la technologie capacitive : peu de paramètres modifiables (seuil de détection et nombre d’échantillons).\n\n- **Seuil de détection :**\n  - Défini par le différentiel de capacité (somme des différentiels) par rapport à un champ de base.\n  - Un seuil élevé permet une détection fine mais rend le déclenchement difficile.\n  - Un seuil faible permet une détection rapide mais augmente les risques de fausses détections.\n\n- **Compromis entre précision et consommation d'énergie :**\n  - Nécessité de trouver un équilibre entre précision de détection et consommation d'énergie, particulièrement pour des applications IoT.\n  - Acquisition d'un grand nombre d'échantillons nécessite une consommation d'énergie élevée, limitant l'autonomie.\n\n- **Expérimentation pour déterminer le point de fonctionnement optimal :**\n  - Étude expérimentale pour établir un compromis entre consommation de batterie et précision.\n\n- **Dépendance de la précision de détection :**\n  - La précision de détection varie en fonction des matériaux recouvrant le capteur (nature, épaisseur, etc.).\n  - Validation des performances de détection sur des systèmes réels après des études en laboratoire sur prototypes.\n\n- **Interprétation des signaux de détection (premier projet) :**\n  - Besoin de détecter des signaux et de les interpréter pour répondre aux commandes utilisateur.\n  - Difficulté à détecter de petits objets (ex. : doigts) sans confondre avec des perturbations (ex. : branches).\n  - Importance de relever fréquemment la capacité tout en maintenant une autonomie suffisante.\n\n- **Second projet :**\n  - Expérimentation de différentes technologies pour détecter une présence sans recourir à des méthodes intrusives.\n  - Étude des cas limites pour réduire la taille du système de détection tout en maintenant une bonne précision de mesure."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDurant ces travaux de R&D, nous nous sommes heurtés à de nombreux verrous concernant la conception d’une solution de détection, l’intégration de cette solution dans un système plus complexe ainsi que le développement des algorithmes dédiés permettant d’interpréter les mesures acquises par les capteurs.\n\nPour les deux projets décrits ci-après, nous avons rencontré de nombreux verrous technologiques concernant dans un premier temps le choix de la technologie de capteurs, puisqu’aucune ne répondait, en l’état à nos besoins et à nos contraintes.  \nUne fois la technologies capacitive choisie pour les deux projets, nous avons dû développer des algorithmes permettant de décliner cette solution à nos deux applications. La technologie capacitive dispose de peu de paramètres modifiables, seul le seuil de détection et le nombre d’échantillons nous permettent d’intervenir sur les détections pour répondre à nos besoins. Ainsi, nous avons dû converger vers la meilleure solution possible en expérimentant les interactions entre ces deux paramètres et les résultats de détection.  \nLe seuil de détection étant le différentiel de capacité (ou plus précisément la somme des différentiels) à partir duquel l’appareil déclenche la détection, il est défini par rapport à un champs de base. Plus il est grand, plus la détection va être fine, et moins il sera facile de déclencher le dispositif. A l’inverse, plus il est petit, plus nous allons capter des objets rapidement, de loin, mais aussi des perturbations.  \nNous devions ainsi déterminer un compromis pour aboutir à une précision de détection compatible avec nos besoins, pour chacun des deux projets et répondant aux contraintes spécifiques à chaque environnement d’utilisation de notre solution.  \nPour ce qui est du nombre d’échantillons ; prendre des échantillons, et donc « scanner » le champs requiert de l’énergie.  \nAinsi, si nous souhaitons acquérir un grand nombre d’échantillons et donc bénéficier d’une bonne précision, l’appareil doit scanner tout le temps, or la consommation de batterie par ce scan risque de limiter l’utilisation de notre solution, tout particulièrement dans le domaine de l’IoT. En effet, une solution « nomade » serait difficilement utilisable si elle a une faible autonomie. Nous devions ainsi déterminer un compromis entre précision et consommation d’énergie par notre système.  \nDans nos deux projets, nous n’avons pu étudier la détermination de ce point de fonctionnement optimal (du point de vue de la consommation de batterie et de la précision) que de manière expérimentale.  \nUne autre difficulté liée à la transposition du développement de notre solution en environnement expérimental à l’environnement réel portait sur la dépendance de la précision de détection aux matériaux (nature, épaisseur, etc.) recouvrant le capteur.  \nSi les études de conception ont été réalisées en laboratoire sur des prototypes dans un premier temps, nous avons dû poursuivre notre expérimentation sur les systèmes sortis d’usine pour valider les performances de détection sur le système final.\n\nDe plus, dans le cadre du premier projet, nous devions en plus de capter les signaux de détection, les interpréter pour comprendre les commandes voulues par l’utilisateur. Cette analyse est gérée purement par du software. Une des difficultés rencontrées était liée au besoin de détecter même des doigts très petits (donc plus petit seuil de détection) mais pas le passage d’une branche par exemple. Aussi, il est important de relever souvent la capacité, pour vérifier si l’utilisateur demande une commande mais de garder une autonomie suffisante.\n\nEnfin, pour notre second projet, nous avons expérimenté différentes technologies, notre but étant de détecter une présence et non un mouvement, sans recourir à des technologies intrusives. Une fois la technologie fixée, nous avons étudié les cas limites nous permettant de réduire la taille du système de détection, tout en assurant une bonne précision de mesure."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Conception initiale** : \n  - Existence d'une première version du thermomètre de cuisson, mais avec des limites et des performances non optimisées.\n\n- **Contraintes environnementales** :\n  - Hautes températures, étanchéité, blindage, etc. ont influencé les choix technologiques validés par des expérimentations.\n\n- **Incertitude sur le troisième point de mesure** :\n  - Nécessité de garantir la fiabilité de la température mesurée à ce point, sans interférence de la conduction thermique des matériaux métalliques de la sonde.\n\n- **Technologies d'acquisition de température** :\n  - Incertitude sur les technologies à utiliser pour assurer une acquisition précise de la température dans des conditions extrêmes.\n  - Développement d'un système d'alerte basé sur les caractéristiques des points de mesure pour notifier l'utilisateur d'un risque de montée en température.\n\n- **Comportement du four comme cage de Faraday** :\n  - Atténuation ou blocage des ondes, nécessitant une solution d'amplification du signal du thermomètre.\n  - Conception d'un socle répéteur de signal pour surmonter l'atténuation lors de l'utilisation.\n\n- **Incertitude sur l'autonomie** :\n  - Doutes concernant l'autonomie de la sonde et du socle associé, avec un besoin partiel de conception pour la recharge de la sonde."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLa conception d’un thermomètre de cuisson connecté nous a confrontés à de nombreuses incertitudes ainsi qu’à des aléas. En effet, si une première conception du thermomètre existait avant nos travaux, de nombreuses limites et des performances non optimisées étaient à déplorer.\nLes contraintes posées par l’environnement de fonctionnement du thermomètre ont également été à l’origine de nombreux choix technologiques que nous avons dû valider à travers des expérimentations : hautes températures, étanchéité, blindage, etc.\nUne première incertitude a été posée par notre volonté d’ajouter un troisième point de mesure sur le thermomètre : nous devions en effet nous assurer que la température en ce point serait fiable et ne serait pas affectée par la conduction de chaleur par les matériaux métalliques dont est faite la sonde.\nUne seconde incertitude portait sur les technologies à mettre en œuvre pour permettre une acquisition précise de la température en chaque point de mesure malgré les conditions extrêmes dans lesquelles la sonde fonctionne. En outre, nous souhaitions développer un système d’alerte en exploitant les caractéristiques de chaque point de mesure pour appréhender la montée en température au sein du four et pouvoir notifier l’utilisateur d’un risque.\nUne autre difficulté à laquelle nous avons été confrontés était posée par le comportement du four en tant que cage de Faraday, atténuant, voire bloquant toutes les ondes. Nous devions ainsi concevoir une solution d’amplification du signal envoyé par le thermomètre connecté afin de permettre au smartphone de l’utilisateur de le capter. Nous avons alors conçu un socle répéteur de signal qui permet de s’affranchir de l’atténuation du signal envoyé par le thermomètre connecté en utilisation.\nEnfin, une dernière incertitude portait sur l’autonomie de la sonde et du socle associé que nous avons partiellement conçu pour répondre au besoin de recharge de la sonde."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Difficultés initiales :** \n  - Conception de la pièce de jonction (connecteur) entre le piètement et le plateau de la table.\n  - Faible épaisseur du plateau en HPL limitant sa planéité et sa fixation par vissage.\n\n- **Maîtrise de la matière :**\n  - Réalisation d'un travail sur les tolérances d'usinage pour la création du plateau.\n  - Création d'un connecteur pour améliorer la tenue du plateau sur le piètement.\n  - Augmentation de la résistance de la pièce de jonction face aux contraintes du plateau.\n\n- **Phase d'étude et de prototypage :**\n  - Développement d'une cinématique de pliage spécifique pour éviter le pincement.\n  - Conception d'un pliage sans éléments libres et sans interstice de 7 à 12 mm (taille de doigt d’un enfant).\n\n- **Utilisation de tubes en acier HLE galvanisé :**\n  - Confrontation à des difficultés lors des processus industriels :\n    - Craquèlement du matériau lors du cintrage.\n    - Soudures compliquées sur des tubes ondulés.\n    - Défauts d’adhérence du traitement de surface avant peinture (problèmes chimiques liés au pH).\n  - Modification du processus industriel de peinture en fonction des produits galvanisés.\n  - Nécessité d'une réflexion approfondie sur le traitement de surface, absence d'antériorité sur les aciers HLE galvanisés pour le mobilier métallique."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours de ce projet, les premières difficultés rencontrées ont été concentrées sur la conception de la pièce de jonction, ou connecteur, entre le piètement et le plateau de la table. Comme évoqué précédemment, la faible épaisseur du plateau en HPL ne permet pas de garantir sa planéité et de permettre sa fixation par vissage. Un travail de maîtrise de la matière a tout d’abord été réalisé pour la création du plateau, notamment en termes de tolérances d’usinage. Nous avons dû ensuite créer le connecteur, améliorer la tenue du plateau sur le piètement et accroître la résistance de la pièce de jonction aux contraintes exercées par le plateau. \n\nD’autre part, nous avons eu une longue phase d’étude et de prototypage pour aboutir à une cinématique particulière de pliage. En effet, afin qu’il n’y ait aucun risque de pincement, nous avons dû réfléchir à un pliage sans éléments libres et dont le système ne comprend pas d’interstice de 7 à 12 mm, taille de doigt d’un enfant.\n\nEnfin, concernant l’utilisation de tubes en acier HLE galvanisé, nous avons été confrontés à diverses difficultés au cours des différents process industriels : craquèlement du matériau au moment du cintrage, soudures difficiles sur des tubes ondulés, défauts d’adhérence du traitement de surface avant peinture (contraintes chimiques liées au pH notamment)... Nous avons dû modifier le process industriel peinture en fonction des produits galvanisés. Pour cela, une importante phase de réflexion a été portée sur le travail de traitement de surface car il n’existait pas d’antériorité sur les aciers HLE galvanisés pour le mobilier métallique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Phase d'Étude et Prototypage :**\n  - Longue phase d'étude pour développer une cinématique de pliage spécifique.\n  - Objectif : Éliminer les zones de pincement tout en assurant une forte compacité une fois la chaise repliée.\n  - Nécessité d'un pliage sans éléments libres et sans interstice de 7 à 12 mm (taille de doigt d’enfant).\n  - Modifications successives de la géométrie de la chaise pour résoudre des problèmes de stabilité.\n\n- **Conception du Système de Coulisse :**\n  - Levée d'incertitudes sur les conditions géométriques optimales : taille, encombrement et course (longueur de déplacement).\n\n- **Phénomènes d'Usure et de Frottement :**\n  - Prise en compte des effets d'usure et de frottement dus aux contacts entre parties solides en mouvement.\n\n- **Soudures Textiles par Haute Fréquence :**\n  - Absence d'antériorité sur l'application des soudures textiles au mobilier au début du projet.\n  - Étude de deux problématiques majeures :\n    - Remplacement des coutures par des soudures.\n    - Soudure des clips élastomères sur la Batyline.\n  - Problème de qualité lié aux coutures textiles : dégradation des fils après exposition prolongée aux éléments naturels (chaleur, soleil, pluie), entraînant un désassemblage de la toile.\n\n- **Défis du Soudage Haute Fréquence :**\n  - Problème rencontré : la Batyline, contenant de l'obsidian et des composants carbonés, produit des flashes et brûle lors du soudage.\n  - Nécessité de réfléchir à l'élimination du noir graphite pour éviter la combustion de la matière.\n  - Travail d'expertise requis pour améliorer le procédé de soudage pour une application dans le mobilier, incluant la modification des paramètres de soudure."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons tout d’abord eu une longue phase d’étude et de prototypage pour aboutir à une cinématique particulière de pliage. En effet, nous souhaitions supprimer les zones de pincement tout en ayant une compacité forte une fois la chaise repliée. Pour cela, nous avons dû réfléchir sur un pliage sans éléments libres et dont le système ne comprend pas d’interstice de 7 à 12 mm, taille de doigt d’un enfant. Nous avons dû modifier à travers des prototypes successifs la géométrie générale de la chaise en raison de problèmes de stabilité.\nD’autre part, la conception du système de coulisse à travers la noix arrière des accoudoirs a nécessité la levée d’incertitudes sur les conditions géométriques optimales de taille, d’encombrement et de course (longueur de déplacement).\nNous avons également dû prendre en compte les phénomènes d’usure et de frottement qui peuvent survenir suite aux contacts permanents ou occasionnels entre les parties solides en mouvement.\n\nConcernant les soudures textiles par haute fréquence appliquées au mobilier, nous n’avons pas trouvé d’antériorité au début du projet. \nNous avons alors étudié deux problématiques majeures :\nLe remplacement des coutures par la soudure ;\nLa soudure des clips élastomères sur la Batyline.\nCes points sont directement liés à un problème qualité dû aux coutures textiles. En effet, après un certain temps d’utilisation et une exposition longue aux éléments naturels (chaleur, soleil, pluie), nous avons constaté que les fils des coutures se dégradaient, entraînant un désassemblage de la toile.\nCependant, en utilisant le soudage haute fréquence, nous avons été confronté au fait que la Batyline, composée d’obsidian et donc de composants carbonés, créé des flashes et brûle. Nous avons ainsi dû mener une réflexion approfondie sur l’élimination du noir graphite afin que la matière ne brûle pas. Cela a nécessité un travail d’expertise pour améliorer le procédé pour une application dans le mobilier et mieux connaître les possibilités en modifiant les paramètres de soudure."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Verrous Techniques sur le Fauteuil Cocoon :**\n  - Difficultés rencontrées lors des travaux sur la structure métallique.\n  - Problèmes de cintrage des aciers à Haute Limite Élastique (HLE) :\n    - Zone de comportement élastique très étendue.\n    - Moyens de cintrage insuffisants pour déformation plastique.\n    - Retour élastique des tubes après déformation.\n\n- **Inexistence d'Abaques :**\n  - Absence d'abaques pour le formage de tubes en acier HLE.\n  - Résultats de certains processus : tubes écrasés pour certains angles de cintrage au lieu d'une incurvation avec section circulaire.\n\n- **Dimensionnement des Tubes :**\n  - Problèmes de dimensionnement des épaisseurs et diamètres pour assurer la résistance du fauteuil.\n  - Nécessité d'adapter le fauteuil pour une utilisation par des enfants et des adultes.\n\n- **Tissu Matelassé HEDONA :**\n  - Destiné à une gamme de produits Lafuma Mobilier.\n  - Complexité d'intégration sur différents produits :\n    - Réévaluation des épaisseurs et densités de la ouate pour le confort et les performances textiles.\n    - Problématiques spécifiques à chaque produit : structure, forme, type d’utilisation, zones de pliage.\n\n- **Développement du Fauteuil Cocoon :**\n  - Problèmes de patronage et d’entoilage :\n    - Fauteuil en forme d'œuf, non pliant, avec piètement en porte à faux.\n    - L’entoilage se fait uniquement le long du tube, créant des contraintes spécifiques.\n\n- **Contraintes Techniques :**\n  - Tenue du tissu matelassé :\n    - Nécessité d'un tissu non élastique, sans couture sur le dossier, sans mauvais plis, et avec un profil adéquat.\n  - Stabilité et résistance mécanique de la structure en acier HLE :\n    - Importance de trouver la bonne épaisseur de tube pour assurer élasticité et résistance.\n    - Structure doit supporter des utilisateurs de différents poids.\n    - Élasticité nécessaire pour permettre une légère déformation et un basculement agréable lors de l’assise."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans le cadre de cet axe de recherche, nous avons eu à lever de nombreux verrous techniques qui se sont surtout posés lors des travaux sur la structure métallique du fauteuil Cocoon.  \nEn effet, nous avons rencontré des difficultés concernant le cintrage des aciers à Haute Limite Elastique. Ceux-ci disposant d’une zone de comportement élastique très étendue, les moyens usuels de cintrage peinent à exercer un effort suffisant pour déformer plastiquement les tubes, et nous avons également rencontré de nombreuses difficultés du fait du retour élastique des tubes une fois ceux-ci déformés.  \nPar ailleurs, nous avons constaté l’inexistence d’abaques concernant le formage de tubes en acier HLE, nos processus ayant mené à l’obtention de tubes écrasés pour certains angles de cintrage au lieu d’une incurvation seulement avec une section de tube restant circulaire.  \nNous avons également été confrontés à des problèmes de dimensionnement des épaisseurs et diamètres de tubes permettant une résistance suffisante du fauteuil pour que celui-ci soit utilisable aussi bien par des enfants que par des adultes.  \n\nNotre tissu matelassé HEDONA a pour visée d’être employé sur toute une gamme de produits Lafuma Mobilier. Or, chaque mobilier est spécifique et nous ne pouvons pas intégrer notre matelas sur tous les produits de la même façon. La complexité réside :  \nD’une part, dans le fait qu’il est nécessaire à chaque produit de réévaluer les épaisseurs et densités de la ouate pour le confort du mobilier et les performances textiles attendues de notre tissu matelassé et ;  \nD’autre part, à chaque produit, sa problématique différente : structure, forme, type d’utilisation, zones de pliage…  \n\nEn particulier, dans le cadre du développement du fauteuil Cocoon, nous avons été confrontés à des problèmes de patronage et d’entoilage du produit. En forme d’œuf, ce fauteuil non pliant a pour spécificité d’avoir un piètement en porte à faux.  \nL’entoilage ne se fait que le long du tube et donc sur du vide : toute l’assise est suspendue dans ce fauteuil. Ainsi, nous avons rencontré des contraintes de :  \nTenue du tissu matelassé : obtenir un tissu non élastique qui soit sans couture sur le dossier, qui ne fasse pas de mauvais plis, et qui donne un certain profil au dossier vu de l’arrière ;  \nStabilité et de résistance mécanique de la structure en acier HLE en raison du porte à faux : trouver la bonne épaisseur de tube qui offre suffisamment d’élasticité pour pouvoir former le fauteuil et plus particulièrement le piètement. La structure doit avoir une résistance suffisante pour accueillir des utilisateurs de tout poids. L’élasticité de la structure doit également permettre une légère déformation du fauteuil et donc, un basculement agréable pour l’utilisateur lors de l’assise."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Développement de la table à platine pivot** : Nécessité de concevoir un piètement spécifique.\n  \n- **Inconnues techniques** :\n  - Résistance mécanique des entretoises dans l’espace disposé.\n  - Nuances d’acier testées : acier auto-trempant et acier doux.\n    - Problème : Les soudures sur l’acier A60 devenaient cassantes lors des essais de charge.\n    - Limite d’élasticité de l’acier A60 très basse, entraînant des cassures lors des soudures.\n\n- **Phase d'étude et de prototypage** :\n  - Longue phase dédiée à l’embranchement central du piètement.\n  - Résistances mécaniques jugées insuffisantes.\n\n- **Stabilité de la table** :\n  - Réflexion sur l’auto-équilibrage pour compenser un sol non droit.\n  - Travail sur le débattement d’un ressort logé à l’intérieur d’un logement écrou.\n  - Prototype non validé lors des tests de stabilité."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement de notre nouvelle table à platine pivot a nécessité la conception d’un piètement spécifique, dont les inconnues techniques étaient les suivantes :  \nLa résistance mécanique des entretoises dans l’espace disposé  \nLes nuances d’acier testés, acier auto-trempant et acier doux, pour qu’à la soudure le matériau ne devienne pas cassant. En effet, lors des essais de charge, les prototypes ne résistaient pas. Les soudures sur l’acier A60 se cassaient car elles devenaient très dures, la limite d’élasticité étant très basse.  \n\nLa conception de la platine pivot a nécessité une longue phase d’étude et de prototypage sur l’embranchement central du piètement au niveau des résistances mécaniques qui étaient insuffisantes.  \nUne réflexion a également été porté sur l’auto-équilibrage de la table pour que cette dernière reste stable lorsque le sol n’est pas droit. Pour cela, nous avons travaillé sur le débattement d’un ressort logé à l’intérieur d’un logement écrou. Cependant, notre prototype ne validait pas les tests de stabilité."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Développement de la table à platine pivot** :\n  - Nécessité de concevoir un piètement spécifique.\n\n- **Inconnues techniques** :\n  - Résistance mécanique des entretoises dans l’espace disposé.\n  - Nuances d’acier testées : acier auto-trempant et acier doux.\n    - Problème : lors de la soudure, le matériau ne doit pas devenir cassant.\n    - Observations : prototypes ne résistaient pas lors des essais de charge.\n    - Soudures sur l’acier A60 se cassaient en raison d’une dureté excessive.\n    - Limite d’élasticité de l’acier A60 jugée très basse.\n\n- **Conception de la platine pivot** :\n  - Longue phase d’étude et de prototypage sur l’embranchement central du piètement.\n  - Problème identifié : résistances mécaniques insuffisantes.\n\n- **Stabilité de la table** :\n  - Réflexion sur l’auto-équilibrage pour compenser un sol non droit.\n  - Travail sur le débattement d’un ressort logé à l’intérieur d’un logement écrou.\n  - Résultat : prototype ne validait pas les tests de stabilité."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement de notre nouvelle table à platine pivot a nécessité la conception d’un piètement spécifique, dont les inconnues techniques étaient les suivantes :  \nLa résistance mécanique des entretoises dans l’espace disposé  \nLes nuances d’acier testés, acier auto-trempant et acier doux, pour qu’à la soudure le matériau ne devienne pas cassant. En effet, lors des essais de charge, les prototypes ne résistaient pas. Les soudures sur l’acier A60 se cassaient car elles devenaient très dures, la limite d’élasticité étant très basse.  \n\nLa conception de la platine pivot a nécessité une longue phase d’étude et de prototypage sur l’embranchement central du piètement au niveau des résistances mécaniques qui étaient insuffisantes. Une réflexion a également été portée sur l’auto-équilibrage de la table pour que cette dernière reste stable lorsque le sol n’est pas droit. Pour cela, nous avons travaillé sur le débattement d’un ressort logé à l’intérieur d’un logement écrou. Cependant, notre prototype ne validait pas les tests de stabilité."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Difficultés rencontrées** : Concentration sur la conception de nouveaux systèmes de coulisse, en particulier au niveau des accoudoirs.\n  \n- **Objectif de conception** : Développer une cinématique spécifique pour :\n  - Supprimer les zones de pincement.\n  - Assurer une forte compacité lorsque la chaise est repliée.\n  \n- **Approche de pliage** : Nécessité de concevoir un pliage sans éléments libres.\n\n- **Système de coulisse** : \n  - Étude approfondie et phase de prototypage pour le système de coulisse à travers la noix arrière des accoudoirs.\n  - Évaluation des conditions géométriques optimales concernant :\n    - Taille.\n    - Encombrement.\n    - Course (longueur de déplacement).\n\n- **Usure et frottement** : Prise en compte des phénomènes d’usure et de frottement dus aux contacts permanents ou occasionnels entre les parties solides en mouvement.\n\n- **Transformation de la chaise** : Conception d'une chaise bridge pliante capable de se transformer en simple chaise pliante (sans accoudoirs).\n\n- **Travail sur la symétrie** : Suppression des contraintes de symétrie des pièces gauche-droite tout en maintenant un châssis et des pièces communs aux deux structures de meuble."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes difficultés rencontrées ont été essentiellement concentrées sur la conception de nouveaux systèmes de coulisse, et en particulier au niveau des accoudoirs. En effet, nous souhaitions aboutir à une cinématique particulière pour supprimer les zones de pincement tout en ayant une compacité forte une fois la chaise repliée. Pour cela, nous avons dû réfléchir sur un pliage sans éléments libres.\n\nD’autre part, le système de coulisse à travers la noix arrière des accoudoirs a requis une longue phase d’étude et de prototypage pour évaluer les conditions géométriques optimales de taille, d’encombrement et de course (longueur de déplacement). De plus, nous avons également dû prendre en compte les phénomènes d’usure et de frottement qui peuvent survenir suite aux contacts permanents ou occasionnels entre les parties solides en mouvement. \n\nEnfin, nous souhaitions concevoir une chaise bridge pliante qui puisse se transformer facilement en simple chaise pliante, c’est-à-dire sans accoudoirs. Pour arriver à ce résultat, nous avons travaillé longuement sur la suppression des contraintes de symétrie des pièces gauche-droite, tout en gardant un châssis et des pièces communs aux deux structures de meuble."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Difficulté principale** : Hyperstatisme dû à la structure du transat, entraînant un meuble bancal et déséquilibré.\n- **Complexité de la forme** : La structure hyperstatique complique la définition d'une forme différente de celle naturellement présente.\n- **Prototypes dissymétriques** : Réalisation de prototypes avec une structure dissymétrique, entraînant des problèmes d'habillage et de pliabilité.\n- **Révision de la structure** : Création d'axes communs pour symétriser l'ensemble du meuble.\n- **Système de blocage arrière** : Nécessité d'une longue phase d'étude et de prototypage pour déterminer les conditions géométriques optimales (taille, encombrement, course) des segments télescopiques.\n- **Modifications structurelles** : Les études sur le système de blocage ont engendré des modifications de la structure pliable.\n- **Phénomènes de frottement** : Prise en compte des frottements dus aux contacts entre les parties solides et la toile en mouvement, qu'ils soient permanents ou occasionnels."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’une des principales difficultés, qui a déjà été évoquée précédemment, provient de l’hyperstatisme dû à la structure du transat. En effet, le meuble se contraint tout seul et cela induit le fait qu’il soit bancal et jamais équilibré. La structure hyperstatique rend ainsi la définition d’une forme autre que ce qu’elle a naturellement très complexe.\n\nAu cours de nos travaux, nous avons réalisé des prototypes avec une structure dissymétrique. Cependant, cela a engendré un problème pour l’habillage et surtout une pliabilité impossible du meuble. Nous avons donc dû revoir la structure en créant des axes communs afin d’avoir une symétrisation de l’ensemble.\n\nD’autre part, le système de blocage arrière a requis une longue phase d’étude et de prototypage pour lever les inconnues techniques liées aux conditions géométriques optimales de taille, d’encombrement et de course (longueur de déplacement) des segments télescopiques, engendrant notamment des modifications de la structure pliable. De plus, nous avons également dû prendre en compte les phénomènes de frottement qui peuvent survenir suite aux contacts permanents ou occasionnels entre les parties solides et la toile en mouvement."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Verrous à lever :**\n  - Conception de la chape de liaison.\n  - Efforts exercés sur la chape.\n\n- **Objectif du projet :**\n  - Modifier une chape métallique soudée par points en un nouveau système de butée en plastique.\n  - Remplacer le soudage par points par un procédé plus fiable sur le long terme en termes de tenue mécanique.\n\n- **Exigences du nouveau système :**\n  - Résistance à des sollicitations mécaniques fortes sans déformation.\n  - Adaptabilité aux différentes positions du fauteuil, en particulier lors d’efforts sur le bord de l’assise (coulissement, rotation, arrachement).\n\n- **Développement technique :**\n  - Création d’un système de triangulation spécifique entre l’assise et les pieds avant et arrière.\n  - Nécessité de plusieurs prototypes successifs pour atteindre les objectifs.\n\n- **Modifications de conception :**\n  - Identification d’un risque de montage inversé des pièces de guidage droite et gauche.\n  - Adaptation de la conception pour rendre le sens de montage indifférencié.\n\n- **Difficultés rencontrées :**\n  - Problèmes liés aux procédés de conception du mécanisme de pivot (écrasement et perçage des tubes).\n  - Réalisation d’essais pour déterminer les valeurs optimales en faisant varier :\n    - Longueur et position d’écrasement.\n    - Diamètre de perçage."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes verrous que nous avons dû lever dans le cadre de ce développement expérimental se rapportent, d’une part à la conception de la chape de liaison, et d’autre part, aux efforts exercés sur celle-ci.\n\nNotre projet visait à modifier de façon substantielle une chape métallique soudée par points par un nouveau système de butée en plastique. En effet, la principale motivation de cette nouvelle conception était de substituer au procédé de soudage par points un procédé plus fiable dans le temps quant à sa tenue mécanique.  \nCe système doit résister à des sollicitations mécaniques fortes sans se déformer selon les différentes positions du fauteuil, notamment en cas d’efforts sur le bord de l’assise : coulissement, rotation, arrachement. Pour cela, nous avons dû développer un système de triangulation spécifique entre l’assise et les pieds avant et arrière, qui a nécessité le développement de plusieurs prototypes successifs.\n\nNous avons également dû modifier le modèle de pièces que nous avions établies lorsque nous avons détecté un risque dans le fait que les pièces de guidage droite et gauche puissent se monter inversées. Nous avons ainsi apporté des modifications à la conception de la pièce pour rendre son sens de montage indifférencié.\n\nPar ailleurs, les procédés de conception du mécanisme assurant le pivot nous ont posé des difficultés, notamment sur l’écrasement et le perçage des tubes. Nous avons alors mené une série d’essais en faisant varier la longueur et la position d’écrasement, de même que le diamètre de perçage, dans le but de déterminer les valeurs optimales."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Développement de la table à platine pivot :**\n  - Nécessité d'un piètement spécifique.\n  \n- **Inconnues techniques :**\n  - Résistance mécanique des entretoises dans l'espace disposé.\n  - Nuances d'acier testées : acier auto-trempant et acier doux.\n    - Problème : soudures devenaient cassantes lors des essais de charge.\n    - Soudures sur acier A60 se cassaient en raison d'une dureté excessive et d'une limite d'élasticité basse.\n\n- **Conception de la platine pivot :**\n  - Longue phase d'étude et de prototypage sur l'embranchement central du piètement.\n  - Résistances mécaniques jugées insuffisantes.\n\n- **Stabilité de la table :**\n  - Réflexion sur l'auto-équilibrage pour compenser un sol non droit.\n  - Développement d'un ressort logé dans un logement écrou.\n  - Prototype ne validait pas les tests de stabilité."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement de notre nouvelle table à platine pivot a nécessité la conception d’un piètement spécifique, dont les inconnues techniques étaient les suivantes :  \nLa résistance mécanique des entretoises dans l’espace disposé  \nLes nuances d’acier testés, acier auto-trempant et acier doux, pour qu’à la soudure le matériau ne devienne pas cassant. En effet, lors des essais de charge, les prototypes ne résistaient pas. Les soudures sur l’acier A60 se cassaient car elles devenaient très dures, la limite d’élasticité étant très basse.  \n\nLa conception de la platine pivot a nécessité une longue phase d’étude et de prototypage sur l’embranchement central du piètement au niveau des résistances mécaniques qui étaient insuffisantes.  \nUne réflexion a également été portée sur l’auto-équilibrage de la table pour que cette dernière reste stable lorsque le sol n’est pas droit. Pour cela, nous avons travaillé sur le débattement d’un ressort logé à l’intérieur d’un logement écrou. Cependant, notre prototype ne validait pas les tests de stabilité."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Développement de la table à platine pivot :**\n  - Nécessité de concevoir un piètement spécifique.\n  \n- **Inconnues techniques identifiées :**\n  - Résistance mécanique des entretoises dans l'espace disposé.\n  - Tests de nuances d'acier : acier auto-trempant et acier doux.\n    - Problème : Les soudures devenaient cassantes lors des essais de charge.\n    - Observations : Les soudures sur l'acier A60 se cassaient en raison d'une dureté excessive et d'une limite d'élasticité très basse.\n\n- **Conception de la platine pivot :**\n  - Longue phase d'étude et de prototypage sur l'embranchement central du piètement.\n  - Résistances mécaniques jugées insuffisantes.\n\n- **Stabilité de la table :**\n  - Réflexion sur l'auto-équilibrage pour compenser un sol irrégulier.\n  - Développement d'un système de débattement d'un ressort logé à l'intérieur d'un logement écrou.\n  - Problème rencontré : Prototype ne validait pas les tests de stabilité."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe développement de notre nouvelle table à platine pivot a nécessité la conception d’un piètement spécifique, dont les inconnues techniques étaient les suivantes :  \nLa résistance mécanique des entretoises dans l’espace disposé  \nLes nuances d’acier testés, acier auto-trempant et acier doux, pour qu’à la soudure le matériau ne devienne pas cassant. En effet, lors des essais de charge, les prototypes ne résistaient pas. Les soudures sur l’acier A60 se cassaient car elles devenaient très dures, la limite d’élasticité étant très basse.  \n\nLa conception de la platine pivot a nécessité une longue phase d’étude et de prototypage sur l’embranchement central du piètement au niveau des résistances mécaniques qui étaient insuffisantes. Une réflexion a également été portée sur l’auto-équilibrage de la table pour que cette dernière reste stable lorsque le sol n’est pas droit. Pour cela, nous avons travaillé sur le débattement d’un ressort logé à l’intérieur d’un logement écrou. Cependant, notre prototype ne validait pas les tests de stabilité."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Verrous et Incertitudes\n\n- **Ambition du projet R&D** :\n  - Objectif : Généraliser la classification des cultures agricoles sur l'ensemble du territoire français.\n\n- **Verrou majeur identifié** :\n  - La plupart des travaux antérieurs se concentrent sur des régions spécifiques (ex. : Inde, Japon, Bretagne).\n  - Cette focalisation régionale crée une lacune pour une approche à l'échelle nationale.\n  - Défi principal : Concevoir un modèle généralisable à l’échelle d’un pays entier.\n\n- **Choix technologique initial** :\n  - Utilisation exclusive de Sentinel-2A.\n  - Limitation : Sensibilité des bandes optiques aux taux de couverture nuageuse.\n    - Passages tous les 4-5 jours, mais une faible couverture nuageuse peut compromettre la qualité des données.\n  - Incertitude sur l'exploitabilité des données pour un modèle performant à l’échelle nationale.\n\n- **Intégration de données complémentaires** :\n  - Association des données de Sentinel-1 (non sensible à la couverture nuageuse) avec celles de Sentinel-2A.\n  - Avantage : Enrichissement des données.\n  - Défis :\n    - Orbite complexe de Sentinel-1 nécessitant un prétraitement difficile.\n    - Intégration et synchronisation des deux sources de données posent des défis.\n    - Fusion des données encore peu explorée dans la littérature, ajoutant une couche d'incertitude au projet."}, {"role": "assistant", "content": "Aléas, verrous et incertitudes\nL’ambition de ce projet R&D est clairement marquée par son désir de généraliser la classification des cultures agricoles sur l’ensemble du territoire français. Cependant, cette ambition se heurte à un verrou majeur dès le départ que nous avons identifié lors de nos recherches sur le sujet. En effet, la plupart des travaux antérieurs axés sur la classification des cultures agricoles se concentrent sur des régions bien spécifiques telles que l’Inde, le Japon ou la Bretagne par exemple. Cette focalisation régionale introduit une lacune notable lorsqu’il s’agit d’aborder la problématique à une échelle plus large telle que le territoire français. Ainsi, nous devons concevoir un modèle capable de généraliser à l’échelle d’un pays entier et non pas à une seule région en particulier c’est le principal défi de ce projet. \nDe plus, le choix initial de s’appuyer exclusivement sur Sentinel-2A a introduit des obstacles spécifiques. En effet, les bandes optiques se heurtent à la limitation de leur sensibilité face aux taux de couverture nuageuse. Avec un passage tous les 4-5 jours sur un point géographique donné, la présence même d’une faible couche de nuages pourrait entraver sérieusement la qualité des données recueillies. Rien ne nous garanti donc que les données sont assez exploitables pour fournir un modèle assez performant sur l’ensemble du territoire français. L’association des données de Sentinel-1, qui n’est pas sensible à la couverture nuageuse, avec celles de Sentinel-2A semble être prometteuse en termes d’enrichissement de l’ensemble de nos données mais n’est pas sans défis. Son orbite complexe nécessite des étapes de prétraitement particulièrement difficiles. L’exploitation simultanée des deux sources de données, malgré leur complémentarité, pourrait engendrer des défis d’intégration et de synchronisation d’autant plus que cette fusion reste assez inexplorée dans la littérature, ajoutant ainsi une autre couche d’incertitude à notre projet."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Verrous et Incertitudes\n\n- **Opportunités en Cybersécurité** :\n  - Les techniques de machine learning pour la détection d’anomalies offrent de nombreuses possibilités.\n\n- **Limites Identifiées** :\n  - La qualité des données est cruciale pour les performances des modèles de machine learning.\n  - L'obtention de données étiquetées est un problème majeur, particulièrement pour l'apprentissage semi-supervisé.\n  - Actuellement, seules des logs de connexions au serveur sont disponibles, sans distinction entre connexions humaines et automatisées.\n\n- **Exploration de Nouvelles Techniques** :\n  - Nécessité d'explorer des algorithmes d'apprentissage non supervisé qui ne nécessitent pas de données étiquetées.\n\n- **Modèles de Deep Learning** :\n  - Bien que puissants, ces modèles exigent une grande puissance de calcul.\n  - Nécessité d'une optimisation méticuleuse des paramètres, comme démontré dans plusieurs études.\n\n- **Analyse de Séries Temporelles** :\n  - Considérée comme une méthode prometteuse pour la détection d’anomalies, mais son efficacité n'est pas garantie.\n\n- **Défis du Domaine de la Cybersécurité** :\n  - Le domaine est très dynamique, avec l'émergence continue de nouvelles techniques d'attaque.\n  - Importance d'adapter et de mettre à jour régulièrement les méthodes de détection pour maintenir leur pertinence et efficacité.\n\n- **Risques Associés** :\n  - Prendre en compte les risques de sur-apprentissage (overfitting) des modèles.\n  - Une focalisation excessive sur un type d'attaque ou de données peut rendre le système vulnérable à des menaces inédites."}, {"role": "assistant", "content": "Aléas, verrous et incertitudes\nLes différentes techniques de machine learning pour la détection d’anomalies présentent de nombreuses opportunités en cybersécurité. Cependant, plusieurs limites découlent de notre état de l’art. Mis à part la qualité des données qui est cruciale pour les performances des modèles de machine learning, l’obtention de données étiquetées, surtout pour l’apprentissage semi-supervisé étudié dans plusieurs articles, est un problème majeur. En effet, nous disposons uniquement de logs de connexions au serveur. Ainsi, nous ne savons pas si le log de connexion provient d’un script automatisé ou d’un humain. Il convient alors d’explorer d’autres techniques de machine learning qui ne nécessite pas de données étiquetées, comme les algorithmes d’apprentissage non supervisé. \n\nNous pouvons aussi nous concentrer sur des modèles de deep learning. Bien que puissants, ces modèles requièrent néanmoins une grande puissance de calcul et une optimisation méticuleuse des paramètres comme nous avons pu le voir dans plusieurs articles. L’analyse de séries temporelles [4] peut être également une méthode prometteuse pour la détection d’anomalies. Malgré cela, rien ne nous garantit l’efficacité de cette méthode. \n\nEnfin, le domaine très dynamique de la cybersécurité présente un défi supplémentaire. Avec l'émergence continue de nouvelles techniques d'attaque, il est impératif d'adapter et de mettre à jour régulièrement les méthodes de détection afin qu’elles soient toujours pertinentes et efficaces. En outre, il est essentiel de prendre en compte les risques associés au sur-apprentissage (overfitting) des modèles. Une focalisation excessive sur un type d'attaque ou de données pourrait rendre le système vulnérable face à des menaces inédites."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Verrous et Incertitudes dans le Projet Graal Systems\n\n- **Défis Technologiques :**\n  - Complexité croissante du partage et de l'isolation des ressources GPU ou FPGA entre plusieurs processus.\n  - Solutions de marché restreintes pour les GPU, principalement proposées par NVIDIA et Intel.\n\n- **Limitations des Solutions Existantes :**\n  - **NVIDIA vGPU et NVIDIA MIG :**\n    - Offrent des fonctionnalités de virtualisation et de partitionnement.\n    - Limitations : coût élevé, licences d'entreprise, manque de flexibilité pour la répartition dynamique des ressources.\n\n- **Verrous Technologiques et Incertitudes :**\n  - **Complexité des Solutions :**\n    - Solutions comme Salus, G-NET et HiveD ajoutent une complexité à leur mise en œuvre.\n    - Exemple : l'algorithme d'allocation de cellules « buddy » de HiveD nécessite une compréhension approfondie.\n\n  - **Dépendance à l'Infrastructure :**\n    - Solutions souvent conçues pour des infrastructures spécifiques (ex. : HiveD optimisé pour clusters GPU).\n    - Adaptations nécessaires si d'autres types de ressources ou infrastructures sont envisagés.\n\n  - **Performance et Scalabilité :**\n    - Évaluation des performances des solutions comme HiveD dans des scénarios réels est cruciale.\n    - Gestion de la scalabilité avec l'augmentation du nombre de tenants ou de tâches reste préoccupante.\n\n  - **Sécurité et Isolation :**\n    - Importance de garantir une isolation robuste dans un environnement multi-tenants.\n    - Solutions comme G-NET assurent l'isolation des données, mais des vulnérabilités potentielles doivent être examinées.\n\n  - **Coûts et Licences :**\n    - Solutions existantes, notamment celles de NVIDIA, sont souvent coûteuses et soumises à des modèles de licence restrictifs.\n    - Adoption difficile pour les PME et TPE en raison des coûts élevés.\n\n- **Analyse du Marché :**\n  - Absence de plateforme basée sur des serveurs non spécialisés et grand public.\n  - Nécessité d'un standard simple, peu onéreux et compréhensible pour le partage de ressources.\n\n- **Conclusion :**\n  - État actuel des connaissances pertinent, mais aucune solution adaptée aux exigences spécifiques et défis uniques du projet Graal Systems.\n  - Limitations des solutions existantes et verrous technologiques identifiés soulignent la nécessité d'un engagement profond dans des travaux de R&D pour surmonter ces défis."}, {"role": "assistant", "content": "Aléas, verrous et incertitudes\nLes défis technologiques auxquels LAYER4 est confronté sont ancrés dans la complexité croissante du partage et de l'isolation des ressources GPU ou FPGA entre plusieurs processus. Contrairement aux CPU, les GPU présentent des solutions de marché restreintes, principalement proposées par des acteurs tels que NVIDIA et Intel. Les solutions existantes, bien que prometteuses, ont leurs propres limitations. Par exemple, NVIDIA vGPU et NVIDIA MIG, bien qu'offrant des fonctionnalités de virtualisation et de partitionnement du GPU, sont souvent limitées à des modèles coûteux, soumis à des licences d'entreprise, et manquent de flexibilité pour une répartition dynamique des ressources.\n\nPlusieurs verrous technologiques et incertitudes entourent notre projet Graal Systems :\nComplexité des solutions : des solutions comme Salus, G-NET et HiveD, bien qu'elles introduisent une complexité supplémentaire dans leur mise en œuvre. Par exemple, l'algorithme d'allocation de cellules « buddy » de HiveD nécessite une compréhension approfondie pour une mise en œuvre efficace.\nDépendance à l'infrastructure : de nombreuses solutions sont spécifiquement conçues pour des infrastructures particulières. HiveD, par exemple, est optimisé pour les clusters GPU. Si nous envisageons d'utiliser d'autres types de ressources ou d'infrastructures, des adaptations pourraient être nécessaires.\nPerformance et Scalabilité : bien que des solutions comme HiveD éliminent certaines anomalies de partage, il est crucial d'évaluer leurs performances dans des scénarios réels. De plus, la manière dont ces solutions gèrent la scalabilité, en particulier avec une augmentation du nombre de tenants ou de tâches, reste une préoccupation.\nSécurité et Isolation : dans un environnement multi-tenants, garantir une isolation robuste est essentiel. Des solutions comme G-NET garantissent l'isolation des données, mais il est crucial de s'assurer qu'il n'y a pas de vulnérabilités potentielles.\nCoûts et licences : les solutions existantes, comme celles proposées par NVIDIA, sont souvent associées à des coûts élevés et à des modèles de licence restrictifs, ce qui pourrait rendre leur adoption difficile pour les PME et TPE.\n\nAinsi, en analysant les éléments de bibliographie et les solutions disponibles sur le marché, il s’avère qu’il n’existe aujourd’hui aucune réponse à une plateforme basée sur des serveurs non spécialisés et grand public, un standard simple, peu onéreux et compréhensible de partage de ressources. Par conséquent, malgré un état actuel des connaissances assez pertinent, aucune solution n'est vraiment adaptée au niveau des exigences spécifiques et des défis uniques du projet Graal Systems. Les limitations des solutions existantes combinées aux verrous technologiques identifiés soulignent la nécessité d'un engagement profond dans des travaux de R&D pour surmonter ces défis et repousser les frontières de ce qui est actuellement réalisable."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Verrous et Incertitudes\n\n- **Incertitudes scientifiques initiales :**\n  - Développement de modules de Natural Language Processing (NLP) et de Semantic Processing performants.\n  - Nécessité de concevoir un algorithme de Machine Learning efficace pour associer les entités identifiées aux bonnes catégories de technologies.\n\n- **Verrou technique :**\n  - Construction d'un ensemble de critères de sélection pour optimiser le modèle.\n  - Importance d'alimenter les algorithmes avec un maximum de données pour affiner l'apprentissage.\n\n- **Utilisation du Machine Learning :**\n  - Élaboration de modèles prédictifs avec des incertitudes liées à l'utilisation de données (CV) en français, alors que les approches NLP sont généralement développées pour des données en anglais.\n\n- **Temps de calcul :**\n  - Constitue une incertitude scientifique majeure, nécessitant un effort de R&D.\n  - Possibilité de réduction du temps de calcul par l'amélioration des algorithmes NLP existants ou la conception de nouveaux.\n\n- **Coût en puissance de calcul :**\n  - Mobilisation de plusieurs machines pour les analyses NLP, entraînant des coûts élevés en ressources.\n\n- **Conformité au RGPD :**\n  - Le RGPD constitue un verrou majeur en raison de la protection des données personnelles.\n  - **Risques liés à la collecte et reconstitution des CV :**\n    - Utilisation de NLP et Semantic Processing pour collecter des CV, potentiellement sans consentement explicite des candidats.\n  - **Traitement des données à la volée :**\n    - Obligation de ne pas stocker les données personnelles, entraînant des défis techniques pour un traitement efficace et rapide.\n  - **Annotation de corpus de textes :**\n    - Construction de corpus annotés comme avantage concurrentiel, mais risque de violation des droits de vie privée si mal géré.\n  - **Campagnes automatisées d’emails :**\n    - Risque d'envoi de communications non désirées sans consentement explicite, pouvant contrevenir aux directives du RGPD.\n  - **Sécurité des données :**\n    - Préoccupation centrale pour protéger les données sensibles contre accès non autorisé, altération ou perte.\n    - Nécessité d'intégrer des mécanismes robustes pour garantir l'intégrité et la sécurité des données.\n\n- **Approche proactive :**\n  - Reconnaissance de la nécessité de collaborer avec des experts en RGPD pour assurer la conformité et inspirer confiance aux utilisateurs et partenaires."}, {"role": "assistant", "content": "Aléas, verrous et incertitudes\nLes premières incertitudes scientifiques que nous avons rencontrées concernaient \n\nDe plus, le développement des modules de Natural Language Processing et de Semantic Processing suffisamment performants pour comprendre et associer de manière efficace les entités identifiées avec les bonnes catégories de technologies. Pour cela il a fallu que nous concevions un algorithme de Machine Learning performant.\nNous avons également dû construire un ensemble de critères de sélection, ce qui a constitué un verrou technique. En effet, pour optimiser le modèle alimenter les algorithmes avec le plus de données possibles afin d’affiner leur apprentissage.\n\nD’autre part, dans le cadre de notre projet, nous utilisons le machine learning pour élaborer des modèles prédictifs. L’incertitude scientifique réside dans le fait que nous ciblons un jeu de données (CV) en français alors que les approches NLP sont pour des données en anglais.\n\nLe temps de calcul constitu également une incertitude scientifique à laquelle nous seront confrontés. En effet, le temps de calcul nécessaire à l’exécution de certaines analyses est l’un des principaux points bloquants dans e projet et réclame à ce titre un effort de R&D. Ce temps peut être réduit en améliorant l’efficacité des algorithmes (NLP) correspondant à l’état de l’art, ou en en concevant de nouveaux. \n\nEn outre, comme nous mobilisons un certain nombre de machines pour ce projet, les analyses NLP sont donc très coûteuses en puissance de calcul.\n\nL’une des autres préoccupations dans le développement de notre plateforme d’automatisation du recrutement est la conformité au Règlement Général sur l Protection des Données (RGPD). Ce règlement européen vise à garantir la protection des données personnelles des individus, et par conséquent, il constitue un verrou majeur dans notre projet compte tenu de plusieurs aspects sensibles que nous traitons :\nCollecte et reconstitution des CV : notre plateforme utilise le NLP et le Semantic Processing pour collecter ou reconstituer des CV de candidats grâce à des recherches internet et des documents papiers. Cette approche pose un risque majeur en matière de RGPD car elle implique potentiellement la collecte de données personnelles sans le consentement explicite des candidats concernés.\nTraitement des données à la volée : afin de respecter les exigences du RGPD, notre plateforme ne doit pas stocker les données personnelles des candidats. Il est donc essentiel de les traiter à la volée. Cependant, cela implique des défis techniques et sécuritaires importants car nous devons assurer que les données soient traitées efficacement et rapidement.\nAnnotation de corpus de textes :  la construction de nos propres corpus annotés constitue un avantage concurrentiel, elle comporte aussi des risques. Les informations extraites pour ces annotations si elles sont mal gérées peuvent violer les droits des individus en termes de vie privée.\nCampagnes automatisées d’emails : l’utilisation de campagnes automatisées pour communiquer avec les candidats augmente le risque d’envoyer des communications non désirées ou non sollicitées. Sans un consentement explicite et un mécanisme d’opt-out clair, cela peut contrevenir aux directives de RGPD.\nSécurité des données : au-delà de la conformité au RGPD, la sécurité des données est une préoccupation centrale. Compte tenu de la nature sensible des informations de recrutement, il est essentiel de s’assurer que les données soient protégées contre tout accès non autorisé, altération ou perte. Notre plateforme doit donc intégrer des mécanismes robustes pour garantir l’intégrité des données et les protéger contre d’éventuelles attaques.\n\nFace à ces préoccupations, nous reconnaissons la nécessité d’adopter une approche proactive et de collaborer étroitement avec des experts en RGPD. Leur expertise sera essentielle pour garantir que notre plateforme respecte les réglementations en vigueur mais aussi qu’elle inspire confiance à nos utilisateurs et partenaires quant à la manière dont nous traitons et protégeons leurs données."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous techniques identifiés :**\n  - Design des process et architecture d'intégration.\n  - Back-End composé de plusieurs modules technologiques différents.\n  - Nécessité de respecter et intégrer les éléments décrits dans le brevet.\n  - Orchestration des process associés avec des structures de données.\n  - Contrôle par les événements contextuels.\n\n- **Flux de données :**\n  - Doivent respecter les spécifications techniques associées à chaque action.\n  - Événements contextuels activés : \n    - Identification\n    - Géolocalisation\n    - Temps\n    - Type de véhicule\n    - Circuit\n\n- **Incertitudes concernant les services géolocalisés :**\n  - Comment créer des clusters en fonction des besoins pour proposer des solutions personnalisées ?\n  - Comment analyser les besoins des utilisateurs ?\n  - Comment lier les données issues des plateformes avec les propositions de service en ligne ?\n\n- **Actions nécessaires :**\n  - Travaux de R&D requis pour lever les verrous techniques et répondre aux incertitudes identifiées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes verrous techniques identifiés dans ce projet résident dans le design des process et de l'architecture d'intégration. En effet, le Back-End est composé de plusieurs modules technologique différentes, le design doit donc respecter et intégrer avec ce qui est décrit dans le brevet, l’orchestration des process associés avec des data structures, contrôlé par les évènements contextuels. Les flux de données développés doivent également respecter les spécifications techniques, associées à chaque action, des évènements contextuels sont activés : Identification, Géolocalisation, Temps, type de Véhicule, Circuit.  \nNous avons également identifié différentes incertitudes concernant la proposition de services géolocalisés : comment créer des clusters en fonction du besoin pour proposer des solutions personnalisées ? Comment analyser les besoins utilisateurs ? Comment lier les données issues des plateformes avec les propositions de service en ligne ?  \nDes travaux de R&D sont alors nécessaires afin de lever ces verrous et répondre à ces incertitudes."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Identification des acteurs performants** : MBT a identifié des acteurs plus efficaces dans divers domaines tels que l’automatisation, la diffusion, le MAM et les moteurs de workflow.\n  \n- **Concept d'atelier flexible** : \n  - Avancée basée sur un backbone unique.\n  - Permet de déployer de nouveaux services sans modifier les infrastructures existantes.\n  - Répond à la nécessité de réduire les coûts et d’implémenter des idées de manière agile.\n\n- **Modularité et coûts** : \n  - Pour être agile et performant, il est essentiel d'utiliser la même modularité.\n  - Les modules existants doivent être compatibles sans régression lors de la mise à jour.\n\n- **Changements à Newsgraph** : \n  - Nécessité de décrire les modifications apportées à l’application d’habillage et d’infographie utilisée par TF1 pour assurer la compatibilité avec la nouvelle architecture.\n\n- **Projets avec grands comptes** : \n  - Mise en œuvre de cette approche pour M6 et France Télévisions via le projet CDE avec Sony.\n  - Media Backbone Conductor de Sony, prévu comme outil central, a rencontré des difficultés d’intégration.\n\n- **Difficultés d’intégration avec Sony** : \n  - Problèmes de réactivité liés à l'architecture technique de l'outil, non aux collaborateurs.\n\n- **Importance de la réactivité en audiovisuel** : \n  - Nécessité d'une approche non séquentielle, favorisant la parallélisation pour une meilleure réactivité dans la résolution d'incidents et les évolutions.\n\n- **Objectifs du projet CDE** : \n  - Solution innovante pour France Télévisions, intégrant le Media Backbone Conductor et suivant les principes de l'architecture orientée services.\n  - Automatisation de l'échange de contenu et de métadonnées entre différentes unités de production.\n\n- **Couverture des départements** : \n  - Système couvrant tous les départements de France Télévisions pour l’acquisition et la diffusion des médias.\n\n- **Technologies utilisées** : \n  - Logiciel d’automatisation MBT et serveurs vidéo Omneon sélectionnés.\n  - Routeurs fournis par Miranda.\n\n- **Gestion de l'ingestion** : \n  - Gérée par le logiciel MBT, avec contrôle qualité et transcodage via des workflows dans Sony Media Backbone Conductor.\n\n- **Changement dans la nature des essences audiovisuelles** : \n  - Dématérialisation des programmes, livraison sous forme de fichiers numérisés.\n  - Transition des cassettes aux fichiers impactant l'organisation du travail.\n\n- **Nouveaux outils de travail** : \n  - Utilisateurs doivent travailler avec des moniteurs vidéo et stations de travail pour accéder aux vidéos dans leur format d’origine ou de consultation.\n\n- **Contrôle éditorial et technique** : \n  - Réalisé via le format de consultation, permettant l’indexation.\n\n- **Nécessité de bouleverser le socle technique** : \n  - Pour être plus agile et économique, il est crucial de modifier le socle des outils, pas seulement les modules.\n\n- **Objectifs des travaux** : \n  - Modifier radicalement l'architecture technique des applications broadcast."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nMBT a parfaitement identifié des acteurs plus performants dans un domaine ou un autre, par exemple l’automatisation, la diffusion, le MAM ou encore les moteurs de workflow.  \n\nMais la notion d’atelier flexible constituerait une avancée spécifique basée sur un backbone unique qui permette de décliner l’alimentation de nouveaux services sans modifier les infrastructures en place. Ce concept répondrait à la nouvelle donne financière dans le domaine. Auparavant, les clients de MBT n’étaient pas limités en coûts du côté technique. Ceux-ci étaient minimes par rapport à la problématique d’achat des droits. Maintenant, dès qu’une idée est lancée, elle doit être mise en place de façon agile et à moindre cout. Or, pour être agile et performant sur les couts il faut utiliser la même modularité.  \n\nIl faut aussi prendre en compte d’autres modules, qui sont jusqu’à présent liés au socle existant, et qui ne doivent pas subir de régression dans cette remise à plat. Nous serons en particulier amenés à décrire les changements apportés à Newsgraph, l’application d’habillage et d’infographie utilisée par TF1, pour rester compatible avec la nouvelle structure architecturale.  \n\nCette approche totalement renouvelée a été mise en place par MBT pour des grands comptes comme M6, ou France Télévisions via le projet CDE géré avec Sony. Nous rappelons que, dans ce dernier projet, le groupe Sony a fourni l’outil Media Backbone Conductor, qui était censé être « le couteau suisse » pour la dématérialisation des essences audiovisuelles. Or, l’intégration n’a pas été facile, et MBT a dû apporter ses propres solutions en substitution de ce produit. Un des principaux handicaps du côté de Sony était le temps de réactivité, non pas dû aux collaborateurs, mais imputable à l’architecture technique de l’outil.  \n\nAu-delà des choix d’implémentation, un élément essentiel en audiovisuel est la réactivité : il faut aller vite et travailler à l’image près. Il faut donc s’éloigner d’une approche informatique classique en excluant tout processus séquentiel. Il faut au contraire aller au maximum dans la parrallélisation pour disposer de cette réactivité dans les résolutions d’incident ou dans les évolutions. La Figure 1-5 illustre l’annonce de la création du futur CDE et de ses objectifs, en mars 2013 :  \n\n« Combining technologies :  \nBertrand Caurier, Sony’s project director takes up the story. “The CDE will be one of the first of its kind in the industry. The solution for France Televisions has Sony’s Media Backbone Conductor at its heart and operates on the principles of Service Orientated Architecture, linking the broadcaster’s new playout facility with the rest of the enterprise. This solution encompasses workflow and service considerations and automates the exchange of content and metadata between the various digital production islands.”  \n\nLe système couvre tous les départements de France Télévisions responsables de l’acquisition et de la diffusion des médias des quatre chaînes premium et d’autres outlets du groupe axés sur des sujets plus spécifiques. Caurier ajoute que le logiciel d’automatisation Media & Broadcast Technologies (MBT) avec les serveurs vidéo Omneon ont été sélectionnés pour l’opération, tandis que les routeurs proviennent de Miranda.  \n\nL’ingestion est gérée par le logiciel MBT. Le mouvement des fichiers, le contrôle qualité et le transcodage sont ensuite gérés via des workflows dans Sony Media Backbone Conductor, qui unifie toutes les différentes fonctions spécialisées pour améliorer l’efficacité opérationnelle.  \n\nSur le plan organique, la difficulté nouvelle vient du changement de nature des essences audiovisuelles. Il faut gérer la dématérialisation des programmes, qui peuvent être livrés par des laboratoires sous forme de fichiers qui sont numérisés par le client. Le passage des cassettes aux fichiers a généré un bouleversement dans l’organisation du travail. Les utilisateurs doivent désormais travailler avec des moniteurs vidéo et accéder par une station de travail, à la vidéo dans le format d’origine, ou dans un format de consultation.  \n\nCe dernier format permet de réaliser du contrôle éditorial et technique, ainsi que de l’indexation.  \n\nAvec cette évolution, cumulée aux autres changements évoqués, on ne peut pas continuer à travailler sur les mêmes fondements. Pour être en plus agile et économique, il faut bouleverser le socle des outils, et plus seulement les modules à brancher sur ce socle.  \n\nOutre les autres objectifs énumérés, les travaux effectués ont donc visé à modifier radicalement le socle technique dans l’architecture d’applications broadcast."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Anticipation de la problématique Big Data (2012)** :\n  - MBT a choisi de se positionner sur une problématique de 2 millions de médias, en visant une pertinence qui garantirait également une efficacité à 300 000 médias.\n  - Un média est défini comme un ensemble de fichiers (vidéos haute/basse résolution, sous-titres, etc.) portant sur le même contenu.\n\n- **R&D et performances** :\n  - Initiatives de R&D lancées en 2012 pour répondre aux problématiques de performances liées à la croissance des contenus.\n  - Passage de la diffusion linéaire à non linéaire, permettant le téléchargement et visionnage flexible des contenus.\n\n- **Verrou technologique du Big Data** :\n  - Réticences persistantes à adopter les technologies Big Data, représentant un verrou majeur pour les travaux de R&D.\n  - Importance de la dimension fonctionnelle sur la dimension volumétrique pour éviter de rendre les données invisibles.\n\n- **Couplage entre Big Data et marquage des flux** :\n  - Difficulté d'optimiser simultanément la production de Big Data et le marquage des flux audiovisuels.\n  - Risque de ralentissement mutuel des deux processus, nécessitant une vigilance accrue sur les performances.\n\n- **Base de travail et performances (2013)** :\n  - Mise en place d'un procédé d'insertion de marqueurs avec un haut niveau de performance.\n  - Exemple : insertion de 27 000 marqueurs en une semaine lors des championnats du monde d’athlétisme avec 5 opérateurs.\n\n- **Fonction des marqueurs** :\n  - Les marqueurs doivent fournir des informations contextuelles : texte, noms d’athlètes, commentaires, performances.\n\n- **Flux non linéaires vs linéaires** :\n  - Utilisation de flux non linéaires pour permettre des ralentis et un traitement efficace des marqueurs.\n  - Les flux linéaires nécessitent des duplications et manipulations longues, entraînant des délais de traitement de 30 à 60 minutes.\n\n- **Délinéarisation et approvisionnement en temps réel** :\n  - Possibilité de ralentir le flux en temps réel pour insérer des marqueurs, puis revenir au flux classique.\n  - Conditions d'approvisionnement en temps réel dépendantes des connexions aux Big Data.\n\n- **Déstructuration et hétérogénéité des données** :\n  - Augmentation des volumes de données déstructurées et hétérogènes pose un défi pour la composition rapide et pertinente de ces données.\n\n- **Réflexions sur l'approche Big Data (2013-2015)** :\n  - Enrichissement de la base de données média avec des moteurs de recherche conceptuels et recherche sémantique.\n  - Spécifications du moteur de recherche en phase de conception en 2013, avec des réflexions sur la gestion des Big Data à la manière de Google, tout en intégrant les spécificités du broadcast."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nAborder la problématique « Big Data » était donc une anticipation de la part de MBT en 2012. La société était positionnée sur une problématique à 300 000 media, mais a choisi de viser une logique pertinente à 2 millions de media. Le raisonnement était le suivant : si on est pertinent à 2 millions de media, on le sera à 300 000 media. Un media est ici un ensemble de fichiers qui peut aller de la haute résolution d’une vidéo à sa basse résolution, intégrer les sous-titres, ou d’autres fichiers correspondant à d’autres problématiques. Il s’agit en donc d’un ensemble virtuel portant sur le même contenu.  \n\nMBT avait donc souhaité, à travers la R&D initiée en 2012, répondre au plus vite aux problématiques de performances qui s’annonçaient. A la croissance des contenus, s’ajoute la représentation d’un même contenu par un nombre croissant de fichiers. Ce phénomène s’explique par le passage de la diffusion linéaire, à savoir la diffusion à l’antenne en chaîne continue, à savoir la diffusion non linéaire, qui traduit la possibilité de télécharger et de regarder un journal entier sur sa tablette. En allant sur un serveur de streaming, on utilise un format de basse résolution, en plus du format de l’opérateur. Il existe en plus des formats spécifiques de VOD par opérateur. C’est par exemple le cas d’Orange.  \n\nMais si les volumétries rencontrées peuvent pousser vers les solutions de Big Data, il existait encore de nos jours, et c’est en partie toujours le cas, de fortes réticences à utiliser les technologies correspondantes. Ce passage de bases de données classiques au Big-Data constitue en lui-même un verrou majeur dans le cadre de nos travaux de R&D.  \n\nLa dimension fonctionnelle est donc encore plus importante que la dimension volumétrique, et toute inversion reviendrait à rendre des données invisibles. Cet aspect est difficile à réussir quand on construit une application « Big Data », et d’autant plus en étant pionnier dans son domaine.  \n\nLa difficulté précédente est amplifiée par le couplage entre production de Big Data et marquage des flux audiovisuels. Nous combinons ici deux processus pour lesquels les performances et la rapidité d’exécution sont vite critiques. On peut donc envisager des situations où ces deux processus se freinent mutuellement dans une sorte de spirale infernale. Cette partie du projet nécessite donc une grande vigilance sur les performances et la pertinence des traitements.  \n\nUne base de travail avait été mise en place en 2013 avec le procédé d’insertion des marqueurs, caractérisé par un haut niveau de performance. Une nouvelle offre de services avait été créée à travers la nouvelle plate-forme mise au point par Quantel et MBT. Aux championnats du monde d’athlétisme, avec seulement 5 opérateurs, il était déjà possible d’insérer 27000 marqueurs dans les flux vidéo avec toutes les données associées, et ce en une semaine de présence de ces 5 personnes.  \n\nLes marqueurs doivent indiquer ce qui se passe, entrer du texte, des noms d’athlètes, des commentaires, des performances.  \n\nLa solution reposait sur les flux non linéaires, qui permettaient de faire des ralentis et des stocks d’images pour ceux qui étaient en train de placer les marqueurs, à partir des fichiers à basse résolution.  \n\nCette approche est impossible avec un flux linéaire, car il faut faire des duplications de fichier et des manipulations assez longues pour traiter le flux, et un temps compris entre une demi-heure et une heure est nécessaire pour pouvoir fournir aux chaînes de télévision des images de l’évènement.  \n\nPar la délinéarisation, on peut ralentir en temps réel. Par exemple, on va avoir besoin de ralentir le flux pendant quelques secondes pour placer entre 200 et 300 marqueurs, puis revenir au flux classique.  \n\nLes chaînes devraient donc pouvoir s’approvisionner en temps réel, mais à condition qu’il n’y ait pas plus tard de délai imposé par les connexions aux Big Data.  \n\nMais en plus ces données vont devenir de plus en plus déstructurées et hétérogènes. D’où la question de l’augmentation très importante des volumes, des compositions rapides et pertinentes de ces données hétérogènes ? C’est tout l’enjeu des réflexions et études engagées par MBT depuis 2013 sur l’approche par Big Data, réflexions qui se sont poursuivies en 2014, avec une extension à 2015.  \n\nNous devons donc enrichir la base de données media en y ajoutant des moteurs de recherche conceptuelle, l’idée étant aussi d’aller vers la recherche sémantique pour avoir une logique basée sur cette occurrence croissante d’informations déstructurées. Les spécifications du moteur de recherche étaient dans la phase de conception et de démarrage en 2013, et on initiait à peine les spécifications de la problématique du moteur de recherche. MBT se posait en particulier la question de gérer les Big Data comme le fait Google, mais en conservant des aspects spécifiques liés au métier du broadcast."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Objectif des travaux (2019)** : Dépasser les limites du prototype d’arbre de décision développé en 2018.\n  \n- **Limites identifiées** : \n  - Problèmes de compatibilité avec les données multivaluées.\n  - Nécessité de relever ce défi technologique pour offrir une solution complète aux utilisateurs.\n\n- **Axe de travail sur la robustesse** : \n  - Résoudre les aléas rencontrés concernant la robustesse de l’arbre développé.\n  - Établir un compromis entre haute performance (précision, fiabilité, robustesse) et interprétabilité de l’algorithme.\n\n- **Verrou sur la lisibilité** : \n  - Améliorer la lisibilité et l’interprétabilité de l’arbre généré.\n\n- **Hypothèses posées** : \n  - Trois hypothèses formulées concernant les attributs de l’arbre de décision et son mode de construction.\n\n- **Incertitude à lever** : \n  - Validation de l'impact neutre (ou positif) des améliorations sur les performances techniques de l’algorithme."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes travaux de développement expérimental que nous nous proposons de réaliser en 2019 visent à dépasser les limites que nous avons pressentir sur le prototype d’arbre de décision développé en 2018. Nous avons pu identifier les limites de la solution en termes de compatibilité avec les données multivaluées et nous souhaitons relever ce défi technologique pour offrir à nos utilisateurs la solution la plus complète possible, capable de les accompagner dans leurs analyses les plus approfondies. Un autre axe de travail visait à résoudre les aléas que nous avons pu rencontrer en termes de robustesse de l’arbre développé. En effet, nous devons déterminer un compromis entre la haute performance (offrant une grande précision, fiabilité et robustesse) de l’algorithme à l’origine de l’arbre développé et l’interprétabilité de ce dernier. Enfin, un dernier verrou à adresser via nos travaux expérimentaux est lié à la lisibilité et à l’interprétabilité de l’arbre généré. Nous avons appréhendé la problématique en posant trois hypothèses quant aux attributs de l’arbre de décision et quant au mode de construction de celui-ci. L’incertitude à lever est la validation de l’impact neutre (ou positif) de ces améliorations sur les performances techniques de notre algorithme."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrou principal** : Échec des méthodes statistiques de régression traditionnelles face à la problématique de retranscription du caractère évolutif des données.\n  \n- **Objectif du projet** : Développer une solution pour analyser et comparer deux jeux de données en utilisant des filtres temporels, géographiques et des variables spécifiques, tout en identifiant les facteurs les plus impactants.\n\n- **Problème identifié** : Multi-colinéarité entre les variables des enquêtes de satisfaction, entraînant des analyses peu fiables.\n\n- **Exploration de la littérature** : Recherche de publications scientifiques sur des données réelles confrontées aux mêmes limites des modèles de statistique paramétrique.\n\n- **Constatations sur les études** : Bien que certaines études proposent des méthodes sophistiquées et performantes, leur complexité calculatoire empêche d'atteindre les spécifications souhaitées.\n\n- **Orientation future** : Nécessité de se diriger vers une modélisation simplifiée pour développer de nouveaux algorithmes permettant la comparaison efficace entre deux jeux de données."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe principal verrou auquel nous avons été confrontés est lié à l’échec des méthodes statistiques de régression « traditionnelles » face à notre problématique. En effet, nous souhaitons développer une solution capable de retranscrire le caractère évolutif d’un jeu de données évalué. Nous souhaitons permettre une analyse entre deux jeux de données selon des filtres temporel, géographique, des variables particulières, etc. et en identifier les facteurs les plus impactants.  \nEn raison de la multi-colinéarité entre les différentes variables qui peuvent être considérées dans les enquêtes de satisfactions soumises aux clients, les analyses qui sont réalisées ne donnent pas de résultats fiables.  \nNous avons ainsi souhaité explorer des publications scientifiques réalisées sur des données réelles dans lesquelles les auteurs ont été confrontés aux mêmes limites des modèles de statistique paramétrique. Nous avons également constaté que certaines études se plaçaient  \nBien que les travaux identifiés mettent en avant des méthodes sophistiquées et performantes pour traduire notre problème, nous avons constaté que leur complexité calculatoire ne permettrait pas d’atteindre les spécifications visées.  \nNous devons ainsi nous orienter vers une modélisation simplifiée de notre problème pour envisager de nouveaux algorithmes permettant la comparaison entre deux jeux de données comme souhaité."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Objectif des travaux** : Concevoir un nouveau type d’analyse pour comparer un indicateur quantitatif entre deux jeux de données d’un même questionnaire, afin d’identifier les principaux facteurs distinctifs.\n  \n- **Continuité des efforts de R&D** : Les travaux s’inscrivent dans la continuité des recherches menées en 2020.\n\n- **Développements expérimentaux de 2020** : Évaluation de l’approche pour déterminer l’analyse en tendance la plus pertinente entre deux jeux de données.\n\n- **Travaux de 2021** : Évaluation de la pertinence des choix précédents à travers des expérimentations sur des données simulées et réelles.\n\n- **Problèmes de fiabilité de l’algorithme** : \n  - L’analyse en tendance, initialement conçue pour comparer les tendances, s'est révélée difficilement interprétable.\n  - Des règles statistiques ont été remises en question, rendant impossible la vérification du cadre d’application des règles.\n\n- **Réorientation de l’algorithme** : \n  - Permet de réduire le nombre de tests statistiques nécessaires.\n  - Diminue le nombre de calculs et les ressources requises pour le fonctionnement de la solution.\n\n- **Difficultés de retranscription** : \n  - Problèmes rencontrés lors du passage de R (utilisé pour la R&D) à Python (utilisé en production).\n  - Absence de plusieurs fonctions dans les librairies Python, nécessitant une programmation manuelle."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nL’objectif de nos travaux, dans la continuité des efforts de R&D consentis en 2020 sur cet axe de recherche, porte sur la conception d’un nouveau type d’analyse permettant de comparer un indicateur quantitatif entre deux jeux de données d’un même questionnaire pour en faire ressortir les principaux facteurs distinctifs.\n\nA la suite de développements expérimentaux menés en 2020 sur l’approche à considérer pour révéler l’analyse en tendance la plus pertinente parmi deux jeux de données, nos travaux de 2021 visaient à évaluer la pertinence des choix réalisés jusqu’alors à travers l’expérimentation sur des données simulées et sur des données réelles.\n\nLa fiabilité de l’algorithme conçu a été remise en question. En effet, l’analyse en tendance était proposée en tant que comparaison de tendances et devenait ainsi difficilement interprétable. En outre, des règles statistiques étaient mises à mal et ne permettaient plus de vérifier le cadre d’application des règles invoquées.\nEn outre, la réorientation de notre algorithme de comparaison des tendances nous a permis de réduire le nombre de tests statistiques à réaliser, réduisant ainsi le nombre de calculs nécessaires et les ressources requises pour faire fonctionner notre solution.\nNous avons en outre été confrontés à quelques difficultés lors de la retranscription de nos développements depuis R (utilisé pour la R&D) vers Python (utilisé en production). En effet, plusieurs fonctions étaient absentes des librairies Python et nécessitaient ainsi d’être programmées manuellement."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Limitations Technologiques des Biocapteurs Génétiques :**\n  - Dépendent de la connaissance de l'écologie des microorganismes ciblés.\n  - Nécessitent des caractéristiques génétiques bien décrites dans les bases de données pour détecter les cibles biologiques.\n  - Ne peuvent détecter que les microorganismes pour lesquels les gènes cibles sont connus.\n  - Les organismes non ciblés dans l'environnement ne sont pas pris en compte.\n\n- **Étude Préalable :**\n  - Recommandée pour cartographier génétiquement le milieu à surveiller.\n  - Permet d'améliorer la connaissance du milieu et d'optimiser les jeux de sondes.\n\n- **Problèmes avec les ARN Ribosomiques (ARNr) :**\n  - Structure non linéaire en trois dimensions, rendant certaines zones inaccessibles.\n  - Risque d'hybridation avec des ARN proches mais différents dans un environnement naturel.\n  - Les tests en laboratoire ne garantissent pas un comportement similaire dans un contexte réel.\n\n- **Dérive Génétique :**\n  - Cause de mutations affectant l'information génétique.\n  - Les sondes ne reconnaissent que des homologues stricts ; un gène cible muté ne sera pas détecté.\n  - Le séquençage de masse fournit des informations sur la diversité génétique, mais est lourd et laborieux.\n\n- **Validation des Sondes :**\n  - Réalisée par expérimentation en laboratoire pour démontrer la spécificité des sondes vis-à-vis des microalgues ciblées.\n\n- **Quantification des Organismes Ciblés :**\n  - Les approches basées sur les ARN apportent des informations supplémentaires mais dépendent de l'état physiologique des cellules.\n  - Nécessité de courbes de calibration pour chaque organisme étudié pour surmonter les contraintes quantitatives.\n  - Tests des sondes confrontés à de nombreuses souches microbiennes et conditions environnementales.\n\n- **Étude CYATOX :**\n  - Développement d'un biocapteur pour détecter les cyanobactéries, qui sont des procaryotes.\n  - Pas de corrélation avérée entre la prolifération des cellules et la quantité d'ARN ribosomique pour les cyanobactéries.\n  - Absence de validation des hypothèses concernant la quantification des cyanobactéries."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes limitations technologiques des biocapteurs génétiques sont en premier lieu liées à l’utilisation de sondes génétiques qui dépendent directement de la connaissance de l’écologie des microorganismes ciblés. En effet, pour éprouver un système de détection, il est impératif de connaître les caractéristiques génétiques des cibles biologiques. Ces technologies ne peuvent donc détecter que les microorganismes pour lesquels les gènes cibles sont suffisamment bien décrits dans les bases de données. Par conséquent, ces technologies ne peuvent détecter que les cibles recherchées, tout autre organisme présent dans l’environnement, qu’il soit d’intérêt ou non, ne sera pas pris en compte. Cet obstacle peut être levé en réalisant une étude préalable qui cartographiera génétiquement le milieu qui requiert une surveillance. Cette approche permet d’incrémenter la connaissance d’un milieu et de perfectionner les jeux de sondes utilisés. Cet aspect est fondamental dans le domaine des pathogènes émergents ou ré-émergents pour lesquels la biologie et le mode de vie sont soit oubliés soit inconnus. Notre objectif est de tester et d’évaluer la spécificité et les propriétés physico-chimiques in silico des sondes génétiques qui seront déterminées pour chaque cible d’intérêt.  \n\nDe plus, les ARN ribosomiques, ou ARNr, à partir desquelles sont conçues les sondes spécifiques à un microorganisme, possèdent une structure non linéaire en trois dimensions. L’efficacité des sondes dépendra donc de la zone de l’ARNr sur laquelle elles ont été définies. Or dans des conditions biologiques réelles, certaines parties de l’ARNr sont inaccessibles, or une sonde conçue pour reconnaître une zone précise de celui-ci pourrait donc ne pas être en mesure de la faire. Par ailleurs, dans un environnement naturel réel, indubitablement plus diversifié que celui du laboratoire, apparaît un risque d’hybridation de nos sondes avec des ARN différents, mais néanmoins très proches, de notre cible. Au vu du grand nombre de sources d’ARN que nous ne prenons pas en compte dans nos essais en laboratoire : les cellules d’autres plantes, d’animaux ou d’êtres humains ; le fait qu’il n’y ait pas d’hybridation dans un contexte artificiel n’assure donc pas un comportement analogue dans un contexte réel.  \n\nUne autre limite de l’utilisation de ces sondes est associée à la dérive génétique, qui est à l’origine de mutations génétiques touchant directement l’information génétique. Les sondes développées pour les biocapteurs sont des copies identiques d’une partie d’un gène. Leurs séquences sont capables de reconnaître, dans un mélange de gènes, leurs homologues stricts. Un gène cible muté ne sera donc pas reconnu par la sonde du même gène cible non muté. Le séquençage de masse d’un échantillon environnemental offre une information génétique précise de quels organismes sont présents et de leur diversité génétique, dérive génétique inclue. Bien que ces approches soient lourdes et laborieuses elles sont actuellement le seul recours pour pallier ces divers problèmes. Nous validons les sondes des biocapteurs désignées par une expérimentation en laboratoire. Il s’agit de démontrer qu’elles sont capables de reconnaître spécifiquement la microalgue ciblée dans différentes conditions expérimentales et à l’aide de cultures pures ou environnementales.  \nEncore un aspect limitant est la capacité de ces technologies à quantifier les organismes ciblés. Les approches utilisant les ARN, transcrits du support génétique, vont apporter de l’information supplémentaire mais vont voir l’aspect quantitatif déterminé par l’état physiologique des cellules ciblées. En effet, le nombre et la durée de vie de ces ARN varient énormément d’un microorganisme à l’autre. Seule l’utilisation de courbes de calibration pour chacun des organismes étudiés peut défaire cette contrainte. Cela représente un travail constant afin d’étudier les variations des signaux émis en fonction du nombre, du type, de l’état physiologique des cellules. Nous testons chaque jeu de sondes en les confrontant au plus grand nombre de souches microbiennes similaires à celle ciblée et au plus grand nombre de conditions environnementales et physiologiques possibles.  \n\nEnfin, dans le cadre de l’étude CYATOX, nous travaillons sur un biocapteur de détection d’un type de microorganismes bien précis que sont les cyanobactéries. A la différence des microalgues, ce sont des procaryotes. Ainsi, contrairement aux eucaryotes, pour lesquels la corrélation entre la prolifération des cellules et la quantité d’ARN ribosomique dans une cellule a été avérée et permet de quantifier la présence d’un organisme donné, aucune relation de ce type n’a été mise en évidence pour les cyanobactéries. Plusieurs hypothèses ont été émises sur le sujet pour certains procaryotes, mais aucune sur le cas précis des cyanobactéries. D’ailleurs, aucune d’entre elles n’a, à ce jour, été corroborée."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Limitations des biocapteurs génétiques** :\n  - Dépendent de la connaissance de l'écologie des microorganismes ciblés.\n  - Nécessitent des caractéristiques génétiques bien décrites dans les bases de données pour détecter des cibles biologiques.\n  - Ne peuvent pas détecter d'autres organismes présents dans l'environnement.\n\n- **Approche pour surmonter les limitations** :\n  - Réalisation d'une étude préalable pour cartographier génétiquement le milieu à surveiller.\n  - Permet d'améliorer la connaissance du milieu et d'optimiser les jeux de sondes.\n\n- **Importance dans le contexte des pathogènes** :\n  - Essentiel pour les pathogènes émergents ou ré-émergents dont la biologie est mal connue.\n\n- **Évaluation des sondes génétiques** :\n  - Tests in silico pour évaluer la spécificité et les propriétés physico-chimiques des sondes pour chaque cible d'intérêt.\n\n- **Complexité des ARN ribosomiques (ARNr)** :\n  - Structure non linéaire en trois dimensions, rendant certaines zones inaccessibles.\n  - Risque d'hybridation avec des ARN similaires dans un environnement naturel diversifié.\n\n- **Dérive génétique** :\n  - Mutations génétiques peuvent empêcher la reconnaissance par les sondes.\n  - Séquençage de masse nécessaire pour obtenir des informations précises sur la diversité génétique.\n\n- **Validation des sondes** :\n  - Expérimentations en laboratoire pour démontrer la capacité des sondes à reconnaître spécifiquement la microalgue ciblée.\n\n- **Quantification des organismes** :\n  - Difficulté à quantifier les organismes ciblés due à la variabilité des ARN en fonction de l'état physiologique des cellules.\n  - Nécessité d'utiliser des courbes de calibration pour chaque organisme étudié.\n\n- **Tests des sondes** :\n  - Confrontation des sondes à un large éventail de souches microbiennes et de conditions environnementales.\n\n- **Étude CYATOX et cyanobactéries** :\n  - Développement d'un biocapteur pour détecter les cyanobactéries, qui sont des procaryotes.\n  - Absence de corrélation avérée entre la prolifération des cyanobactéries et la quantité d'ARN ribosomique, contrairement aux eucaryotes.\n  - Aucune hypothèse sur les cyanobactéries n'a été corroborée jusqu'à présent."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLes limitations technologiques des biocapteurs génétiques sont en premier lieu liées à l’utilisation de sondes génétiques qui dépendent directement de la connaissance de l’écologie des microorganismes ciblés. En effet, pour éprouver un système de détection, il est impératif de connaître les caractéristiques génétiques des cibles biologiques. Ces technologies ne peuvent donc détecter que les microorganismes pour lesquels les gènes cibles sont suffisamment bien décrits dans les bases de données. Par conséquent, ces technologies ne peuvent détecter que les cibles recherchées, tout autre organisme présent dans l’environnement, qu’il soit d’intérêt ou non, ne sera pas pris en compte. Cet obstacle peut être levé en réalisant une étude préalable qui cartographiera génétiquement le milieu qui requiert une surveillance. Cette approche permet d’incrémenter la connaissance d’un milieu et de perfectionner les jeux de sondes utilisés. Cet aspect est fondamental dans le domaine des pathogènes émergents ou ré-émergents pour lesquels la biologie et le mode de vie sont soit oubliés soit inconnus. Notre objectif est de tester et d’évaluer la spécificité et les propriétés physico-chimiques in silico des sondes génétiques qui seront déterminées pour chaque cible d’intérêt. \n\nDe plus, les ARN ribosomiques, ou ARNr, à partir desquelles sont conçues les sondes spécifiques à un microorganisme, possèdent une structure non linéaire en trois dimensions. L’efficacité des sondes dépendra donc de la zone de l’ARNr sur laquelle elles ont été définies. Or dans des conditions biologiques réelles, certaines parties de l’ARNr sont inaccessibles, or une sonde conçue pour reconnaître une zone précise de celui-ci pourrait donc ne pas être en mesure de la faire. Par ailleurs, dans un environnement naturel réel, indubitablement plus diversifié que celui du laboratoire, apparaît un risque d’hybridation de nos sondes avec des ARN différents, mais néanmoins très proches, de notre cible. Au vu du grand nombre de sources d’ARN que nous ne prenons pas en compte dans nos essais en laboratoire : les cellules d’autres plantes, d’animaux ou d’êtres humains ; le fait qu’il n’y ait pas d’hybridation dans un contexte artificiel n’assure donc pas un comportement analogue dans un contexte réel.\n\nUne autre limite de l’utilisation de ces sondes est associée à la dérive génétique, qui est à l’origine de mutations génétiques touchant directement l’information génétique. Les sondes développées pour les biocapteurs sont des copies identiques d’une partie d’un gène. Leurs séquences sont capables de reconnaître, dans un mélange de gènes, leurs homologues stricts. Un gène cible muté ne sera donc pas reconnu par la sonde du même gène cible non muté. Le séquençage de masse d’un échantillon environnemental offre une information génétique précise de quels organismes sont présents et de leur diversité génétique, dérive génétique inclue. Bien que ces approches soient lourdes et laborieuses elles sont actuellement le seul recours pour pallier ces divers problèmes. Nous validons les sondes des biocapteurs désignées par une expérimentation en laboratoire. Il s’agit de démontrer qu’elles sont capables de reconnaître spécifiquement la microalgue ciblée dans différentes conditions expérimentales et à l’aide de cultures pures ou environnementales. \nEncore un aspect limitant est la capacité de ces technologies à quantifier les organismes ciblés. Les approches utilisant les ARN, transcrits du support génétique, vont apporter de l’information supplémentaire mais vont voir l’aspect quantitatif déterminé par l’état physiologique des cellules ciblées. En effet, le nombre et la durée de vie de ces ARN varient énormément d’un microorganisme à l’autre. Seule l’utilisation de courbes de calibration pour chacun des organismes étudiés peut défaire cette contrainte. Cela représente un travail constant afin d’étudier les variations des signaux émis en fonction du nombre, du type, de l’état physiologique des cellules. Nous testons chaque jeu de sondes en les confrontant au plus grand nombre de souches microbiennes similaires à celle ciblée et au plus grand nombre de conditions environnementales et physiologiques possibles. \n\nEnfin, dans le cadre de l’étude CYATOX, nous travaillons sur un biocapteur de détection d’un type de microorganismes bien précis que sont les cyanobactéries. A la différence des microalgues, ce sont des procaryotes. Ainsi, contrairement aux eucaryotes, pour lesquels la corrélation entre la prolifération des cellules et la quantité d’ARN ribosomique dans une cellule a été avérée et permet de quantifier la présence d’un organisme donné, aucune relation de ce type n’a été mise en évidence pour les cyanobactéries. Plusieurs hypothèses ont été émises sur le sujet pour certains procaryotes, mais aucune sur le cas précis des cyanobactéries. D’ailleurs, aucune d’entre elles n’a, à ce jour, été corroborée."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Limitations des Biocapteurs Génétiques :**\n  - Dépendent de la connaissance de l'écologie des microorganismes ciblés.\n  - Nécessité de connaître les caractéristiques génétiques des cibles biologiques pour la détection.\n  - Détection limitée aux microorganismes avec gènes cibles bien décrits dans les bases de données.\n  - Autres organismes présents dans l'environnement non pris en compte.\n\n- **Étude Préalable Requise :**\n  - Réalisation d'une étude pour cartographier génétiquement le milieu à surveiller.\n  - Permet d'améliorer la connaissance du milieu et de perfectionner les sondes utilisées.\n  - Essentiel pour les pathogènes émergents ou ré-émergents dont la biologie est peu connue.\n\n- **Propriétés des Sondes Génétiques :**\n  - Évaluation in silico de la spécificité et des propriétés physico-chimiques des sondes pour chaque cible d’intérêt.\n  - Les ARN ribosomiques (ARNr) ont une structure non linéaire en trois dimensions, affectant l'efficacité des sondes.\n  - Risque d'inaccessibilité de certaines parties de l'ARNr dans des conditions biologiques réelles.\n\n- **Risques d'Hybridation :**\n  - Possibilité d'hybridation des sondes avec des ARN similaires d'autres organismes.\n  - Environnement naturel plus diversifié que les conditions de laboratoire, augmentant les risques d'hybridation.\n\n- **Dérive Génétique :**\n  - Mutations génétiques peuvent affecter la reconnaissance par les sondes.\n  - Les sondes sont des copies identiques d'une partie d'un gène et ne reconnaissent pas les gènes cibles mutés.\n  - Séquençage de masse nécessaire pour obtenir des informations précises sur la diversité génétique.\n\n- **Validation des Sondes :**\n  - Validation par expérimentation en laboratoire pour démontrer la spécificité des sondes.\n  - Tests dans différentes conditions expérimentales avec cultures pures ou environnementales.\n\n- **Quantification des Organismes :**\n  - Les approches utilisant les ARN apportent des informations supplémentaires mais sont limitées par l'état physiologique des cellules.\n  - Variabilité du nombre et de la durée de vie des ARN selon les microorganismes.\n  - Utilisation d'abaques (courbes de calibration) nécessaire pour chaque organisme étudié.\n\n- **Études sur le Phytoplancton Marin :**\n  - Étude de la faisabilité du phytoplancton comme bioindicateur de l'état de santé de l'écosystème marin.\n  - Peu de documentation scientifique sur l'utilisation du phytoplancton comme bioindicateur.\n  - Identification par microscopie ne reflète pas l'état physiologique cellulaire.\n\n- **Cyanobactéries comme Biocapteur :**\n  - Biocapteur spécifique pour les procaryotes (cyanobactéries) contrairement aux eucaryotes.\n  - Aucune corrélation avérée entre prolifération des cellules et quantité d'ARN ribosomique pour les cyanobactéries.\n  - Hypothèses sur la quantification des procaryotes non corroborées pour les cyanobactéries."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLes limitations technologiques des biocapteurs génétiques sont en premier lieu liées à l’utilisation de sondes génétiques qui dépendent directement de la connaissance de l’écologie des microorganismes ciblés. En effet, pour éprouver un système de détection, il est impératif de connaître les caractéristiques génétiques des cibles biologiques. Ces technologies ne peuvent donc détecter que les microorganismes pour lesquels les gènes cibles sont suffisamment bien décrits dans les bases de données. Par conséquent, ces technologies ne peuvent détecter que les cibles recherchées, tout autre organisme présent dans l’environnement, qu’il soit d’intérêt ou non, ne sera pas pris en compte. Cet obstacle peut être levé en réalisant une étude préalable qui cartographiera génétiquement le milieu qui requiert une surveillance. Cette approche permet d’incrémenter la connaissance d’un milieu et de perfectionner les jeux de sondes utilisés. Cet aspect est fondamental dans le domaine des pathogènes émergents ou ré-émergents pour lesquels la biologie et le mode de vie sont soit oubliés soit inconnus. Notre objectif est de tester et d’évaluer la spécificité et les propriétés physico-chimiques in silico des sondes génétiques qui seront déterminées pour chaque cible d’intérêt. \n\nDe plus, les ARN ribosomiques, ou ARNr, à partir desquels sont conçues les sondes spécifiques à un microorganisme, possèdent une structure non linéaire en trois dimensions. L’efficacité des sondes dépendra donc de la zone de l’ARNr sur laquelle elles ont été définies. Or dans des conditions biologiques réelles, certaines parties de l’ARNr sont inaccessibles ; ainsi, une sonde conçue pour reconnaître une zone précise de celui-ci pourrait donc ne pas être en mesure de le faire. Par ailleurs, dans un environnement naturel réel, indubitablement plus diversifié que celui du laboratoire, apparaît un risque d’hybridation de nos sondes avec des ARN différents, mais néanmoins très proches, de notre cible. Au vu du grand nombre de sources d’ARN que nous ne prenons pas en compte dans nos essais en laboratoire : les cellules d’autres plantes, d’animaux ou d’êtres humains ; le fait qu’il n’y ait pas d’hybridation dans un contexte artificiel n’assure donc pas un comportement analogue dans un contexte réel.\n\nUne autre limite de l’utilisation de ces sondes est associée à la dérive génétique, qui est à l’origine de mutations génétiques touchant directement l’information génétique. Les sondes développées pour les biocapteurs sont des copies identiques d’une partie d’un gène. Leurs séquences sont capables de reconnaître, dans un mélange de gènes, leurs homologues stricts. Un gène cible muté ne sera donc pas reconnu par la sonde du même gène cible non muté. Le séquençage de masse d’un échantillon environnemental offre une information génétique précise de quels organismes sont présents et de leur diversité génétique, dérive génétique incluse. Bien que ces approches soient lourdes et laborieuses elles sont actuellement le seul recours pour pallier ces divers problèmes. Nous validons les sondes des biocapteurs désignées par une expérimentation en laboratoire. Il s’agit de démontrer qu’elles sont capables de reconnaître spécifiquement la microalgue ciblée dans différentes conditions expérimentales et à l’aide de cultures pures ou environnementales.\nEncore un aspect limitant est la capacité de ces technologies à quantifier les organismes ciblés. Les approches utilisant les ARN, transcrits du support génétique, vont apporter de l’information supplémentaire mais vont voir l’aspect quantitatif déterminé par l’état physiologique des cellules ciblées. En effet, le nombre et la durée de vie de ces ARN varient énormément d’un microorganisme à l’autre. Seule l’utilisation d’abaques (des courbes de calibration) pour chacun des organismes étudiés peut défaire cette contrainte. Cela représente un travail constant afin d’étudier les variations des signaux émis en fonction du nombre, du type et de l’état physiologique des cellules. Nous testons chaque jeu de sondes en les confrontant au plus grand nombre de souches microbiennes similaires à celle ciblée et au plus grand nombre de conditions environnementales et physiologiques possibles.\nEn 2020, nous sommes avons particulièrement étudié la faisabilité et la pertinence du phytoplancton marin comme bioindicateur du bon état de santé de l’écosystème marin. Les études concernant l’utilisation de ce microorganisme comme bioindicateur de l’état de l’eau sont peu documentées dans la littérature scientifique. La plupart des études consiste à identifier et dénombrer les espèces présentes par microscopie ce qui n’indique pas l’état physiologique cellulaire. Les espèces visées sont souvent les plus nombreuses et ne sont pas forcément représentatives de l’état de l’écosystème. Il n’existe actuellement pas de référence de bio-indication en milieu marin, seulement quelques recommandations pour raisons sanitaires.\nEnfin, dans le cadre de l’étude des cyanobactéries, il s’agit un biocapteur de détection d’un type de microorganismes bien précis. En effet, à la différence des microalgues, les organismes étudiés sont des procaryotes. Ainsi, contrairement aux eucaryotes, pour lesquels la corrélation entre la prolifération des cellules et la quantité d’ARN ribosomique dans une cellule a été avérée et permet de quantifier la présence d’un organisme donné, aucune relation de ce type n’a été mise en évidence pour ce type de cellules, dont font partie les cyanobactéries. Plusieurs hypothèses ont été émises sur le sujet pour certains procaryotes, mais aucune sur le cas précis des cyanobactéries. D’ailleurs, aucune d’entre elles n’a, à ce jour, été corroborée."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Incertitude scientifique principale :**\n  - Détermination des paramètres de décision.\n  - Mauvais choix de paramètres impacte la précision des modèles et des prédictions.\n\n- **Difficulté liée aux modèles :**\n  - Conception de modèles pour mettre en avant les aspects pertinents des représentations.\n  - Nécessité de faciliter la prise de décision sur les représentations à considérer ou à rejeter.\n  - Équilibre à trouver entre :\n    - Compression de la représentation (nombre de dimensions de l'espace latent).\n    - Caractéristiques pertinentes des représentations en entrée.\n\n- **Verrou technologique :**\n  - Entraînement des modèles (MLP et AutoEncoder) sur la collection test MovieLens20M.\n  - Traitement de matrices de nombreuses dimensions (matrices creuses).\n  - Conséquences : augmentation des coûts et du temps d'exécution."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLa principale incertitude scientifique à laquelle nous avons fait face a été de déterminer les paramètres de décision. En effet, le mauvais choix de ces paramètres impacte la précision des modèles et donc des prédictions.\n\nUne autre difficulté à laquelle nous avons fait face était liée aux modèles conçus pour mettre en exergue les aspects des représentations pertinents pour faciliter la prise de décision quant aux représentations à prendre en considération et celles à rejeter. Nous étions donc amenés, dans nos modèles, à mettre en balance les deux critères suivants pour lesquels nous devions trouver un juste équilibre :\n\nLa compression de la représentation, mesurée par le nombre de dimensions de l'espace lattent  \nLes caractéristiques pertinentes des représentations en entrée.\n\nUn autre verrou technologique auquel nous avons été confrontés concernait l’entraînement des modèles (MLP et AutoEncoder) sur notre collection test MovieLens20M. En effet, cet entraînement nécessitait un traitement de matrices s’étendant sur de nombreuses dimensions (matrices creuses), ce qui engendre une augmentation des coûts et du temps d’exécution."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Impact de MIRPHAB** :\n  - Réduction de la fragmentation de la chaîne d'approvisionnement MIR, obstacle à la pénétration des technologies MIR.\n\n- **Obstacles à la pénétration des technologies MIRPHAB** :\n  1. **Sécurité et réglementation** :\n     - Marché des appareils de contrôle et surveillance des émissions fortement réglementé.\n     - Appareils soumis à des certifications coûteuses (temps et argent).\n     - Déploiement des systèmes de surveillance lié à l'application des réglementations.\n     - Nouvelles règles de réglementation appliquées lorsque des solutions innovantes sont disponibles.\n\n  2. **Applications médicales** :\n     - Instruments soumis à des procédures de validation longues et coûteuses (tests sur patients, validation clinique).\n\n- **Avantages de MIRPHAB** :\n  - Réduction du délai de mise sur le marché grâce à un prototypage rapide.\n  - Procédures de fabrication normalisées et produits entièrement qualifiés.\n  - Réduction des risques associés à la phase de développement.\n  - Coûts de fabrication abordables grâce à des procédures partagées.\n\n- **Obstacles liés à l'acceptation des technologies** :\n  - Utilisation habituelle de technologies traditionnelles (FTIR et GC) par les décideurs et utilisateurs finaux.\n  - Nécessité de convaincre la communauté des avantages des nouvelles technologies MIR basées sur le laser.\n  - Arguments en faveur des technologies traditionnelles considérées comme « à la pointe de la technologie ».\n  \n- **Stratégies de réduction des obstacles** :\n  - Planification d'activités de diffusion et d'exploitation agressives par le consortium.\n  - Conviction que les technologies laser ciblées par MIRPHAB rencontreront une croissance exponentielle dans les années à venir."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL'impact positif de MIRPHAB sur la réduction de la fragmentation de la chaîne d'approvisionnement MIR, actuellement considéré comme un obstacle sérieux à une plus grande pénétration du marché des technologies MIR, a déjà été discuté en détail dans les paragraphes précédents. Il existe deux autres obstacles principaux à la pénétration des technologies MIRPHAB:\n\nLa sécurité et la sûreté, le contrôle et la surveillance des émissions sont des segments de marché fortement réglementés ; ainsi, les appareils destinés à ces applications sont soumis à des certifications coûteuses en temps et en argent. En conséquence, le déploiement des systèmes de surveillance se produit lorsque les réglementations spécifiques sont appliquées par la loi. En effet, l'application de nouvelles règles de réglementation commence généralement lorsque des solutions innovantes deviennent disponibles sur le plan technique et commercial.  \nDe même, dans les applications médicales et dans le secteur de la santé en général, les nouveaux instruments doivent être soumis à des procédures de validation longues et coûteuses, qui peuvent inclure des tests sur des patients ou une validation clinique.  \nMIRPHAB réduira considérablement le délai de mise sur le marché en offrant une voie rapide pour le prototypage basé sur des procédures de fabrication normalisées et des produits entièrement qualifiés, réduisant ainsi les risques associés à la phase de développement de l'outil. En outre, les procédures de fabrication partagées entraîneront des coûts de fabrication abordables, réduisant ainsi l'impact économique du prototypage du système. Tous ces aspects peuvent permettre l’apparition de nouvelles réglementations.\n\nObstacles et limites imposés par l'acceptation des technologies :  \nDans le scénario de marché existant pour la détection analytique des produits chimiques, les décideurs et les utilisateurs finaux sont habitués à utiliser les technologies traditionnelles reposant principalement sur la détection FTIR et GC. Même si de nombreux utilisateurs précoces ont déjà commencé à utiliser ces nouvelles technologies MIR basées sur le laser, un effort important reste nécessaire pour convaincre la communauté des avantages considérables de la détection par laser et de la démarche suivie par MIRPHAB.  \nPour contrebalancer les arguments utilisés par pro pour les technologies actuelles utilisées depuis une décennie et toujours considérées comme « à la pointe de la technologie », devront être contrebalancés par les nouvelles opportunités générées par l’utilisation des technologies MIR fournies par MIRPHAB. Afin de réduire ces obstacles, le consortium a planifié des activités de diffusion et d’exploitation agressives. Nous sommes convaincus que les nouvelles technologies laser, telles que ciblées dans le MIRPHAB, comportant un laser pour des mesures multicomposantes en ligne rapides rencontreront des taux de croissance exponentiels dans les années à venir.  \n\nQuels étaient les aléas spécifiques aux travaux de mirsense ?"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques du Projet MIR-Spec\n\n- **Objectif du projet** : Développer une technologie de photonique du silicium exploitant des membranes silicium sous-longueur d’onde pour couvrir le Near-IR et Mid-IR.\n- **Production** : Utilisation des installations de fabrication de microélectronique pour une production rentable et en grande quantité.\n  \n#### Développements Prévus\n- **Dispositifs passifs** : Circuits à bande ultra-large, capteurs à ondes évanescentes, sources non linéaires à bandes larges.\n- **Plage de transparence** : Accès complet de 1,1 µm à 8 µm grâce à des matériaux de haute qualité et des processus de fabrication matures (SOI).\n- **Caractéristiques des circuits** :\n  - Contraste élevé pour circuits ultra-compacts.\n  - Efficacité non linéaire améliorée.\n  - Degrés de liberté pour l’adaptation des propriétés de propagation de la lumière (dispersion, confinement).\n\n#### Innovations Techniques\n- **Guides d’ondes à membranes** : Surmonter le compromis entre grand champ évanescent et faible perte de propagation, avec sensibilités inférieures à un milliard par rapport aux capteurs d’espace libre.\n- **Réduction de l’indice de réfraction** : Délocalisation du mode optique et réduction de l’affaiblissement dû à la rugosité.\n- **Performance expérimentale** : Guides d'ondes Near-IR avec perte de propagation d’environ 2 dB/cm, permettant des sensibilités de 100 ppb.\n\n#### Sources Non-Linéaires\n- **Développement** : Sources non-linéaires à bandes larges (peignes de fréquence et super continuum) utilisant le confinement étroit entre le silicium et l’air.\n- **Ingénierie de dispersion** : Adaptation flexible des conditions de dispersion et de phase des guides d’ondes.\n\n#### Plate-forme de Spectroscopie\n- **Membrane de guides d’ondes** : Développement d'une plateforme fonctionnant dans la fenêtre de transparence du silicium.\n- **Source externe** : Utilisation d'une source à bande étroite pour élargissement spectral non-linéaire.\n- **Micro fluidique** : Apport de l’analyte proche d’un guide d’ondes à faibles pertes pour maximiser l’interaction lumière-matière.\n\n#### Spectromètre Intégré\n- **Fonctionnalité** : Résolution précise de la réponse en fréquence de la lumière à bande large, utilisant des photodétecteurs Mid-IR à larges bandes externes.\n\n### Piliers du Projet\n1. **Guides d’ondes à membranes de silicium** : Fonctionnement dans la plage de transparence complète du silicium.\n2. **Source à bandes larges** : Comprend une pompe externe à bande étroite et une puce à système spectral non-linéaire.\n3. **Spectromètre d’absorption Mid-IR intégré** : Haute sensibilité des interactions lumière-matière.\n\n### Enjeux et Risques\n- **Avancées substantielles** : Connaissances sur les technologies photoniques Near-IR, Mid-IR et spectroscopie d’absorption à l’échelle d’une puce.\n- **Objectifs ambitieux** : Enjeux à hauts risques avec potentiel de rentabilité et impact sur la connaissance interdisciplinaire.\n\n### Jalons de Développement\n- **Nanostructuration** : Étude de nouvelles stratégies pour un composant photonique passif à bande ultra-large.\n- **Guides d’ondes en silicium** : Optimisation du mode de confinement et de la dispersion pour capteurs Mid-IR.\n- **Intégration pratique** : Développement de stratégies pour capteurs Mid-IR utilisant les installations de fabrication CMOS."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’objectif du projet MIR-Spec est d’exploiter le concept de membrane silicium sous-longueur d’onde pour construire une nouvelle technologie de photonique du silicium qui s'étendant du Near-IR au Mid-IR, en tirant parti des installations de fabrication de la microélectronique pour une production rentable et en grande quantité. Dans ce contexte, les dispositifs passifs à bande ultra-large, les capteurs hautes performances à ondes évanescentes et des sources non linéaires à bandes larges seront développées pour exploiter toute la plage de transparence du silicium.  \nLe projet MIR-Spec exploite les caractéristiques uniques de sous-longueur d’ondes des membranes en silicium pour le développement de circuits à photoniques de silicium et de spectromètres à absorption Near-IR et Mid-IR intégrés :  \n\nAccès à la plage de transparence complète du Si (longueurs d'onde de 1,1 µm à 8 µm), en exploitant des matériaux de haute qualité et les processus de fabrication matures de SOI.  \n\nContraste élevé sans égal permettant des circuits ultra-compacts et une efficacité non linéaire améliorée.  \n\nDegrés de liberté supplémentaires pour l’adaptation des propriétés de propagation de la lumière (dispersion, confinement, etc.)  \n\nNous envisageons via ce projet d’utiliser des guides d’ondes à membranes à sous-longueur d’ondes afin de surmonter le compromis entre grand champ évanescent et faible perte de propagation de manière à surpasser les dispositifs intégrés de pointes existants dans l’état de l’art actuel, générant des sensibilités inférieures à un milliard par rapport aux capteurs d’espace libre.  \n\nL’approche en sous-longueur d’ondes réduit l’indice de réfraction synthétisé du guide d’ondes en délocalisant le mode optique tout en réduisant l’affaiblissement de propagation dû à la rugosité du flanc. En effet, des guides d'ondes avec une sous-longueur d'onde Near-IR avec des modes optiques fortement déconfinés ont démontré expérimentalement une perte de propagation d’environ 2 dB / cm [49], permettant ainsi des sensibilités de l’ordre de 100 ppb [50], surpassant les équivalents actuels. Par ailleurs, le projet explore la réalisation de sources non-linéaires à bandes larges (peignes de fréquence et super continuum) en tirant parti du confinement étroit fourni par le grand contraste d’indice entre le silicium et l’air, ainsi que l’adaptation flexible des conditions de dispersion et d’adaptation de phase des guides d’ondes permis par le travail en sous-longueur d'onde.  \nLa plate-forme de spectroscopie d'absorption envisagée est développée sur une membrane de guides d’ondes disposant de la longueur d’onde du silicium et fonctionnant dans la fenêtre de transparence complète du silicium. Une source externe à bande étroite est utilisée pour permettre un élargissement spectral non-linéaire s’appuyant sur des travaux avancés en ingénierie de la dispersion pour générer la source à bande large. La micro fluidique sur puce à haut débit est utilisée pour apporter l’analyte proche d’un guide d’ondes à faibles pertes prenant en charge un mode largement délocalisé afin de maximiser l’interaction lumière-matière et donc l’absorption caractéristique des molécules cibles.  \nLe spectromètre intégré permet de résoudre avec précision la réponse en fréquence de la lumière à bande large marquée par l’empreinte des composants de l’analyte, utilisant des photodétecteurs Mid-IR à larges bandes externes.  \n\nCe projet repose ainsi sur les trois piliers suivants :  \n\nGuides d’ondes à membranes de silicium à sous-longueur d’onde permettant le fonctionnement dans la plage de transparence complète du silicium via l’élimination du dioxyde de silicium par le bas.  \nSource à bandes larges comprenant une pompe externe à bande étroite et une puce à système spectral non-linéaire en s’appuyant sur une ingénierie de pointe en matière de dispersion par sous-longueur d’onde.  \nMise en œuvre d’un spectromètre d’absorption Mid-IR intégré reposant sur une haute sensibilité des interactions lumière-matière via une puce exploitant l’ingénierie de la sous-longueur d’onde pour maximiser le champ évanescent et minimiser les pertes par propagation.  \n\nChacun de ces piliers constitue une avancée substantielle en termes de connaissances concernant la mise en œuvre de technologies photoniques Near-IR, Mid-IR et de spectroscopie d’absorption à l’échelle d’une puce. Ces objectifs ambitieux en font un enjeu à hauts risques avec un potentiel de rentabilité à l’échelle de la complexité mais également un impact fondamental sur la connaissance interdisciplinaire. Les avancés suivants ces différents axes se concrétisent par le passage de plusieurs jalons nécessitant chacun un haut niveau de recherche et de développement :  \n\nÉtude de nouvelles stratégies de nanostructuration des sous-longueur d'onde pour réaliser un composant photonique passif à bande ultra large couvrant l’intégralité de la plage de transparence du silicium (1,1 µm - 8 µm).  \nDéveloppement de guides d’ondes en silicium nanostructuré afin d’optimiser le mode de confinement et la dispersion du guide d’ondes dans les capteurs de champ évanescents Mid-IR à bandes larges, de source non-linéaire et à hautes performances (peigne de fréquence et supercontinuum).  \nDéveloppement de stratégies d’intégration permettant la mise en œuvre pratique de capteurs Mid-IR tirant parti des installations de fabrication de grandes séries CMOS."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Objectif des travaux de recherche (2022)** : Optimisation et intégration de solutions technologiques avancées en détection et criminalistique.\n  \n- **Cibles des avancées** : \n  - Renforcement de la prévention et des enquêtes sur les incidents terroristes avec explosifs artisanaux.\n  - Détection en temps réel des précurseurs d'explosifs dans l'eau et l'air.\n\n- **Élément clé** : Sélection et personnalisation de capteurs pour quantifier de faibles concentrations de précurseurs d'explosifs dans l'eau.\n\n- **Amélioration visée** : Optimisation des réseaux de surveillance de la qualité de l'eau en Europe.\n\n- **Technologie utilisée** : \n  - Développement autour de la solution MULTISENSE de Mirsense.\n  - Spectroscopie photoacoustique utilisant un laser à cascade quantique (QCL) pour l’analyse des traces de gaz.\n  - Avantages : compacité, sensibilité, rapidité, grande bande passante optique.\n\n- **Potentiel d'intégration** : Instruments répondant à des enjeux environnementaux, de défense et de sûreté.\n\n- **Verrous technologiques rencontrés** :\n  - Manque de capteurs sensibles et fiables à coût raisonnable pour applications aquatiques.\n  - Dispositifs d'analyse coûteux nécessaires pour garantir la qualité de l'eau, entraînant des investissements financiers élevés.\n  \n- **Défis techniques** :\n  - Mesures rapides nécessitant manipulation manuelle (préparation d'échantillons, analyse chimique, stockage), augmentant les risques de contamination.\n  - Limitations des capteurs commerciaux pour l'analyse en ligne : sensibilité aux variations de température et d'humidité, délais de réponse lents.\n  \n- **Exemples de limitations** :\n  - Capteurs électrochimiques : dépendance à la tension d'alimentation, température, humidité, et perte de sensibilité annuelle.\n  - Analyseurs conventionnels d'ammonium et de sulfures : reposent sur des mesures photométriques hors ligne et nécessitent des réactifs chimiques spécifiques.\n\n- **Solutions prometteuses** : Dispositifs optifluidiques basés sur l’interféromètre de Mach-Zehnder pour surveiller la concentration chimique totale, bien qu’ils ne puissent pas identifier des substances spécifiques.\n\n- **But des travaux** : Surpasser les contraintes et limites technologiques en combinant un QCL infrarouge et une cellule photoacoustique pour une détection précise des signatures spectrales des gaz vaporisés dans l'eau."}, {"role": "assistant", "content": "Les travaux de recherche que nous avons menés en 2022 avaient pour objectif l'optimisation et l'intégration d'une série de solutions technologiques de pointe dans le domaine de la détection et de la criminalistique. Ces avancées visent à renforcer la prévention et les enquêtes liées aux incidents terroristes impliquant des explosifs artisanaux, ainsi qu'à permettre la détection en temps réel des précurseurs d'explosifs dans l'eau et dans l'air. L'élément indispensable de cette procédure sera la sélection et la personnalisation des capteurs capables de quantifier de faibles concentrations de précurseurs d'explosifs dans l'eau dans le but d’améliorer les réseaux de surveillance de la qualité de l'eau dans différents pays européens. Nos capteurs sont développés autour de la personnalisation de la solution MULTISENSE de Mirsense, une technologie de spectroscopie photoacoustique basée sur un laser à cascade quantique QCL pour l’analyse des traces de gaz. Cette technologie permet d’allier compacité, sensibilité, rapidité et grande bande passante optique. Pour ces raisons, elle a le potentiel d’être intégrée dans des instruments permettant de répondre à de multiples enjeux liés à l’environnement, la défense et la sureté. Nous avons été confrontés à de nombreux verrous technologiques quant à la réalisation de ces travaux. Ces défis sont liés au manque de capteurs sensibles et fiables à coût raisonnable pour les applications aquatiques. En particulier, la sécurité de la qualité de l'eau fournie nécessite généralement des dispositifs d'analyse et des capteurs spécifiques et coûteux, dont les coûts impliquent des investissements financiers plus importants. En termes de technologie, les mesures rapides exigent souvent une manipulation manuelle, y compris la préparation de l'échantillon, l'analyse chimique par voie humide et le stockage de l'échantillon, ce qui augmente les risques de contamination de l'échantillon. D'autre part, l'utilisation de capteurs disponibles dans le commerce pour l'analyse de l'eau en ligne est limitée par leur plage de fonctionnement en ce qui concerne la température et l'humidité et les délais de réponse causés par des changements brusques des paramètres ambiants. Par exemple, l'application des capteurs électrochimiques dépend fortement de la tension d'alimentation, de la température, de l'humidité et des autres composants du mélange gazeux. En outre, les capteurs électrochimiques qui permettent une concentration automatisée de l'ammonium et des sulfures dans l'eau nécessitent non seulement un étalonnage fréquent pour tenir compte des changements de température, mais présentent également une perte de sensibilité significative par an. En outre, les analyseurs conventionnels d'ammonium et de sulfures reposent sur des mesures photométriques hors ligne et des réactifs chimiques spécifiques. Par ailleurs, les dispositifs optifluidiques basés sur l’interféromètre de Mach-Zehnder qui utilisent les indices des réfractions comme indicateur pour surveiller la concentration chimique totale et la variance, semblent être des solutions prometteuses. Bien qu'ils ne puissent pas identifier des substances spécifiques, ces dispositifs peuvent être utilisés pour indiquer la présence de contaminants dans l'eau, ce qui déclenche des alarmes pour les sociétés de services publics et leur permet d'éviter une deuxième contamination croisée dans le réseau. Nos travaux visent donc à surpasser ces différentes contraintes et limites technologiques, en combinant un QCL infrarouge et une cellule photoacoustique, permettant ainsi la détection précise de la signature spectrale des gaz vaporisés dans un échantillon d’eau."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Électronique Embarquée\n- **Axe 1 : Mise à jour des calculateurs**\n  - Conception d'une solution compatible avec toutes les marques de véhicules.\n  - Verrous identifiés :\n    - Absence de standard de communication partagé par les constructeurs.\n    - Différents modes d'accès aux mises à jour.\n    - Débit et latence de communication des données.\n    - Outils des constructeurs pour la mise à jour et accès aux bases de données.\n\n- **Axe 2 : Protocoles de réparation des calculateurs**\n  - Étude de différentes technologies de calculateurs.\n  - Difficulté principale : diversité des technologies nécessitant des protocoles personnalisés.\n\n#### Management des Fluides\n- **Développement d'une solution d'aspiration et distribution de fluides**\n  - Prototype conçu en 2016, essais révélant des axes d'amélioration.\n  - Verrous rencontrés :\n    - Manipulation de fluides sans mélange pour éviter contamination.\n    - Choix de conception orienté vers un système plus simple et moins coûteux, nécessitant des fonctions manuelles.\n    - Réduction de l'encombrement pour installation dans tous les centres Norauto.\n  \n- **Problématiques en 2018**\n  - Dimensionnement pour centres de plus grande taille avec activité accrue.\n  - Nécessité de dédier des points d'aspiration à un seul type de fluide.\n  - Amélioration du fonctionnement via objets connectés.\n\n#### Diagnostic en Atelier\n- **Conception d'outils de diagnostic d'usure**\n  - Contraintes d'encombrement pour un diagnostic sans démontage des pneus.\n  - Difficultés liées au jeu des têtes de mesure déportées affectant les valeurs.\n\n#### Marqueur de Fluide Frigorigène\n- **Développements et tests en 2018**\n  - Pollution du dispositif de test par dégagement d'huile lors de l'interfaçage.\n  - Rupture mécanique du dispositif sur certains modèles avec pression de fluide élevée.\n\n#### Cale de Pont\n- **Conception d'une cale de pont aimantée**\n  - Inconnues à résoudre :\n    - Force magnétique nécessaire pour utilisation équivalente aux cales traditionnelles.\n    - Résistance mécanique des aimants et choix des matériaux.\n    - Problèmes d'échauffement des aimants lors du moulage du plastique.\n\n#### Banc TPMS\n- **Communication avec valves TPMS**\n  - Verrou principal : communication entre le banc de lecture et les valves TPMS.\n  - Études antérieures sur technologies de valves pour protocoles de communication.\n  \n- **Améliorations en 2018**\n  - Automatisation des recherches d'informations et recueil de données.\n  - Conception d'une infrastructure pour soutenir l'automatisation et assurer l'interopérabilité des données.\n\n#### Borne de Gonflage\n- **Développement d'une borne de gonflage intelligente**\n  - Inconnues technologiques sur pressions à délivrer et processus de gonflage.\n  - Solution informatisée utilisant des capteurs pour données en temps réel.\n  \n- **Défis supplémentaires**\n  - Fonction gonflage pour deux pneus simultanément.\n  - Évaluation des performances pneumatiques des modules de gonflage en configuration double.\n  - Risque de variation de pression délivrée impactant le gonflage des pneumatiques."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nÉlectronique embarquée  \nNous avons étudié deux axes de développement de solutions dédiées à l’électronique embarquée dans les véhicules que nous entretenons au sein du réseau Norauto.  \nLe premier axe d’étude porte sur la conception d’une solution de mise à jour des calculateurs qui soit compatible avec toutes les marques de véhicules du parc automobile. Nous avons été confrontés à de nombreux verrous posés par l’inexistence d’un standard de communication partagé par tous les constructeurs, mais aussi les différents modes d’accès aux mises à jour auprès des constructeurs. Les difficultés se sont notamment posées sur le débit de communication des données lors du transfert des données, mais aussi la latence de celui-ci. Par ailleurs, des aléas ont été posés par les outils proposés par les constructeurs pour la réalisation de l’opération de mise à jour des calculateurs, mais aussi pour l’accès aux bases de données contenant les mises à jour à installer.  \nNotre second travail consiste à étudier différentes technologies de calculateurs afin de développer des protocoles de réparation de ceux-ci. La principale difficulté à laquelle nous avons été confrontés lors de ce travail était la multitude de technologies qui existent et le besoin de développer un protocole personnalisé pour appréhender chaque type de calculateur.  \nManagement des fluides  \nLes réalisations liées au management des fluides mis en jeu dans les opérations de maintenance des véhicules consistaient à développer une solution d’aspiration centralisée et de distribution de fluides.  \nLes travaux réalisés en 2016 nous ont permis de concevoir une première solution prototype, cependant nos essais sur l’installation-pilote nous ont permis de relever de nombreux axes de conception à améliorer.  \nLe principal verrou auquel nous avons été confrontés en 2017 était lié à la manipulation de différents fluides dont il faut éviter tout mélange. En effet, un mélange de ces fluides présente des risques lors de l’interfaçage avec un véhicule du fait de la contamination des circuits mais aussi lors de l’élimination de ces liquides qui sont chacun récupéré et éliminé différemment.  \nUne autre difficulté était liée aux choix de conception du premier prototype. Nous avons décidé d’orienter la conception de la nouvelle version de notre prototype vers un système plus simple à utiliser, plus simple à maintenir et moins coûteux. Nous devions alors dégrader certaines fonctions automatisées en fonctions manuelles pour réduire les coûts tout en gardant les mêmes performances et les mêmes modes de sécurité sur le système, notamment les pompes et les cuves.  \nDe plus, un autre défi était lié à la réduction de l’encombrement du système central pour permettre d’installer la solution dans tous les centres Norauto, quelle que soit leur configuration et la superficie disponible.  \nEn 2018, nous avons été confrontés à de nouvelles problématiques de performances lors du dimensionnement de notre installation pour un centre de plus grande taille avec une activité d’aspiration/distribution de fluides plus soutenue.  \nNous avons notamment été confrontés au besoin de dédier certains points d’aspiration à un seul type de fluide en raison des contaminations inévitables. Nous avons aussi souhaité améliorer le fonctionnement de cette installation via des objets connectés.  \nDiagnostic en atelier  \nLa principale contrainte à l’origine des défis que nous avons rencontrés lors de la conception de nos outils de diagnostic d’usure était liée à l’encombrement réduit dans lequel ces outils doivent passer. En effet, nous souhaitons que les techniciens puissent réaliser leur diagnostic sans nécessiter de démonter les pneus du véhicule contrôlé et que ces outils soient compatibles avec l’ensemble du parc automobile.  \nNous avons par ailleurs été confrontés à des difficultés de jeu au niveau des têtes de mesure déportées qui faussaient les valeurs acquises.  \nMarqueur de fluide frigorigène  \nA la suite de la conception du dispositif en cours d’année 2017, nous avons poursuivi les développements et tests en 2018. Ceux-ci nous ont confrontés à deux difficultés majeures lors de l’expérimentation en conditions réelles.  \nEn effet, nous avons été confrontés à une pollution du dispositif de test en raison d’un dégagement d’huile lors de l’interfaçage du dispositif avec le véhicule. Nous avons également constaté une rupture mécanique du dispositif lors de son utilisation sur certains modèles de véhicules qui présentaient une pression de fluide plus élevée que la pression nominale.  \nCale de pont  \nL’objectif de notre projet était de concevoir une cale de pont aimantée, qui présente la même fonctionnalité et les mêmes caractéristiques que les cales de pont traditionnelles, notamment sa durabilité.  \nLes inconnues que nous avons dû résoudre portaient sur la force magnétique à générer pour permettre une utilisation dans les mêmes conditions que les cales traditionnelles, la résistance mécanique des aimants à apposer au sein de la cale, mais aussi le type d’aimant tenant compte de l’ensemble du cycle de vie de la cale. Nous avons été confrontés à des problématiques d’échauffement des aimants lors du moulage du plastique les entourant, à l’origine d’une montée en température des aimants en néodyme qui éliminait leur force magnétique.  \nBanc TPMS  \nLe principal verrou auquel nous avons été confrontés était lié à la communication du banc de lecture avec les valves TPMS des pneus du véhicule à contrôler. En effet, cette problématique avait déjà été identifiée et rencontrée en 2017, et nous avions tenté de la résoudre en étudiant les différentes technologies de valves TPMS afin de développer des protocoles de communication compatibles. Certaines technologies de valves nécessitent de recevoir un premier stimulus de la part du banc de lecture afin d’initier la connexion, puis l’échange de données de pression des pneumatiques.  \nEn 2018, nous avons décidé d’automatiser un grand nombre de recherches d’informations et d’améliorer nos procédés de recueil de données (détection du véhicule, reconnaissance de la combinaison marque/modèle). Nous avons également conçu une infrastructure capable de soutenir nos initiatives d’automatisation afin de rendre possible la communication entre les antennes de notre banc de lecture et les valves TPMS des véhicules à contrôles. La conception de cette infrastructure a également nécessité la levée d’inconnues sur l’interopérabilité des formats des données manipulées ainsi que sur les processus à développer afin d’atteindre des performances satisfaisantes vis-à-vis du scénario d’utilisation.  \nBorne de gonflage  \nL’étude et développement de notre nouvelle borne de gonflage intelligente nous a confrontés à plusieurs inconnues technologiques portant sur la détermination des pressions à délivrer, mais aussi le gonflage lui-même.  \nNous nous sommes orientés vers une solution très informatisée, utilisant de nombreux capteurs afin de recueillir un maximum de données en temps réel lors de l’approche du véhicule. Ces caractéristiques seront alors croisées avec des bases de données spécialisées permettant de connaitre précisément la pression nominale à délivrer. Le défi est de parvenir à gérer le caractère temps-réel de ces requêtes et à développer une solution fiable.  \nUn autre verrou est également posé par la fonction gonflage que nous souhaitons doubler afin que l’automate puisse prendre en charge les deux pneus de chaque train en même temps. L’inconnue à laquelle nous sommes confrontés porte sur les performances pneumatiques des modules de gonflage s’ils sont réunis sur une même borne. Nos développements auront pour but de valider la faisabilité technique de ce regroupement d’une part, et son éventuel impact sur les performances de gonflage d’autre part. En effet, en doublant le nombre de sorties d’air comprimé, nous pouvons nous attendre à une variation de la pression délivrée par rapport à la pression commandée, qui pourrait résulter en un défaut de gonflage des pneumatiques."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### Électronique embarquée\n- **Axe 1 : Mise à jour des calculateurs**\n  - Conception d'une solution compatible avec toutes les marques de véhicules.\n  - Verrous identifiés :\n    - Absence de standard de communication entre constructeurs.\n    - Modes d'accès variés aux mises à jour.\n    - Débit de communication et latence problématiques lors des transferts de données.\n  - Aléas liés aux outils des constructeurs pour la mise à jour et l'accès aux bases de données.\n  - Solution envisagée : principe USB/IP, avec expérimentations en cours.\n  - Incertitudes à lever :\n    - Limitations de débit USB et latence IP.\n    - Sécurité du réseau dédié.\n\n- **Axe 2 : Véhicule étendu (ExVe)**\n  - Étude exploratoire sur les principes de communication des véhicules.\n  - Objectif : lever des inconnues technologiques et expérimenter des approches concrètes.\n\n#### Management des fluides\n- Défis rencontrés :\n  - Non-contamination croisée des fluides.\n  - Performances requises pour la vitesse d’aspiration.\n  - Compatibilité avec tous les fluides et véhicules.\n- Prototypes développés pour évaluer les performances en conditions réelles.\n\n#### Atelier « augmenté »\n- **Automatisation du montage/démontage des pneus**\n  - Défis :\n    - Variabilité des types de pneus à manipuler.\n    - Solutions existantes adaptées à la grande série, nécessitant une nouvelle conception pour tous types de pneus.\n    - Contraintes d’encombrement du système en atelier.\n    - Conception d'un système sans intervention humaine pénible.\n  - Validation du prototype requise en conditions réelles.\n\n- **Cale de pont**\n  - Objectif : concevoir une cale de pont aimantée avec durabilité équivalente aux cales traditionnelles.\n  - Inconnues à résoudre :\n    - Force magnétique nécessaire.\n    - Résistance mécanique des aimants.\n    - Problématiques d’échauffement des aimants lors du moulage.\n    - Usure élevée des matériaux, nécessitant une itération de conception pour robustesse.\n\n#### Baie de contrôles actifs\n- **Banc TPMS**\n  - Verrou principal : communication entre le banc de lecture et les valves TPMS.\n  - Historique :\n    - Problématique identifiée en 2017, tentatives de développement de protocoles de communication.\n    - Automatisation des recherches d’informations et amélioration des procédés en 2018.\n    - Conception d'une infrastructure pour soutenir l'automatisation et la communication.\n  - En 2019, investigation des défauts de lecture et expérimentation pour évaluer la fiabilité des lectures automatiques.\n\n- **Scan de carrosserie, pneus et châssis**\n  - Difficultés liées aux échanges de données entre systèmes d’acquisition.\n  - Problèmes de communication entre le système de scan des pneus et les caméras.\n  - Nécessité de structurer les données par véhicule et non par série d’acquisition.\n  - Améliorations requises pour les prises de vue en conditions d’atelier.\n\n- **Borne de gonflage**\n  - Inconnues technologiques sur la détermination des pressions à délivrer et le gonflage.\n  - Solution informatisée avec capteurs pour recueillir des données en temps réel.\n  - Défi de gestion des requêtes en temps réel et fiabilité de la solution.\n  - Verrou : gonflage simultané des deux pneus de chaque train, impact potentiel sur performances.\n  - Prototype développé en 2019, confronté aux véhicules réels, nécessitant une gestion manuelle des cas non conformes."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\nÉlectronique embarquée  \nNous avons étudié deux axes de développement de solutions dédiées à l’électronique embarquée dans les véhicules que nous entretenons au sein du réseau Norauto.  \n\nTéléchargement des mises à jour de calculateurs  \nLe premier axe d’étude porte sur la conception d’une solution de mise à jour des calculateurs qui soit compatible avec toutes les marques de véhicules du parc automobile. Nous avons été confrontés à de nombreux verrous posés par l’inexistence d’un standard de communication partagé par tous les constructeurs, mais aussi les différents modes d’accès aux mises à jour auprès des constructeurs. Les difficultés se sont notamment posées sur le débit de communication des données lors du transfert des données, mais aussi la latence de celui-ci. Par ailleurs, des aléas ont été posés par les outils proposés par les constructeurs pour la réalisation de l’opération de mise à jour des calculateurs, mais aussi pour l’accès aux bases de données contenant les mises à jour à installer. Nous avons envisagé une solution basée sur un principe USB/IP que nous avons expérimentée.  \nDe plus, la solution envisagée pose des incertitudes, liées aux limitations de débit USB et à la latence IP, mais aussi à la sécurité du réseau dédié. Nos travaux visent ainsi à lever ces incertitudes et expérimenter des pistes de solution en vue de la conception de notre système.  \n\nVéhicule étendu  \nLe second axe d’étude porte quant à lui sur l’étude exploratoire du nouveau concept de véhicule étendu (ExVe : Extended Vehicle). Cette exploration technologique de cette solution visait à lever des inconnues sur les principes de communication et à expérimenter une première approche concrète de la communication des véhicules.  \n\nManagement des fluides  \nDans le cadre du développement de nos solutions de management des fluides, les principaux défis que nous avons rencontrés découlaient des contraintes de non-contamination croisée des fluides manipulés et des performances à atteindre pour la vitesse d’aspiration et la compatibilité avec tous les fluides et tous les véhicules du parc automobile. Le développement de prototypes nous a permis d’évaluer les performances de nos solutions en conditions réelles.  \n\nAtelier « augmenté »  \nAutomatisation du montage/démontage des pneus  \nLes défis auxquels nous sommes confrontés dans le cadre de cette conception sont principalement liés à la variabilité des types de pneus manipulés par la solution automatisée à concevoir. En effet, les solutions existantes sont destinées à la grande série, qui manipule ainsi la même référence de pneus en grand nombre avant d’être configurée à nouveau pour une utilisation avec un autre type de pneus. Nous devions ainsi concevoir une solution permettant le serrage de pneus de toutes tailles.  \nLa difficulté de conception à adresser était également posée par la contrainte d’encombrement du système qui doit être installé en atelier de réparation.  \nDe plus, nous avons été confrontés à des difficultés dans la conception d’un système capable de réaliser les opérations de démontage et de montage des pneus, sans nécessiter d’intervention humaine qui soit pénible pour le technicien.  \nEnfin, la conception du système devra faire l’objet d’une expérimentation pour valider le prototype en conditions réelles.  \n\nCale de pont  \nL’objectif de notre projet était de concevoir une cale de pont aimantée, qui présente la même fonctionnalité et les mêmes caractéristiques que les cales de pont traditionnelles, notamment sa durabilité.  \nLes inconnues que nous avons dû résoudre portaient sur la force magnétique à générer pour permettre une utilisation dans les mêmes conditions que les cales traditionnelles, la résistance mécanique des aimants à apposer au sein de la cale, mais aussi le type d’aimant tenant compte de l’ensemble du cycle de vie de la cale. Nous avons été confrontés à des problématiques d’échauffement des aimants lors du moulage du plastique les entourant, à l’origine d’une montée en température des aimants en néodyme qui éliminait leur force magnétique.  \nPar ailleurs, la composition des matériaux choisis pour les cales nous avait confrontés à une usure élevée des cales : nous avons ainsi procédé à une nouvelle itération de conception pour améliorer la robustesse des cales, notamment dans le cas où elles seraient accidentellement utilisées à l’envers pour que la feuillure des véhicules ne détériore pas les aimants.  \n\nBaie de contrôles actifs  \nBanc TPMS  \nLe principal verrou auquel nous avons été confrontés était lié à la communication du banc de lecture avec les valves TPMS des pneus du véhicule à contrôler. En effet, cette problématique avait déjà été identifiée et rencontrée en 2017, et nous avions tenté de la résoudre en étudiant les différentes technologies de valves TPMS afin de développer des protocoles de communication compatibles. Certaines technologies de valves nécessitent de recevoir un premier stimulus de la part du banc de lecture afin d’initier la connexion, puis l’échange de données de pression des pneumatiques.  \nEn 2018, nous avons décidé d’automatiser un grand nombre de recherches d’informations et d’améliorer nos procédés de recueil de données (détection du véhicule, reconnaissance de la combinaison marque/modèle). Nous avons également conçu une infrastructure capable de soutenir nos initiatives d’automatisation afin de rendre possible la communication entre les antennes de notre banc de lecture et les valves TPMS des véhicules à contrôles. La conception de cette infrastructure a également nécessité la levée d’inconnues sur l’interopérabilité des formats des données manipulées ainsi que sur les processus à développer afin d’atteindre des performances satisfaisantes vis-à-vis du scénario d’utilisation.  \nEn 2019, nous avons capitalisé sur les développements réalisés autour de l’automatisation de la détermination du protocole de communication à déployer pour échanger avec un type de valves en particulier, toutefois, nous avons été confrontés à de nouvelles difficultés posées par les lectures des données. Nous avons ainsi dû investiguer pour trouver l’origine des défauts de lecture (inversion, absence de lecture, etc.) et avons ainsi mené une nouvelle expérimentation permettant d’évaluer le niveau de fiabilité des lectures automatiques.  \n\nScan de carrosserie, pneus et châssis  \nDans le cas des scans de carrosserie, châssis et pneumatiques, les difficultés rencontrées étaient principalement liées aux échanges de données entre les systèmes d’acquisition. En particulier, la communication entre le système de scan de la bande de roulement des pneus avec les caméras et notre système d’information a été à l’origine de nombreuses difficultés. En outre, nous avons été confrontés au besoin de structurer les données acquises pour permettre une considération par véhicule plutôt que par série d’acquisition : nous devions ainsi piloter l’acquisition progressive des différentes mesures (train avant, train arrière). Enfin, les conditions d’atelier ont également nécessité de réaliser des travaux d’amélioration des prises de vue afin de garantir leur exploitabilité.  \n\nBorne de gonflage  \nL’étude et le développement de notre nouvelle borne de gonflage intelligente nous ont confrontés à plusieurs inconnues technologiques portant sur la détermination des pressions à délivrer, mais aussi le gonflage lui-même.  \nNous nous sommes orientés vers une solution très informatisée, utilisant de nombreux capteurs afin de recueillir un maximum de données en temps réel lors de l’approche du véhicule. Ces caractéristiques seront alors croisées avec des bases de données spécialisées permettant de connaitre précisément la pression nominale à délivrer. Le défi est de parvenir à gérer le caractère temps-réel de ces requêtes et à développer une solution fiable.  \nUn autre verrou est également posé par la fonction gonflage que nous souhaitons doubler afin que l’automate puisse prendre en charge les deux pneus de chaque train en même temps. L’inconnue à laquelle nous sommes confrontés porte sur les performances pneumatiques des modules de gonflage s’ils sont réunis sur une même borne. Nos développements auront pour but de valider la faisabilité technique de ce regroupement d’une part, et son éventuel impact sur les performances de gonflage d’autre part. En effet, en doublant le nombre de sorties d’air comprimé, nous pouvons nous attendre à une variation de la pression délivrée par rapport à la pression commandée, qui pourrait résulter en un défaut de gonflage des pneumatiques.  \nEn 2019, nous avons développé un prototype que nous avons mis en expérimentation. Cette confrontation aux véhicules réels nous a permis de constater que dans certains cas, les utilisateurs n’avaient pas respecté les montes préconisées par le constructeur : nous devions ainsi proposer une solution pour gérer ce type de cas, manuellement."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### Électronique Embarquée\n- **Axes de développement** : Solutions pour l’électronique embarquée dans les véhicules du réseau Norauto.\n  \n- **Téléchargement des mises à jour de calculateurs** :\n  - Conception d'une solution de mise à jour compatible avec toutes les marques de véhicules.\n  - **Verrous identifiés** :\n    - Absence de standard de communication entre constructeurs.\n    - Divers modes d'accès aux mises à jour.\n    - Débit de communication et latence lors du transfert des données.\n    - Outils des constructeurs pour la mise à jour et accès aux bases de données.\n  - **Solution envisagée** : Principe USB/IP expérimenté.\n  - **Incertitudes** :\n    - Limitations de débit USB et latence IP.\n    - Sécurité du réseau dédié.\n  - **Travaux de 2020** : Dissipation des incertitudes sur le protocole de connexion entre le module USB-IP et les systèmes d'information.\n\n- **Véhicule Étendu (ExVe)** :\n  - Étude exploratoire sur la communication des véhicules.\n  - **Travaux de 2020** :\n    - Évaluation de la disponibilité des données via lecture des clés.\n    - Hétérogénéité des données accessibles.\n    - Mise à jour, lecture et écriture sur calculateurs spécifiques.\n    - Développement d'un protocole combinant fonctions électroniques et manipulations physiques.\n\n#### Atelier « Augmenté »\n- **Pied à coulisse connecté** :\n  - Incertitudes technologiques sur le mécanisme de mesure : fiabilité, précision, stabilité, insensibilité à l'environnement métallique et aux températures.\n  - Limitation : impossibilité de blindage électromagnétique en raison de la nécessité de communication avec le système d'information de l'atelier.\n\n- **Baie de contrôles actifs** :\n  - **Banc TPMS** :\n    - Verrou principal : communication entre le banc de lecture et les valves TPMS.\n    - Problématiques identifiées depuis 2017, nécessitant des protocoles de communication adaptés.\n    - Automatisation des recherches d'informations et amélioration des procédés de recueil de données (détection du véhicule, reconnaissance marque/modèle).\n    - Développement d'une infrastructure pour la communication entre le banc de lecture et les valves TPMS.\n    - **Travaux de 2019** : Résolution des difficultés de lecture des données (inversion, absence de lecture).\n    - **Travaux de 2020** :\n      - Développement d'un système pour moduler la puissance de communication des antennes.\n      - Mise à jour de la base de données pour classifier les véhicules par taille, améliorant le temps de réponse.\n\n#### Décontamination à l’Ozone\n- **Objectif** : Développer une solution de décontamination par ozone, évitant l'utilisation de produits nocifs.\n- **Incertitudes scientifiques** :\n  - Méconnaissance des caractéristiques du virus COVID-19 ciblé pour la décontamination.\n  - Restrictions d'accès aux souches de virus pour tests biologiques.\n  - Collaboration avec l'institut Pasteur de Lille pour les expériences.\n- **Plan d'expériences** :\n  - Validation expérimentale des performances de décontamination en variant les paramètres : durée d'exposition, concentration en ozone, température.\n  - Validation des performances des solutions commercialisées dans les conditions d'utilisation envisagées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nÉlectronique embarquée\nNous avons étudié deux axes de développement de solutions dédiées à l’électronique embarquée dans les véhicules que nous entretenons au sein du réseau Norauto.\nTéléchargement des mises à jour de calculateurs\nLe premier axe d’étude porte sur la conception d’une solution de mise à jour des calculateurs qui soit compatible avec toutes les marques de véhicules du parc automobile. Nous avons été confrontés à de nombreux verrous posés par l’inexistence d’un standard de communication partagé par tous les constructeurs, mais aussi les différents modes d’accès aux mises à jour auprès des constructeurs. Les difficultés se sont notamment posées sur le débit de communication des données lors du transfert des données, mais aussi la latence de celui-ci. Par ailleurs, des aléas ont été posés par les outils proposés par les constructeurs pour la réalisation de l’opération de mise à jour des calculateurs, mais aussi pour l’accès aux bases de données contenant les mises à jour à installer. Nous avons envisagé une solution basée sur un principe USB/IP que nous avons expérimentée.\nDe plus, la solution envisagée pose des incertitudes, liées aux limitations de débit USB et à la latence IP, mais aussi à la sécurité du réseau dédié.\nNos travaux visent ainsi à lever ces incertitudes et expérimenter des pistes de solution en vue de la conception de notre système.\nNos travaux de 2020 visaient à dissiper des incertitudes quant au protocole à dérouler en termes de connexion entre le module USB-IP que nous avons conçu et les systèmes d’information de part et d’autre pour que les exigences en termes de temps de latence notamment soient respectées et permettent d’accéder aux services des différents logiciels des constructeurs.\nVéhicule étendu\nLe second axe d’étude porte quant à lui sur l’étude exploratoire du nouveau concept de véhicule étendu (ExVe : Extended Vehicle). Cette exploration technologique de cette solution visait à lever des inconnues sur les principes de communication et à expérimenter une première approche concrète de la communication des véhicules.\nLes travaux que nous avons menés en 2020 visaient à poursuivre notre exploration technologique des communications électroniques en multimarques. En effet, nous avons premièrement établi un tour d’horizon de la disponibilité des données via la lecture des clés. Nous avons pu constater une grande hétérogénéité au niveau des données accessibles.\nEn parallèle de ce travail, nous avons également étudié expérimentalement la mise à jour, la lecture et l’écriture sur des calculateurs particuliers. Nous avons ainsi dû lever des verrous liés au blocage constructeur et avons dû développer un protocole mêlant fonctions électroniques et manipulations physiques sur la carte électronique pour parvenir à lire le contenu de certains calculateurs.\nAtelier « augmenté »\nPied à coulisse connecté\nCette nouvelle phase de conception du pied à coulisse connecté nous a confrontés à des incertitudes technologiques quant au mécanisme de mesure qui soit fiable, précis, stable et surtout insensible à l’environnement essentiellement métallique dans lequel il sera amené à être utilisé ainsi qu’aux températures. Toutefois, nous ne pouvons pas réaliser de blindage électromagnétique autour de l’ensemble des composants du pied à coulisse car celui-ci doit communiquer avec le système d’informations de l’atelier (pas Bluetooth ou par Wi-Fi).\nEn outre, l’encombrement réduit dans lequel l’ensemble des fonctions doivent être assurées nous confronte à de nombreux aléas quant à la conception mécanique du système.\nBaie de contrôles actifs\nBanc TPMS\nLe principal verrou auquel nous avons été confrontés était lié à la communication du banc de lecture avec les valves TPMS des pneus du véhicule à contrôler. En effet, cette problématique avait déjà été identifiée et rencontrée en 2017, et nous avions tenté de la résoudre en étudiant les différentes technologies de valves TPMS afin de développer des protocoles de communication compatibles. Certaines technologies de valves nécessitent de recevoir un premier stimulus de la part du banc de lecture afin d’initier la connexion, puis l’échange de données de pression des pneumatiques.\nEn 2018, nous avons décidé d’automatiser un grand nombre de recherches d’informations et d’améliorer nos procédés de recueil de données (détection du véhicule, reconnaissance de la combinaison marque/modèle). Nous avons également conçu une infrastructure capable de soutenir nos initiatives d’automatisation afin de rendre possible la communication entre les antennes de notre banc de lecture et les valves TPMS des véhicules à contrôles. La conception de cette infrastructure a également nécessité la levée d’inconnues sur l’interopérabilité des formats des données manipulées ainsi que sur les processus à développer afin d’atteindre des performances satisfaisantes vis-à-vis du scénario d’utilisation.\nEn 2019, nous avons capitalisé sur les développements réalisés autour de l’automatisation de la détermination du protocole de communication à déployer pour échanger avec un type de valves en particulier, toutefois, nous avons été confrontés à de nouvelles difficultés posées par les lectures des données. Nous avons ainsi dû investiguer pour trouver l’origine des défauts de lecture (inversion, absence de lecture, etc.) et avons ainsi mené une nouvelle expérimentation permettant d’évaluer le niveau de fiabilité des lectures automatiques.\nEn 2020, nous avons surmonté le verrou posé par les puissances de communication avec les différents véhicules en agissant en parallèle sur deux axes : d’une part, le développement d’un nouveau système permettant de moduler la puissance de communication des antennes afin de réveiller les valves TPMS sur tous les véhicules du parc roulant et d’autre part, la mise à jour de la base de données pour intégrer une classification des véhicules selon leur taille (petit/moyen/grand) permettant d’activer le bon module de puissance sur l’antenne. Dans un souci d’amélioration du temps de réponse de cette requête, nous avons intégré le K-type des véhicules dans la base de données permettant d’obtenir une correspondance plus ciblée entre une plaque d’immatriculation scannée et le scénario de communication à mettre en œuvre.\nDécontamination à l’ozone\nL’objectif de nos travaux était d’étudier la faisabilité du développement d’une solution de décontamination de surfaces et d’espaces par l’ozone pour éviter l’utilisation de tout produit nettoyant nocif pour la santé et pour l’environnement et pas toujours compatible avec la nature des éléments à nettoyer. L’incertitude scientifique à laquelle nous avons été confrontés était notamment la méconnaissance des caractéristiques du virus COVID-19 que nous souhaitions particulièrement cibler dans le cadre des décontaminations pour assurer un environnement sain et sécurisé pour nos techniciens d’atelier ainsi que pour nos clients.\nEn raison de l’urgence sanitaire publique, les souches de virus étaient exclusivement réservées aux projets de R&D à des fins de développement d’un médicament ou d’un vaccin contre le virus ciblé. Nous n’avons ainsi pas pu acquérir de souches pour mener des tests biologiques nous-mêmes. Nous avons ainsi décidé de collaborer avec l’institut Pasteur de Lille pour la conduite de ces expériences.\nNous avons défini un plan d’expériences avec divers scénarios visant à valider expérimentalement les performances en termes de décontamination en faisant varier les paramètres expérimentaux disponibles : durée d’exposition, concentration en ozone, température, etc.\nNous avons en outre souhaité valider expérimentalement les performances avancées par des solutions commercialisées dans les conditions d’utilisation envisagées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Électronique Embarquée\n- **Axes de développement :** Solutions dédiées à l’électronique embarquée pour véhicules au sein du réseau Norauto.\n  \n- **Téléchargement des mises à jour de calculateurs :**\n  - **Objectif :** Concevoir une solution de mise à jour compatible avec toutes les marques de véhicules.\n  - **Verrous identifiés :**\n    - Absence de standard de communication partagé entre constructeurs.\n    - Divers modes d’accès aux mises à jour des constructeurs.\n    - Débit de communication et latence lors du transfert de données.\n    - Outils des constructeurs pour la mise à jour et accès aux bases de données.\n  - **Solution envisagée :** Principe USB/IP expérimenté.\n  - **Incertitudes :**\n    - Limitations de débit USB et latence IP.\n    - Sécurité du réseau dédié.\n  - **Travaux de 2020 :** Dissipation des incertitudes sur le protocole de connexion entre module USB-IP et systèmes d’information.\n  - **Travaux de 2021 :** Choix technologiques pour améliorer la latence et étude de la virtualisation des fonctions.\n\n#### Véhicule Étendu (ExVe)\n- **Objectif :** Étude exploratoire sur le concept de véhicule étendu pour lever les inconnues sur les principes de communication.\n- **Travaux de 2020 :**\n  - Tour d’horizon de la disponibilité des données via la lecture des clés, constatation d’hétérogénéité des données accessibles.\n  - Étude expérimentale de la mise à jour, lecture et écriture sur calculateurs particuliers.\n  - Développement d’un protocole combinant fonctions électroniques et manipulations physiques pour lire certains calculateurs.\n- **Travaux de 2021 :** Collaboration envisagée avec un équipementier pour le remplacement de calculateurs impossibles à cloner.\n\n#### Atelier « Augmenté » : Pied à Coulisse Connecté\n- **Objectif :** Conception d’un pied à coulisse connecté.\n- **Incertitudes technologiques :**\n  - Mécanisme de mesure fiable, précis, stable et insensible à l’environnement métallique et aux températures.\n  - Impossibilité de réaliser un blindage électromagnétique complet.\n- **Aléas :** Encombrement réduit des fonctions à concevoir.\n- **Travaux de 2021 :** Résolution de problèmes d’instabilité dans la précision de mesure par des développements itératifs.\n\n#### Banc TPMS\n- **Verrou principal :** Communication entre le banc de lecture et les valves TPMS des pneus.\n- **Historique des travaux :**\n  - **2017 :** Identification de la problématique et étude des technologies de valves TPMS pour développer des protocoles de communication.\n  - **2018 :** Automatisation des recherches d’informations et amélioration des procédés de recueil de données.\n    - Conception d’une infrastructure pour soutenir l’automatisation et communication entre le banc de lecture et les valves.\n    - Levée d’inconnues sur l’interopérabilité des formats de données.\n  - **2019 :** Investigation des défauts de lecture (inversion, absence de lecture) et expérimentation pour évaluer la fiabilité des lectures automatiques.\n  - **2020 :** Surmontée des verrous de communication en développant un système modulant la puissance des antennes et mise à jour de la base de données pour classifier les véhicules.\n  - **2021 :** Mise en expérimentation des développements en conditions réelles pour envisager le déploiement dans les ateliers."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nÉlectronique embarquée\nNous avons étudié deux axes de développement de solutions dédiées à l’électronique embarquée dans les véhicules que nous entretenons au sein du réseau Norauto.\nTéléchargement des mises à jour de calculateurs\nLe premier axe d’étude porte sur la conception d’une solution de mise à jour des calculateurs qui soit compatible avec toutes les marques de véhicules du parc automobile. Nous avons été confrontés à de nombreux verrous posés par l’inexistence d’un standard de communication partagé par tous les constructeurs, mais aussi les différents modes d’accès aux mises à jour auprès des constructeurs. Les difficultés se sont notamment posées sur le débit de communication des données lors du transfert des données, mais aussi la latence de celui-ci. Par ailleurs, des aléas ont été posés par les outils proposés par les constructeurs pour la réalisation de l’opération de mise à jour des calculateurs, mais aussi pour l’accès aux bases de données contenant les mises à jour à installer. Nous avons envisagé une solution basée sur un principe USB/IP que nous avons expérimentée.\nDe plus, la solution envisagée pose des incertitudes, liées aux limitations de débit USB et à la latence IP, mais aussi à la sécurité du réseau dédié.\nNos travaux visent ainsi à lever ces incertitudes et expérimenter des pistes de solution en vue de la conception de notre système.\nNos travaux de 2020 visaient à dissiper des incertitudes quant au protocole à dérouler en termes de connexion entre le module USB-IP que nous avons conçu et les systèmes d’information de part et d’autre pour que les exigences en termes de temps de latence notamment soient respectées et permettent d’accéder aux services des différents logiciels des constructeurs.\nEn 2021, nous avons poursuivi nos développements de la solution en arbitrant les choix technologiques à réaliser en termes de connectivité pour adresser le verrou de latence posé par les infrastructures sécurisées de notre système d’information. Nous avons en outre étudié l’intérêt de la virtualisation de certaines fonctions pour gagner en temps de réponse.\nVéhicule étendu\nLe second axe d’étude porte quant à lui sur l’étude exploratoire du nouveau concept de véhicule étendu (ExVe : Extended Vehicle). Cette exploration technologique de cette solution visait à lever des inconnues sur les principes de communication et à expérimenter une première approche concrète de la communication des véhicules.\nLes travaux que nous avons menés en 2020 visaient à poursuivre notre exploration technologique des communications électroniques en multimarques. En effet, nous avons premièrement établi un tour d’horizon de la disponibilité des données via la lecture des clés. Nous avons pu constater une grande hétérogénéité au niveau des données accessibles.\nEn parallèle de ce travail, nous avons également étudié expérimentalement la mise à jour, la lecture et l’écriture sur des calculateurs particuliers. Nous avons ainsi dû lever des verrous liés au blocage constructeur et avons dû développer un protocole mêlant fonctions électroniques et manipulations physiques sur la carte électronique pour parvenir à lire le contenu de certains calculateurs. En 2021, nous nous sommes intéressés aux calculateurs dont le clonage est impossible en envisageant une collaboration avec un équipementier pour faciliter le remplacement de ces calculateurs via des outils d’origine.\nAtelier « augmenté » : Pied à coulisse connecté\nCette nouvelle phase de conception du pied à coulisse connecté nous a confrontés à des incertitudes technologiques quant au mécanisme de mesure qui soit fiable, précis, stable et surtout insensible à l’environnement essentiellement métallique dans lequel il sera amené à être utilisé ainsi qu’aux températures. Toutefois, nous ne pouvons pas réaliser de blindage électromagnétique autour de l’ensemble des composants du pied à coulisse car celui-ci doit communiquer avec le système d’informations de l’atelier (pas Bluetooth ou par Wi-Fi).\nEn outre, l’encombrement réduit dans lequel l’ensemble des fonctions doivent être assurées nous confronte à de nombreux aléas quant à la conception mécanique du système.\nEn 2021, nous avons particulièrement été confrontés à un problème d’instabilité dans la précision de mesure. Nos travaux visaient ainsi l’exploration de l’origine de ce problème et sa résolution à travers des développements itératifs.\nBanc TPMS\nLe principal verrou auquel nous avons été confrontés était lié à la communication du banc de lecture avec les valves TPMS des pneus du véhicule à contrôler. En effet, cette problématique avait déjà été identifiée et rencontrée en 2017, et nous avions tenté de la résoudre en étudiant les différentes technologies de valves TPMS afin de développer des protocoles de communication compatibles. Certaines technologies de valves nécessitent de recevoir un premier stimulus de la part du banc de lecture afin d’initier la connexion, puis l’échange de données de pression des pneumatiques.\nEn 2018, nous avons décidé d’automatiser un grand nombre de recherches d’informations et d’améliorer nos procédés de recueil de données (détection du véhicule, reconnaissance de la combinaison marque/modèle). Nous avons également conçu une infrastructure capable de soutenir nos initiatives d’automatisation afin de rendre possible la communication entre les antennes de notre banc de lecture et les valves TPMS des véhicules à contrôles. La conception de cette infrastructure a également nécessité la levée d’inconnues sur l’interopérabilité des formats des données manipulées ainsi que sur les processus à développer afin d’atteindre des performances satisfaisantes vis-à-vis du scénario d’utilisation.\nEn 2019, nous avons capitalisé sur les développements réalisés autour de l’automatisation de la détermination du protocole de communication à déployer pour échanger avec un type de valves en particulier, toutefois, nous avons été confrontés à de nouvelles difficultés posées par les lectures des données. Nous avons ainsi dû investiguer pour trouver l’origine des défauts de lecture (inversion, absence de lecture, etc.) et avons ainsi mené une nouvelle expérimentation permettant d’évaluer le niveau de fiabilité des lectures automatiques.\nEn 2020, nous avons surmonté le verrou posé par les puissances de communication avec les différents véhicules en agissant en parallèle sur deux axes : d’une part, le développement d’un nouveau système permettant de moduler la puissance de communication des antennes afin de réveiller les valves TPMS sur tous les véhicules du parc roulant et d’autre part, la mise à jour de la base de données pour intégrer une classification des véhicules selon leur taille (petit/moyen/grand) permettant d’activer le bon module de puissance sur l’antenne. Dans un souci d’amélioration du temps de réponse de cette requête, nous avons intégrer le K-type des véhicules dans la base de données permettant d’obtenir une correspondance plus ciblée entre une plaque d’immatriculation scannée et le scénario de communication à mettre en œuvre. \nEn 2021, nous avons été confrontés à la mise en expérimentation de nos développements en conditions réelles afin d’envisager un déploiement de la solution dans les ateliers de notre réseau."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Mise en Réseau d’Équipements d’Atelier\n- **Difficultés rencontrées :**\n  - Spécificité des langages de communication des différents fabricants.\n  - Hétérogénéité du parc d'équipements.\n  - Mise en réseau de 380 centres Norauto.\n- **Problèmes spécifiques :**\n  - Chaque fabricant utilise un langage et un format de données distincts, rendant la récupération de fichiers complexe.\n  - Nécessité d'évoluer le DMS (Dealer Management System) pour intégrer les équipements et mutualiser les fichiers.\n- **Expérimentation :**\n  - Besoin d'un terrain d'expérimentation pour tester le dimensionnement de l'outil informatique pour la collecte de données.\n\n#### Calculateurs – Électronique Embarquée\n- **Difficultés principales :**\n  - Multiplicité des technologies chez les différents constructeurs.\n  - Aléas sur la copie et la manipulation des données dans les zones mémoires des calculateurs.\n  - Accès limité aux données des calculateurs, avec documentation technique souvent indisponible.\n- **Problèmes de mise à jour :**\n  - Fermeture des constructeurs à l'idée de télécharger des mises à jour depuis leurs sites.\n  - Identification des problèmes de téléchargement et d'installation des mises à jour sur les calculateurs.\n\n#### Management des Fluides\n- **Objectif :**\n  - Développer une solution d’aspiration centralisée et de distribution de fluides pour la maintenance des véhicules.\n- **Prototypage :**\n  - Prototype initial conçu en 2016, avec de nombreux axes d'amélioration identifiés lors des essais sur l'installation-pilote.\n- **Verrous rencontrés :**\n  - Manipulation de différents fluides sans mélange, pour éviter contamination et complications d'élimination.\n  - Choix de conception orienté vers un système plus simple, nécessitant une dégradation de certaines fonctions automatisées en manuelles pour réduire les coûts.\n  - Réduction de l’encombrement du système central pour installation dans tous les centres Norauto.\n- **Essais futurs :**\n  - Réalisation d'essais sur la nouvelle installation-pilote pour vérifier la performance par rapport au prototype initial.\n\n#### Diagnostic en Atelier\n- **Plateforme d’assistance technique interne :**\n  - Difficulté dans la structuration de la base de données.\n- **Verrou de conception :**\n  - Trouver un compromis entre une structure générique pour englober tous les cas de pannes et une structure précise pour éviter la confusion entre pannes similaires.\n- **Alimentation de la base de données :**\n  - Difficultés dans la caractérisation des cas de pannes, essentielle pour la qualité des données.\n  - Problèmes liés à la manipulation du langage naturel pour définir les symptômes, avec risque de doublons dus à des synonymes ou à un langage trop spécifique."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nMise en réseau d’équipements d’atelier\nLa mise en réseau des équipements des ateliers Norauto a comporté de nombreuses difficultés, liées à trois aspects : la nature des équipements, l’hétérogénéité du parc et enfin la mise en réseau de 380 centres.\n\nLa principale difficulté était liée à la spécificité de chaque constructeur pour le langage de communication de ses équipements, voire l’absence de communication initialement prévue sur l’équipement. En effet, les trois fabricants avec qui nous avons collaboré disposaient chacun d’un langage spécifique et d’un format de données distinct. La difficulté réside dans la récupération de fichiers de la part de chacun de ces équipements.\nPar ailleurs, nous avons été confrontés à la nécessité de faire évoluer notre DMS (Dealer Management System) – le système d’information de tous les ateliers Norauto afin d’y intégrer les différents équipements et permettre la mise en commun des fichiers manipulés par les différents équipements.\nEnfin, pour tester nos développements, nous avons eu besoin d’un terrain d’expérimentation pour évaluer le dimensionnement de l’outil informatique à mettre en œuvre pour recueillir les remontées de données issues des équipements installés dans 380 centres Norauto. Nous avons alors envisagé une expérimentation sur différents sites Norauto.\nCalculateurs – Électronique embarquée\nLa principale difficulté que nos travaux visaient à adresser portait sur la multitude de technologies qui existent chez les différents constructeurs.\nDu point de vue des technologies de calculateurs, nous rencontrons de nombreux aléas sur la copie et la manipulation de données présentes dans les zones mémoires de ces composants. En effet, les équipementiers sont plus ou moins ouverts au partage des données présentes sur les calculateurs et l’accès à celles-ci est parfois ardu. De plus, les sites des constructeurs ne mettent pas tous à disposition la documentation technique pour adresser ces difficultés.\nEnfin, nous avons également constaté que les constructeurs automobiles restent très fermés à l’idée des téléchargements des mises à jour des calculateurs depuis leurs sites. Notre travail consiste aujourd’hui à identifier les problèmes rencontrés lors du téléchargement de ces mises à jour et lors de leur installation sur les calculateurs (notamment concernant les outils nécessaires).\nManagement des fluides\nLes réalisations liées au management des fluides mis en jeu dans les opérations de maintenance des véhicules consistaient à développer une solution d’aspiration centralisée et de distribution de fluides.\nLes travaux réalisés en 2016 nous ont permis de concevoir une première solution prototype, cependant nos essais sur l’installation-pilote nous ont permis de relever de nombreux axes de conception à améliorer.\n\nLe principal verrou auquel nous avons été confrontés au cours de nos développements était lié à la manipulation de différents fluides dont il faut éviter tout mélange. En effet, un mélange de ces fluides présente des risques lors de l’interfaçage avec un véhicule du fait de la contamination des circuits mais aussi lors de l’élimination de ces liquides qui sont chacun récupéré et éliminé différemment.\n\nUne autre difficulté était liée aux choix de conception du premier prototype. Nous avons décidé d’orienter la conception de la nouvelle version de notre prototype vers un système plus simple à utiliser, plus simple à maintenir et moins coûteux. Nous devions alors dégrader certaines fonctions automatisées en fonctions manuelles pour réduire les coûts tout en gardant les mêmes performances et les mêmes modes de sécurité sur le système, notamment les pompes et les cuves.\n\nEnfin, un autre défi était lié à la réduction de l’encombrement du système central pour permettre d’installer la solution dans tous les centres Norauto, quelle que soit leur configuration et la superficie disponible.\n\nDes essais seront réalisés sur la nouvelle installation-pilote afin de nous assurer de la non-dégradation des performances mesurées avec le prototype initial.\nDiagnostic en atelier\nPlateforme d’assistance technique interne\nLa première difficulté était liée à la structuration de la base de données sous-jacente.\nLe principal verrou à adresser lors de la conception de cette base de données se trouvait dans la recherche d’un compromis entre une structure suffisamment générique pour pouvoir englober tous les cas de pannes et une structure suffisamment précise pour que deux pannes présentant des similitudes ne soient pas considérées comme identiques.\nDe plus, l’alimentation de cette base de données a été à l’origine de nombreuses difficultés. La caractérisation des cas de pannes était le point central pour assurer une qualité des données contenues dans la base de cas. Nous avons alors été confrontés à la difficulté de la manipulation du langage naturel pour définir les symptômes d’une panne. Nous ne souhaitions en effet pas avoir de doublons de cas qui seraient liés à l’utilisation de synonymes ou encore qu’un cas de panne saisi soit inexploitable en raison d’un langage trop spécifique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Verrous et Incertitudes\n\n- **Innovation Principale**: \n  - Traitement des eaux par une approche biologique utilisant des champignons, contrairement à l'état de l'art qui utilise principalement des bactéries.\n\n- **État de la Recherche**: \n  - Les systèmes de traitement des eaux par champignons sont peu documentés, principalement dans des publications scientifiques.\n  - Très peu de références sur des prototypes existants.\n  - Application de cette technologie à des effluents d'atelier automobile est totalement inédite.\n\n- **Complexité des Effluents**:\n  - Composition des effluents de Norauto est complexe et difficile à identifier.\n  - Dépend des produits chimiques utilisés pour les lavages et les prestations d’entretien mécanique.\n  - Concentration des produits varie selon le mode de travail des mécaniciens et le volume de prestations réalisées.\n\n- **Défi Scientifique**:\n  - Développement d'une solution efficace et standardisée pour tous les ateliers représente un véritable défi.\n\n- **Économie Circulaire**:\n  - La solution doit être conçue exclusivement à partir de matières recyclées.\n  - Installation du système ne doit pas nécessiter de travaux chez les clients."}, {"role": "assistant", "content": "Aléas, verrous et incertitudes  \nLa principale innovation apportée par notre projet réside dans le traitement des eaux par une approche biologique : Norauto souhaite utiliser des champignons pour traiter les eaux alors que dans l'état de l'art, ce sont principalement des bactéries qui sont utilisées.  \nDe plus, les systèmes de traitement des eaux par champignons ont uniquement été décrits dans des publications scientifiques : même en tant que prototype, très peu de références évoquent ces travaux et l'application de cette solution à des effluents de type atelier automobile est totalement inédit. En effet, la composition des effluents de Norauto est complexe et difficile à identifier puisque sa composition dépend des produits chimiques utilisés pour les lavages et les prestations d’entretien mécanique réalisées. La concentration relative des différents produits dépend en outre de la façon de travailler du mécanicien et du volume de prestations réalisées au sein du centre étudié. Développer une solution efficace standard pour tous nos ateliers est un véritable défi scientifique.  \n\nUne autre difficulté du projet est de créer une solution liée à l'économie circulaire : le système à concevoir doit exclusivement être réalisé en matières recyclées et son installation ne doit pas nécessiter de travaux chez les futurs clients."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Verrous et Incertitudes\n\n- **Innovation Principale** : Utilisation de champignons pour le traitement des eaux, contrairement à l'état de l'art qui privilégie les bactéries.\n  \n- **État de la Recherche** : \n  - Systèmes de traitement des eaux par champignons peu documentés dans la littérature scientifique.\n  - Très peu de références sur des prototypes existants.\n\n- **Application Inédite** : \n  - Première application des champignons pour traiter les effluents des ateliers automobiles, en particulier ceux de Norauto.\n  \n- **Complexité des Effluents** : \n  - Composition des effluents variable et difficile à identifier, dépendant des produits chimiques utilisés pour les lavages et les prestations d’entretien.\n  - Concentration des produits influencée par les méthodes de travail des mécaniciens et le volume des prestations.\n\n- **Défi Scientifique** : \n  - Nécessité de développer une solution efficace et standardisée pour tous les ateliers, ce qui représente un défi considérable.\n\n- **Économie Circulaire** : \n  - La solution doit être conçue exclusivement à partir de matières recyclées.\n  - Installation du système ne doit pas nécessiter de travaux majeurs dans les ateliers."}, {"role": "assistant", "content": "Aléas, verrous et incertitudes  \nLa principale innovation apportée par notre projet réside dans l’approche biologique du traitement des eaux : Nous souhaitons utiliser des champignons pour cet effet, alors que dans l'état de l'art, ce sont principalement des bactéries qui sont utilisées.\n\nDe plus, bien que les systèmes de traitement des eaux par champignons aient été principalement décrits dans des publications scientifiques (même en tant que prototype, très peu de références évoquent ces travaux), leur application aux effluents des ateliers automobiles est totalement inédite. En effet, la composition des effluents de Norauto est complexe et difficile à identifier puisque sa composition dépend des produits chimiques utilisés pour les lavages et les prestations d’entretien mécanique réalisées au moment considéré. La concentration relative des différents produits dépend en outre de la façon de travailler du mécanicien et du volume de prestations réalisées au sein du centre étudié. Par conséquent, développer une solution efficace standard pour tous nos ateliers est un véritable défi scientifique.\n\nUne autre difficulté du projet réside dans la création d'une solution liée à l'économie circulaire : le système à concevoir doit être exclusivement réalisé à partir de matières recyclées, et son installation ne doit pas nécessiter de travaux majeurs dans les ateliers souhaitant s’y équiper."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques et verrous technologiques\n\n- **Orientation des travaux 2019** : Choix d'axes de recherche en réponse aux défis identifiés.\n\n- **Cardinalité de l’abonnement** :\n  - Modèle d'abonnement basé sur la division nécessite que chaque joueur soit abonné à de nombreuses cellules, pouvant atteindre plusieurs milliers.\n  - Les déplacements des joueurs entraînent des mises à jour fréquentes des abonnements, générant un volume élevé de demandes d'abonnements et de désabonnements.\n  - Bien que les cellules soient regroupées dans des PubSubArea (PSA) pour réduire le nombre de demandes, chaque joueur interagit avec environ une centaine de PSA.\n  - Problème de mémoire : chaque joueur doit conserver un historique des messages d'intérêt public en contact avec lui.\n  - Chaque PSA est lié à tous les joueurs abonnés, ce qui complique la gestion des abonnements.\n\n- **Constructions multiples** :\n  - Pour permettre la visibilité des joueurs dans les bâtiments, ceux-ci doivent s'abonner aux cellules de ces constructions.\n  - Un joueur ne publie ses mouvements que dans la construction à laquelle il est rattaché.\n  - Dans des zones avec de nombreuses constructions (ex : villes), cela entraîne un nombre élevé d'abonnements supplémentaires.\n  - Les mouvements des constructions ne sont publiés que dans la cellule racine, ce qui nuit à l'évolutivité.\n  - Les joueurs abonnés à une construction reçoivent tous ses mouvements (ConstructUpdate), ce qui contredit les gains de mise à jour à distance via PlayerUpdate.\n\n- **Flux des publications** :\n  - Chaque joueur envoie uniquement sa position, mais reçoit la position de tous les joueurs visibles.\n  - Nécessité de contrôler soigneusement le trafic sortant.\n  - Les PSA doivent publier des événements de manière optimale, considérant chaque message comme un message diffusé à plusieurs destinataires.\n  - Les messages doivent être transformés en messages unicast pour chaque joueur avec leur propre adresse IP.\n  - Transformation des messages doit être effectuée le plus tard possible, idéalement dans le serveur le plus proche du client.\n  - Actuellement, les processus du nœud étendent la publication, ce qui exerce une pression sur l'interface réseau sortante.\n  - Les processus front sont mieux adaptés à gérer un trafic sortant important."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques et verrous technologiques\nAux vues de ces éléments, nous avons choisi d’orienter nos travaux de l’année 2019 sur les axes suivants :\n\nCardinalité de l’abonnement :\n\nEn raison du modèle d’abonnement basée sur la division, un joueur doit être abonnée à de nombreuses cellules pouvant ainsi atteindre plusieurs milliers. De plus, lorsque celui-ci se déplace, la division doit être mise à jour générant une grande quantité de demandes d’abonnements et de désabonnements. Même si les cellules ont été regroupées dans PubSubArea (PSA) afin de réduire le nombre de demandes d’abonnements, chaque joueur doit tout de même interagir avec une centaine de PSA. Se pose ainsi la problématique de la mémoire. En effet, chaque joueur doit conserver un historique de l’ensemble des messages considérés d’intérêt public avec lequel il est en contact. De plus, chaque PSA est lié et suit chacun des joueurs abonnés à celui-ci.\n\nConstructions multiples :\n\nAfin de permettre la visibilité des joueurs se trouvant à l’intérieur de bâtiments par les autres joueurs, ceux-ci doivent s’abonner aux cellules de cette construction. En effet, un joueur ne publie ses mouvements que dans la construction à laquelle il est rattaché. Néanmoins, lorsque le joueur se trouve dans une zone constituée de nombreuses constructions telle une ville, cela génère une grande quantité d’abonnements supplémentaires. De plus, pour voir une construction bouger, les joueurs doivent y être abonnés, le mouvement des constructions n’étant publié que dans la cellule racine de celle-ci, influant ainsi négativement sur l’évolutivité. De ce fait, tout joueur voyant une construction recevra tous ses mouvements (ConstructUpdate). Cependant, cela va à l’encontre des gains obtenus via le PlayerUpdate qui permets lier le nombre de mise à jour à distance du joueur.\n\nFlux des publications :\n\nChaque joueur envoie uniquement sa position mais reçoit la position de tous les joueurs qu’il voit. Cela signifie que le trafic sortant doit être soigneusement contrôlé. Lorsque le PSA publie des évènements à ses abonnés, il doit le faire de la manière qui lui convient le mieux. Une publication issue du PSA doit être considérée comme un message diffusé avec plusieurs destinataires. Cependant, ce message doit s’adresser à des joueurs distincts avec leur propre adresse IP. A ces fins, le message diffusé doit se transformer en plusieurs messages unicast. Cette transformation doit cependant être effectuée le plus tard possible dans le processus, idéalement dans le processus serveur le plus proche du client du joueur. Jusqu’à présent, ce sont les processus du nœud qui étendent la publication, exerçant ainsi une pression du réseau sur l’interface réseau sortante du nœud. Les processus front sont mieux équipés avec un trafic de sortie important."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Gestion des données Voxel :**\n  - Utilisation de MongoDB sur Amazon Web Services pour le stockage.\n  - Problèmes de fiabilité et de performance identifiés lors de la mise en production.\n  - Bug dans la remontée des informations de charge serveur entraînant une saturation prématurée du stockage.\n\n- **Algorithme de répartition des joueurs sur différents nœuds :**\n  - Architecture distribuée nécessitant une maximisation de l'utilisation des serveurs.\n  - Limitation par la capacité individuelle de chaque serveur.\n  - Difficulté à simuler le déplacement réaliste des joueurs.\n  - Règles de distribution géométrique jugées insuffisantes pour modéliser le comportement des joueurs.\n\n- **Design et scalabilité des industries :**\n  - Approche basée sur la matérialisation des entrées et sorties via des conteneurs de stockage.\n  - Modélisation des processus industriels par des abonnements et désabonnements selon l'état des conteneurs (plein ou vide).\n  - Trafic élevé d'abonnements et désabonnements générant une forte charge sur le CPU.\n  - Nécessité d'un état des lieux des conteneurs à chaque démarrage du serveur."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes incertitudes liées à cette opération de R&D sont présentées sous forme de différents axes :  \nGestion des données Voxel :  \nLe système de stockage se basait principalement sur la solution MongoDB dans un environnement Amazon Web Services. Ce choix technologique a causé une problématique au cours de la mise en production, mettant en lumière un certain nombre de problèmes de fiabilité et de performance. En effet, un bug dans la remontée des informations concernant la charge serveur générait une saturation prématurée du stockage.  \nAlgorithme de répartition des joueurs sur différents nœuds :  \nL’architecture utilisant un modèle distribué nécessite de maximiser l’utilisation de chaque serveur mais cela est limité par la capacité de chacun d’entre eux. L’approche proposée s’est basée sur des règles de répartition spécifiques au déplacement du joueur. Cependant, il était difficile de simuler le déplacement des joueurs d’une manière réaliste. Les règles de distribution géométrique utilisées n’étaient pas suffisantes pour révéler le comportement des joueurs.  \nDesign et scalabilité des industries :  \nL’approche adoptée se base sur la matérialisation des entrées et des sorties par des conteneurs de stockage et la modélisation des processus des industries par des abonnements et désabonnements en fonction de l’état des conteneurs (plein ou vide). Cependant, le trafic élevé d’abonnements et de désabonnement provoquait une forte charge sur le CPU (central processing unit) et nécessitait un état des lieux de l’ensemble des conteneurs à chaque démarrage du serveur."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### Axes de travail pour l'année 2020\n\n- **Gestion des données Voxel :**\n  - Initialement basé sur MongoDB dans un environnement Amazon Web Services.\n  - Problèmes rencontrés :\n    - Fiabilité et performance insuffisantes en production.\n    - Bug dans la remontée des informations de charge serveur entraînant une saturation prématurée du stockage.\n  - Solutions développées :\n    - Création d'une solution interne pour gérer plusieurs Téraoctets de données.\n    - Amélioration de la fiabilité des métriques.\n    - Réduction significative des coûts opérationnels.\n\n- **Algorithme de répartition des joueurs sur différents nœuds :**\n  - Architecture distribuée nécessitant une gestion efficace de la répartition des joueurs.\n  - Objectif : Maximiser l'utilisation des serveurs sans dépasser leurs capacités.\n  - Développement de règles de répartition basées sur la proximité des joueurs.\n  - Hypothèses envisagées : clustering, géographique.\n  - Difficultés rencontrées :\n    - Simulation réaliste du déplacement des joueurs.\n    - Règles de distribution géométrique initiales non représentatives du comportement réel des joueurs.\n\n- **Design et scalabilité des industries :**\n  - Objectif : Permettre la création et gestion d'un grand nombre d'industries avec chaînage possible.\n  - Contraintes prises en compte dans l'architecture serveur.\n  - Choix de conception :\n    - Contrairement à d'autres jeux (ex. : MineCraft), pas de fonctionnement des industries basé sur la visibilité du joueur.\n    - Utilisation de conteneurs de stockage pour matérialiser les entrées et sorties.\n    - Modélisation des processus industriels par abonnements et désabonnements selon l'état des conteneurs (plein ou vide).\n  - Problèmes identifiés :\n    - Trafic élevé d'abonnements et désabonnements entraînant une forte charge CPU.\n    - Nécessité d'un état des lieux des conteneurs à chaque démarrage du serveur.\n  - Solution envisagée :\n    - Création d'un couplage entre conteneurs et industries pour permettre la reconnaissance des industries par les conteneurs abonnés."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques et verrous technologiques\nAux vues de ces éléments, nous avons choisi d’orienter nos travaux de l’année 2020 sur les axes suivants :\n\nGestion des données Voxel :\n\nLe système de stockage était initialement basé sur la solution MongoDB dans un environnement Amazon Web Services. Ce choix technologique s’est révélé problématique au cours de la mise en production, mettant en lumière un certain nombre de problèmes de fiabilité et de performance.\nEn effet, un bug dans la remontée des informations concernant la charge serveur générait une saturation prématurée du stockage. Nous avons ainsi été amené à développer notre propre solution permettant d’instancier plusieurs Téraoctets données afin de résoudre cette problématique de fiabilité des métriques. \n\nNous avons également profité de ce nouveau développement pour travailler sur la réduction drastique des coûts opérationnels. \n\nAlgorithme de répartition des joueurs sur différents nœuds : \n\nNotre architecture utilisant un modèle distribué, nous devons gérer la répartition des joueurs sur les différents serveurs en maximisant l’utilisation de chaque serveur tout en ne dépassant pas les limites de capacité de chacun d’entre eux.\nNous avons donc dû développer des règles de répartition spécifiques en nous basant sur la proximité des joueurs. A cet effet plusieurs hypothèses ont été envisagées (clustering, géographique,).\nNous avons alors été confronté à la difficulté de simuler le déplacement des joueurs de manière réaliste. En effet, les règles de distribution géométrique utilisées dans un premier temps ne se sont pas révélées suffisamment proches du véritable comportement des joueurs.\n\nDesign et scalabilité des industries :\n\nNotre objectif étant de proposer un gameplay permettant la création et la gestion d’un très grand nombre d’industries avec une possibilité de chaînage entre elles, nous avons dû tenir compte de cette contrainte dans nos travaux sur l’architecture et le fonctionnement de notre serveur.\nSi certains jeux tels que MineCraft ont choisi d’adapter le fonctionnement de l’industrie selon la distance du joueur en ne fonctionnant que lorsque celle-ci est visible, nous n’avons pas souhaité retenir cette solution qui ne s’intègre pas dans la stratégie de Dual Universe. Nous avons ainsi préféré matérialiser les entrées et les sorties par des conteneurs de stockage et en modélisant les processus des industries par des abonnements et désabonnements en fonction de l’état des conteneurs (plein ou vide).\nLa problématique de cette solution est le trafic particulièrement élevé d’abonnements et de désabonnement provoquant une forte charge sur le CPU et nécessitant un état des lieux de l’ensemble des conteneurs à chaque démarrage du serveur.\nNous avons donc envisagé la création d’un couplage entre les conteneurs et les industries permettant ainsi une reconnaissance des industries par les conteneurs abonnés à celles-ci."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques et verrous technologiques\n\n- **Incertitudes scientifiques** : \n  - Trois difficultés techniques principales associées aux thématiques étudiées.\n\n- **Génération procédurale** :\n  - Problématique centrale dans le jeu Dual Universe.\n  - Implique la synchronisation entre le client et le serveur.\n  - Enjeux : temps de génération procédurale et synchronisation.\n  - Contraintes internes et externes (code et données d’entrée) augmentent le temps de génération.\n  - Impact négatif sur l'expérience utilisateur.\n  - Travaux de 2022 : poursuite du développement et enrichissement de la solution.\n\n- **Moteur de rendu** :\n  - Amélioration liée à l'intégration de divers éléments (sols, océans, brouillard, ombres, etc.) pour un niveau de détail élevé.\n  - Nécessité de plusieurs ressources, entraînant une consommation accrue de la mémoire GPU.\n  - Difficulté : objectifs antinomiques entre amélioration du moteur de rendu et surconsommation de mémoire GPU.\n  - Besoin de travaux de R&D pour résoudre cette problématique.\n\n- **Voxel** :\n  - Problématique de la distance limite maximale de détection des éléments par les joueurs.\n  - Les éléments doivent être visibles à différentes distances selon leur importance.\n  - Gestion de la grille voxel et des opérations associées pose de nombreuses problématiques.\n  - Coûts d’exploitation élevés dus à la génération procédurale en temps réel.\n  - Objectif : garantir un temps de génération procédurale suffisamment bas pour une bonne expérience de jeu.\n  - En 2022 : expérimentation d'une solution de stockage des différences des voxels au lieu d'une génération procédurale systématique.\n  - Nouveaux verrous et incertitudes rencontrés, notamment en cas de taille de différence \"anormalement\" grande."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques :\nLes incertitudes scientifiques de cette opération de R&D sont liées essentiellement à trois difficultés techniques associées aux thématiques étudiées.\n\nGénération procédurale : \nLa génération procédurale en temps réel est une problématique au cœur du jeu Dual Universe. Cette dernière se fait à la fois au niveau du client et du serveur. La synchronisation entre ces deux éléments et le temps de génération procédurale sont les enjeux les plus compliqués. En effet, les contraintes internes et externes à la fois du code et des données d’entrée causent un temps de génération plus longue ce qui nuit à l’expérience de l’utilisateur. Les travaux de 2022 visaient à poursuivre le développement et l’enrichissement de cette solution.\n\nMoteur de rendu :\nL’amélioration du moteur de rendu est liée à l’utilisation de plusieurs éléments (sols, océans, brouillard, ombres, etc.) pour assurer un niveau de détail élevé. La dynamique qui lie tous ces éléments nécessite plusieurs ressources qui sont toutes associées à la consommation de la mémoire GPU (Graphics Processing Unit).\nLa difficulté réside donc dans les objectifs antinomiques de cet axe de travail qui a comme but l’amélioration du moteur de rendu à travers l’ajout de plusieurs éléments et la surconsommation de la mémoire GPU. Des travaux de R&D sont nécessaires pour répondre à cette problématique.\n\nVoxel :\nL’une des problématiques associées à cette thématique repose notamment sur la distance limite maximale de détection des éléments. En effet, selon l’importance des éléments, ces derniers doivent pouvoir être vus de plus ou moins loin par les joueurs. Ces contraintes nous ont conduits à devoir faire face à de nombreuses problématiques liées à la gestion de la grille voxel et des opérations associées à celle-ci. La principale repose sur les coûts d’exploitation engendrés par une génération procédurale en temps réel. De plus, afin d’atteindre nos objectifs en matière d’expérience de jeu, nous devons garantir un temps de génération procédurale suffisamment bas.\nEn 2022, nous avons expérimenté le développement d’une solution de stockage des différences des voxels plutôt qu’une génération procédurale à chaque fois. Cette solution nous a toutefois confrontés à de nouveaux verrous et des incertitudes, par exemple si la taille de la différence est « anormalement » grande."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Problématiques de fiabilisation des données routières :**\n  - Identification de deux verrous principaux :\n    - Inhérents à la nature des données traitées.\n    - Relatifs au contexte de la solution proposée.\n\n- **Premier Verrou : Données GPS et FCD**\n  - Les données GPS (FCD - Floating Car Data) fournissent uniquement :\n    - Localisation des véhicules.\n    - Vitesse des véhicules.\n    - Sens de déplacement des véhicules.\n  - Limitation des données FCD :\n    - Représentent en moyenne seulement 10 % du volume total de voitures sur un axe donné.\n    - Impossibilité de déduire directement le nombre total de voitures sur une route.\n  - Questions soulevées :\n    - Est-il possible de remplacer une boucle magnétique par des données GPS ?\n    - Combien de boucles peuvent être remplacées tout en conservant une estimation représentative ?\n\n- **Objectif du Projet :**\n  - Développer un algorithme pour prédire le nombre de voitures sur un axe.\n  - Utilisation minimale de capteurs classiques au profit des données GPS.\n\n- **Fonctionnement de l'Algorithme :**\n  - Considération des données des capteurs sélectionnés.\n  - Traitement d'un volume important de données GPS en temps réel.\n\n- **Comparaison avec le Système Actuel :**\n  - Système SIRIUS fournit un comptage toutes les 20 secondes.\n  - Nécessité pour le prototype final d'offrir une estimation précise dans un temps court.\n\n- **Deuxième Verrou : Temps de Calcul**\n  - Importance du facteur « temps de calcul » pour la performance de l'algorithme."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes problématiques de fiabilisation des données routières ont soulevé deux verrous principaux, à la fois inhérents à la nature des données que l’on traite et relatifs au contexte dans lequel s’inscrit notre solution.\n\nLe premier verrou réside dans le fait que les données GPS, et plus précisément les données FCD (pour Floating Car Data, ou traduit de façon littérale « données de véhicules flottants »), ne permettent que de connaître les caractéristiques de localisation, de vitesse et de sens de déplacement des véhicules. De plus, les données FCD auxquelles nous avons accès ne représentent en moyenne que 10 % du volume de voitures sur un axe donné. Il est donc a priori impossible de déduire directement de ces données le nombre de voitures sur une route. Deux questions se posent alors : est-il seulement possible de remplacer ne serait-ce qu’une seule boucle magnétique par des données GPS ? Et si c’est le cas, combien de boucles peut-on remplacer tout en conservant une estimation représentative de la réalité ?\n\nNous rappelons que l’objectif de ce projet est de développer un algorithme capable de prédire le nombre de voitures circulant sur un axe en employant un minimum de capteurs classiques au profit de l’utilisation de données GPS. Ledit algorithme devra, à chaque itération, considérer les données des capteurs que l’on décide de conserver et le volume très important de données GPS produites en temps réel. Or, à titre de comparaison, le système actuel SIRIUS est capable de fournir un comptage toutes les 20 secondes. Même sans essayer de reproduire une pareille efficacité, il faudra que la version finale de notre prototype soit capable de retourner une estimation précise dans un laps de temps court. Le facteur « temps de calcul » est donc le deuxième verrou majeur de ce projet."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Séparation des Plans** :\n  - Difficulté à définir et séparer le plan de contrôle du plan de données en fonction des Cores et des Threads.\n  - Nécessité de fixer des listes de tâches pour chaque plan et d'assigner des Cores ou Threads spécifiques.\n  - Importance de séparer les plans selon les types d'utilisation lancés.\n\n- **Problème de Dépendance/Indépendance** :\n  - Transition vers le mode multicœur crée des problèmes de dépendance entre le plan de données et le plan de contrôle.\n  - Dans le mode mono-cœur, les deux plans étaient gérés par le même Core, entraînant une forte dépendance.\n  - En mode multicœur, chaque Core peut gérer un plan indépendamment, mais nécessite une communication constante entre les deux plans.\n\n- **Gestion des Cores et OS Embarqué** :\n  - Chaque type de plan doit fonctionner sur son propre Core via un OS embarqué différent.\n  - Pour améliorer la performance, le plan de données et le plan de contrôle doivent être gérés par des Cores distincts, ce qui complique la communication en temps réel.\n\n- **Synchronisation de l'Horloge** :\n  - Difficulté à synchroniser l'horloge automatiquement en fonction des services actifs sur le routeur.\n\n- **Configuration Matérielle** :\n  - Nécessité d'une carte fille en plus de la carte mère pour gérer les composants supplémentaires.\n  - L'ajout d'une carte fille entraîne des problèmes de gestion et de maîtrise des ressources Cores et Threads.\n\n- **Communication Matérielle/Logicielle** :\n  - Développement nécessaire d'une communication précise et prudente pour éviter les collisions de paquets et de threads.\n\n- **Adaptation de l'OS Linux Embarqué** :\n  - L'OS Linux doit être adapté aux spécifications techniques du nouveau routeur, incluant :\n    - Variété des Cores.\n    - Séparation des plans de contrôle et de données.\n    - Communication en temps réel entre la carte mère et la carte fille.\n    - Communication en mode multithreading.\n\n### Questions Techniques à Résoudre\n\n- **Optimisation de la Mémoire** :\n  - Quel est le meilleur processus pour optimiser la mémoire du OneOS6 ?\n\n- **Différences entre API** :\n  - Comment annoter les différences entre l’API et la compatibilité des comportements ?\n\n- **Impact de la Compatibilité** :\n  - Quel est l'impact d'une compatibilité sur notre solution ? Est-elle limitée à un sous-ensemble de l’API ?\n\n- **Relations de Dépendance** :\n  - Comment exprimer les relations de dépendance pour assurer la construction de logiciels cohérents ?\n\n- **Identification des Modules et Composants** :\n  - Comment identifier et extraire tous les modules et composants du système OneOS6 ? Nécessité de développer une approche pour récupérer l'ensemble des composants."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nVoici la liste des principaux verrous techniques qui ont accompagné la mise en œuvre d’une architecture de type multicœur multithreads sur nos routeurs :\n\nAvec un routeur doté de telles caractéristiques (multithread multicœur) il est difficile de définir et séparer le plan de contrôle du plan de données en fonction des Cores (Processeur cœur) et des Threads. Déjà pour définir chaque plan, il faut fixer les listes des tâches pour chaque plan et avec quel Core ou Thread traiter, ensuite, il faut séparer les plans en fonction des types d’utilisation lancés.\n\nLa transition au mode multicœur génère un problème de dépendance/indépendance entre le plan de données et le plan de contrôle. Dans l’ancien mode mono-cœur, les plans de données et de contrôle étaient très dépendants dans le sens où ils étaient gérés par le même Core. Mais avec la transition au mode multicœur où chaque Core peut gérer indépendamment un plan, la question de la dépendance/indépendance entre les plans se pose, sachant que les deux plans doivent tout le temps communiquer entre eux.\n\nChaque type de plan doit tourner avec son propre Core via l’OS embarqué différent. En fait, pour booster la performance d’un équipement réseau au bout, chaque plan (plan de données et de contrôle) sera géré par un Core différent de l’autre, mais cela posera un problème de communication en temps-réel entre les deux plans.\n\nEn présence du concept multithread multicœur, il est très difficile de synchroniser l’horloge automatiquement en fonction des services qui tournent au niveau du routeur.\n\nPour gérer un tel routeur puissant (multithread multicœur), il faut proposer en plus de la carte mère, une carte fille qui embarquera quelques autres composants. Mais l’embarquement d’une telle carte va générer des problèmes de gestion et de maîtrise d’utilisation des ressources Cores et Threads.\n\nLa communication matérielle/logicielle dans un tel environnement complexe doit être développée d’une manière très précise et très prudente afin d’éviter la collision des paquets, des threads, …\n\nL’OS Linux embarqué choisi pour piloter le routeur doit être adapté avec les spécifications techniques du nouveau routeur (variété des Cores, séparation des plans de contrôle et de données, communication en temps-réel entre carte mère et carte fille, communication en mode multithreading, etc.).\n\nQuelle est le meilleur process qui permet une meilleure optimisation de la mémoire du OneOS6 ?\n\nComment annoter les différences entre l’API et la compatibilité des comportements ?\n\nQuel est l’impact d’une compatibilité sur notre solution ? Cette compatibilité est-elle limitée à un sous-ensemble de l’API ?\n\nComment exprimer les relations de dépendance, qui permettent d’assurer la construction de logiciels cohérents ?\n\nComment identifier et extraire tous les modules et composants de notre système OneOS6 ? Il est nécessaire ici de développer une approche qui permet de récupérer l’ensemble des composants du système."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### Développement d’une Chain of Trust\n- **Incertitude principale** : Concevoir un Trusted Platform Module capable de gérer des attaques futures inconnues.\n  - Nécessité de mises à jour non-invasives après intégration.\n  - Capacité à répondre aux futures normes via des mises à jour faciles.\n\n- **Deuxième incertitude** : Processus d’authentification du logiciel sur un équipement.\n  - Détermination du moment optimal pour déclencher des vérifications.\n\n#### Redéfinition de l’architecture pour supporter le trafic voix\n- **Constat initial** : Problèmes de mise en file d'attente dans l'architecture existante lors de la montée en puissance.\n- **Incertitude scientifique** : Identification du type d'architecture capable de résoudre ces problèmes.\n  - Architecture initiale : Noyau à plusieurs cœurs, jugée peu performante.\n  - Envisagement d'une écriture multicœur sur les anneaux de mémoire DDSP.\n    - Nécessité de validation pour assurer les performances.\n  \n- **Objectif à long terme** : Suppression de la file d’attente, ajoutant une incertitude scientifique.\n- **Problème supplémentaire** : Arrivée de paquets de données du même flux sur des cœurs de chemin de données différents, entraînant de nouvelles incertitudes.\n\n#### Amélioration des performances de trafic multicast\n- **Observation initiale** : Problèmes de congestion avec l'architecture prévoyant plusieurs flux parallèles.\n- **Incertitude scientifique** : Conception d'une architecture pour résoudre les problèmes de congestion.\n  - Envisagement de remplacer les flux parallèles par un flux unique réparti au dernier point commun.\n    - Cette solution introduit des incertitudes supplémentaires.\n    - Nécessité de gérer plusieurs flux pour l'acheminement des données, impliquant des copies de flux.\n    - Problématique de coût élevé dans le cadre de dispositifs embarqués."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nDéveloppement d’une Chain of Trust\nLa principale incertitude à laquelle nous avons fait face lors du développement de notre Trusted Platform Module était de concevoir une plateforme capable de gérer de futurs types d’attaques inconnus ou inexistants au moment de nos développements. En effet, une fois que notre élément de sécurité sera intégré à nos équipements, nous devrons être en mesure de le mettre à jour de manière non-invasive.\nDe même, notre module doit pouvoir être facilement mis à jour pour répondre aux futures normes. \n\nDe plus, une seconde incertitude concernait le processus d’authentification du logiciel chargé sur un équipement. Nous avons ainsi dû déterminer à quel moment de ce processus il était le plus pertinent de déclencher des actions de vérification. \nRedéfinition de l’architecture pour supporter le trafic voix\nLes travaux sur ce sujet sont partis du constat que l’architecture existante de notre solution présentait des problèmes de mise en file d'attente lors de la montée en puissance de notre équipement. La principale incertitude scientifique à laquelle nous avons fait dans le cadre de cas développement a donc été de déterminer quel type d’architecture permettrait de résoudre ces problèmes. \n\nL’architecture initiale était basée sur un noyau à plusieurs cœurs. Cette architecture étant manifestement peut performante, nous avons envisagé de transformer l'écriture multicœur sur les anneaux de mémoire DDSP.\nCependant, nous n’étions pas certains de la performance d’une telle architecture, nous avons donc dû la valider afin d’assurer les performances lors de la montée en puissance de notre solution. \nDe plus, nous souhaitons à terme supprimer la file d’attente, ce qui constitue une incertitude scientifique supplémentaire. \n\nEn outre, le fait que des paquets de données issus du même flux puissent arriver sur des cœurs de chemin de données différents entraîne de nouvelles incertitudes. \nAmélioration des performances de trafic multicast\nNotre architecture initiale prévoyait plusieurs flux parallèles. Cependant, lors de l’utilisation, nous avons observé que cette architecture présentait des problèmes de congestion. L’incertitude scientifique était donc de concevoir et de développer une architecture qui résolve ces problèmes. \nNous avons envisagé de remplacer les flux parallèles par un flux unique qui est réparti au dernier point commun. Cependant, une telle solution pose également des incertitudes. L’introduction d’un dernier point commun suppose que plusieurs flux se chargeront de l’acheminement des données aux interfaces, ce qui implique de faire des copies du flux. Dans le cadre de dispositifs embarqués, cela est problématique car très coûteux."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Limitation des volumes de transactions sur blockchains publiques :**\n  - Les solutions actuelles d'OWNEST sont limitées par l'utilisation de blockchains publiques.\n  - Validation des transactions : \n    - En moyenne 15 secondes (Ethereum) à 10 minutes (Bitcoin).\n    - Besoin d'OWNEST pour des transactions quasi-instantanées sur le terrain.\n  - Coût des transactions :\n    - Système de consensus “trustless” entraîne des coûts variables.\n    - Nécessité de maintenir des coûts de transaction faibles et quasi constants.\n  - Taille limitée du registre de transactions (blockchain) :\n    - Doit permettre à chacun d'avoir un nœud complet pour garantir la décentralisation.\n    - Limitation du nombre de transactions par seconde, impactant les cas d'usage à grande échelle (centaines de milliers de transferts par jour).\n    - Congestion du réseau entraîne des délais de validation accrus et potentiels rejets de transactions.\n\n- **Difficultés d’interactions multi-blockchain :**\n  - Limitations technologiques dans l'interaction entre différentes blockchains.\n  - Variabilité des caractéristiques des blockchains rend les solutions spécifiques non transférables.\n  - Absence de contrat régissant l'obligation de résultat sur les blockchains publiques augmente la dépendance et le risque technique.\n  - Complexité de la mise en place d’une solution multi-blockchain :\n    - Multiplicité des protocoles et architecture serveur nécessaires.\n    - Difficulté à relier les données de transactions dispersées sur différentes blockchains.\n\n- **Variation des coûts d’utilisation des blockchains publiques :**\n  - Les frais de transaction varient, impactant la validation rapide des transactions.\n  - Insuffisance des frais peut entraîner des délais de validation inacceptables pour les utilisateurs.\n  - Surestimation des frais peut rendre la solution économiquement non viable.\n  - Nécessité d'ajuster en temps réel les frais pour optimiser les coûts et garantir la pérennité de la solution.\n\n- **Absence d’un protocole global de transfert de responsabilités :**\n  - Développement de normes différentes par chaque système de certification entraîne incompatibilités.\n  - Risque d'affecter la traçabilité des biens, essentielle à la promesse de la solution d'OWNEST."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLimitation des volumes de transactions sur les blockchains publiques\nLes limites des solutions actuelles auxquelles fût confronté OWNEST sont liées à l’utilisation de blockchains publiques, notamment pour passer à l’échelle, les réseaux de blockchains ne sont pas suffisants en eux seuls.\n\nEn effet, la validation d’une transaction prend au moins 15 seconde en moyenne (fork Ethereum) et peut aller jusqu’à 10 min (Bitcoin), ceci pour laisser aux blocs de transactions le temps d’être propagés sur l’ensemble du réseau et garder une chaîne principale cohérente et sécurisée. OWNEST a cependant besoin de transactions quasi-instantanées pour que le logiciel soit utilisable sur le terrain.\n\nDe plus, le réseau est sécurisé par un système de consensus “trustless”, et donc intéressé économiquement. Chaque transaction a donc un certain coût et celui-ci est d’autant plus élevé que le réseau est sécurisé et “impossible à modifier”. OWNEST a besoin que le coût d’une transaction soit le plus faible possible et quasi constant. \n\nUn troisième verrou réside dans le fait que le registre de transaction, la blockchain, doit être d’une taille limitée, ceci afin de permettre à tout un chacun d’avoir chez soi un nœud (avec toute la blockchain), et donc de garantir un fort niveau de décentralisation pour le réseau blockchain. Ceci provoque donc une limitation du nombre de transaction par seconde, or OWNEST a besoin de traiter de très nombreuses transactions pour pouvoir être appliqué à des cas d’usage de plusieurs centaines de milliers de transferts par jour.\nAinsi lorsqu’une blockchain supporte un nombre de transactions de plus en plus important, la congestion du réseau qui en découle génère un accroissement du délai de validation des transactions qui peut aller jusqu’au rejet en cas de saturation.\n\nDifficultés d’interactions multi-blockchain\nNous avons aussi rencontré de nombreuses limitations technologiques concernant l’interaction multi-blockchain. En effet, les blockchains publiques ont toutes des caractéristiques différentes et une solution développée sur une blockchain particulière ne fonctionnera pas nécessairement sur les autres. Compte tenu de leur caractère public, aucun contrat ne régit une quelconque obligation de résultat à l’utilisation d’une blockchain et personne n’en garantit sa viabilité. Faire reposer sa solution sur une blockchain en particulier crée une relation de dépendance très forte, augmente le risque technique et nuit à la pérennité de la solution.\n\nLa difficulté d’établir une solution multi-blockchain réside dans la multiplicité des protocoles des blockchains, ainsi que dans l’architecture serveur nécessaire, mais également la récolte de ces données dispersées. En effet, les données de transactions étant réparties sur différentes blockchains il faut pouvoir relier les preuves d’interactions sur un actif malgré leur isolement dans une blockchain. \n\nVariation des coûts d’utilisation des blockchains publiques\nEnsuite, au cours de nos travaux de recherche et développement, des verrous technologiques supplémentaires ont été soulevés par la variation des coûts d’utilisation des blockchains publiques. \nSur une blockchain, les transactions sont accompagnées de frais de transaction, ceux-ci devant être suffisamment importants pour s’assurer que la transaction soit rapidement validée. Si le montant que l’application alloue à ces frais est insuffisant, la transaction pourra n’être validée que longtemps après, causant un désagrément important au client qui attend la confirmation de celle-ci pour s’assurer que sa responsabilité a bien été transmise. Au-delà de l’impact négatif sur la perception de l’utilisateur, ceci pourrait également aboutir à l’absence d’écriture de la transaction dans le registre distribué, créant une brèche dans l’historique des responsables d’un produit.\nDe plus, surestimer ces frais génère une hausse des coûts pouvant rendre la solution économiquement non viable. Il est donc nécessaire d’ajuster en temps réel le niveau de ces frais pour en optimiser le coût. Pour permettre la pérennité de sa solution, OWNEST doit anticiper ces variations afin que les transactions soient émises à un coût absorbable, tout en limitant le temps d’attente jusqu’à la validation de la transaction.\n\nAbsence d’un protocole global de transfert de responsabilités\nEnfin, si chaque système de certification de transferts de responsabilités développe sa norme, cela conduira à une incompatibilité des systèmes. Or, au vu du nombre important d’acteurs dans les réseaux logistiques cela aurait une grande probabilité d’affecter directement la traçabilité des biens, promesse essentielle de notre solution."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Verrous et Incertitudes\n\n- **Carence en connaissances** : Les projets de R&D en 2022 ont été confrontés à des défis scientifiques dus à un manque de connaissances existantes et accessibles sur les sujets abordés.\n\n- **Migration de données blockchain** :\n  - Verrou : Migration complexe de centaines de milliers de données d'une blockchain à une autre.\n  - Défi : Maintien d'une synchronisation totale entre les deux blockchains.\n\n- **Optimisation des coûts** :\n  - Contexte : Technologie récente et peu documentée.\n  - Défi : Réduction des coûts de gaz tout en assurant un fonctionnement efficace de la plateforme.\n\n- **Normes/Standards de smart contracts** :\n  - Verrou : Détection et gestion des différentes normes de smart contracts.\n  - Problème : Les types de métadonnées varient selon les implémentations (web, IPFS/Arweave, json, base64, etc.), malgré des standards guidant l'écriture des contrats.\n\n- **Assemblages multi niveaux** :\n  - Obstacle : Gestion des assemblages multi niveaux, non mentionnée dans la littérature.\n  - Défi : Garantir la fluidité du code face à un nombre potentiellement illimité d'assemblages, nécessitant un travail de recherche important.\n\n- **Sécurisation des wallets** :\n  - Problème : Les systèmes de génération de wallets manquent de mécanismes de chiffrement additionnels.\n  - Conséquence : Difficulté à sauvegarder sans risque les secrets d'accès aux wallets."}, {"role": "assistant", "content": "Aléas, verrous et incertitudes\nLors de nos projets de R&D en 2022, nous avons été confrontés à plusieurs verrous qui ont présenté des défis scientifiques à surmonter, notamment du fait de la carence en connaissances existantes et/ou accessibles sur les sujets abordés. \n\nLe premier verrou auquel nous avons été confrontés concernait la migration complexe de centaines de milliers de données produits d'une blockchain à une autre. En effet, cette migration nécessitait de maintenir une synchronisation totale entre les deux blockchains. \n\nDe plus, l'optimisation des coûts dans un contexte de technologie récente et peu documentée posait un défi supplémentaire. En effet, nous avons dû réduire le gaz tout en assurant un fonctionnement efficace de la plateforme.\n\nEnsuite, la détection et la gestion de différentes normes/standards de smart contract a aussi représenté un verrou. En effet, les modes d’écriture des contrats, bien que guidés par des standards, dépendent beaucoup de l’implémentation finale choisie par ses créateurs, notamment pour l'obtention des métadonnées. Nous avons donc été confrontés au fait que les types des métadonnées ne sont pas uniformes d’une collection à l’autre (web, IPFS/Arweave, json, base64, etc).\n\nUn autre obstacle majeur auquel nous avons été confrontés était la gestion des assemblages multi niveaux, dont la littérature ne fait nullement mention. En effet, du fait d’un nombre potentiellement illimité d’assemblages, garantir la fluidité du code a requis un travail de recherche important. \n\nEnfin, nous avons été confrontés, lors de la sécurisation des wallets, au fait que les systèmes de génération de wallet n’incorporent pas de mécanismes de chiffrement additionnels pour permettre la sauvegarde sans risque des secrets permettant l’accès aux wallet."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Outils et littérature** : Les outils existants dans la littérature, en particulier les bibliothèques logicielles, ne sont pas adaptés aux processeurs quantiques à atomes neutres.\n  \n- **Travaux précurseurs** : Nécessité de réaliser des travaux innovants et complexes pour répondre à cette problématique spécifique.\n\n- **Écosystème de programmation quantique** : Développement d'un premier écosystème pour la programmation quantique, impliquant l'élaboration et la mise en œuvre de toutes les briques de base nécessaires.\n\n- **Difficulté de la tâche** : La création de cet écosystème représente une tâche extrêmement difficile.\n\n- **Simulation numérique** : Étude de l'impact des imperfections sur la conception des processeurs quantiques via la simulation numérique.\n\n- **Implémentation d'algorithmes** : L'implémentation d'algorithmes sur les processeurs quantiques est un domaine de recherche encore émergent.\n\n- **Approche méthodologique** : Nécessité de développer une approche méthodologique pour aborder cette problématique, avec des résultats incertains à l'avance."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nComme évoqué avec l’état de l’art, les outils présentés dans la littérature, notamment les bibliothèques logicielles, n’ont pas été adressés aux processeurs quantiques à atomes neutres. Il nous fallait ainsi réaliser, des travaux à la fois précurseurs et complexes, en vue donc de répondre à cette problématique.  \n\nNous nous sommes également intéressés au développement d’un premier écosystème pour la programmation quantique. Il nous fallait ainsi élaborer ou mettre en œuvre toutes les briques de base nécessaires à cet effet, ce qui représente une tâche extrêmement difficile.  \n\nNous souhaitions également, via la simulation numérique, étudier l’impact des imperfections quant à la conception des processeurs quantiques. L’implémentation d’algorithmes sur les processeurs quantiques à cet effet, et même de manière générale, est un domaine de recherche qui est encore à ses premières heures. Il nous fallait donc dérouler une approche méthodologique visant à adresser cette problématique, et dont les résultats ne peuvent être connus à l’avance."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Intégration des épisomes :**\n  - Les épisomes peuvent s'intégrer dans le génome de la cellule réceptrice.\n  - Nécessité de contrôler la qualité des cellules iPSC pour éviter l'intégration de séquences exogènes.\n\n- **Utilisation d'ARNm :**\n  - Adoption d'ARNm pour éliminer le risque d'intégration des vecteurs.\n  - Transduction quotidienne des cellules requise selon les ARNm utilisés.\n  - Fragilité des cellules sanguines constitue un défi.\n\n- **Reprogrammation des cellules sanguines :**\n  - Reprogrammation nécessaire sur un lit de fibroblastes embryonnaires murins (MEF) pour un bon rendement.\n  - MEF d'origine animale présentent une variabilité importante entre les lots, rendant leur maîtrise difficile.\n\n- **Robotisation du procédé de production des iPSC :**\n  - Difficulté à robotiser due à la période de \"crise\" où les iPSC nouvellement formées se différencient spontanément.\n  - La différenciation spontanée compromet la reprogrammation et affecte l'homogénéité et la stabilité des produits.\n  - Retrait quotidien et manuel des cellules différenciées est chronophage et limite la production d’iPSC.\n\n- **Solutions de R&D :**\n  - Développement de procédures opérationnelles standard (SOP) pour le passage enzymatique des iPSC.\n  - Objectif de réaliser le passage enzymatique dès l'apparition des colonies post-reprogrammation pour lever le verrou de la robotisation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes épisomes peuvent présenter des événements d’intégration dans le génome de la cellule réceptrice. Il est alors nécessaire de multiplier les étapes du contrôle qualité des iPSC pour vérifier qu’aucune séquence exogène ne soit intégrée. Afin de pallier cette contrainte, nous nous sommes orientés vers l’utilisation d’ARNm qui élimine tout risque d'intégration des vecteurs. Cependant, cette approche nécessite de transduire quotidiennement les cellules selon les ARNm utilisés et nous avons été confrontés à la fragilité des cellules sanguines.\n\nUn facteur limitant de la qualité des iPSC est également la nécessité de reprogrammer les cellules sanguines sur un lit MEF afin d’obtenir un bon rendement de reprogrammation. Or, les MEF sont d’origine animale et montrent une grande variabilité de lot à lot : il n’y a aucune maîtrise sur cette variabilité.\n\nLa difficulté de robotiser l'ensemble du procédé de production des iPSC réside dans la période appelée \"crise\", durant laquelle les iPSC nouvellement formées, encore en voie de stabilisation, ont tendance à se différencier spontanément, ce qui réverse leur reprogrammation et perturbe l’homogénéité et la stabilité des produits obtenus. La stabilisation de la reprogrammation impliquait de retirer quotidiennement et manuellement les cellules différenciées des boîtes de culture, étape très chronophage pour le personnel et qui limite la quantité d’iPSC produites. La robotisation de cette étape nécessitait de transformer les passages manuels en passages enzymatiques. Afin de lever ce verrou, nos efforts de R&D se sont portés sur le développement de Standard Operating Procedures (SOP) pour le passage enzymatique des iPSC au plus tôt après l’apparition des colonies post-reprogrammation."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Intégration génomique des épisomes** : \n  - Risque d'intégration d'événements dans le génome de la cellule réceptrice.\n  - Nécessité de multiplier les étapes de contrôle qualité des iPSC pour vérifier l'absence de séquences exogènes.\n\n- **Utilisation d'ARNm** :\n  - Solution pour éliminer le risque d'intégration des vecteurs.\n  - Nécessité de transduire quotidiennement les cellules selon les ARNm utilisés.\n  - Fragilité des cellules sanguines constitue un défi.\n\n- **Reprogrammation des cellules sanguines** :\n  - Nécessité de reprogrammer sur un lit MEF pour un bon rendement.\n  - Variabilité des lots de MEF d'origine animale, entraînant un manque de maîtrise sur cette variabilité.\n\n- **Difficulté de robotisation du procédé de production des iPSC** :\n  - Période de \"crise\" où les iPSC nouvellement formées se différencient spontanément.\n  - Création d'un micro-environnement qui inverse la reprogrammation et perturbe l'homogénéité et la stabilité des produits.\n  - Stabilisation de la reprogrammation nécessite un retrait quotidien et manuel des cellules différenciées, étape chronophage.\n\n- **Limitation de la production d’iPSC** :\n  - Le processus manuel limite la quantité d’iPSC produites.\n  - Robotisation requiert transformation des passages manuels en passages enzymatiques.\n\n- **Développement de SOP** :\n  - Efforts de R&D pour établir des Standard Operating Procedures (SOP) pour le passage enzymatique des iPSC.\n  - Objectif : effectuer le passage au plus tôt après l’apparition des colonies post-reprogrammation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLes épisomes peuvent présenter des événements d’intégration dans le génome de la cellule réceptrice. Il est alors nécessaire de multiplier les étapes du contrôle qualité des iPSC pour vérifier qu’aucune séquence exogène ne soit intégrée. Afin de pallier cette contrainte, nous nous sommes orientés vers l’utilisation d’ARNm qui élimine tout risque d'intégration des vecteurs. Cependant, cette approche nécessite de transduire quotidiennement les cellules selon les ARNm utilisés et nous avons été confrontés à la fragilité des cellules sanguines.\n\nUn facteur limitant de la qualité des iPSC est également la nécessité de reprogrammer les cellules sanguines sur un lit MEF afin d’obtenir un bon rendement de reprogrammation. Or, les MEF sont d’origine animale et montrent une grande variabilité de lot à lot : il n’y a aucune maîtrise sur cette variabilité.\n\nLa difficulté de robotiser l'ensemble du procédé de production des iPSC réside dans la période appelée \"crise\", durant laquelle les iPSC nouvellement formées, encore en voie de stabilisation, ont tendance à se différencier spontanément, créant un micro-environnement qui réverse leur reprogrammation et perturbe l’homogénéité et la stabilité des produits obtenus. La stabilisation de la reprogrammation impliquait de retirer quotidiennement et manuellement les cellules différenciées des boîtes de culture sur plusieurs semaines, étape très chronophage pour le personnel et qui limite la quantité d’iPSC produites. La robotisation de cette étape nécessitait de transformer les passages manuels en passages enzymatiques. Afin de lever ce verrou, nos efforts de R&D se sont portés sur le développement de Standard Operating Procedures (SOP) pour le passage enzymatique des iPSC au plus tôt après l’apparition des colonies post-reprogrammation."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Intégration des épisomes** : \n  - Risque d'intégration dans le génome de la cellule réceptrice.\n  - Nécessité de multiplier les étapes de contrôle qualité des iPSC pour vérifier l'absence de séquences exogènes.\n\n- **Utilisation d’ARNm** :\n  - Élimine le risque d'intégration des vecteurs.\n  - Nécessite une transduction quotidienne des cellules selon les ARNm utilisés.\n  - Fragilité des cellules sanguines constitue un défi.\n\n- **Reprogrammation des cellules sanguines** :\n  - Nécessité d'un lit MEF pour un bon rendement de reprogrammation.\n  - Variabilité des MEF d’origine animale, sans maîtrise sur cette variabilité.\n\n- **Difficulté de robotisation** :\n  - Période de \"crise\" : iPSC nouvellement formées se différencient spontanément, perturbant la reprogrammation.\n  - Création d'un micro-environnement défavorable à l'homogénéité et à la stabilité des produits.\n  - Retrait quotidien et manuel des cellules différenciées, étape chronophage limitant la production d’iPSC.\n\n- **Solutions proposées** :\n  - Transformation des passages manuels en passages enzymatiques pour faciliter la robotisation.\n  - Développement de Standard Operating Procedures (SOP) pour le passage enzymatique des iPSC, à réaliser dès l'apparition des colonies post-reprogrammation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLes épisomes peuvent présenter des événements d’intégration dans le génome de la cellule réceptrice. Il est alors nécessaire de multiplier les étapes du contrôle qualité des iPSC pour vérifier qu’aucune séquence exogène ne soit intégrée. Afin de pallier cette contrainte, nous nous sommes orientés vers l’utilisation d’ARNm qui élimine tout risque d'intégration des vecteurs. Cependant, cette approche nécessite de transduire quotidiennement les cellules selon les ARNm utilisés et nous avons été confrontés à la fragilité des cellules sanguines.\n\nUn facteur limitant de la qualité des iPSC est également la nécessité de reprogrammer les cellules sanguines sur un lit MEF afin d’obtenir un bon rendement de reprogrammation. Or, les MEF sont d’origine animale et montrent une grande variabilité de lot à lot : il n’y a aucune maîtrise sur cette variabilité.\n\nLa difficulté de robotiser l'ensemble du procédé de production des iPSC réside dans la période appelée \"crise\", durant laquelle les iPSC nouvellement formées, encore en voie de stabilisation, ont tendance à se différencier spontanément, créant un micro-environnement qui réverse leur reprogrammation et perturbe l’homogénéité et la stabilité des produits obtenus. La stabilisation de la reprogrammation impliquait de retirer quotidiennement et manuellement les cellules différenciées des boîtes de culture sur plusieurs semaines, étape très chronophage pour le personnel et qui limite la quantité d’iPSC produites. La robotisation de cette étape nécessitait de transformer les passages manuels en passages enzymatiques. Afin de lever ce verrou, nos efforts de R&D se sont portés sur le développement de Standard Operating Procedures (SOP) pour le passage enzymatique des iPSC au plus tôt après l’apparition des colonies post-reprogrammation."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Conception de la génératrice** :\n  - Nécessité de lever de nombreux verrous techniques pour une conception robuste et à coût compétitif.\n  \n- **Encombrement restreint** :\n  - La génératrice doit fournir 20 kW dans un espace limité près du bloc moteur du char.\n  \n- **Échauffement des pièces mécaniques** :\n  - Température ambiante de 120 °C, avec des pertes internes au moteur d'environ 100 kW, entraînant des températures locales jusqu'à 200 °C.\n  - Flux d'air limités, rendant la dissipation de chaleur insuffisante.\n  \n- **Système de refroidissement** :\n  - Besoin de concevoir un système de refroidissement efficace pour abaisser la température ambiante des composants.\n  - Absence de réseau de circulation de fluide, contrairement à la concurrence (SERMAT) qui utilise un refroidissement par bain d'huile.\n  - Nécessité de développer un système compact de refroidissement et de gérer son alimentation.\n  \n- **Sensibilité des composants électroniques** :\n  - Nécessité de refroidir une zone stratégique pour implanter l'électronique de puissance ou de développer une solution de refroidissement spécifique pour ces composants.\n  \n- **Variabilité de la puissance** :\n  - Exigence d'une génératrice capable de fournir 20 kW avec une variation de vitesse de 2500 à 7050 tours/minute.\n  - Conception structurée pour gérer les variations de courant dues aux variations de vitesse.\n  - Moins d'aléas techniques à vitesse fixe, mais complexité accrue avec des variations.\n  \n- **Pertes Joule et échauffement** :\n  - Forts courants entraînant un échauffement important et des pertes de rendement.\n  - Réflexion sur la limitation des pertes de puissance et de l'échauffement.\n  - Choix du mode de redressement orienté vers la minimisation des pertes : redressement triphasé plus pertinent qu'hexaphasé.\n  \n- **Exigences du cahier des charges** :\n  - Tenue mécanique et minimisation des opérations de maintenance comme contraintes de développement.\n  - Choix d'une machine sans balais pour réduire les pièces d'usure et faciliter la maintenance.\n  \n- **Tenue mécanique des pièces tournantes** :\n  - Réflexion sur l'assemblage des tôles feuilletées pour éviter la casse due aux forces centrifuges.\n  - Risque de projection de tôle dans le moteur, pouvant entraîner une casse moteur."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe travail de conception de cette génératrice nécessite la levée de nombreux verrous techniques liés aux choix technologiques à réaliser pour aboutir à une conception robuste et à coût compétitif d’une génératrice de puissance élevée.\n\nL’un de ces verrous est lié à l’encombrement restreint dans lequel notre génératrice doit fournir une puissance de 20 kW. Cette contrainte spatiale est due au faible espace disponible à cet effet localisé à proximité du bloc moteur dans le char.  \nLa proximité moteur et la présence de différentes sources de pertes de puissance sont à l’origine d’un échauffement important des pièces mécaniques et d’une augmentation de la température sous le capot : l’environnement est à 120 °C mais les pertes à l’intérieur du moteur (100 kW environ) créent beaucoup de chaleur, entraînant des températures locales avoisinant les 200 °C.  \nDu fait de l’espace restreint, les flux d’air sont limités et la dissipation d’énergie par ce vecteur est insuffisante. Des incertitudes subsistaient quant à la résistance des composants de la génératrice et à leur efficacité. Nous devions ainsi concevoir un système de refroidissement capable d’évacuer la chaleur afin d’abaisser la température ambiante dans laquelle fonctionnent les composants pour atteindre leurs températures optimales de travail assurant un bon rendement.  \nLa difficulté dans la conception de ce système de refroidissement est liée à l’absence de réseau de circulation de fluide à cet effet. Notre concurrent, SERMAT, disposait en effet d’un réseau d’alimentation en huile pressurisée pour réaliser le refroidissement par bain d’huile.  \nNous devions dans notre étude concevoir un nouveau système compact de refroidissement et gérer l’alimentation de celui-ci.  \nDe plus, l’environnement extrême a posé d’autres difficultés pour l’architecture et la tenue en fonctionnement de l’ensemble. Les composants électroniques étant sensibles aux hautes températures, nous devions trouver une solution pour : soit refroidir une zone stratégique et y implanter l’électronique de puissance et d’asservissement, soit développer une solution de refroidissement spécifique réservée à ces composants.  \n\nUn autre verrou dans la conception de notre génératrice est lié au besoin de notre client. Les spécifications techniques exigent une génératrice capable de fournir une puissance de 20 kW avec une variation de vitesse importante, entre 2500 et 7050 tours/minute. Cette contrainte a été à l’origine de travaux sur la structuration de la machine pour que celle-ci soit capable de gérer les variations de courant qui résultent de ces variations de vitesse et délivrer une puissance de 20 kW à une tension continue. A vitesse fixe, les aléas techniques auraient été moindres puisque nous n’aurions pas à combiner toutes les modalités pour les paramètres régissant le fonctionnement de la machine.  \nEn particulier, nous avons été soumis à de forts courants qui, du fait des pertes Joule, engendrent un échauffement important et de grandes pertes dans l’ensemble générateur, minimisant donc son rendement. Nous avons ainsi dû porter une réflexion pour limiter l’impact de ces courants sur les pertes de puissance et l’échauffement. Le mode de redressement a en particulier été orienté par notre volonté de borner ces pertes. Après une étude approfondie, un redressement de la tension en triphasé s’est révélé plus pertinent qu’un redressement en hexaphasé.  \n\nEnfin, les exigences du cahier des charges en termes de tenue mécanique et de minimisation des opérations de maintenance nécessaires ont été à l’origine de nouvelles contraintes de développement. L’un de ces choix a en particulier été de privilégier une machine sans balais du fait qu’elle ne comporte pas de pièces d’usure et que la maintenance s’en trouve facilitée. La tenue mécanique des pièces tournantes a de même été au cœur de nos réflexions, notamment concernant l’assemblage des tôles feuilletées. Nous devions nous assurer que les forces centrifuges n’engendreraient pas de casse sur les éléments en rotation. En effet, les tôles feuilletées sont soumises à un effort considérable de cisaillement qui a tendance à les séparer, surtout à grande vitesse. Une projection de tôle dans le moteur serait désastreuse et provoquerait une casse moteur."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Conception de la génératrice :**\n  - Nécessité de lever de nombreux verrous techniques pour une conception robuste et à coût compétitif d'une génératrice de puissance élevée.\n\n- **Contexte des travaux :**\n  - Continuation des travaux de 2017, confrontés à des verrous technologiques spécifiques à la génératrice.\n\n- **Verrou lié à l'encombrement :**\n  - La génératrice doit fournir une puissance de 20 kW dans un espace restreint près du bloc moteur.\n  - Environnement à 120 °C avec des pertes de puissance dans le moteur (environ 100 kW) entraînant des températures locales jusqu'à 200 °C.\n  - Flux d'air limités, rendant la dissipation de chaleur insuffisante.\n\n- **Système de refroidissement :**\n  - Nécessité de concevoir un système de refroidissement pour abaisser la température ambiante des composants à leurs niveaux optimaux.\n  - Absence de réseau de circulation de fluide pour le refroidissement, contrairement à un concurrent (SERMAT) qui utilise un réseau d'huile pressurisée.\n  - Développement d'un système compact de refroidissement et gestion de son alimentation.\n  - Sensibilité des composants électroniques aux hautes températures, nécessitant une solution de refroidissement spécifique pour l'électronique de puissance et d'asservissement.\n\n- **Verrou lié aux spécifications techniques :**\n  - Exigence d'une génératrice capable de fournir 20 kW avec une variation de vitesse entre 2500 et 7050 tours/minute.\n  - Travaux sur la structuration de la machine pour gérer les variations de courant dues aux variations de vitesse.\n  - Problèmes d'échauffement et de pertes Joule à cause de forts courants, impactant le rendement de la génératrice.\n  - Choix du mode de redressement : redressement triphasé plus pertinent qu'hexaphasé pour limiter les pertes.\n\n- **Exigences du cahier des charges :**\n  - Tenue mécanique et minimisation des opérations de maintenance comme contraintes de développement.\n  - Choix d'une machine sans balais pour réduire l'usure et faciliter la maintenance.\n  - Réflexions sur la tenue mécanique des pièces tournantes, notamment l'assemblage des tôles feuilletées.\n  - Assurance que les forces centrifuges ne provoquent pas de casse des éléments en rotation, en raison des efforts de cisaillement sur les tôles feuilletées, qui pourraient entraîner une projection de tôle dans le moteur, causant une casse moteur."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\n\nLe travail de conception de cette génératrice nécessite la levée de nombreux verrous techniques liés aux choix technologiques à réaliser pour aboutir à une conception robuste et à coût compétitif d’une génératrice de puissance élevée.\n\nDans la continuité des travaux réalisés lors de l’année 2017, nous avons dû nous confronter à de nombreux verrous technologiques liés aux spécificités de notre génératrice.\n\nL’un de ces verrous est lié à l’encombrement restreint dans lequel notre génératrice doit fournir une puissance de 20 kW. Cette contrainte spatiale est due au faible espace disponible à cet effet localisé à proximité du bloc moteur dans le char. La proximité du moteur et la présence de différentes sources de pertes de puissance sont à l’origine d’un échauffement important des pièces mécaniques et d’une augmentation de la température sous le capot : l’environnement est à 120 °C mais les pertes à l’intérieur du moteur (100 kW environ) créent beaucoup de chaleur, entraînant des températures locales avoisinant les 200 °C. Du fait de l’espace restreint, les flux d’air sont limités et la dissipation d’énergie par ce vecteur est insuffisante. Des incertitudes subsistaient quant à la résistance des composants de la génératrice et à leur efficacité. Nous devions ainsi concevoir un système de refroidissement capable d’évacuer la chaleur afin d’abaisser la température ambiante dans laquelle fonctionnent les composants pour atteindre leurs températures optimales de travail assurant un bon rendement. La difficulté dans la conception de ce système de refroidissement est liée à l’absence de réseau de circulation de fluide à cet effet. Notre concurrent, SERMAT, disposait en effet d’un réseau d’alimentation en huile pressurisée pour réaliser le refroidissement par bain d’huile. Nous devions dans notre étude concevoir un nouveau système compact de refroidissement et gérer l’alimentation de celui-ci. De plus, l’environnement extrême a posé d’autres difficultés pour l’architecture et la tenue en fonctionnement de l’ensemble. Les composants électroniques étant sensibles aux hautes températures, nous devions trouver une solution pour : soit refroidir une zone stratégique et y implanter l’électronique de puissance et d’asservissement, soit développer une solution de refroidissement spécifique réservée à ces composants.\n\nUn autre verrou dans la conception de notre génératrice est lié au besoin de notre client. Les spécifications techniques exigent une génératrice capable de fournir une puissance de 20 kW avec une variation de vitesse importante, entre 2500 et 7050 tours/minute. Cette contrainte a été à l’origine de travaux sur la structuration de la machine pour que celle-ci soit capable de gérer les variations de courant qui résultent de ces variations de vitesse et délivrer une puissance de 20 kW à une tension continue. A vitesse fixe, les aléas techniques auraient été moindres puisque nous n’aurions pas à combiner toutes les modalités pour les paramètres régissant le fonctionnement de la machine. En particulier, nous avons été soumis à de forts courants qui, du fait des pertes Joule, engendrent un échauffement important et de grandes pertes dans l’ensemble générateur, minimisant donc son rendement. Nous avons ainsi dû porter une réflexion pour limiter l’impact de ces courants sur les pertes de puissance et l’échauffement. Le mode de redressement a en particulier été orienté par notre volonté de borner ces pertes. Après une étude approfondie, un redressement de la tension en triphasé s’est révélé plus pertinent qu’un redressement en hexaphasé.\n\nEnfin, les exigences du cahier des charges en termes de tenue mécanique et de minimisation des opérations de maintenance nécessaires ont été à l’origine de nouvelles contraintes de développement. L’un de ces choix a en particulier été de privilégier une machine sans balais du fait qu’elle ne comporte pas de pièces d’usure et que la maintenance s’en trouve facilitée. La tenue mécanique des pièces tournantes a de même été au cœur de nos réflexions, notamment concernant l’assemblage des tôles feuilletées. Nous devions nous assurer que les forces centrifuges n’engendreraient pas de casse sur les éléments en rotation. En effet, les tôles feuilletées sont soumises à un effort considérable de cisaillement qui a tendance à les séparer, surtout à grande vitesse. Une projection de tôle dans le moteur serait désastreuse et provoquerait une casse moteur."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contrainte d'encombrement pour la génératrice (20 kW)** :\n  - Espace restreint et proximité du bloc moteur entraînent un échauffement des pièces mécaniques.\n  - Température sous le capot augmente de 120°C à 200°C.\n  - Dissipation d'énergie insuffisante en raison de l'espace limité.\n  - Incertitudes sur la résistance et l'efficacité des composants dans ces conditions.\n  - Nécessité de concevoir un système de refroidissement compact pour abaisser la température.\n  - Absence de réseau de circulation de fluide complique la gestion thermique.\n\n- **Gestion de la variation de vitesse (2500 à 7050 tours/minute à 20 kW en tension continue)** :\n  - Besoin de structurer la machine pour gérer les variations de courant dues aux changements de vitesse.\n  - Courants de forte intensité causent un échauffement important par effet Joule, entraînant des pertes de rendement.\n  - Réflexion nécessaire pour limiter l'impact des courants sur les pertes de puissance et l'échauffement.\n\n- **Tenue mécanique des composants et maintenance** :\n  - Exigences du cahier des charges imposent des contraintes sur la tenue mécanique des éléments en rotation.\n  - Tôles feuilletées soumises à un effort de cisaillement élevé, risquant de se séparer à haute vitesse.\n  - Assurance nécessaire que les forces centrifuges ne provoquent pas de casse des éléments, pour éviter des dommages au moteur."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques et verrous technologiques \nDans la continuité des travaux menés en 2018, les spécificités de notre génératrice nous ont conduits à lever de nombreux verrous technologiques :\n\nEncombrement restreint dans lequel la génératrice doit fournir une puissance de 20 kW :\n\nCette contrainte spatiale due au faible espace disponible et la proximité du bloc moteur du char sont à l’origine d’un échauffement important des pièces mécaniques entraînant une augmentation importante de la température sous le capot passant ainsi de 120 à 200°C. De plus, la dissipation d’énergie via les flux d’air est insuffisante eu égard à l’espace restreint disponible. De ce fait, des incertitudes subsistent quant à la résistance et l’efficacité des composants de la génératrice dans ces conditions. Par ailleurs, la conception d’un système de refroidissement capable d’évacuer la chaleur permettant ainsi d’abaisser la température de l’environnement afin d’atteindre les conditions optimales de fonctionnement des composants se heurte à l’absence de réseau de circulation de fluide disponible à cet effet. Notre étude devait donc intégrer la conception d’un nouveau système compact de refroidissement ainsi que la gestion de l’alimentation de celui-ci.\n\nGénératrice capable de gérer une variation de vitesse importante entre 2500 et 7050 tours/minute à une puissance de 20 kW en tension continue :\n\nCette contrainte a été à l’origine de travaux sur la structuration de la machine pour que celle-ci soit capable de gérer les variations de courant qui résultent de ces variations de vitesse. En effet, les courants de forte intensité engendrent un échauffement important dû aux pertes par effet Joule, générant de grandes pertes dans l’ensemble générateur et minimisant ainsi son rendement. De ce fait, nous avons dû porter une réflexion afin de limiter l’impact de ces courants sur les pertes de puissance et l’échauffement.\n\nTenue mécanique des composants et minimisation des opérations de maintenance :\n\nLes exigences du cahier des charges ont été à l’origine de nouvelles contraintes de développement notamment en ce qui concerne la tenue mécanique des éléments en rotation. En effet, les tôles feuilletées étant soumises à un effort de cisaillement considérable ayant tendance à les séparer à haute vitesse, nous avons dû nous assurer que les forces centrifuges n’entraînaient pas de casse sur ces éléments car une projection de tôle dans le moteur provoquerait une casse de ce dernier."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Verrous et Incertitudes\n\n- **Conception et Développement de la Génératrice de 20 kW :**\n  - Difficulté à prédire le comportement et à vérifier les performances (tension de régulation, puissance de sortie) sans R&D.\n\n- **Défis du Second Prototype (2023) :**\n  - Problèmes non anticipés par rapport au prototype N°1.\n  - Utilisation de sections de cuivre plus épaisses que d'habitude, nécessitant de nouveaux outillages pour la mise en forme.\n\n- **Partenariat avec Cleo pour le Développement d’un Moteur Spécifique :**\n  - Objectif : Moteur fonctionnant à 200°C et résistant à des rayonnements radiatifs ionisants élevés (γ : 1,9 kGy ; neutron : 0,32 Gy).\n  - Manque de documentation disponible, car les développements similaires sont réalisés dans le secteur de la défense.\n  - Nécessité de démarches de recherche pour répondre aux contraintes de température et d’environnement radiatif ionisant.\n\n- **Résultats des Calculs et Simulations :**\n  - Incapacité à atteindre la contrainte de température maximale de 200°C.\n  - Vitesse de rotation du moteur non assurée au-delà de 100°C.\n  - Continuité du service non garantie à 200°C."}, {"role": "assistant", "content": "Aléas, verrous et incertitudes\nNos travaux des années précédentes nous ont amenés à travailler sur la conception et le développement de notre génératrice spécifique de 20 kW. Cependant, il nous était impossible, pour une génératrice de si grande puissance, de prédire son comportement et de vérifier ses performances (tension de régulation, puissance de sortie, etc..) sans avoir recours à des travaux de R&D.\n\nEn 2023, lors de la réalisation du second prototype de la génératrice, nous avons été confrontés à un défi que nous n’avions pas envisagé. En effet, pour pallier les problèmes rencontrés sur le prototype N°1, nous avons, entre autres, eu à utiliser des sections de cuivre beaucoup plus épaisses que celles avec lesquelles nous avions l’habitude de travailler et pour lesquelles nous n’étions donc pas équipés Nous avons donc eu à réaliser de nouveaux outillages pour nous permettre de mettre en forme le cuivre.\n\nDe plus, nous avons également rencontré des incertitudes lors de notre partenariat avec Cleo pour le projet de développement d’un moteur spécifique, pouvant fonctionner à une température de 200°C et pour des rayonnements radiatifs ionisants élevés (γ : 1,9 kGy ; neutron : 0,32 Gy). En effet, le sujet est très peu documenté car les développements de ce type sont réalisés dans le secteur de la défense et les résultats ne sont pas divulgués. En l’absence de documentation, nous avons dû entreprendre des démarches de recherche pour développer une solution répondant aux contraintes de température et d’environnement radiatif ionisant. Nous avons réalisé de nombreux calculs et simulations pour lever cette incertitude, sans parvenir pour l’instant à tenir la contrainte de température maximale (200°C) ; la vitesse de rotation du moteur n’est plus assurée dès 100°C et la continuité du service n’est pas encore garantie pour 200°C."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contrainte d'encombrement pour la génératrice (20 kW)** :\n  - Espace restreint à proximité du bloc moteur du char.\n  - Échauffement des pièces mécaniques : température sous le capot augmente de 120°C à 200°C.\n  - Insuffisance de dissipation d'énergie par flux d'air en raison de l'espace limité.\n  - Incertitudes sur la résistance et l’efficacité des composants de la génératrice dans ces conditions.\n  - Nécessité de concevoir un système de refroidissement compact pour évacuer la chaleur.\n  - Absence de réseau de circulation de fluide pour le refroidissement à intégrer dans l'étude.\n\n- **Gestion des variations de vitesse (2500 à 7050 tours/minute à 20 kW en tension continue)** :\n  - Nécessité de structurer la machine pour gérer les variations de courant dues aux changements de vitesse.\n  - Échauffement important causé par des courants de forte intensité, entraînant des pertes par effet Joule.\n  - Réflexion nécessaire pour limiter l'impact des courants sur les pertes de puissance et l'échauffement.\n\n- **Tenue mécanique des composants et maintenance** :\n  - Exigences du cahier des charges imposant des contraintes sur la tenue mécanique des éléments en rotation.\n  - Tôles feuilletées soumises à un effort de cisaillement important à haute vitesse.\n  - Assurance nécessaire que les forces centrifuges ne provoquent pas de casse des éléments.\n  - Risque de projection de tôle dans le moteur entraînant une casse de celui-ci."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques et verrous technologiques \nDans la continuité des travaux menés en 2019, les spécificités de notre génératrice nous ont conduits à lever de nombreux verrous technologiques :\n\nEncombrement restreint dans lequel la génératrice doit fournir une puissance de 20 kW :\n\nCette contrainte spatiale due au faible espace disponible et la proximité du bloc moteur du char sont à l’origine d’un échauffement important des pièces mécaniques entraînant une augmentation importante de la température sous le capot passant ainsi de 120 à 200°C. De plus, la dissipation d’énergie via les flux d’air est insuffisante eu égard à l’espace restreint disponible. De ce fait, des incertitudes subsistent quant à la résistance et l’efficacité des composants de la génératrice dans ces conditions. Par ailleurs, la conception d’un système de refroidissement capable d’évacuer la chaleur permettant ainsi d’abaisser la température de l’environnement afin d’atteindre les conditions optimales de fonctionnement des composants se heurte à l’absence de réseau de circulation de fluide disponible à cet effet. Notre étude devait donc intégrer la conception d’un nouveau système compact de refroidissement ainsi que la gestion de l’alimentation de celui-ci.\n\nGénératrice capable de gérer une variation de vitesse importante entre 2500 et 7050 tours/minute à une puissance de 20 kW en tension continue :\n\nCette contrainte a été à l’origine de travaux sur la structuration de la machine pour que celle-ci soit capable de gérer les variations de courant qui résultent de ces variations de vitesse. En effet, les courants de forte intensité engendrent un échauffement important dû aux pertes par effet Joule, générant de grandes pertes dans l’ensemble générateur et minimisant ainsi son rendement. De ce fait, nous avons dû porter une réflexion afin de limiter l’impact de ces courants sur les pertes de puissance et l’échauffement. \n\nTenue mécanique des composants et minimisation des opérations de maintenance :\n\nLes exigences du cahier des charges ont été à l’origine de nouvelles contraintes de développement notamment en ce qui concerne la tenue mécanique des éléments en rotation. En effet, les tôles feuilletées étant soumises à un effort de cisaillement considérable ayant tendance à les séparer à haute vitesse, nous avons dû nous assurer que les forces centrifuges n’entraînaient pas de casse sur ces éléments car une projection de tôle dans le moteur provoquerait une casse de ce dernier."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Objectif principal :**\n  - Obtenir des embeddings spécialisés en finance intégrant les sentiments.\n  - Entraîner des modèles de machine learning sur un large corpus d'actualités financières.\n\n- **Développement du modèle :**\n  - Capacité à prédire l'affect (sentiment ou émotion) d'une phrase ou d'un texte.\n  \n- **Difficultés à surmonter :**\n  - **Difficultés lexicales :**\n    - Exemples : « good » et « bad » utilisés dans le même contexte mais avec des sens opposés.\n  - **Ambiguïté lexicale :**\n    - Problèmes causés par les homonymes.\n  - **Relations complexes :**\n    - Exemple : « king – man + woman = queen » à prédire par le modèle.\n  - **Jeux de données d’entraînement limités :**\n    - Difficulté à obtenir des données annotées.\n\n- **Utilisation de l'analyse des sentiments :**\n  - Prédire les évolutions du marché financier.\n  \n- **Corrélation entre marchés et sentiments :**\n  - Déterminer si les marchés sont corrélés aux sentiments extraits des sources textuelles.\n  \n- **Représentation des données :**\n  - Nécessité d'une représentation pertinente pour l'apprentissage des modèles de machine learning.\n  - Définir de nouvelles variables explicatives (features) pour réduire la redondance de l’information et établir une corrélation claire.\n  \n- **Stabilité et précision des corrélations :**\n  - La corrélation doit être stable pour permettre des prédictions précises.\n  - Objectif : tirer une analyse du marché et déduire des stratégies d’investissement efficaces."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous souhaitons obtenir des embeddings spécialisés dans le domaine de la finance et tenant compte des sentiments, et entraîner des modèles de machine learning sur un très large corpus d’actualités et informations financières en se basant sur ces embeddings. Nous devons concevoir un modèle capable de prédire l’affect (sentiment ou émotion) d’une phrase, voire d’un texte contenant plusieurs phrases, en tenant compte : \nDes difficultés lexicales dues au contexte (« good » et « bad » sont toujours utilisés dans le même contexte, mais ont un sens opposé) ;\nDe l’ambiguïté de certaines mots (par exemple, les homonymes) ;\nDes relations complexes comme « king – man + woman = queen », que notre modèle doit être capable de prédire ;\nEt ce avec des jeux de données d’entraînement limités, puisqu’obtenir des données annotées est difficile. \n\nNous devons par la suite utiliser cette analyse de sentiments pour prédire les évolutions du marché financier :\nIl nous faudra déterminer si les marchés sont bien corrélés aux sentiments et émotions extraits de nos différentes sources textuelles ;\nLes modèles de machine learning nécessitent une représentation des données pertinente afin d’apprendre correctement. Il nous faudra donc définir de nouvelles variables explicatives (features) pertinentes afin de réduire la redondance de l’information et d’établir une corrélation claire ;\nLa corrélation établie doit être stable et permettre des prédictions suffisamment précises pour pouvoir en tirer une véritable analyse du marché et en déduire de bonnes stratégies d’investissement."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Défi scientifique principal** : Obtenir des embeddings spécialisés en finance prenant en compte les sentiments pour entraîner des modèles de machine learning sur un large corpus d'actualités financières.\n  \n- **Modèle d'analyse de sentiments** : \n  - Capacité à prédire l'affect (sentiment ou émotion) d'une phrase ou d'un texte.\n  - Prise en compte des difficultés lexicales dues au contexte et à l'ambiguïté (ex. homonymes).\n  - Utilisation de jeux de données d'entraînement limités, nécessitant des augmentations de données.\n\n- **Analyse de sentiments et marché financier** :\n  - Détermination de la corrélation entre les sentiments/emotions et les évolutions du marché.\n  - Questions à se poser :\n    - Limites des sentiments et émotions.\n    - Métriques à utiliser et leur validité dans le domaine financier.\n    - Représentation pertinente des données pour l'apprentissage des modèles.\n    - Définition de nouvelles variables explicatives (features) pour réduire la redondance et établir une corrélation claire.\n    - Prise en compte de la variance inhérente à la finance et du risque systématique.\n    - Stabilité de la corrélation pour des prédictions précises et stratégies d'investissement efficaces.\n    - Validité des modèles comparés aux modèles généraux.\n\n- **Spécialisation des modèles de NLP** :\n  - Les modèles de Natural Language Processing (NLP) sont souvent génériques et peinent à se spécialiser.\n  - Développement d'un modèle de Deep Learning spécifique à la finance, appelé FinancialBERT, avec incertitudes sur ses performances.\n  - Utilisation d'un dictionnaire courant peut être contreproductive en raison du jargon technique spécifique à la finance (ex. termes \"long\" et \"short\").\n\n- **Évaluation des critères ESG** :\n  - Absence actuelle d'évaluation automatique des critères ESG par analyse de sentiments.\n  - Développement interne comme levée d'un verrou technologique, axe de recherche prioritaire chez SESAMm.\n  - Questions de périmètre d'analyse :\n    - Définition d'un scandale ESG et de son importance (volume médiatique, pertes financières, impact environnemental, conséquences salariales).\n  - Domaine peu abordé dans la littérature, nécessitant la recherche de métriques adaptées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNotre premier défi scientifique porte sur l’obtention d’embeddings spécialisés dans le domaine de la finance tenant compte des sentiments, et permettant d’entraîner des modèles de machine learning sur un très large corpus d’actualités et informations financières en se basant sur ces embeddings. Pour cela, nous nous proposons de concevoir un modèle capable de prédire l’affect (sentiment ou émotion) d’une phrase, voire d’un texte contenant plusieurs phrases, en tenant compte des difficultés lexicales dues au contexte ou à l’ambiguïté de certains mots (par exemple, les homonymes) et ce avec des jeux de données d’entraînement limités, puisqu’obtenir des données annotées est difficile (d’autant plus si les jeux de données nécessaires sont spécialisés), nous amenant donc à se questionner sur des augmentations de données, par exemple.\n\nNous devons par la suite utiliser cette analyse de sentiments pour prédire les évolutions du marché financier. Il nous faut ainsi déterminer si les marchés sont bien corrélés aux sentiments et émotions extraits de nos différentes sources textuelles, ce qui est difficile de façon inhérente à l’analyse de sentiments : quelles sont les limites des sentiments et des émotions ? Quelles métriques positionner ? Si des métriques existent, restent-elles valables pour le domaine particulier de la finance (en particulier, les gens expriment-ils leurs sentiments de la même façon dans ce domaine particulier que dans la vie courante ou dans les autres bases de données d’entraînement de modèles) ? Les modèles de machine learning nécessitent une représentation des données pertinente afin d’apprendre correctement. Il est donc nécessaire de définir de nouvelles variables explicatives (features) pertinentes afin de réduire la redondance de l’information et d’établir une corrélation claire. Aussi, quand bien même ces features sont trouvées (sans overfitting ni underfitting), prennent-elles en compte la variance inhérente à la finance, avec notamment un risque systématique ? La corrélation établie doit être stable et permettre des prédictions suffisamment précises pour pouvoir en tirer une véritable analyse du marché et en déduire de bonnes stratégies d’investissement. Il nous faut donc nous poser des questions en permanence sur la validité de nos modèles, car ils sont difficilement comparables aux modèles généraux, conséquence de notre activité.\n\nDe plus, les modèles de Natural Langage Processing ont tendance à être génériques et peinent souvent lorsqu’on doit les spécialiser. Ainsi, un modèle de Deep Learning spécialisé au domaine de la finance, ou FinancialBERT comme il sera appelé par la suite, serait une nouveauté et nous sommes encore incertains sur ses potentielles performances. Aussi, le monde de la finance utilise des termes spécifiques différents du langage courant (par exemple long et short) et a en général un jargon technique particulier : utiliser un dictionnaire courant peut être contreproductif et il est nécessaire de trouver des alternatives.\n\nFinalement, l’évaluation automatique des critères ESG (Environnementaux Sociaux et de Gouvernance) par analyse de sentiments n’existe pour l’instant pas encore et son développement en interne représenterait la levée d’un verrou technologique, c’est ainsi un axe de recherche prioritaire chez SESAMm. Ici aussi se posent des questions de périmètre de l’analyse : si nous cherchons à détecter les scandales, comment définir un scandale ESG et plus précisément son importance (volume médiatique, pertes financières pour l’entreprise, impact sur l’environnement, conséquences salariales etc.). Il s’agit d’un domaine très peu abordé par la littérature et trouver des métriques correspondantes à ce domaine représente un fort verrou pour notre analyse."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte de Recherche (2020)**:\n  - Multiples verrous technologiques et limitations de l'état de l'art.\n  - Mobilisation des ingénieurs et chercheurs pour surmonter ces défis via tests et évaluations.\n\n- **Problèmes avec Modèles Historiques**:\n  - Utilisation de SVM et XGBoost pour la classification de sentiments.\n  - Carences identifiées :\n    - Implémentation en Weka limitant flexibilité et portabilité.\n    - Évaluation non-standard des performances.\n    - Manque de vectorisation entraînant une exécution lente, incompatible avec le DataLake.\n    - Non-transposabilité des architectures à d'autres langues.\n\n- **Défis de la Tâche SST-2**:\n  - Non-adéquation avec la problématique de classification de sentiments :\n    - SST-2 est binaire (positif/négatif) alors que besoin de trois indicateurs (neutre, positif, négatif).\n    - Données issues de revues de films, éloignées des cas d'application liés à l'investissement financier.\n    - Disponibilité limitée aux données en anglais.\n  - Algorithmes performants basés sur des modèles Transformers, nécessitant une infrastructure coûteuse et posant des problèmes de latence.\n\n- **Choix de Modèle et Hyperparamètres**:\n  - Question cruciale : quel modèle choisir pour équilibrer état de l'art et efficacité en production ?\n  - Incertitudes sur :\n    - Existence d'une maquette commune pour différentes langues.\n    - Performance en qualité et latence des modèles.\n    - Estimation du meilleur modèle pour chaque langue.\n    - Reproductibilité pour de nouvelles langues.\n\n- **Contraintes de Tokenisation**:\n  - Nécessité d'une tokenisation couvrant un vocabulaire large, incluant données d'entraînement et validation.\n  - Intersections complètes avec les embeddings requises.\n\n- **Variabilité des Tailles de Phrases**:\n  - Traitement de séquences de taille variable, complexité computationnelle des Long Short Term Memory Networks.\n  - Adoption du padding avec question du seuil à choisir.\n\n- **Paramétrage des Modèles**:\n  - Questions sur la taille du filtre (kernel size) et le nombre de filtres.\n  - Risques de sous-performance ou sur-apprentissage selon le nombre de filtres.\n\n- **Estimation des Modèles**:\n  - Modèles à estimer pour chaque langue : classification de sentiments et d'émotions.\n  - Variabilité de la qualité des données d'entraînement selon les langues.\n  - Hyperparamètres à calibrer : learning rate (lr), batch size (B), nombre de filtres (F), proportion de dropout (a).\n  - Espace de recherche (H) trop grand pour une évaluation rapide.\n\n- **Goulots d'Étranglement dans l'Annotation**:\n  - Coûts élevés et subjectivité des campagnes d'annotation.\n  - Modèle exploitable via deux binaires : tokenizer et modèle, avec implications sur la containerisation.\n\n- **Choix d'Embeddings pour Transfer Learning**:\n  - Long processus de sélection d'embeddings pré-entraînés pour plus de 250 langues, difficilement réalisable dans un temps limité.\n\n- **Limitations Technologiques sur SignalReveal**:\n  - Association de données sur l'indice du dollar et industries import/export.\n  - Dataset limité (63 industries, 108 timestamps) pour la modélisation.\n  - Difficultés liées à un grand nombre de features dépendantes nécessitant réduction de dimension.\n  - Transformation des valeurs continues en cibles binaires pour classificateurs probabilistes.\n  - Séparation du training set et test set pour éviter l'over-fitting, avec défis pour les séries chronologiques financières.\n  - Validation des résultats avec ROC-AUC, mais insuffisante pour évaluer stratégies financières.\n  - Problèmes de biais d'anticipation lors de la validation croisée."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours de nos travaux de recherche et développement de l’année 2020, nous avons rencontré de nombreux verrous technologiques et limitations de l’état de l’art qui nous ont obligé à mobiliser l’expérience et les connaissances de nos ingénieurs et chercheurs, ainsi que la réalisation de nombreux tests et évaluations. \nEn effet, dans un premier temps, les modèles utilisés historiquement par SESAMm pour la classification de sentiments comme SVM ou XGBoost présentaient plusieurs carences. Citons entre autres : \nUne implémentation des algorithmes d’apprentissage en Weka, résultant en une perte de flexibilité, ainsi qu’une architecture peu portative ;\nUne évaluation non-standard des performances des modèles ;\nUn manque de vectorisation, conduisant à une moindre parallélisation lors de l’apprentissage et de la prédiction, et donc à des exécutions plus lentes, incompatibles dans la pratique avec le DataLake de SESAMm ;\nLes architectures ne sont pas transposables à d’autres langues.\n\nAussi, si l’on se concentre sur la tâche SST-2 des jeux de tâches GLUE et SUPERGLUE, se rapprochant de notre problématique de classification de sentiments. Un premier obstacle est que celle-ci n’est pas en adéquation totale avec notre problématique. En effet :\nD’une part, SST-2 est une tâche de détection binaire, positif vs. négatif. Or, nous avons besoin de trois indicateurs : neutre, positif ou négatif ;\nD’autre part, les données issues de SST-2 sont originellement issues de revues de films, indicateurs éloignés de nos cas d'applications liés à l’investissement financier ;\nLe jeu de données n’est disponible qu’en anglais. Les autres langues sont indisponibles.\nEnfin, si l’on observe les algorithmes les plus performants sur la résolution de cette tâche, ils sont issus de la famille de modèles des Transformers. Or, si ces modèles s’avèrent puissants, leur apprentissage ainsi que leur mise en production s’avèrent contraintes par une infrastructure très coûteuse. Lors de la prédiction notamment, une faible latence n’est pas garantie.\nCe verrou peut être donc résumé comme suit : quel modèle élire afin d’assurer un équilibre entre l’état de l’art et efficacité lors de la mise en production, tout en s’assurant d’un respect des contraintes exigibles des indicateurs souhaités ? Une fois la famille de modèle choisie, comment estimer les meilleurs hyperparamètres ?\n\nAu-delà des problématiques évoquées ci-avant, le choix de l’implémentation du Deep Learning résulte de la volonté de SESAMm de laisser plus de flexibilité et de garantir plus de performances au modèle. Le problème réside dans le choix du modèle en question : parmi la pléthore des architectures réalisées chaque jour, laquelle sera la plus à même de résoudre le problème de latence et de performance ?\nLes incertitudes furent de quatre ordres : Dans un premier temps, sachant que la problématique est commune aux langues traitées, existe-t-il une maquette commune ? De plus, comment assurer une performance en termes de qualité des modèles, ainsi qu’en termes de latence ? Il fût aussi question de comment estimer le meilleur modèle pour chacune des langues. Enfin, comment assurer la reproductibilité liée à la couverture de nouvelles langues ?\n\nNous avons aussi rencontré des contraintes concernant la tokenisation dans notre architecture. En effet, il fallait que celle-ci couvre le vocabulaire le plus large possible, du fait de la volumétrie importante, que celle-ci couvre les données d’entraînement et de validation et que l’intersection avec les embeddings soit complète.\n\nUne autre difficulté rencontrée consiste en la variabilité des tailles des phrases : en effet, le nombre de mots varie pour chaque phrase. Si certains procédés existent pour traiter ces séquences de taille variable, telles que les Long Short Term Memory Networks, ceux-ci s’avèrent computationnellement complexes et difficilement mettables à l’échelle. Nous avons donc opté pour le padding, pour plus de pragmatisme. Néanmoins, pour le padding, il se pose de même la question du seuil à choisir. \n\nDe plus, deux questions de paramétrage se sont donc posées. La première concernait la taille du filtre, ou kernel size : sachant que nous scannons consécutivement les tokens, à partir de combien de tokens peut-on considérer qu’une expression est différente d’une autre ? Typiquement, cette question se rapproche de l’utilisation des n-grammes en NLP. La seconde question concernait le nombre de filtres appris conjointement. Ces questions résument nos problématiques de recherche, où les articles académiques ne s’appesantissent pas sur ces calibrations.\n\nPour ce qui est de l’estimation des modèles, nous nous sommes retrouvés confrontés à un nombre de modèles à estimer qui n’est pas négligeable : chacune des langues traitées contient un modèle de classification de sentiment et un modèle de classification d’émotions. De plus, chacun des modèles peut potentiellement être disponible sous plusieurs versions. Aussi, les données d’entraînement sont de qualité variable suivant les différentes langues. Certains paramètres, tels que le kernel_size, ont dû être calibrés par une connaissance experte, néanmoins, une incertitude demeure en ce qui concerne :\nle learning rate lr\nla taille d’un lot, ou batch size B\nle nombre de filtres F\nenfin, la proportion du dropout, a\n\nD’autre part, nous notons h = (lr, F,B, a) le quadruplet associé aux hyperparamètres. Cet hyperparamètre h appartient à un espace que nous nommerons H, l’espace de recherche, ou search space. Or, une incertitude présente chez SESAMm a consisté à contraindre cet espace, trop grand pour évaluer rapidement un modèle.\ndu réseau de neurones ainsi que des ressources GPU utilisées.\n\nPour ce qui est du nombre de filtres, il représente le nombre de convolutions apprises par un modèle donné. En effet, reprenant leur fonction de créateur de variables, un nombre faible exhiberait trop peu de variables et conduirait le modèle à sous-performer. Inversement, un nombre élevé de filtres peut conduire à un sur-apprentissage ainsi qu’à un apprentissage lent. L’une des contraintes porte sur la divisibilité du nombre de filtres par 4 : en effet, les librairies actuelles d’apprentissage profond tirent profit des architectures multicœurs des processeurs.\n\nEn ce qui concerne le projet d’universalisation des indicateurs, les principaux verrous technologiques motivant le projet d’apprentissage d’un algorithme universel d’indicateurs de sentiments sont cités ci-dessous. \nPremièrement, un goulot d’étranglement persiste à la phase d’estimation des modèles : celui de l’annotation. Pour rappel, une campagne d’annotation consiste à choisir des phrases représentatives des cas d’usage du DataLake SESAMm, et à labelliser celles-ci par divers indicateurs, afin d’appliquer par suite un algorithme d’apprentissage supervisé conduisant à un modèle prédictif.\nUne campagne d’annotation présente plusieurs risques et inconvénients :\nelle est coûteuse : en effet, les plateformes de labellisation de donnée proposent des processus coûteux de labellisation. En outre, pour les langues différentes de l’anglais, l’annotation peut être plus coûteuse ;\nelle est subjective : sur une phrase donnée, il peut ne pas y avoir de consensus entre deux annotateurs pour annoter le sentiment de cette phrase comme étant positive, neutre ou négative.\nDe plus, du point de vue des systèmes informatiques sous-jacents, un modèle de sentiments - ou d’émotions - est concrètement exploitable par deux binaires :\nun tokenizer : ce binaire va transformer une séquence de mots en une séquence d’entiers.\nun modèle : une suite de paramètres appliqués à cette séquence de nombres entiers.\nLors de la mise en production, ces deux binaires peuvent s’avérer volumineux. Or, notre infrastructure doit être fiable, sécurisée et efficace. Le volume de ces binaires, multiplié par le nombre de langues traitées, grève la containerisation des productions, c’està-dire la mise en place de systèmes isolés et fiables dans la durée. \n\nEnfin, dans l’optique du transfer learning, nous avons fait le choix d’encoder les informations sémantiques extérieures par le biais des embeddings. Or, pour une langue donnée, le choix d’un embedding pré-entraîné requiert plusieurs semaines de curation et de test pour une tâche spécifique, ainsi qu’un nombre considérable de connaissances linguistiques préalable. Or, pour plus de 250 langues répertoriées au sein du DataLake, il est impossible, dans la pratique, d’effectuer une telle sélection dans un laps de temps limité.\n\nFinalement, au cours des travaux sur SignalReveal, nous  aussi rencontré de nombreuses limitations technologiques. Dans un premier temps, nous devions associer des données basées sur les variations de l'indice du dollar (facteur de risque macroéconomique lié à la classe d'actifs des devises) et les données des industries des importations/exportations (caractéristique fondamentale et économique liée à la classe d'actifs des actions). \nDe plus, pour la modélisation à l’aide d’un algorithme de Machine Learning, nous disposions d’un dataset de 63 industries avec 108 timestamps, ce qui est relativement restreint pour la plupart des algorithmes d’apprentissage  d’autant plus que nous devions en extraire notre set de test.\nDe plus, en ajoutant à ces données les données marchés, nous nous sommes retrouvés avec un très grand nombre de features, dépendantes les unes des autres, ce qui nous a causé des difficultés supplémentaires (notamment un besoin de réduction de dimension). \nAussi, pour la cible de l’algorithme, l'objectif était de transformer les valeurs continues de la cible en une cible binaire qui sera injectée dans un classificateur probabiliste. Il fallait alors évaluer la bonne méthodologie pour ne pas perdre de l’information et qui soit cohérente avec les besoins. \nUne des principales difficultés rencontrées fût la séparation du training set et du test set pour notamment éviter l’over-fitting. Les splits CV classiques qui tirent au hasard parmi toutes les observations échouent lorsqu'ils sont appliqués à des séries chronologiques financières multivariées. Deux observations simultanées dépendent de l'information mutuelle, de sorte qu'un problème de surajustement se produira en raison de la fuite d'information si nous ne garantissons pas que les observations de formation et de test sont indépendantes et identiquement distribuées.\nPour la validation de ces résultats, nous avons observé que le ROC-AUC est une bonne métrique à priori mais ne permet pas d’évaluer les stratégies financières. \nEnfin, la période de validation croisée est un peu problématique car nous affinons notre pipeline de modèles en fonction de cette période pour sélectionner les pipelines ayant les meilleurs scores de validation croisée. Cela peut donc conduire à un biais d'anticipation, car dans un backtesting en temps réel, nous n'aurions pas accès à toutes ces données."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Verrous technologiques et limitations de l’état de l’art** : \n  - Nombreux défis rencontrés, motivant les travaux de R&D.\n\n- **Amélioration des indicateurs ESG** :\n  - Objectif : Définir des indicateurs ESG pour entreprises avec peu de données.\n  - Nécessité de tester plusieurs méthodes pour évaluer leur efficacité.\n  - Problème de corrélation entre indicateurs, les rendant inexploitables.\n  - Identification d'un biais de similarité à corriger par transformation des indicateurs.\n\n- **Recherche thématique et machine learning** :\n  - Modèles supervisés nécessitent des jeux de données annotés.\n  - Absence de jeu de données labellisé sur documents financiers pour SESAMm.\n  - Annotation manuelle théoriquement réalisable, mais pratiquement trop chronophage.\n  - Développement d'algorithmes non-supervisés nécessaire.\n  - Difficulté à différencier les thèmes majeurs à cause de chevauchement dans les documents.\n\n- **Récupération d'informations de bases de données** :\n  - Objectif : Extraire des données de Crunchbase et Wikidata pour guider les clients.\n  - Données mises à jour en continu, nécessitant un algorithme de récupération régulière.\n  - Extraction de données « safe for work » pour éviter contenu frauduleux.\n\n- **Gestion des données et Datalake** :\n  - Plusieurs milliards de documents disponibles pour les clients.\n  - Chaque itération des modèles demande temps et ressources informatiques.\n  - Amélioration du Datalake pour fournir des données prétraitées avec indicateurs calculés.\n  - Problème d'augmentation constante des données, nécessitant des plateformes de calcul spécialisées.\n  - Préparation des données : détermination de la taille maximale des phrases pour uniformité.\n  - Vérification de la capacité des modèles à traiter des langages inconnus lors de l'entraînement.\n  - Problème de temps de calcul, nécessitant des méthodes de simplification des modèles.\n  - Défi : compresser la taille des modèles tout en maintenant les performances.\n\n- **Détection d’entités** :\n  - Blocages lors de la détection d'entités dans des langues inconnues.\n  - Manque de points de comparaison pour évaluer les modèles.\n  - Inhomogénéité de la répartition des entités dans les langues connues.\n  - Développement d'indexes et utilisation de librairies spécialisées nécessaires.\n\n- **Attribution d’un score de qualité** :\n  - Différences linguistiques (français, anglais, portugais vs chinois, japonais) posent un verrou.\n  - Définition de KPI distincts pour chaque famille de langues.\n  - Extension du calcul du score de qualité à divers types de texte.\n  - Tests sur l'influence de l'ajout de textes et redéfinition des KPI sur le score.\n\n- **Détection de controverse** :\n  - Problèmes d'attribution d'un score de controverse à cause de republications d'articles.\n  - Nécessité de regrouper les articles similaires pour optimiser l'analyse.\n  - Intégration d'un score de réputation aux indicateurs ESG soulève des défis de pondération dans le temps pour privilégier les articles récents."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons rencontré de nombreux verrous technologiques et limitations de l’état de l’art. Ces derniers sont à l’origine des travaux de R&D réalisés par nos ingénieurs et chercheurs.\n\nDans un premier temps, nous avons souhaité améliorer la définition des indicateurs ESG pour les entreprises, et plus spécifiquement celles pour lesquelles nous avions un faible volume de données. Il nous a alors fallu définir plusieurs méthodes et tester leurs efficacités respectives et combinées pour définir les indicateurs ESG. Nous nous sommes ensuite confrontés à un problème de corrélation des indicateurs, les rendant inexploitables. Nous avons identifié un biais de similarité que nous avons souhaité gommer en testant plusieurs approches. L’idée sous-jacente visait à transformer les indicateurs afin de supprimer la corrélation qui existait entre eux.\n\nNous avons aussi rencontré des obstacles au cours de nos travaux sur la recherche thématique. En effet, les modèles supervisés de machine learning reposent sur l’utilisation de jeux de données annotés dont les modèles se servent pour réaliser leurs prédictions sur des données qu’ils n’ont encore jamais rencontrées. Dans le cas de SESAMm, un jeu de données labellisé sur des documents en lien avec la finance n’existe tout simplement pas. Au-delà de cette limite, la tâche d’annotation manuelle de notre part était en théorie réalisable, mais inenvisageable en pratique de par l’aspect extrêmement chronophage de la tâche à réaliser. Il nous a donc fallu développer des algorithmes non-supervisés. Un second aléas majeur résidait dans notre capacité à différencier les thèmes majeurs détectés par notre modèle. Nous avons en effet rencontré des problèmes de chevauchement de plusieurs thèmes au sein d’un même document, complexifiant l’extraction des thèmes principaux. \n\nNous avons aussi souhaité récupérer les informations de plusieurs milliers de sociétés à partir des bases de données Crunchbase et Wikidata afin de guider nos clients dans leurs stratégies d’investissement. En revanche, les informations de ces bases de données sont mises à jour continuellement. Il nous a alors fallu créer un algorithme nous permettant de récupérer les données mises à jour de manière régulière. Nous nous sommes aussi confrontés à la nécessité d’extraire des données dites « safe for work » afin que nos clients ne soient pas confrontés à du contenu frauduleux ou modifié de manière malicieuse. \n\nSESAMm tient à sa disposition plusieurs milliards de documents que nos clients peuvent exploiter via nos différents produits. Néanmoins, chaque itération de nos modèles demande du temps et de la ressource informatique. Pour répondre à cette problématique, nous avons souhaité améliorer notre Datalake en fournissant des données prétraitées à partir desquels des indicateurs auraient déjà été calculés. Nous nous sommes confrontés à un soucis d’augmentation constante de nos données, nécessitant l’adoption de plateformes de calcul spécialisées pour les datasets comme le nôtre. Concernant le traitement en lui-même de nos modèles, il nous a fallu réaliser un gros travail de préparation des données. En particulier, il nous a fallu déterminer le seuil de taille maximale des phrases, les modèles ne pouvant traiter que des phrases faisant toutes la même taille. Nos modèles devaient également être applicables sur des langages qu’ils n’avaient jamais rencontré au cours de leur entrainement. Il nous a donc fallu vérifier cette capacité de nos modèles. Nous avons aussi été confronté à un problème de temps de calcul de nos modèles. Compte-tenu des milliards de documents dont dispose SESAMm, un temps de calcul trop long constituait un verrou majeur à l’utilisation massive de nos algorithmes. Nous avons alors expérimenté des méthodes de simplification des modèles, dont le principal obstacle résidait en notre capacité à compresser la taille de nos modèles tout en conservant leurs performances de calcul initiales. \n\nNotre travail autour de la détection d’entités dans une phrase (i.e. Person, Organization, Localization) a fait l’objet de plusieurs points de blocage. D’abord, lorsque des entités doivent être détectées dans un langage que le modèle ne connait pas, nous n’avions pas de point de comparaison pour évaluer le modèle. Aussi, pour les langues connues, les entités que le modèle peut repérer ne sont pas réparties de manière suffisamment homogène. Il nous a alors fallu développer des indexes et utiliser des librairies spécifiquement dédiées à ces problèmes pour les résoudre.\n\nPour ce qui est de l’attribution d’un score de qualité pour nos textes, les différences entre les langues comme le français, l’anglais ou le portugais par rapport à des langues comme le chinois et le japonais constituait un premier verrou. Il nous a alors fallu définir des Key Performance Indicator (KPI) distincts pour ces deux familles de langues afin de prendre en compte leurs spécificités respectives. Par la suite, il nous a fallu étendre le calcul du score de qualité à différents types de texte. Dans le but d’améliorer la prédiction du score de qualité, nous avons testé dans quelle mesure l’ajout de textes et/ou la redéfinition des KPI influençaient ce score. \n\nConcernant nos travaux portant sur la détection de controverse, nous nous sommes heurtés à des verrous sur l’attribution d’un score de controverse. En effet, de nombreux articles, traités par notre algorithmes, sont une simple republication d’un article initial. Plusieurs occurrences du même article et du même texte sont ainsi traitées, et il nous a fallu trouver une méthode de regroupement de ces articles afin d’optimiser leurs analyses. L’intégration d’un score de réputation aux indicateurs ESG a également soulevé le verrou de pondération des articles au cours du temps afin que les articles les plus récents aient le plus de poids dans notre calcul."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Analyse d'Imagerie Médicale**  \n  - Traitement de grands volumes de données nécessaires pour un diagnostic fiable.\n  - Adaptation des algorithmes de deep learning pour :\n    - Paralléliser et optimiser le traitement entre un cluster Spark de CPU et un GPU.\n    - Réduire le nombre d'initialisations des modèles de deep learning sur le GPU pour améliorer les performances.\n\n- **Spectrométrie de Masse**  \n  - Chaque échantillon possède des milliers de descripteurs significatifs, nécessitant un traitement sur plusieurs machines.\n  - Importance du prétraitement des données de masse pour :\n    - Homogénéiser les formats.\n    - Effectuer des calculs pertinents.\n  - Nécessité de transformer rapidement les données pour un traitement parallélisé efficace.\n\n- **Outils de Big Data**  \n  - Outils comme Hadoop et bases de données comme HBase conçus pour un usage hors-ligne.\n  - Inadéquation pour le calcul en ligne, entraînant des temps de réponse insatisfaisants pour le traitement en temps réel.\n  - Besoin de développer des modules pour adapter ces technologies à :\n    - Traitement en temps réel des informations.\n    - Mise à jour des données clients et calcul d'agrégations pour détecter et prédire des comportements frauduleux.\n\n- **Évolution des Données Produits en Retail**  \n  - Les données produits changent fréquemment, rendant difficile l'apprentissage des algorithmes de deep learning.\n  - Nécessité de :\n    - Éviter le réapprentissage complet des modèles prédictifs.\n    - Réutiliser l'apprentissage sur l'ancien jeu de données pour l'adapter au nouveau.\n  - Conception d'un algorithme pour :\n    - Reclassifier les nouvelles données produits.\n    - Assurer une réutilisation pertinente de l'apprentissage passé."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans le cadre de nos travaux de R&D nous avons été confrontés à un ensemble d’incertitudes scientifiques et de verrous techniques, que nous citons dans la suite :  \nL’analyse d’imagerie médicale implique de traiter un grand volume de données. Si un médecin peut tirer une conclusion en étudiant rapidement un échantillon, une machine doit, pour obtenir un diagnostic fiable, traiter des centaines de milliers de points de cet échantillon. Il va donc falloir adapter nos algorithmes de deep learning pour paralléliser et optimiser le traitement de ces données entre un cluster Spark de CPU et un GPU. En particulier, il faudra réduire le nombre d’initialisations des modèles de deep learning nécessaires sur le GPU pour augmenter significativement les performances de notre solution ;  \nDans le cas de la spectrométrie de masse, le problème de la volumétrie est du au fait que chaque échantillon possède des milliers de descripteurs significatifs, ce qui implique de traiter chaque échantillon sur plusieurs machines. La spécificité des données manipuler implique un prétraitement des masses obtenues par spectrométrie afin d’homogénéiser les formats et de faire des calculs qui ont du sens. Il faut donc pouvoir transformer rapidement ces données pour les manipuler aisément et permettre, enfin, un traitement parallélisé rapide et performant ;  \nEnfin, les outils de Big Data existants, comme Hadoop, et les bases de données usuelles, comme HBase, sont conçus pour être utilisés hors-ligne. Ces outils ne sont pas adaptés au calcul en ligne et ne fournissent pas de temps de réponses satisfaisants pour du calcul en temps réel. C’est pourtant un aspect essentiel, notamment dans le cas de la lutte contre la fraude en ligne. Nos algorithmes doivent traiter en temps réel un grand nombre d’informations pour mettre à jour les données clients et calculer des agrégations de données importantes permettant de détecter et prédire les comportements frauduleux. Il nous faudra donc développer des modules permettant d’adapter ces technologies à notre besoin de traitement en temps réel ;  \nEnfin, certains des cas réels auxquels nous sommes confrontés, comme le retail, voient leurs données produits constamment évoluer. Les algorithmes de deep learning nécessitent un grand nombre de données dans leur phase d’apprentissage, et nos modèles de prédiction ne peuvent pas suivre des changements trop fréquents de ces données d’apprentissage. Il faut donc trouver un moyen de ne pas systématiquement réapprendre entièrement le modèle prédictif, mais plutôt de réutiliser l’apprentissage sur l’ancien jeu de données pour l’adapter au nouveau jeu de données, qui est souvent assez proche (produits d’une même gamme, etc.). Il nous faudra donc concevoir un algorithme permettant de reclassifier les nouvelles données produits pour les faire correspondre aux nouvelles, et ainsi réutiliser de façon pertinente l’apprentissage passé."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### Développement de nouvelles fonctionnalités au sein de la plateforme Gargantua\n- Solution sous forme d’API pour soumettre des documents et obtenir des données structurées et résultats de contrôles.\n- Doit répondre à des demandes en temps réel pour intégration sur des sites Internet grand public.\n- Capacité de traitement : documents de plusieurs pages (jusqu’à 100) en moins de 15 secondes.\n- Étapes de traitement :\n  - Extraction des pages PDF sous forme d’images.\n  - Rotation et redressement des images (deskew).\n  - Nettoyage des images (amélioration de la netteté, suppression du bruit, adaptation de la résolution).\n  - Extraction de type OCR.\n  - Structuration des données.\n  - Contrôle sur les données.\n- Techniques classiques entraînent des temps de traitement de plusieurs dizaines de secondes par page, nécessitant la parallélisation des calculs pour réduire le temps total.\n\n#### Développement de nouvelles fonctionnalités au sein de la plateforme Zeus\n- TMS (Système de Gestion de Transport) conçu pour superviser divers types de systèmes.\n- Difficultés rencontrées lors de la conception :\n  - Hétérogénéité des domaines métier, évitant une définition ontologique trop complexe.\n  - Évolution des systèmes avec ajout/suppression de propriétés, nécessitant l'évitement des langages orientés objet ou à typage fort.\n  - Organisation hiérarchique des éléments des systèmes, devant être conservée dans le design.\n  - Multiples acteurs utilisant le système, nécessitant des perspectives personnalisées.\n- TMS doit gérer des volumes de données générés par un bâtiment (centaines d'instruments et capteurs, milliers de points de mesure).\n- Incertitude scientifique concernant l’exécution des règles lors de la réception de messages, introduisant une latence excessive et ralentissant les débits.\n\n#### Réalisation d’une étude de stockage et de requêtage massif de données\n- Première solution : stockage brut en cluster Hadoop, inutilisable par les employés en raison de la lenteur de l’upload d’images et de la recherche de maximums d'intensité.\n- Dégradation des performances constatée lors d’utilisations simultanées par plusieurs utilisateurs.\n- Limite technique : téléchargement de spectres limité par la saturation de la ligne réseau à 1 Gb/s.\n- Recherche sur l’application des réseaux de neurones à convolution auto-encodeur a révélé des travaux sur la détection de membranes/cellules, mais aucune application au requêtage massif de tissus d'animaux, représentant un aléas et verrou technique significatif."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nDéveloppement de nouvelles fonctionnalités au sein de notre plateforme Gargantua\nCette solution se présente sous la forme d’une API à laquelle il est possible de soumettre un ensemble de documents, et qui renvoie les données structurées ainsi que le résultat des contrôles. Notre solution doit être en mesure de répondre à des demandes en temps réel de manière à pouvoir intégrer les fonctionnalités de contrôle de document sur des sites Internet grand public. \n\nAinsi, notre solution doit être en mesure de traiter un document de plusieurs pages, jusqu’à une centaine, en moins de 15 secondes. Le traitement comprend les étapes suivantes : \nExtraction de toutes les pages du PDF sous forme d’images ;\nRotation et redressement de l’image (« deskew » de l’image) ;\nNettoyage de l’image (améliorer la netteté, enlever le bruit, adapter la résolution, etc.) ;\nExtraction de type OCR ;\nStructuration des données ; \nContrôle sur les données. \nL’utilisation de techniques classiques (normalisation des documents, amélioration du contraste, redressement, etc.) implique des temps de traitement de plusieurs dizaines de secondes par page de document. Ces approches étant séquentielles, il faut parfois plusieurs minutes pour traiter un document : la nécessité de parallélisation des calculs s’impose donc. \nDéveloppement de nouvelles fonctionnalités au sein de notre plateforme Zeus\nLe TMS est prévu pour pouvoir superviser n’importe quel type de systèmes. \nLors de sa conception, nous avons été confrontés aux difficultés suivantes :\n \nLes domaines métier des systèmes sont hétérogènes. Il fallait donc éviter de rentrer dans une définition « ontologique » du domaine qui serait trop lourde. \nLes systèmes évoluent dans le temps : ajout / suppression de nouvelles propriétés brutes ou dérivées à tout moment. Nous devons donc éviter les langages orientés objet ou à typage fort. \nLes systèmes sont composés d’éléments organisés hiérarchiquement. Nous avons donc dû conserver cette organisation dans le design des systèmes. \nEn complément, les systèmes sont « utilisés » par plusieurs acteurs. Nous devons donc permettre à chacun de ces acteurs de voir le système avec sa propre perspective. \n \nPar ailleurs, le TMS doit supporter des volumes générés par un bâtiment. Un bâtiment est composé de centaines d’instruments et capteurs qui correspondent à des milliers de points de mesure. Toutefois, nous avons remarqué que le calcul des propriétés dérivées et l’exécution des règles métier à chaque réception de message introduisaient une latence trop importante et ralentissait les débits. \nNous avons donc eu une incertitude scientifique face à l’exécution des règles de la réception de messages. \nRéalisation d’une étude de stockage et de requêtage massif de données\nNotre première solution était un stockage brut en cluster Hadoop. Cependant, cette solution était inutilisable par les employés de notre client. L’upload d’images et la recherche de maximums d'intensité sur une partie d’une image prenait trop de temps.\n\nDe plus, étant donné que le projet était destiné à être utilisé par plusieurs utilisateurs en même temps, nous avons été confrontés à la dégradation des performances en utilisation simultanée. \n\nUne autre limite technique importante à laquelle nous avons fait face était liée au téléchargement de spectres qui était limité par la saturation de la ligne réseau à 1Gb/s.\n\nFinalement, dans le cadre de nos recherches sur l’état de l’art de l’application des réseaux de neurones à convolution auto-encodeur, nous avons trouvé des travaux dans le cadre de la détection de membranes ou de cellules, mais jamais appliqués au requêtage massif de tissus d'animaux, ce qui consistait donc un aléas et verrou technique fort."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Travaux de Recherche et Développement (2020)** :\n  - Poursuite des travaux sur les plateformes Gargantua et Zeus.\n  \n- **Plateforme Gargantua** :\n  - Objectif : Créer des fonctionnalités pour :\n    - Déterminer automatiquement le type de documents en entrée.\n    - Segmenter et traiter individuellement les documents pour :\n      - Extraire des informations textuelles et visuelles.\n      - Réorganiser les informations.\n      - Appliquer des contrôles d’authenticité pour détecter la fraude.\n  - Performance requise : Traiter jusqu’à 100 pages en moins de 15 secondes.\n\n- **Plateforme Zeus** :\n  - Type : Thing Management System (TMS).\n  - Objectif : Gérer des données volumineuses et effectuer des calculs prédictifs en Machine Learning.\n  - Fonctionnalités : Affichage et exploitation des données en temps réel et de manière interactive via un outil de dataviz.\n  - Performance requise : Supporter un grand volume de données avec une latence minimale.\n\n- **Problématiques liées à l’Apprentissage Automatisé** :\n  - Complexité et évolution de la langue et des métiers.\n  - Variabilité des dénominations pour un même métier ou compétence, parfois sans mots communs.\n  - Nécessité de différencier :\n    - Nouvelles compétences des anciennes.\n    - Algorithme requis pour identifier des similarités entre compétences et métiers, même sans mots communs.\n\n- **Détection Automatique de Blanchiment d’Argent** :\n  - Limitations technologiques rencontrées :\n    - Problème d’apprentissage avec des classes déséquilibrées (blanchiment représentant un faible pourcentage des données).\n    - Tendance des modèles à prédire la classe dominante (flux légitimes).\n  - Approche utilisée :\n    - Combinaison de techniques d’IA :\n      - Apprentissage supervisé : Support Vector Machine, réseaux de neurones récurrents (LSTM).\n      - Apprentissage non-supervisé : Auto-encodeurs.\n  - Résultat : Identification de 90% des cas avérés de blanchiment à partir de quelques milliers de cas suspects sur 4 ans d’historique.\n\n- **Traitement de Données Instantanées pour le Routage des Mails** :\n  - Division des travaux en trois modules :\n    - Analyse des emails en langage naturel.\n    - Extraction des noms et prénoms.\n    - Affectation de documents à un compte client.\n  - Difficultés rencontrées :\n    - Diversité des formulations dans les emails (vocabulaire commun, technique, juridique, spécifique aux offres).\n    - Besoin de spécialisation de l’analyse du contenu.\n    - Prise en compte de l’historique de conversation et du langage « parlé » (fautes d’orthographe, approche phonétique).\n    - Absence de format standardisé pour les pièces jointes, souvent regroupées sous forme de fichiers."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques et verrous technologiques\n\nAu cours de nos travaux de recherche et développement de l’année 2020, nous avons notamment continué les travaux sur les plateformes Gargantua et Zeus. Dans le cadre de ces travaux, dont les objectifs, étaient, pour la plateforme Gargantua, de créer des fonctionnalités permettant de déterminer automatiquement le type d’une suite de documents en entrée, de les segmenter et les traiter individuellement pour extraire des informations textuelles, visuelles, les réorganiser, et appliquer des contrôles d’authenticité pour détecter une éventuelle fraude. La plateforme Zeus, quant à elle est une plateforme de Thing Management System (TMS) permettant d’avoir une vue complète sur un système à superviser. Elle devrait permettre de gérer très simplement des données extrêmement volumineuses, de faire des calculs prédictifs du domaine du Machine Learning sur ces données, et de les afficher et les exploiter en temps réel et de manière interactive dans un outil de dataviz. Dans le cadre de ces deux plateformes, nous avons mobilisé nos équipes de recherche et développement pour développer de nouvelles fonctionnalités complexes tout en ayant un temps d’exécution moindre. La plateforme Gargantua devant être en mesure de traiter un document de plusieurs pages, jusqu’à une centaine, en moins de 15 secondes, et la plateforme Zeus, devant déjà supporter un très grand nombre de données tout en évitant une latence trop importante. \n\nDans le cadre du projet de regroupement de métier et de compétences par apprentissage automatisé, nous avons rencontré de nombreuses problématiques liées à la complexité de la langue mais aussi à l’évolution de la langue et des métiers. En effet, que ce soit pour décrire un même métier ou une même compétence, plusieurs dénominations sont possibles, avec parfois aucuns mots en communs. De plus, de nouvelles compétences apparaissant régulièrement, il fallait également pouvoir faire la différence entre une expression désignant une \"nouvelle compétence\" et une expression nouvelle qui désigne une \"ancienne compétence\". Le même raisonnement s'applique pour les métiers. Ainsi, il a été nécessaire de créer un algorithme capable de trouver des similarités entre des compétences similaires ou des métiers similaires, et qui soit donc capable de trouver des similitudes entre des expressions n'ayant aucun mot commun.\n\nAussi, pour le traitement de données en temps réel pour la détection automatique de blanchiment d’argent, nous avons été confrontés à des limitations technologiques, principalement quant à l’apprentissage de classes déséquilibrés. En effet, les cas de blanchiment représentant un pourcentage très faible de la totalité de données, ce déséquilibre conduit souvent à une mauvaise prédiction. Les modèles usuels auront toujours tendance à prédire la classe dominante, par exemple ici les flux légitimes. Nous avons ainsi dû combiner plusieurs techniques d’IA par apprentissage supervisé (un algorithme d'apprentissage de type support vector machine, les réseaux de neurones récurrents de type Long-short Term Memory) et non-supervisés (réseaux de neurones de type auto-encodeurs) afin d’identifier des opérations à risque de blanchiment. Cette approche de combinaison de différents modèles a permis de valider sur l’historique de 4 ans d’opérations de Oney et d’identifier 90% des cas avérés de blanchiment en ne remontant que quelques milliers de cas suspects, ce qui constitue une avancée décisive dans le processus. \n\nEnfin, dans le cadre des traitements de données instantanées pour le routage des mails, nous avons divisé les travaux en trois modules : l’analyse des emails en langage naturel, l’extraction des noms et prénoms et l’affectation de document à un compte client. Les difficultés rencontrées sont liées à la diversité des formulations utilisées dans les emails (combinant à la fois du vocabulaire commun, du vocabulaire technique issu du domaine métier de la banque/assurance ou même juridique, et du vocabulaire spécifique lié aux offres du client) impliquant une spécialisation de l’analyse du contenu des email,  la prise en compte de l’historique de conversation, le langage « parlé », avec des fautes d’orthographe et une approche phonétique du langage rendant l’usage d’approches classiques de NLP impossible. De plus, les pièces jointes n’ont pas de format standardisé, et sont fréquemment envoyées regroupées sous forme de fichiers."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif du Projet**: Automatisation de la plateforme de mise en relation entre voituriers et voyageurs.\n\n#### Questions et Défis Abordés\n\n1. **Estimation du Nombre de Voituriers Nécessaires**:\n   - Difficultés liées à l'enchaînement des courses.\n   - Modélisation de la durée globale d'une course selon le type de mission et le lieu.\n\n2. **Organisation des Courses**:\n   - Problématiques d'optimisation des tournées des voituriers.\n   - Complexité accrue due à la multitude de parkings disponibles.\n   - Couplage avec l'allocation automatique des places de parking via un algorithme.\n\n3. **Choix d'une Place de Parking**:\n   - Problématique de matching entre offre et demande complexes.\n   - Complexité du matching couplée à l'optimisation des tournées.\n\n4. **Estimation de l'Heure de Rencontre entre Voiturier et Voyageur**:\n   - Difficulté principale : modélisation du parcours du voyageur, notamment dans les aéroports (de l'atterrissage à la sortie).\n\n5. **Automatisation de la Vérification des Horaires**:\n   - Normalisation des formats de données des horaires de trains et de vols.\n   - Vérification en temps réel des horaires communiqués par le voyageur.\n\n6. **Amélioration de la Stabilité de la Plateforme Ector Parking**:\n   - Objectif : amélioration substantielle de la stabilité via l'utilisation de GraphQL.\n   - Difficultés rencontrées : documentation restreinte et incompatibilités avec les outils de référencement naturel (SEO).\n\n7. **Scalabilité du Système**:\n   - Objectif : rendre le système scalable pour supporter un pic d'affluence.\n   - Expérimentation d'une technologie de serverless computing pour des ressources serveur variables selon les besoins."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNos travaux visaient à adresser différents verrous qui se sont posés pour atteindre notre objectif d’automatisation de la plateforme de mise en relation entre voituriers et voyageurs. Les travaux que nous avons réalisés visaient à répondre aux questions suivantes :\n\nComment estimer le nombre de voituriers nécessaires ?  \nLes difficultés portaient sur l’enchainement des courses et la modélisation de la durée globale d’une course selon le type de mission et le lieu.  \nComment organiser les courses ?  \nLe défi se rapportait à des problématiques d’optimisation des tournées des voituriers. A la difficulté classiquement rencontrée dans ce type de problème s’ajoutait une inconnue supplémentaire du fait de la multitude de parkings disponibles. Ce problème était ainsi couplé au problème d’allocation automatique des places de parking par notre algorithme.  \nComment choisir une place de parking pour une réservation au meilleur prix et générant le moins de perte de temps pour le voiturier ?  \nUne problématique de matching entre une offre et une demande complexes était à adresser ici. Le matching est d’autant plus complexe qu’il est couplé à notre optimisation des tournées.  \nComment estimer de manière précise l’heure de rencontre du voiturier et du voyageur ?  \nNotre principale difficulté portait sur la modélisation du parcours du voyageur, en particulier dans les aéroports, entre l’atterrissage de l’avion et la sortie du voyageur.  \nDe plus, d’autres travaux visaient à automatiser la vérification des horaires de trains et de vols à partir de la référence communiquée par le voyageur : un travail de normalisation du format de ces données puis de vérification en temps réel a été lancé.  \nComment améliorer la stabilité de la plateforme Ector Parking ?  \nNous souhaitons améliorer de manière substantielle la stabilité de notre plateforme en ayant recours à une nouvelle technologie : le graphQL. Cette nouvelle technologie est très récente, la documentation est ainsi très restreinte, notamment sur les bonnes pratiques. Nous avons en particulier rencontré des difficultés pour adresser les incompatibilités de ce langage avec les outils de référencement naturel SEO.  \nComment rendre notre système scalable et en particulier supporter un pic d’affluence ?  \nPour répondre à cet objectif, nous expérimentons une nouvelle technologie de serverless computing qui offre des ressources variables de serveurs selon le besoin."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Objectif du projet** : Automatisation de la plateforme de mise en relation entre voituriers et voyageurs.\n\n- **Développement des algorithmes** :\n  - Création de règles de matching intelligentes pour le staffing et la rémunération des voituriers.\n  - Constitution de regroupements pertinents sous forme de créneaux de travail à proposer aux voituriers.\n\n- **Difficultés rencontrées** :\n  - Problèmes de staffing dus à des missions ajoutées à la dernière minute ou à des changements d'horaires de voyageurs.\n  - Nécessité de déterminer la meilleure association de missions à des créneaux existants en maximisant la marge globale.\n\n- **Amélioration du taux de rétention des voituriers-partenaires** :\n  - Développement d'un nouveau mode de rémunération basé sur la performance et la qualité, influençant les gains des voituriers.\n\n- **Amélioration du service** :\n  - Proposition d'alternatives aux clients en cas d'indisponibilité d'un voiturier au créneau souhaité.\n  - Défi d'atteindre des performances satisfaisantes en termes de temps de réponse pour fournir rapidement des résultats aux clients.\n  - Adoption d'une solution de parallélisation pour améliorer les temps de réponse.\n\n- **Précision des horaires de missions** :\n  - Ajustement des horaires de prise en charge et de restitution des véhicules en fonction des retards des trains et vols.\n  - Difficultés liées à l'hétérogénéité des formats de données et aux écarts entre différentes sources de données.\n  - Développement de tables de recherches exhaustives pour accéder aux données pertinentes."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNos travaux visaient à adresser différents verrous qui se sont posés pour atteindre notre objectif d’automatisation de la plateforme de mise en relation entre voituriers et voyageurs. Lors des travaux de développement des algorithmes de staffing et de rémunération des voituriers, notre principal objectif était de développer des règles de matching intelligentes entre les courses afin de constituer des regroupements pertinents sous forme de créneaux de travail à proposer aux voituriers. Nous avons été confrontés à des difficultés de staffing lorsque des missions s’ajoutent à la dernière minute ou lorsqu’un voyageur décale son heure de rencontre. Notre problématique était alors de déterminer la meilleure façon d’associer ces missions à des créneaux existants, via des considérations de maximisation de la marge globale. En parallèle de ces travaux, nous avons également mené une expérimentation permettant d’améliorer le taux de rétention de nos voituriers-partenaires. Nous avons pour cela développé un nouveau mode de rémunération des voituriers qui mette en avant la performance et la qualité de chacun et qui impacte leurs gains. Un autre axe d’amélioration de notre service visait spécifiquement notre taux de disponibilité. Nous souhaitions proposer une alternative aux clients lorsqu’aucun voiturier n’était disponible au créneau qu’ils recherchaient initialement. Notre principal défi était posé par l’atteinte de performances satisfaisantes en termes de temps de réponse afin de proposer aux clients des résultats rapidement. Nous nous sommes ainsi orientés vers une solution de parallélisation. Enfin, le dernier axe d’amélioration de notre outil visait une amélioration de la précision des horaires de missions, grâce à un ajustement des horaires de prise en charge et de restitution des voitures selon les retards des trains et vols des voyageurs. Nous avons été confrontés à des difficultés liées à l’hétérogénéité des formats de données mais aussi en raison d’écarts existants entre les différentes sources de données. Nous avons ainsi été contraints de développer des tables de recherches exhaustives permettant de remonter aux données pertinentes."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Objectif principal** : Automatisation de la plateforme de mise en relation entre voituriers et voyageurs.\n  \n- **Développement d'algorithmes** :\n  - Création de règles de matching intelligentes pour le staffing et la rémunération des voituriers.\n  - Constitution de regroupements pertinents sous forme de créneaux de travail.\n\n- **Difficultés rencontrées** :\n  - Problèmes de staffing dus à des missions ajoutées à la dernière minute ou à des changements d'horaires par les voyageurs.\n  - Nécessité de maximiser la marge globale lors de l'association de nouvelles missions à des créneaux existants.\n\n- **Amélioration du taux de rétention des voituriers** :\n  - Développement d'un nouveau mode de rémunération basé sur la performance et la qualité, influençant les gains des voituriers.\n\n- **Amélioration du taux de disponibilité** :\n  - Proposition d'alternatives aux clients lorsque aucun voiturier n'est disponible au créneau recherché.\n  - Défi principal : atteindre des performances satisfaisantes en termes de temps de réponse pour fournir rapidement des résultats.\n  - Orientation vers une solution de parallélisation pour améliorer les temps de réponse.\n\n- **Précision des horaires de missions** :\n  - Ajustement des horaires de prise en charge et de restitution des voitures en fonction des retards des trains et vols.\n  - Difficultés liées à l'hétérogénéité des formats de données et aux écarts entre différentes sources de données.\n  - Développement de tables de recherches exhaustives pour accéder aux données pertinentes."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNos travaux visaient à adresser différents verrous qui se sont posés pour atteindre notre objectif d’automatisation de la plateforme de mise en relation entre voituriers et voyageurs. Lors des travaux de développement des algorithmes de staffing et de rémunération des voituriers, notre principal objectif était de développer des règles de matching intelligentes entre les courses afin de constituer des regroupements pertinents sous forme de créneaux de travail à proposer aux voituriers. Nous avons été confrontés à des difficultés de staffing lorsque des missions s’ajoutent à la dernière minute ou lorsqu’un voyageur décale son heure de rencontre. Notre problématique était alors de déterminer la meilleure façon d’associer ces missions à des créneaux existants, via des considérations de maximisation de la marge globale. En parallèle de ces travaux, nous avons également mené une expérimentation permettant d’améliorer le taux de rétention de nos voituriers-partenaires. Nous avons pour cela développé un nouveau mode de rémunération des voituriers qui mette en avant la performance et la qualité de chacun et qui impacte leurs gains. Un autre axe d’amélioration de notre service visait spécifiquement notre taux de disponibilité. Nous souhaitions proposer une alternative aux clients lorsqu’aucun voiturier n’était disponible au créneau qu’ils recherchaient initialement. Notre principal défi était posé par l’atteinte de performances satisfaisantes en termes de temps de réponse afin de proposer aux clients des résultats rapidement. Nous nous sommes ainsi orientés vers une solution de parallélisation. Enfin, le dernier axe d’amélioration de notre outil visait une amélioration de la précision des horaires de missions, grâce à un ajustement des horaires de prise en charge et de restitution des voitures selon les retards des trains et vols des voyageurs. Nous avons été confrontés à des difficultés liées à l’hétérogénéité des formats de données mais aussi en raison d’écarts existants entre les différentes sources de données. Nous avons ainsi été contraints de développer des tables de recherches exhaustives permettant de remonter aux données pertinentes."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Automatisation du Staffing des Voituriers**\n  - Confrontation en 2019 aux absences des voituriers durant leurs créneaux.\n  - Restrictions juridiques sur le statut d’indépendant limitant l’intervention.\n  - Nécessité d’assurer la précision et la performance de la solution pour des alertes rapides.\n\n- **Recherche de Créneaux Alternatifs**\n  - Poursuite des efforts de 2018 pour développer une solution de recherche de créneaux alternatifs.\n  - Développement d'un algorithme pour sonder rapidement les plannings des voituriers.\n  - Objectif : afficher la disponibilité en quasi-temps réel pour les horaires choisis.\n  - En 2019, ajout de la recherche de créneaux alternatifs autour de l’horaire initial.\n  - Nécessité de paralléliser la recherche pour limiter l’impact sur le temps de réponse.\n\n- **Développement d’un Outil de Vérification des Horaires des Vols**\n  - Recherche continue depuis 2017 d'une solution pour recevoir en temps réel les mises à jour des vols.\n  - Objectif : assurer la présence du voiturier à la sortie du voyageur sans attente excessive.\n  - Modélisation du parcours des voyageurs dans divers aéroports pour optimiser les missions des voituriers.\n  \n- **Complexité des Données Aériennes**\n  - Disponibilité des données ferroviaires via une base de données ouverte, mais complexité des données aériennes.\n  - Problèmes d’hétérogénéité des processus des compagnies aériennes :\n    - Vols communs avec plusieurs identifiants.\n    - Incohérences entre le code de vol et le numéro de billet passager.\n  \n- **Changement de Paradigme**\n  - Décision de développer un moteur de recherche interne pour récupérer les données des API des aéroports.\n  - En 2019, prototypage d’un outil de recherche de vols correspondant à un voyage et un horaire, affichant les informations aux voyageurs."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nAutomatisation du staffing des voituriers  \nLa difficulté à laquelle nous avons été confrontés en 2019 est liée aux absences des voituriers lors des créneaux qu’ils sont censés assurer. Les restrictions juridiques liées au statut d’indépendant limitaient notre champ d’action. Ainsi, une contrainte de non-intrusion s’est exercée sur notre approche. Nous devions toutefois nous assurer de la précision et de la performance de la solution afin qu’elle nous transmette une alerte dès que possible.  \nRecherche de créneaux alternatifs  \nCes travaux s’inscrivent dans la continuité des efforts consentis en 2018 pour le développement d’une solution de recherche de créneaux alternatifs lorsqu’aucun voiturier n’est disponible. Nous souhaitions en effet développer un algorithme permettant de sonder, très rapidement, les plannings des voituriers et identifier une disponibilité pour proposer un créneau aux voyageurs. Le temps de réponse de cette solution a été à l’origine de nombreux défis car nous souhaitons afficher en quasi-temps réel la disponibilité ou non à la date et l’horaire choisis. En 2019, nous avons souhaité poursuivre nos efforts en proposant des créneaux alternatifs autour de l’horaire initialement sélectionné. Afin de limiter l’impact de cette recherche sur le temps de réponse, nous devions paralléliser cette recherche.  \nDéveloppement d’un outil de vérification des horaires des vols  \nDepuis le démarrage de nos travaux en 2017, nous avons été continuellement à la recherche d’une solution permettant de recevoir en temps réel toutes les mises à jour concernant les vols de nos clients voyageurs afin de s’assurer que le voiturier soit là à la sortie du voyageur mais qu’il ne l’attende pas trop longtemps (optimisation du temps d’attente).  \nAprès un travail de modélisation du parcours des voyageurs dans les différents aéroports dans lesquels nous sommes présents, nous avons pu développer des modèles qui nous ont permis d’optimiser les regroupements de missions des voituriers et la planification de leurs créneaux de travail.  \nSi les données ferroviaires liées aux horaires des trains sont disponibles via une base de données ouverte, les données aériennes sont plus complexes. En effet, nous avons été confrontés à des aléas posés par l’hétérogénéité des processus des compagnies aériennes. Nous pouvons notamment citer les vols en commun entre plusieurs compagnies aériennes qui peuvent avoir de multiples identifiants, ou encore des vols dont le code et le numéro de billet passager ne concordent pas.  \nFace à l’ensemble de ces écarts, nous avons été confrontés au besoin de changer de paradigme. Nous avons ainsi décidé d’appréhender notre problématique via le développement d’un moteur de recherche interne à notre plateforme qui récupère les données issues des API des aéroports, plutôt que les informations de vols issues des compagnies aériennes. En 2019 nous avons ainsi prototypé un outil permettant de rechercher les vols correspondant à un voyage et un horaire et d’afficher l’ensemble aux voyageurs."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques et verrous technologiques\n\n#### Custom Progress\n- **Efficacité de l'apprentissage mobile** : Les travaux existants montrent l'intérêt de l'apprentissage sur mobile, mais ne traitent pas de la personnalisation du parcours pour maintenir l'intérêt de l'apprenant.\n- **Incertitudes scientifiques** : Difficulté à prédire le comportement des utilisateurs lors d'interactions sur mobile.\n- **Objectif** : Concevoir un système motivant pour encourager la persévérance des apprenants dans leur formation via un système de score.\n- **Mesure des connaissances** : Nécessité de modéliser le niveau de connaissance des apprenants sur différents sujets de formation.\n- **Connaissance de la mémoire** : Compréhension du fonctionnement de la mémoire essentielle pour maximiser le potentiel d'apprentissage.\n- **Modèle de suivi** : Développement d'un modèle permettant de mesurer les connaissances et de définir des parcours personnalisés pour chaque apprenant.\n\n#### Segmentation des apprenants\n- **Système de segmentation** : Doit être basé sur une logique business configurable par l'utilisateur, sans complexité visible pour l'apprenant.\n- **Finalité** : Filtrer le contenu des cours et jeux en fonction des apprenants.\n- **Performance requise** : Système doit fonctionner en temps réel sur une base de données PostgreSQL avec des requêtes traitées en moins de 300 ms.\n- **Complexité des requêtes** : Création de requêtes à n jointures sur des tables à m entrées.\n- **Versatilité et évolutivité** : Système doit s'adapter à divers types d'organisations (hiérarchiques, matriciels, hybrides) et évoluer avec les changements organisationnels des clients.\n- **Déploiement** : Système doit pouvoir être déployé sans changement de code source et être configurable par le client depuis le back office.\n\n#### Génération automatique de contenus\n- **Approches de résumé automatique** : Deux méthodes - abstractive (via TextSum) et extractive.\n- **Méthode abstractive** : Modèle open source non entraîné ; nécessite plus d'un million d'itérations pour des résultats satisfaisants. Google Brain a utilisé 7000 heures GPU pour un niveau de performance acceptable, ce qui n'est pas viable pour le projet.\n- **Approche extractive** : Librairies existantes uniquement en Python, alors que l'infrastructure est en JavaScript.\n- **Développement d'une librairie de NLP** : Nécessité de créer une librairie en JavaScript pour atteindre des performances adéquates, représentant un défi en conception et programmation.\n- **Qualité des textes** : Assurer que la qualité des textes générés est suffisante."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nCustom Progress\nLes travaux existants concernant l’apprentissage sur mobile attestent de l’efficacité et l’intérêt de ce paradigme, mais n’étudient pas la meilleure façon de conserver l’intérêt d’un apprenant à travers une expérience mobile, par exemple en personnalisant son parcours pour le rendre plus intéressant. Les incertitudes scientifiques tiennent dans la difficulté à prédire le comportement de l’utilisateur en fonction d’une interaction sur mobile. Notre but est donc de concevoir un système capable de motiver les apprenants à persévérer dans le jeu afin de poursuivre leur formation. Un système de score pourrait stimuler l’utilisateur au cours de sa formation et l’inciter à persister. \n\nEn particulier, nous souhaitons mesurer et modéliser précisément le niveau de connaissance des apprenants dans les différents sujets des formations qu’ils suivent. Une bonne connaissance du fonctionnement de la mémoire est nécessaire afin de maximiser le potentiel d’apprentissage de la solution. Pouvons-nous alors concevoir un modèle de suivi de l’apprentissage des utilisateurs permettant de mesurer leurs connaissances et de définir un parcours personnalisé à chaque individu ?  \nSegmentation des apprenants \nNous devons délivrer un système de segmentation basé sur une logique business dont la complexité est entièrement configurée par l’utilisateur. Cependant, cette complexité ne doit pas être visible de l’utilisateur final (l’apprenant).\n\nLa finalité de notre système de segmentation est donc de filtrer des ensembles d’entités, typiquement le contenu des cours et jeux en fonction des apprenants. Notre système doit donc en particulier être suffisamment performant pour fonctionner en temps réel sur une base de données PostgreSQL. Il nous faudra créer une requête à n jointures sur des tables a m entrées. Ces requêtes devront être traitées dans un délai inférieur à 300ms afin d'être considérées comme effectuées en temps réel.\n\nLa difficulté de concevoir un système de segmentation qui soit assez versatile pour s’adapter à tous types d’organisations (hiérarchiques, matriciels ou hybrides), assez évolutifs pour accompagner les changements d’organisations de nos clients et assez complet pour pouvoir coller à toutes les combinaisons possibles en termes de ciblage utilisateurs.  \nLe système doit en outre pouvoir être déployé dans changement dans le code source et configurable directement par le client depuis le back office. \nGénération automatique de contenus\nComme nous l’avons présenté plus haut, il existe deux approches pour résumer automatiquement un texte : la méthode abstractive et la méthode extractive. La méthode abstractive peut notamment être implémentée en utilisant la librairie TextSum. Le modèle proposé est disponible en open source mais n’est pas entrainé, l’équipe de TensorFlow recommandant de l’entraîner sur plus d’un million d’itérations pour avoir des résultats satisfaisants. L’équipe Google Brain a utilisé l’équivalent de 7000 heures GPU pour atteindre un niveau de performance acceptable. Cette approche n’est donc pas envisageable dans le cadre de notre projet. \n\nLes librairies existantes pour implémenter l’approche extractive n’existent qu’en Python. Notre infrastructure complète étant codée en JavaScript, nous avons choisi de développer notre propre librairie de NLP pour obtenir des performances suffisantes dans ce langage. Cela constitue un défi important à relever en termes de conception et de programmation. Par ailleurs, il faut également s’assurer que la qualité des textes produits est suffisante."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Identification des Verrous Technologiques :**\n  - Plusieurs verrous technologiques et incertitudes ont été identifiés en lien avec les thématiques du projet.\n\n- **Génération Automatique de Questions :**\n  - Objectif : Fournir une solution générant automatiquement des questions à partir d’un texte.\n  - Problématique : Les clients fournissent des textes riches en connaissances métier.\n  - Difficulté : Construire une question à partir d’un texte contenant des éléments de réponse pour tester les connaissances des utilisateurs.\n  - État actuel : Aucune solution documentée dans la littérature scientifique pour cette tâche.\n\n- **Recherche d’Images à partir de Mots-Clés :**\n  - Besoin client : Recherches d’images régulières pour la création d'infographies.\n  - Problèmes rencontrés : \n    - Processus de recherche fastidieux et chronophage.\n    - Images obtenues souvent non conformes aux attentes.\n  - Solution envisagée : Développer un système de recherche d’images à partir d’un texte.\n  - Verrou identifié : Textes généralement courts rendant l’identification des mots-clés pertinents complexe.\n  - Actions entreprises : \n    - Travail sur l’identification des mots-clés les plus pertinents.\n    - Implémentation de l'algorithme dans des bases de données d’images (Google Images, Unsplash).\n\n- **Refonte de l’Architecture Back-Office :**\n  - Objectif : Créer une architecture réseau hexagonale.\n  - Problématique : Librairies utilisées éditées par des tiers, avec risques de mises à jour ou d’arrêt de support.\n  - Incertitudes rencontrées : \n    - Capacité à découpler le code métier des librairies externes.\n  - Verrous infrastructurels : \n    - Nécessité de lancer plusieurs entités sur le serveur (entité métier et entité pour autres applications).\n    - Impact sur la base de données : Nécessité de maintenir à jour deux entités au lieu d’une seule.\n  - Exigence : L’application SPARTED doit rester disponible pour les utilisateurs durant le processus de changement d’architecture réseau."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons identifié plusieurs verrous technologiques et incertitudes en lien avec les thématiques que nous venons de présenter.\n\nDans un premier temps, nous souhaitions fournir à nos clients une solution générant de manière automatique des questions à partir d’un texte. Nos clients nous fournissent des textes remplis de connaissances en lien avec les données métier de leur entreprise. Si la création de différentes réponses à partir d’une question est relativement simple à réaliser, notre verrou s’est porté sur l’étape inverse. Il nous fallait, à partir d’un texte contenant des éléments de réponse, construire une question permettant de tester les connaissances des utilisateurs sur une thématique donnée. A ce jour, aucun élément de la littérature scientifique ne permet de réaliser cette tâche. \n\nDans un second temps, la recherche d’image à partir de mots-clés résulte d’un besoin client. Ces derniers avaient en effet besoin d’effectuer des recherches d’images de manière régulière afin de réaliser des infographies. Ces recherches étaient fastidieuses et chronophage, et les images obtenues par nos clients n’étaient pas celles attendues. Nous avons donc souhaité développer un système de recherche d’images à partir d’un texte. Le verrou que nous avons rencontré réside dans le fait que les textes fournis sont généralement courts et l’identification des mots-clés les plus pertinents était donc complexe. Nous avons donc travaillé sur l’identification des mots-clés les plus pertinents et sur l’implémentation de notre algorithme dans des bases de données d’image tels que Google Images et Unsplash.\n\nUn troisième objectif était la refonte de notre architecture back-office. Les librairies que nous utilisons sont éditées par des personnes externes à SPARTED. Les mises à jour de ces librairies voire l’arrêt de leurs mises à jour pourraient alors compromettre le fonctionnement du serveur en l’état actuel. C’est pourquoi nous avons souhaité créer une architecture réseau hexagonale. Les incertitudes que nous avons rencontrées résidaient en notre capacité à découpler le code métier des librairies utilisées. Aussi, des verrous en termes d’infrastructure ont été identifiés puisque nous avions besoin de lancer plusieurs entités sur notre serveur : une entité métier et une entité pour les autres applications. Cela impactait directement le fonctionnement de notre base de données puisque ces deux entités devaient être maintenues à jour au lieu d’une seule auparavant. Enfin, l’application SPARTED devaient également rester disponible auprès de nos utilisateurs tout au long du processus de changement d’architecture réseau."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Verrous Identifiés :**\n  - Accès limité aux métadonnées du texte extrait, rendant difficile la détermination précise des attributs (taille, police, effets appliqués aux caractères).\n  - Détection compliquée des zones de titre, affectant la délimitation des sections du contenu.\n  - Fiabilité des tailles des boîtes délimitant les caractères : certaines lettres affichent des boîtes de taille différente de la taille réelle.\n  - Précision limitée dans la détection des titres, surtout pour les caractères petits ou les titres répartis sur plusieurs paragraphes.\n\n- **Utilisation du Modèle Davinci d'OpenAI :**\n  - Coût élevé et limite de longueur du texte pouvant être traité en une seule fois.\n  - Nécessité de solutions spécifiques pour gérer les textes trop longs, afin d'assurer la cohérence du contenu de formation en sortie du modèle.\n\n- **Développement d'un Système de Sauvegarde et Gestion des Conflits de Version :**\n  - Défis techniques liés à la création et à la validation des commandes.\n  - Complexité du processus de vérification des commandes pour garantir la validité du parcours.\n  - Importance de l'identification et de la résolution des conflits potentiels entre différentes commandes pour maintenir l'intégrité des parcours de formation.\n  - Difficultés liées à la gestion des micro-knowledge et à la mise à jour cohérente du contenu, essentielles pour une expérience utilisateur fluide et sans erreur."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons identifié plusieurs verrous technologiques et incertitudes en lien avec les thématiques que nous venons de présenter.\n\nLors du développement du modèle OCR, nous avons fait face à plusieurs verrous. Premièrement, l'accès limité aux métadonnées du texte extrait rendait difficile la détermination précise des attributs tels que la taille, la police ou les effets appliqués aux caractères. Cette limitation a compliqué la détection des zones de titre dans le contenu nous permettant de délimiter les différentes sections. Deuxièmement, la fiabilité de la taille des boîtes délimitant les caractères a posé des problèmes, avec certaines lettres affichant des boîtes légèrement différentes de la taille réelle du caractère. Ces contraintes ont entraîné une précision limitée dans la détection des titres, surtout lorsque les caractères étaient petits ou les titres coupés en plusieurs paragraphes.\n\nNous nous sommes aussi basés sur le modèle de traitement du langage Davinci proposé par OpenAI pour la création de contenu sous format « story ». La principale difficulté rencontrée résidait dans le coût élevé et la limite de longueur du texte pouvant être traité en une seule fois par le modèle. La gestion des cas où le texte était trop long et devait être découpé en parties plus courtes nécessitait des solutions spécifiques pour assurer la cohérence du contenu de formation que nous obtenions en sortie du modèle.\n\nEn ce qui concerne le développement d’un système de sauvegarde des parcours et de gestion des conflits de version, la création des commandes et leur validation ont posé de réels défis techniques. La complexité du processus de vérification des commandes afin de garantir la validité du parcours a nécessité une conception rigoureuse des règles de validation. L'identification et la résolution des conflits potentiels entre les différentes commandes étaient essentielles pour assurer l'intégrité des parcours de formation des utilisateurs. Les difficultés liées à la gestion des micro-knowledge et à la mise à jour cohérente du contenu ont également été un défi à relever pour garantir une expérience utilisateur fluide et sans erreur."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Verrous, Aléas, Incertitudes - Projet DOLPHIN\n\n#### Incertitudes a priori :\n- **Continuité du service** : Assurer le service aux utilisateurs tout en implémentant des changements.\n- **Migration des données** : \n  - Risques de corruption si des technologies comme les ORM changent.\n- **Incompatibilités** : \n  - Risques d'incompatibilités lors de la migration de dépendances obsolètes avec le code existant ou d'autres dépendances.\n- **Validation des choix technologiques** : Nécessité de valider les technologies choisies pour le projet.\n- **Performance** : \n  - Problèmes de performance potentiels liés aux modèles de données complexes et aux requêtes, notamment dans le cadre de défis internes (battles).\n\n#### Aléas (liés aux axes A, B, C des travaux) :\n- **Transverse** : \n  - Manque de spécifications entraînant un rétro-engineering pour comprendre le fonctionnement actuel.\n\n##### A. Suppression de repositories :\n- **Identification** : Déterminer les repositories redondants ou peu utilisés pour fusionner ou supprimer.\n- **Migration** : Envisager une migration vers un repository monolithique pour simplifier la gestion du code.\n- **Adaptation du code** : \n  - Adapter le code pour assurer son bon fonctionnement après la consolidation des repositories, en particulier la gestion des permissions d’accès.\n- **Pipelines CI/CD** : Adapter les pipelines d’intégration continue et de déploiement continu pour gérer les changements dans la structure du repository.\n\n##### B. Suppression de dead features :\n- **Analyse du code** : Identifier les fonctionnalités obsolètes ou non utilisées via une analyse statique ou dynamique.\n- **Suppression** : Retirer les fonctionnalités identifiées tout en préservant le reste de l’application.\n- **Prévention des régressions** : S’assurer que la suppression de code n'introduit pas de régressions ou de nouveaux bugs.\n\n##### C. Refactoring des core features :\n- **Identification des complexités** : Identifier les parties du code complexes, peu lisibles ou fortement interdépendantes.\n- **Spécifications** : Clarifier le fonctionnement des fonctionnalités existantes (ex. : calcul des scores dans les battles).\n- **Stabilisation du data model** : Simplifier le modèle de données pour éviter la corruption des données.\n- **Mise à jour technologique** : Remplacer les technologies obsolètes (ex. : mithril et loopback) par des alternatives modernes et maintenues."}, {"role": "assistant", "content": "Verrous, aléas, incertitudes\n\nDOLPHIN\nIncertitudes a priori :\n\nAssurer la continuité du service pour les utilisateurs tout en faisant les changements\nMigration des Données : Si des technologies comme les ORM (Object-Relational Mapping) changent, il peut y avoir des défis significatifs pour migrer les données sans corruption.\nIncompatibilités : Migrer des dépendances obsolètes peut entraîner des incompatibilités avec le code existant ou d'autres dépendances.\nValider les choix technologiques\n\nPerf sur les sujets de traductions ou battles : modèles de données complexes et requêtes peuvent poser des pb de perf : battles (défi entre collègues sur une série de 5qst puis score)/ on fait 1on1 donc 2 parmi tous les collab, incertitude sur les perf même sur l’existant\n\nAléas (en lien avec les axes A B C des travaux) :\n\nTransverse : Manque de specs donc retro engineering pour savoir comment \n\nA/ Suppression de repositories\nIdentifier les repositories redondants ou peu utilisés pour les fusionner ou supprimer conflits de code potentiels\n\nMigrer vers un repository monolithique pour simplifier la gestion du code ?\n\nAdapter le code pour fonctionner correctement après la consolidation des repositories : en particulier, gestion des permissions d’accès\n\nLes pipelines d’intégration continue et de déploiement continu doivent être adaptés pour gérer les changements dans la structure du repository.\n\nB/ Suppression de dead features\nAnalyser le code pour identifier les features obsolètes ou non utilisées / analyse statique ou dynamique du code pour tracer les utilisations\nSupprimer les features identifiées, en veillant à ne pas casser le reste de l’application.\nS’assurer que la suppression de code n’introduit pas de régressions ou de nouveaux bugs dans le système.\n\nC/Refacto des core features\nIdentifier les parties du code qui sont complexes, peu lisibles, ou fortement interdépendantes : \nSpecs (comme mentionné au-dessus) : comment une fcté est censée marcher, déduire à partir de ce qui existe (calcul des scores était opaque ; calcul dans les battles sur qui a répondu en premier, comment les questions sont choisies, etc.)\nStabiliser et simplifier le data model pour éviter la corruption des données.\nRemplacer les technologies obsolètes (comme mithril et loopback) par des alternatives modernes et maintenues."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Limites des connaissances** : \n  - Présence de lacunes dans la littérature scientifique à plusieurs niveaux.\n\n- **Détection des émotions** :\n  - Absence de méthode consensuelle au sein de la communauté scientifique pour la représentation des signaux audio.\n\n- **Reconnaissance automatique de la parole** :\n  - Manque de méthode consensuelle, entraînant des expérimentations de plusieurs approches.\n  - Considération de la combinaison des approches pour améliorer la fiabilité du module.\n\n- **Difficultés rencontrées** :\n  - Absence de corpus dédiés aux cas d'usage ciblés.\n  - Si un corpus est constitué, l'annotation des données pose des défis.\n  - Nécessité de formuler des hypothèses sur l'approche d'annotation à adopter.\n\n- **Impact de l'annotation** :\n  - La performance des modèles et leur capacité de généralisation dépendent de la qualité des annotations.\n\n- **Travaux expérimentaux** :\n  - Réalisation d'expérimentations sur les jeux de données et sur l'annotation.\n  - Identification de biais dans les modèles, avec un objectif de les éliminer dans les travaux futurs."}, {"role": "assistant", "content": "Limites des connaissances, verrous technologiques, aléas  \nLes limites des connaissances exposées dans la littérature scientifiques se situent à plusieurs niveaux.  \nSur le périmètre de la détection des émotions, nous pouvons citer l’absence de méthode consensuelle au sein de la communauté scientifique quant à la représentation des signaux audio en vue de leur exploitation.  \nPour la reconnaissance automatique de la parole, là encore, l’absence de méthode consensuelle nous a amenés à expérimenter plusieurs approches et nous avons considéré leur combinaison pour fiabiliser notre module.  \nPour l’un et l’autre axes de travail que nous avons menés, nous pouvons citer en les difficultés liées à l’absence de corpus dédiés aux cas d’usage que nous visons, et dans le cas où un corpus peut être constitué, l’annotation des données pose des défis et nécessite de poser des hypothèses quant à l’approche à adopter. En effet, la performance des modèles et leur capacité de généralisation est dépendante de la qualité des annotations. Nous avons pu mener des expérimentations sur les jeux de données et sur l’annotation et ce travail nous a permis de relever des biais dans nos modèles que nos prochains travaux viseront à éliminer."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Limites des connaissances** :\n  - Absence de méthode consensuelle pour la représentation des signaux audio dans la détection des émotions.\n  - Manque de consensus dans la reconnaissance automatique de la parole, nécessitant l'expérimentation de plusieurs approches et leur combinaison pour fiabiliser le module.\n\n- **Difficultés rencontrées** :\n  - Absence de corpus dédiés aux cas d'usage ciblés.\n  - Annotation des données complexe, nécessitant des hypothèses sur l'approche à adopter.\n  - Performance des modèles dépendante de la qualité des annotations.\n\n- **Expérimentations réalisées** :\n  - Analyse des jeux de données et des annotations, identification de biais dans les modèles à corriger dans les travaux futurs.\n\n- **Investigations de 2022** :\n  - Étude approfondie des connaissances sur la transcription, annotation et exploitation des corpus pour la compréhension du langage naturel.\n  - Confrontation aux limites des modèles actuels pour transposer efficacement les données en input pour la compréhension du langage naturel.\n\n- **Incertitudes identifiées** :\n  - Événements ponctuels imprévus lors des échanges, comme l'épellation de noms propres ou de coordonnées de contact (numéro de téléphone, adresse mail).\n  - Nécessité pour le modèle de transcrire ces données avec précision.\n\n- **Défis supplémentaires** :\n  - Difficultés liées aux accents et intonations des locuteurs non natifs.\n  - Détection des intentions dans des situations réelles (non scriptées) sans mots-clés ou phrases prédéfinies, rendant la classification des intentions plus complexe."}, {"role": "assistant", "content": "Limites des connaissances, verrous technologiques, aléas\n\nLes limites des connaissances exposées dans la littérature scientifiques se situent à plusieurs niveaux. Sur le périmètre de la détection des émotions, nous pouvons citer l’absence de méthode consensuelle au sein de la communauté scientifique quant à la représentation des signaux audio en vue de leur exploitation. Pour la reconnaissance automatique de la parole, là encore, l’absence de méthode consensuelle nous a amenés à expérimenter plusieurs approches et nous avons considéré leur combinaison pour fiabiliser notre module. Pour l’un et l’autre axes de travail que nous avons menés en 2021, nous pouvons citer les difficultés liées à l’absence de corpus dédiés aux cas d’usage que nous visons, et dans le cas où un corpus peut être constitué, l’annotation des données pose des défis et nécessite de poser des hypothèses quant à l’approche à adopter. En effet, la performance des modèles et leur capacité de généralisation est dépendante de la qualité des annotations. Nous avons pu mener des expérimentations sur les jeux de données et sur l’annotation et ce travail nous a permis de relever des biais dans nos modèles que nos prochains travaux viseront à éliminer. En 2022, nous avons investigué de manière plus approfondie l’état des connaissances en termes de transcription, annotation et exploitation de ces corpus de textes à des fins de compréhension du langage naturel. Nous avons été confrontés aux limites relevées par la littérature quant aux capacités des modèles actuels à transposer les données de manière performante pour envisager l’utilisation de ces données en input d’un modèle de compréhension du langage naturel. Nous avons également pu établir les incertitudes de la littérature scientifique quant aux évènements ponctuels pouvant se dérouler de manière spontané lors d’un échange tel que celui que nous visons dans le cas d’usage expérimental que nous étudions qui est celui de la prise de rendez-vous dentaires. Des occurrences d’épellation de noms propres ou de coordonnées de contact (numéro de téléphone, adresse mail, etc.) peuvent survenir et notre modèle doit ainsi être capable de transcrire ces données avec justesse. Les accents et intonations de locuteurs non natifs sont également à l’origine de difficultés que nos travaux visent à adresser. Enfin, la détection des intentions pose également des défis dans la littérature scientifique. Dans le contexte de notre étude, nous sommes confrontés à des situations réelles (non scriptées) : nous devons ainsi d’autant plus détecter avec justesse la classification des intentions sans qu’il n’y ait de mots-clés ou de phrases prédéfinies pour nous guider vers le scénario conversationnel auquel rattacher la situation."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous technologiques à surmonter :**\n  - Problème de volumétrie de données : nécessité de grandes quantités de données pour la précision des algorithmes de machine learning.\n  - Intégration de données provenant de diverses sources (INSEE, temps de transport, réseaux sociaux) pour enrichir l'analyse.\n\n- **Normalisation des données :**\n  - Les données collectées ont des formats variés et doivent être normalisées pour être exploitables par un modèle unique.\n  - Capacité du modèle à apprendre à partir des données d'une ville et à les appliquer à d'autres villes ou pays, notamment en zones frontalières.\n\n- **Analyse locale vs globale :**\n  - Conception d'un modèle permettant de passer d'une analyse locale à une analyse globale, en ignorant les différences locales entre villes ou pays.\n\n- **Objectif final :**\n  - Utilisation des données normalisées pour prédire le chiffre d'affaires d'un magasin en fonction de son emplacement.\n  - Catégorisation des populations selon leurs données démographiques et habitudes d'achat.\n  - Corrélation des variables démographiques et comportementales avec les chiffres d'affaires des magasins dans un secteur donné.\n\n- **Capacités du modèle :**\n  - Inférer les variables générales impactant le chiffre d'affaires pour permettre des prédictions précises dans divers contextes."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nPour mener à bien ce projet de recherche, nous devons dépasser un certain nombre de verrous technologiques. Tout d’abord, nos travaux soulèvent un problème important de volumétrie de donnée : les algorithmes de machine learning ont besoin d’énormément de données pour être précis. Nous devons donc être capable d’en récolter suffisamment pour que nos algorithmes puissent effectivement apprendre et inférer des relations entre les variables intéressantes pour notre problématique. Nous disposons de plusieurs sources différentes : l’INSEE, les données sur les temps de transport, etc. Nous devons à terme être capables d’intégrer des informations issues des réseaux sociaux quant aux habitudes de déplacement et d’achats des potentiels consommateurs. Ces données ont donc des formes différentes et nécessitent d’être normalisées pour être exploitées par un seul modèle. En particulier, notre modèle devrait être capable d’apprendre sur les données d’une ville et de les appliquer à une autre, ainsi que de traiter de la même manière les données issues de villes différentes ou de pays différents en zones frontalières. Plus généralement, nous devons donc concevoir un modèle qui permet de passer d’une analyse locale à une analyse globale pour répondre à notre problème en faisant abstraction des différences locales entre des villes ou même des pays. Enfin, notre but est d’utiliser ces données normalisées pour prédire le chiffre d’affaires d’un magasin en fonction de son emplacement. Notre solution doit donc permettre d’une part de catégoriser différents types de population en fonction de leurs données démographiques et de leurs habitudes d’achats, pour dans un second temps corréler ces variables aux chiffres d’affaires des magasins dans un secteur donné. Notre modèle doit donc être capable d’inférer quelles variables générales impactent le chiffre d’affaires pour faire une prédiction précise n’importe où."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Identification des pays à partir des coordonnées géographiques :**\n  - Nécessité d'une source de données sur les frontières, y compris maritimes.\n  - Difficultés rencontrées avec les jeux de données accessibles :\n    - Frontières simplifiées, ne respectant pas la topologie.\n    - Absence de données sur les frontières maritimes.\n    - Simplifications ne couvrant pas le territoire exact.\n  - Exploration des données d'OpenStreetMap :\n    - Données détaillées et non simplifiées.\n    - Besoin d'un mécanisme de simplification géographique en raison du volume de données.\n  - Exigence de temps d'exécution :\n    - Identification d'un pays en moins de 10 millisecondes nécessitant l'utilisation de la RAM plutôt qu'une base de données distante.\n\n- **Désagrégation de recensements locaux :**\n  - Verrou lié au temps d'affichage des résultats dans l'outil.\n  - Objectif d'afficher les résultats en moins de 5 secondes.\n  - Volume important de données rendant le stockage à chaud dans une base traditionnelle inadapté.\n  - Expérimentation de la solution « Elasticsearch » pour la visualisation des données.\n\n- **Normalisation des données de flux :**\n  - Méthode établie pour transformer les traces GPS en données affichables rapidement.\n  - Complexité de l'approche pour les clients :\n    - Variabilité des échantillons de traces GPS d'un jour à l'autre.\n    - Manque d'uniformité entre les pays.\n  - Développement d'une approche manuelle pour calculer un indice de normalisation :\n    - Objectif de mieux appréhender la problématique et d'expérimenter une solution à automatiser.\n    - Indice validé manuellement avec un intervalle de confiance spécifique.\n  - Absence de solution technique complètement satisfaisante pour cette tâche.\n\n- **Labélisation des Points d'Intérêt (POI) :**\n  - Problème d'uniformité des POI provenant de différentes sources.\n  - Nécessité de développer une méthodologie pour identifier les POI de manière uniforme, indépendamment de leur origine."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLors de la réalisation de nos travaux de recherche, nous avons rencontré de nombreux verrous technologiques. \n\nEn effet, quant à l’identification du pays, nous avons identifié les verrous suivants : pour identifier un pays à partir de ses coordonnées géographiques, il nous fallait identifier une source de données contenant des informations sur les frontières de chaque pays, y compris les frontières maritimes. Cependant, en se basant sur les jeux de données accessibles immédiatement, nous avons rencontré plusieurs difficultés : les frontières des pays étaient simplifiées individuellement, ne respectant pas la topologie des frontières, ce qui rend difficile une identification. De plus, ces données ne comprenaient pas les frontières maritimes, et les simplifications ne couvrent pas le territoire exact du pays. D’autre part, en explorant les données de frontières de pays disponibles dans OpenStreetMap, nous avons constaté que ces données étaient assez détaillées et non simplifiées. Cependant, en raison du volume considérable de ces données, il était impératif de proposer un mécanisme de simplification géographique pour les rendre utilisables dans nos applications. A ces contraintes s’ajoute le facteur de temps d’exécution. En effet, pour pourvoir identifier un pays en moins de 10 millisecondes, il nous fallait exploiter directement la mémoire vive (RAM) de la machine exécutant la requête et non pas une base de données déportée sur une autre machine accessible via le réseau.\n\nD’autre part, concernant la désagrégation de recensements locaux, nous avons également été confrontés à verrou compte tenu du temps d’affichage des résultats dans l’outil. Afin d’afficher ces résultats immédiatement dans l'outil, en moins de 5 secondes, et étant donné le volume important de données, le stockage à chaud dans une base de données traditionnelle ne serait pas adapté. Nous avons ainsi dû envisager le recours à une technologie plus adaptée à des données de cette échelle, nous avons ainsi expérimenté la solution « Elasticsearch » pour visualiser cette donnée.\n\nNous avons également identifié un défi supplémentaire lors de la normalisation des données de flux. En effet, lors de nos travaux précédents, nous avions établi une méthode pour transformer les traces GPS en données affichables presque immédiatement dans notre plateforme, indépendamment du nombre de dispositifs ou de la surface observée. Cependant, notre approche s'est avérée complexe à comprendre pour nos clients. En effet, nos échantillons de traces GPS variaient d'un jour à l'autre et ne présentaient pas une uniformité constante d'un pays à l'autre. En effet, notre échantillon de traces GPS étant variable de jour en jour et n'étant pas uniforme d'un pays à l'autre, il nous fallait donc déterminer un moyen de convertir ces échantillons en une estimation du nombre de personnes traversant les rues. Nous avons alors suivi une nouvelle approche manuelle qui consiste à calculer un indice de normalisation : ce travail vise à mieux appréhender la problématique et expérimenter une approche que nous devrons par la suite automatiser. Cet indice est validé manuellement et appliqué dans un intervalle de confiance spécifique. Bien que cela ait amélioré notre compréhension du problème, nous n'avons pas encore déterminé de solution technique complètement satisfaisante pour cette tâche.\n\nEnfin, nous avons été confrontés à un autre verrou quant à la labélisation des POI provenant de différentes sources, ce qui pose un problème au niveau de leur uniformité. Il nous fallait donc développer une méthodologie qui permet d’identifier tous les POI de la même manière, indépendamment de leur origine."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Labellisation de Points d’Intérêt (POI) :**\n  - Limitations des approches existantes en termes de :\n    - Nombre de POI labélisés.\n    - Temps de traitement nécessaire.\n  - Nécessité de développer une nouvelle approche à l’échelle planétaire avec :\n    - Vitesse de traitement significativement plus élevée.\n  \n- **Verrous rencontrés :**\n  - Gestion d'une grande quantité de données géographiques et de POI à l’échelle mondiale.\n  - Impact sur la vitesse de traitement des données.\n  - Besoin de solutions robustes en termes de :\n    - Stockage et traitement des données.\n    - Parallélisation.\n    - Correspondance en temps réel avec les catégories définies.\n  - Développement d'algorithmes puissants et d'une architecture de données efficace pour maintenir la rapidité du processus.\n\n- **Prédiction du chiffre d'affaires :**\n  - Difficulté liée au bruit introduit par différentes variables.\n  - Adoption d'une approche générant des données aléatoires pour :\n    - Garantir l'absence de corrélation (ou corrélation d'ordre premier ou second) entre la variable en question et les variations de la variable cible.\n\n- **Suggestions de réseaux de points de vente :**\n  - Incertitude sur l'approche à adopter par rapport à un objectif fixé.\n  - Approche naïve : cibler les zones à forte densité de population ou flux piéton.\n    - Limitation : négligence du phénomène de cannibalisation entre points de vente.\n  - Importance de l'expertise pour élaborer des scénarios de chiffre d'affaires en utilisant :\n    - Sources de données directes et indirectes.\n  - Proposition d'une approche incrémentale :\n    - Évaluation isolée des performances prévisionnelles de chaque nouveau point de vente.\n    - Analyse de l'impact (positif ou négatif) sur les performances des autres points de vente du réseau envisagé."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLors de la réalisation de nos travaux de recherche, nous avons été confrontés à plusieurs verrous.\n\nEn effet, nous avons travaillé l’année dernière sur la labellisation de Points d’Intérêt (POI). Jusqu’à présent, les approches citées dans la littératures présentes plusieurs limites en termes de ordres de grandeur du nombre de POI labélisés et le temps de traitement nécessaire. Ainsi, il fallait développer une nouvelle approche en étendant notre projet à l’échelle planétaire, avec une vitesse significativement plus élevée. Dans ce contexte, nous avons été confronté aux plusieurs verrous. En effet, l’expansion du projet à l’échelle mondiale nécessite la gestion d’une quantité considérable de données géographiques et des POI répartis sur toute la planète ce qui infecte par conséquent la vitesse de traitement. Donc , il nous fallait développer des solutions robustes en termes de stockage et de traitement des données, de parallélisation et de correspondance en temps réel avec les catégories définies. De plus, pour atteindre une vitesse de traitement significativement élevée, il nous fallait développer des algorithmes puissants et suivre une architecture de données et de traitement efficace pour maintenir la rapidité du processus.\n\nD'autre part, en ce qui concerne la prédiction du chiffre d'affaires, nous avons été confrontés à une difficulté liée au bruit introduit par différentes variables. Afin de surmonter cette problématique, nous avons dû générer des données aléatoires en adoptant une approche où des données aléatoires sont intégrées pour garantir l'absence de corrélation, ou du moins de corrélation d'ordre premier ou second, entre la variable en question et les véritables variations de la variable cible.\n\nNous avons également été confrontés à l'incertitude quant à l'approche à adopter pour les suggestions de réseaux de points de vente par rapport à un objectif fixé. Une approche naïve consisterait à cibler les zones à forte densité de population ou présentant un flux piéton important, similaire à ce qu'un expert humain pourrait envisager. Cependant, cette approche serait incomplète car elle négligerait le phénomène de cannibalisation entre les différents points de vente, un aspect crucial dans la détermination du réseau. L'enjeu réside dans la continuité de l'offre de toute son expertise, exploitant une multitude de sources de données directes et indirectes pour élaborer des scénarios de chiffre d'affaires. Ainsi, il nous a fallu évaluer comment traduire cette approche multidimensionnelle en proposant une approche incrémentale. À chaque nouveau point de vente envisagé pour être ajouté au réseau, ses performances prévisionnelles sont évaluées de manière isolée, tout en analysant son impact, qu'il soit positif ou négatif, sur les performances des autres points de vente du réseau envisagé."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous technologiques à surmonter** : Nécessité de dépasser plusieurs obstacles techniques pour la réussite du projet de recherche.\n\n- **Analyse de tendance** :\n  - Basée sur le nombre d'appareils connectés dans une zone géographique spécifique.\n  - Variabilité quotidienne du nombre d'appareils connectés, rendant difficile l'établissement de tendances locales.\n  - Besoin de dégager une tendance locale indépendante de la tendance globale pour obtenir des données pertinentes et exploitables.\n\n- **Prédiction post-COVID** :\n  - Modèles prédictifs existants obsolètes en raison de la crise sanitaire.\n  - Modèles précédemment développés basés sur des tendances \"normales\" ne s'appliquent pas à la situation actuelle.\n\n- **Données de recensement** :\n  - Recensement effectué sur des cellules définies par la densité des zones, pas sur des cellules de taille constante.\n  - Nécessité de retraitement des données pour fournir des informations pertinentes aux clients.\n  - Disponibilité de 200 variables de recensement individuel, entraînant un volume de données important à traiter.\n  - Besoin de réduire la quantité de données pour accéder à l'information de manière efficace et rapide."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nPour mener à bien ce projet de recherche, nous devons dépasser un certain nombre de verrous technologiques.\n\nPour les travaux d’analyse de tendance, nous nous basons sur le nombre d’appareils connectés sur une zone géographique donnée. Cependant le nombre total d’appareils connectés peut varier très fortement d’un jour à l’autre, il faudrait donc pouvoir dégager une tendance locale indépendante de la tendance globale afin d’avoir des données pertinentes et exploitables. De plus, la prédiction de la reprise d’activité post covid pose un second problème : les modèles prédictifs développés au préalable ou proposés dans la littérature sont totalement obsolètes face à la situation inédite de la crise sanitaire, car ayant été entrainés sur des tendances « normales ».\n\nConcernant, les données de recensement, un des principaux biais est que le recensement est effectué sur des cellules définies en fonction de la densité des zones et non des cellules de taille constante, ce qui ne nous permet pas, sans retraitement, de fournir une information pertinente à nos clients. De plus, il existe 200 variables de recensement individuel disponibles, ce qui implique un grand volume de données à traiter, particulièrement si l’on souhaite un maillage fin, nous devons donc trouver un moyen de réduire la quantité de données pour pouvoir accéder à l’information en un temps raisonnable."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### Mascaras\n- **Absence d'échelle standard** : Pas de méthode uniforme pour évaluer l'effet des mascaras sur les cils (volume, allongement, densité, recourbement).\n- **Protocole de mesure manquant** : Études existantes ne comparent pas les résultats de perception à des mesures instrumentales.\n- **Limites des études** : Certaines quantifient des paramètres des cils, mais manquent de méthodologie rigoureuse pour soutenir les arguments marketing.\n- **Combinaison ressentis et mesures** : Objectif de modéliser et classifier les mascaras en intégrant les ressentis des testeurs et des mesures objectives.\n- **Développement d'une échelle** : Nécessité de définir des paramètres précis pour les mascaras (volume, allongement, courbure).\n- **Évaluation subjective** : Exploration de l'influence de facteurs sur la perception des résultats des mascaras via un protocole basé sur des photographies.\n- **Uniformité de l'application** : Élaboration d'une méthode d'application uniforme et d'un format standardisé pour les photos pour éviter les biais.\n- **Échelle visuelle** : Création d'une échelle de comparaison des caractéristiques des mascaras.\n\n#### Panel sensoriel\n- **Développement d'un panel** : Création d'un panel discriminant, consensuel et reproductible pour les analyses sensorielles.\n- **Complexité de la prévision** : Difficulté à anticiper l'appréhension des produits cosmétiques par les panélistes en raison de la subjectivité des descripteurs.\n- **Évaluation continue** : Nécessité d'évaluer la flexibilité des hypothèses et la pertinence des questionnaires à chaque phase des tests.\n- **Choix des descripteurs** : Sélection de descripteurs clairs, non redondants et précis pour qualifier les produits.\n- **Études antérieures** : Réussite en 2021 sur la caractérisation des textures des crèmes, extension en 2022 à l'évaluation visuelle et olfactive.\n- **Panel externe** : Développement d'un panel externe en 2023 basé sur des descripteurs établis pour une caractérisation discriminante.\n- **Consensus non atteint** : Malgré le travail théorique et empirique, le panel n'est pas encore consensuel sur certains descripteurs.\n\n#### Panel d’évaluation d’injection d’acide hyaluronique\n- **Critères d'inclusion/exclusion** : Difficulté à recruter des participants répondant à des critères d'inclusion spécifiques (âge, sexe, état de santé) et d'exclusion rigoureux (contre-indications médicales).\n- **Dépendance aux participants** : Disponibilité des participants cruciale pour l'étude.\n- **Recrutement difficile** : Challenge continu de recruter un nombre suffisant de participants dévoués, respectant les exigences de l'étude et comprenant les risques et bénéfices, surtout pour des procédures médicales invasives comme les injections d'acide hyaluronique."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**  \nMascaras  \nNos recherches de l’état de l’art ont révélé l'absence d'une échelle standard pour évaluer l'effet des mascaras sur les cils (volume, allongement, densité, recourbement) ainsi que l'absence de protocole de mesure standardisé. Une étude utilisant une approche psychophysique pour évaluer l'illusion d'agrandissement de l'œil par le maquillage a été identifiée, mais elle ne propose pas d’évaluation spécifique de critères précis ni ne compare ces résultats de perception à des mesures instrumentales.  \n\nCertaines études quantifient divers paramètres des cils, mais elles ne proposent pas de méthodologie rigoureuse pour étayer les arguments marketing concernant les promesses des produits et excluent les aspects subjectifs des testeurs de mascaras en se limitant à des mesures instrumentales. Notre défi consiste à combiner les ressentis des testeurs avec des mesures objectives pour modéliser et classifier les mascaras.  \n\nLe premier obstacle à surmonter est le développement d’une échelle et de paramètres décrivant précisément les caractéristiques des mascaras, notamment le volume, l’allongement et la courbure des cils. Étant donné la complexité de l’évaluation subjective des utilisateurs, nous explorons l'influence de divers facteurs hypothétiques sur la perception des résultats des mascaras et élaborons un protocole pour évaluer cette hypothèse à travers des photographies.  \n\nL'élaboration de ce protocole nécessite la définition d’une méthode d'application uniforme des mascaras et l’adoption d’un format standardisé pour les photos afin d’éviter tout biais de comparaison. Nous souhaitons également créer une échelle visuelle de comparaison des caractéristiques des mascaras.  \nPanel sensoriel  \nPar ailleurs, dans le cadre du développement d’un panel discriminant, consensuel et reproductible pour nos analyses sensorielles, nous avons été confrontés à d’autres défis. La prévision de l’appréhension des produits cosmétiques par les panélistes est complexe en raison de la nature subjective et personnelle du choix des descripteurs. Chaque étape de notre étude nous contraint à évaluer la flexibilité de nos hypothèses pour obtenir des résultats exploitables à chaque phase des tests de produits. Nous devons constamment évaluer la pertinence de nos questionnaires pour appréhender la perception des produits par les panélistes et recueillir leurs impressions. Nous sommes également confrontés à la tâche délicate de choisir des descripteurs qui soient clairs pour l’ensemble du panel, non redondants entre eux et surtout capables de qualifier de manière précise un produit.  \n\nBien que nous ayons réussi en 2021 à mener une étude expérimentale sur la caractérisation des textures des crèmes, et que nous ayons étendu cette approche en 2022 à l'évaluation visuelle et olfactive, nous avons poursuivi notre extension en 2023 en développant un panel externe en plus de notre panel interne. Ce développement s'est appuyé sur les descripteurs établis dans le but d'obtenir une caractérisation discriminante, consensuelle et reproductible. Cependant, malgré un travail théorique et empirique visant à déterminer des échelles pour positionner un échantillon par rapport aux différentes dimensions évaluées, notre panel n'est toujours pas consensuel sur certains descripteurs.  \nPanel d’évaluation d’injection d’acide hyaluronique  \nEnfin, concernant la définition d’un panel spécifique pour les études cliniques auxquelles nous avons participé, nous avons été confrontés au défi de trouver des participants répondant à des critères d'inclusion très spécifiques (âge, sexe, état de santé, antécédents médicaux, etc.) et à des critères d'exclusion rigoureux (contre-indications médicales, conditions préexistantes, etc.). Cette étude dépend fortement des participants et leur disponibilité est cruciale. Par conséquent, recruter un nombre suffisant de participants véritablement dévoués à l'étude, respectant ses exigences et comprenant pleinement ses risques et ses bénéfices, constitue un défi continu, surtout dans le cas d'une étude impliquant des procédures médicales invasives comme les injections d'acide hyaluronique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Objectifs de R&D** : La réussite du projet dépend de la levée de nombreux verrous technologiques.\n  \n- **Modélisation de la Satisfaction** :\n  - Difficulté à modéliser la satisfaction en tant que résultat des attentes et de la perception du produit par le consommateur.\n  - Les attentes sont influencées par les expériences passées du consommateur.\n  - La perception est dynamique et évolue au cours des tests.\n  - Nécessité d'évaluer la souplesse des hypothèses tout au long de l'étude pour obtenir des résultats exploitables.\n\n- **Évaluation des Questionnaires** :\n  - Importance de continuellement évaluer la pertinence des questionnaires pour comprendre la perception du produit par les panélistes.\n  - Recueil des impressions des panélistes est crucial.\n\n- **Développement des Bases de Données** :\n  - Principal verrou : structuration des données pour permettre l'analyse des données de test.\n  - Les données doivent être analysées pour éliminer les incohérences et éviter des conclusions erronées sur l'appréciation des produits.\n  - Évaluation de la signification des écarts relevés dans les résultats.\n\n- **Fiabilité des Panélistes** :\n  - Évaluation de la fiabilité des panélistes et de leurs résultats.\n  - Sensibilités individuelles des panélistes peuvent nuancer les résultats (ex. : inconfort, intolérance).\n  - Ces sensibilités peuvent entraîner l'arrêt des tests.\n\n- **Engagement des Panélistes** :\n  - Besoin de renforcer l'engagement des panélistes à travers des initiatives de gamification.\n\n- **Méthodologie d'Enquête** :\n  - Défi de concevoir une méthodologie pour identifier l'axe le plus pertinent à discuter lors d'enquêtes qualitatives de groupe."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’atteinte des objectifs fixés dans ce projet de R&D est tributaire de la levée de nombreux verrous. Une première difficulté réside dans la modélisation de la satisfaction comme résultat des attentes et de la perception d’un produit par le consommateur. Or les attentes dépendent de ses expériences passées et sa perception est une variable dynamique au cours du test. La nature de ces deux composantes nous contraint à évaluer la souplesse de nos hypothèses tout au long de notre étude afin d’obtenir des résultats exploitables aux différents stades des tests de produits. Nous devons alors continuellement évaluer la pertinence de nos questionnaires pour appréhender la perception du produit par les panélistes et recueillir leurs impressions. Concernant le développement des bases de données, le principal verrou que nos travaux visaient à lever portait sur la structuration des données afin de permettre les analyses des données de test obtenues auprès des panélistes. Les données devaient elles-aussi subir diverses analyses afin d’écarter les incohérences menant à des conclusions aberrantes quant à l’appréciation des produits ou évaluer si un écart relevé est signifiant ou non. De plus, une difficulté supplémentaire s’adjoignant à celle de la modélisation de la satisfaction et l’analyse des données recueillies réside dans l’évaluation de la fiabilité des panélistes participant à l’étude et de leurs résultats. Les sensibilités des différents panélistes viennent en effet nuancer leurs résultats, notamment les réactions d’inconfort ou d’intolérance, qui peuvent être à l’origine d’un arrêt du test. Enfin, nous avons également été confrontés au besoin de renforcer l’engagement de nos panélistes par des initiatives de gamification. Notre défi portait également sur la conception d’une méthodologie permettant d’identifier l’axe le plus pertinent à proposer au débat dans le cadre d’enquêtes qualitatives de groupe."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Inexistence de Standards**: \n  - Absence de standards de classification des mascaras identifiée lors des recherches de l'état de l'art.\n  - Syres propose de combler cette lacune par des travaux de R&D.\n\n- **Caractéristiques Non Standardisées**:\n  - Aucune échelle pour caractériser l'effet des mascaras sur les cils.\n  - Pas de protocole de mesure standardisé pour les caractéristiques mises en avant par les fabricants (volume, allongement, densité, recourbement).\n\n- **Études Existantes**:\n  - Une étude psychophysique sur l'effet du maquillage sur l'illusion d'agrandissement de l'œil, sans évaluation spécifique ni mesure instrumentale.\n  - Des études sur la modélisation des cils, mais sans méthodologie rigoureuse pour les arguments marketing et excluant les aspects subjectifs du porteur.\n\n- **Défi de Modélisation et Classification**:\n  - Coupler le ressenti des testeurs avec les caractéristiques mesurées pour modéliser et classifier les mascaras.\n  - Développement d'une échelle et définition de paramètres pour décrire les caractéristiques des mascaras, en commençant par le volume des cils.\n\n- **Facteurs d'Influence sur le Ressenti**:\n  - Hypothèse que le point de vue peut altérer la perception des résultats des mascaras.\n  - Protocole de test proposé pour comparer les perceptions via trois modes d'évaluation :\n    - Auto-évaluation par les experts.\n    - Observation du mascara sur d'autres experts.\n    - Évaluation sur des photos.\n\n- **Difficultés de Comparaison**:\n  - Nécessité d'un protocole d'application commun pour garantir la similitude des conditions d'évaluation.\n  - Définition d'un format commun pour les photos afin d'éviter l'hétérogénéité des conditions (luminosité, zoom, orientation).\n\n- **Instrumentation et Photographie**:\n  - Recherche d'un partenaire avec une instrumentation performante pour les évaluations.\n  - Photographie choisie pour constituer une échelle visuelle des caractéristiques des mascaras.\n  - Traduction des termes marketing en métriques.\n\n- **Contraintes**:\n  - Reproductibilité des conditions de photographie et indépendance vis-à-vis du prestataire pour l'analyse des photos.\n  - Limitations des outils du marché et nécessité de traitement d'image pour les photos exploitables.\n  - Problème d'écrasement des cils par certains instruments de mesure.\n\n- **Influence des Attentes des Panélistes**:\n  - Le ressenti des panélistes influencé par leurs attentes basées sur des expériences précédentes.\n  - Développement de bases de données pour répertorier les produits testés par les panélistes, avec des travaux de fiabilisation pour leur exploitabilité.\n\n- **Projections Futures**:\n  - Affinement du modèle en introduisant de nouveaux axes de modélisation du rendu des mascaras (forme de la brosse, longueur, matériau des picots, etc.)."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNos recherches de l’état de l’art nous ont permis de mettre en évidence l’inexistence de standards de classification des mascaras. Syres se propose de combler cette lacune par des travaux de R&D via une approche originale couplant perception des résultats et mesure instrumentale.  \nEn effet, nos recherches bibliographiques sur l’état de l’art en la matière nous ont permis de constater qu’aucune échelle n’existe pour caractériser l’effet des mascaras sur les cils, et qu’aucun protocole de mesure des principales caractéristiques mises en avant par les fabricants (volume, allongement, densité, recourbement) n’a été défini comme un standard.  \nAu cours de notre étude bibliographique, nous avons identifié une étude adoptant une approche psychophysique de l’effet du maquillage sur l’illusion d’agrandissement de l’œil, mais cette étude ne propose pas d’évaluation spécifique d’un critère précis, et elle ne propose pas de confronter ces résultats de perception à une mesure instrumentale. De plus, cette étude joue sur les effets psychophysiques induits par l’enchainement des photos proposées à la comparaison par une méthodologie particulière (méthode des escaliers, bien connue parmi les méthodes d’évaluation en psychophysique).  \nEn outre, nous avons identifié des études expérimentant la modélisation des cils en définissant différents paramètres, qui sont quantifiés instrumentalement. Cependant, ces études ne présentent aucune méthodologie rigoureuse pour définir les arguments marketing qui sont mis en avant. Une autre limite de ces études vis-à-vis de notre ambition, est leur périmètre restreint uniquement à la mesure instrumentale. En particulier, tous les aspects subjectifs du porteur du mascara sont écartés.  \n\nNotre défi est de coupler la mesure du ressenti des testeurs avec les caractéristiques mesurées en vue d’une modélisation et d’une classification des différents mascaras.  \nLe premier défi qui se pose dans ce projet est lié au développement d’une échelle et à la définition de paramètres qui permettent de décrire univoquement les caractéristiques des mascaras, et dans un premier temps, uniquement le volume des cils procuré par une référence de mascara étudiée.  \n\nPar ailleurs, le ressenti procuré par un mascara étant difficile à appréhender, nous avons souhaité étudier les différents facteurs d’influence pouvant s’exercer sur celui-ci. Nous avons ainsi formulé l’hypothèse selon laquelle le point de vue peut altérer la perception des résultats procurés par les mascaras. Notre défi était alors de mettre en œuvre un protocole de test permettant de détecter et comparer les éventuelles nuances qui ressortent de la perception des résultats d’un mascara via trois modes d’évaluation :  \nSur les experts eux-mêmes en auto-évaluation ;  \nPar observation du mascara sur d’autres experts ayant appliqué le mascara ;  \nPar évaluation sur des photos.  \n\nL’étude de cette hypothèse a été à l’origine de nouvelles difficultés.  \n\nEn effet, la comparaison des caractéristiques d’un mascara vues par les experts sur eux-mêmes et sur d’autres modèles suppose la définition d’un protocole d’application commun des mascaras afin de garantir la similitude des conditions d’évaluation. Ce point sera exploré ultérieurement en vue de la formalisation d’un protocole d’application.  \n\nL’évaluation de photos pose quant à elle le besoin de la définition d’un format commun pour l’ensemble des photos constituant la photothèque afin de ne pas influencer les comparaisons du fait d’hétérogénéité des conditions (luminosité, zoom, orientation des modèles, etc.). La difficulté que nous devons surmonter est donc de trouver un partenaire proposant une instrumentation suffisamment performante pour notre besoin.  \nLe choix de la photographie comme troisième mode d’évaluation des caractéristiques des mascaras a également été motivé par notre désir de constituer une échelle visuelle pour comparer les caractéristiques des mascaras du marché. En effet, les arguments et appellations du marché proposent des termes tels que « extra volume » ; « effet faux-cils », etc., et l’une de nos ambitions est de traduire ces appellations en métriques.  \n\nParmi nos contraintes, le caractère reproductible des conditions de photographie s’ajoute à notre besoin d’indépendance vis-à-vis du prestataire, notamment concernant l’analyse des photos. En effet, nos recherches de partenaires nous ont permis de constater que les outils du marché restaient limités sur certains aspects et les photos prises doivent subir un traitement d’image afin d’être exploitables. En particulier, l’un des problèmes auxquels nous avons été confrontés était l’écrasement des cils par certains instruments de mesure.  \n\nPar ailleurs, le ressenti des panélistes vis-à-vis des rendus des produits testés est influencé par leurs attentes. Celles-ci résultent de leurs expériences précédentes. Nous avons ainsi consenti l’effort de développer des bases de données permettant de répertorier les produits testés par les panélistes.  \nDes travaux de fiabilisation de celles-ci ont ainsi dû être menés, notamment pour permettre leur exploitabilité via des requêtes complexes.  \n\nEnfin, suite à la levée des principaux verrous ici décrits, nous projetons d’affiner notre modèle en introduisant de nouveaux axes de modélisation du rendu des mascaras tels que l’étude de l’impact de la forme de la brosse ou encore sa longueur, le matériau des picots de celle-ci, etc. sur le résultat final."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Inexistence de standards** : Aucune classification standard des mascaras n'existe, ce qui constitue une lacune que SYRES souhaite combler par des travaux de R&D.\n  \n- **Absence de protocoles** : Aucun protocole standardisé pour mesurer les caractéristiques des mascaras (volume, allongement, densité, recourbement) n'a été défini.\n\n- **Études existantes** :\n  - Une étude psychophysique sur l'effet du maquillage sur l'illusion d'agrandissement de l'œil, sans évaluation spécifique ni mesure instrumentale.\n  - D'autres études modélisent les cils avec des paramètres quantifiés, mais manquent de méthodologie rigoureuse pour les arguments marketing et se concentrent uniquement sur la mesure instrumentale, négligeant les aspects subjectifs.\n\n- **Défi principal** : Coupler la mesure du ressenti des testeurs avec les caractéristiques mesurées pour modéliser et classifier les mascaras.\n\n- **Développement d'une échelle** : Nécessité de définir des paramètres pour décrire les caractéristiques des mascaras, en commençant par le volume des cils.\n\n- **Facteurs d'influence sur le ressenti** : Hypothèse que le point de vue peut altérer la perception des résultats des mascaras. Protocole de test proposé :\n  - Auto-évaluation par les experts.\n  - Observation sur d'autres experts ayant appliqué le mascara.\n  - Évaluation sur des photos.\n\n- **Comparaison des caractéristiques** : Nécessité d'un protocole d'application commun pour garantir la similitude des conditions d'évaluation.\n\n- **Évaluation par photos** : Besoin d'un format commun pour les photos afin d'éviter l'hétérogénéité (luminosité, zoom, orientation).\n\n- **Partenariat pour instrumentation** : Recherche d'un partenaire avec une instrumentation performante pour répondre aux besoins d'évaluation.\n\n- **Échelle visuelle** : Objectif de créer une échelle visuelle pour comparer les caractéristiques des mascaras, traduisant les termes marketing en métriques.\n\n- **Contraintes** :\n  - Reproductibilité des conditions de photographie.\n  - Indépendance vis-à-vis du prestataire pour l'analyse des photos.\n  - Limitations des outils du marché et nécessité de traitement d'image pour les photos.\n\n- **Affinement du modèle** : Après levée des verrous, introduction de nouveaux axes de modélisation (forme de la brosse, longueur, matériau des picots).\n\n- **Influence des attentes des panélistes** : Les attentes des panélistes, basées sur leurs expériences passées, influencent leur ressenti.\n\n- **Bases de données des produits testés** : Développement de bases de données pour répertorier les produits testés, avec des travaux de fiabilisation réguliers.\n\n- **Recrutement des panélistes** :\n  - Longs délais de recrutement et manque d'engagement des panélistes.\n  - Données parfois incomplètes ou obsolètes.\n  - Attente prolongée avant d'être contactés pour des études.\n\n- **Hypothèse de recrutement alternatif** : Conversion de visiteurs en testeurs pour améliorer l'engagement et la réactivité.\n\n- **Conception d'un \"living lab\"** : Création d'un \"Bar à tests\" pour proposer un nouveau processus d'évaluation des produits et lever les incertitudes sur cette approche par rapport aux méthodes traditionnelles."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNos recherches de l’état de l’art nous ont permis de mettre en évidence l’inexistence de standards de classification des mascaras. SYRES se propose de combler cette lacune par des travaux de R&D via une approche originale couplant perception des résultats et mesure instrumentale.\nEn effet, nos recherches bibliographiques sur l’état de l’art en la matière nous ont permis de constater qu’aucune échelle n’existe pour caractériser l’effet des mascaras sur les cils, et qu’aucun protocole de mesure des principales caractéristiques mises en avant par les fabricants (volume, allongement, densité, recourbement) n’a été défini comme standard.\nAu cours de notre étude bibliographique, nous avons identifié une étude adoptant une approche psychophysique de l’effet du maquillage sur l’illusion d’agrandissement de l’œil, mais cette étude ne propose pas d’évaluation spécifique d’un critère précis, et elle ne propose pas de confronter ces résultats de perception à une mesure instrumentale. De plus, cette étude joue sur les effets psychophysiques induits par l’enchainement des photos proposées à la comparaison par une méthodologie particulière (méthode des escaliers, bien connue parmi les méthodes d’évaluation en psychophysique).\nEn outre, nous avons identifié des études expérimentant la modélisation des cils en définissant différents paramètres, qui sont quantifiés instrumentalement. Cependant, ces études ne présentent aucune méthodologie rigoureuse pour définir les arguments marketing qui sont mis en avant. Une autre limite de ces études vis-à-vis de notre ambition, est leur périmètre restreint uniquement à la mesure instrumentale. En particulier, tous les aspects subjectifs du porteur du mascara sont écartés.\n\nNotre défi est de coupler la mesure du ressenti des testeurs avec les caractéristiques mesurées en vue d’une modélisation et d’une classification des différents mascaras.\nLe premier défi qui se pose dans ce projet est lié au développement d’une échelle et à la définition de paramètres qui permettent de décrire univoquement les caractéristiques des mascaras, et dans un premier temps, uniquement le volume des cils procuré par une référence de mascara étudiée.\n\nPar ailleurs, le ressenti procuré par un mascara étant difficile à appréhender, nous avons souhaité étudier les différents facteurs d’influence pouvant s’exercer sur celui-ci. Nous avons ainsi formulé l’hypothèse selon laquelle le point de vue peut altérer la perception des résultats procurés par les mascaras. Notre défi était alors de mettre en œuvre un protocole de test permettant de détecter et comparer les éventuelles nuances qui ressortent de la perception des résultats d’un mascara via trois modes d’évaluation :\nSur les experts eux-mêmes en auto-évaluation ;\nPar observation du mascara sur d’autres experts ayant appliqué le mascara ;\nPar évaluation sur des photos.\n\nL’étude de cette hypothèse a été à l’origine de nouvelles difficultés.\nEn effet, la comparaison des caractéristiques d’un mascara vues par les experts sur eux-mêmes et sur d’autres modèles suppose la définition d’un protocole d’application commun des mascaras afin de garantir la similitude des conditions d’évaluation. Ce point sera exploré ultérieurement en vue de la formalisation d’un protocole d’application.\n\nL’évaluation de photos pose quant à elle le besoin de la définition d’un format commun pour l’ensemble des photos constituant la photothèque afin de ne pas influencer les comparaisons du fait d’hétérogénéité des conditions (luminosité, zoom, orientation des modèles, etc.). La difficulté que nous devons surmonter est donc de trouver un partenaire proposant une instrumentation suffisamment performante pour notre besoin.\nLe choix de la photographie comme troisième mode d’évaluation des caractéristiques des mascaras a également été motivé par notre désir de constituer une échelle visuelle pour comparer les caractéristiques des mascaras du marché. En effet, les arguments et appellations du marché proposent des termes tels que « extra volume » ; « effet faux-cils », etc., et l’une de nos ambitions est de traduire ces appellations en métriques.\n\nParmi nos contraintes, le caractère reproductible des conditions de photographie s’ajoute à notre besoin d’indépendance vis-à-vis du prestataire, notamment concernant l’analyse des photos. En effet, nos recherches de partenaires nous ont permis de constater que les outils du marché restaient limités sur certains aspects et les photos prises doivent subir un traitement d’image afin d’être exploitables. En particulier, l’un des problèmes auxquels nous avons été confrontés était l’écrasement des cils par certains instruments de mesure.\nDe plus, à la suite de la levée des principaux verrous ici décrits, nous projetons d’affiner notre modèle en introduisant de nouveaux axes de modélisation du rendu des mascaras tels que l’étude de l’impact de la forme de la brosse ou sa longueur, ou encore le matériau des picots de celle-ci, etc. sur le résultat final.\n\nPar ailleurs, le ressenti des panélistes vis-à-vis des rendus des produits testés est influencé par leurs attentes. Celles-ci résultent de leurs expériences précédentes. Nous avons ainsi consenti l’effort de développer des bases de données permettant de répertorier les produits testés par les panélistes. Des travaux de fiabilisation de celles-ci sont régulièrement menés, notamment pour permettre leur exploitabilité via des requêtes complexes qui manipulent des données dont le volume croît sans cesse.\nNous disposons ainsi de nombreuses données sur les panélistes (de l’ordre de 200 entrées par panéliste) que nous soumettons aux différents filtres en vue de sélectionner les profils pouvant convenir pour une étude sur le point de démarrer.\nToutefois ce type de recrutement a montré ses limites :\nLes temps de recrutement sont longs et certains panélistes sélectionnés ne répondent pas à la proposition d’étude, décourageant ainsi les chargés de recrutement ;\nLes données des panélistes sont parfois incomplètes ou obsolètes ;\nLes panélistes inscrits au sein de nos bases peuvent être amenés à attendre longtemps avant d’être contactés pour répondre à une enquête correspondant à leur profil, ainsi, un manque d’engagement a été constaté.\n\nNous avons ainsi émis l’hypothèse que la spontanéité et la curiosité de visiteurs qui seraient convertis en testeurs permettraient d’adresser ces difficultés. La réflexion que nous avons menée en 2019 s’est ainsi focalisée sur la conception d’une solution offrant cette nouvelle expérience en vue de l’évaluation de produits.\nNous avons ainsi conçu le premier « living lab », sous forme de « Bar à tests » afin de proposer un nouveau processus d’évaluation des produits et lever nos incertitudes sur l’apport d’une telle approche en termes de résultats et comment concilier ce type d’étude avec nos approches traditionnelles de recrutement des panélistes."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Objectifs du projet** : L'atteinte des objectifs de R&D dépend de la levée de nombreux verrous.\n  \n- **Modélisation de la satisfaction** :\n  - Difficulté à modéliser la satisfaction en tant que résultat des attentes et perceptions des consommateurs.\n  - Les attentes sont influencées par les expériences passées des consommateurs.\n  - La perception est dynamique et évolue durant les tests.\n  - Nécessité d'évaluer la souplesse des hypothèses tout au long de l'étude pour obtenir des résultats exploitables.\n\n- **Évaluation des questionnaires** :\n  - Importance de continuellement évaluer la pertinence des questionnaires pour comprendre la perception du produit par les panélistes.\n\n- **Développement des bases de données** :\n  - Principal verrou : structuration des données pour analyses des tests auprès des panélistes.\n  - Nécessité d'analyser les données pour éliminer les incohérences et évaluer la signification des écarts.\n\n- **Fiabilité des panélistes** :\n  - Difficulté à évaluer la fiabilité des panélistes et leurs résultats.\n  - Sensibilités variées des panélistes peuvent influencer les résultats (réactions d’inconfort/intolérance pouvant interrompre les tests).\n\n- **Influence des attentes des panélistes** :\n  - Le ressenti des panélistes est influencé par leurs attentes, basées sur des expériences antérieures.\n  - Développement de bases de données pour répertorier les produits testés.\n  - Travaux de fiabilisation réguliers pour assurer l'exploitabilité des données via des requêtes complexes.\n\n- **Recrutement des panélistes** :\n  - Environ 200 entrées de données par panéliste soumises à des filtres pour sélectionner des profils adaptés.\n  - Limites du recrutement :\n    - Temps de recrutement longs.\n    - Certains panélistes ne répondent pas aux propositions d’étude.\n    - Données parfois incomplètes ou obsolètes.\n    - Attente prolongée pour les panélistes avant d'être contactés, entraînant un manque d'engagement.\n\n- **Hypothèse de recrutement alternatif** :\n  - Proposition de convertir des visiteurs spontanés en testeurs pour surmonter les difficultés de recrutement.\n  - Conception d'une solution innovante pour l'évaluation des produits.\n\n- **Living Lab - Bar à tests** :\n  - Création d'un premier « living lab » sous forme de « Bar à tests » pour évaluer les produits.\n  - Objectif : lever les incertitudes sur l’efficacité de cette approche et son intégration avec les méthodes traditionnelles.\n\n- **Engagement des panélistes** :\n  - Besoin de renforcer l'engagement des panélistes à travers des initiatives de gamification.\n  - Défi de concevoir une méthodologie pour identifier les axes pertinents à débattre lors d'enquêtes qualitatives de groupe."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**  \nL’atteinte des objectifs fixés dans ce projet de R&D est tributaire de la levée de nombreux verrous.  \nUne première difficulté réside dans la modélisation de la satisfaction comme résultat des attentes et de la perception d’un produit par le consommateur. Or les attentes dépendent de ses expériences passées et sa perception est une variable dynamique au cours du test.  \nLa nature de ces deux composantes nous contraint à évaluer la souplesse de nos hypothèses tout au long de notre étude afin d’obtenir des résultats exploitables aux différents stades des tests de produits. Nous devons alors continuellement évaluer la pertinence de nos questionnaires pour appréhender la perception du produit par les panélistes et recueillir leurs impressions.  \nConcernant le développement des bases de données, le principal verrou que nos travaux visaient à lever portait sur la structuration des données afin de permettre les analyses des données de test obtenues auprès des panélistes. Les données devaient elles-aussi subir diverses analyses afin d’écarter les incohérences menant à des conclusions aberrantes quant à l’appréciation des produits ou évaluer si un écart relevé est signifiant ou non.  \nDe plus, une difficulté qui s’ajoute à celle de la modélisation de la satisfaction et l’analyse des données recueillies réside dans l’évaluation de la fiabilité des panélistes participant à l’étude et de leurs résultats. Les sensibilités des différents panélistes viennent en effet nuancer leurs résultats, notamment les réactions d’inconfort ou d’intolérance, qui peuvent être à l’origine d’un arrêt du test.  \n\nPar ailleurs, le ressenti des panélistes vis-à-vis des rendus des produits testés est influencé par leurs attentes. Celles-ci résultent de leurs expériences précédentes. Nous avons ainsi consenti l’effort de développer des bases de données permettant de répertorier les produits testés par les panélistes. Des travaux de fiabilisation de celles-ci sont régulièrement menés, notamment pour permettre leur exploitabilité via des requêtes complexes qui manipulent des données dont le volume croît sans cesse.  \nNous disposons ainsi de nombreuses données sur les panélistes (de l’ordre de 200 entrées par panéliste) que nous soumettons aux différents filtres en vue de sélectionner les profils pouvant convenir pour une étude sur le point de démarrer.  \nToutefois ce type de recrutement a montré ses limites :  \nLes temps de recrutement sont longs et certains panélistes sélectionnés ne répondent pas à la proposition d’étude, décourageant ainsi les chargés de recrutement ;  \nLes données des panélistes sont parfois incomplètes ou obsolètes ;  \nLes panélistes inscrits au sein de nos bases peuvent être amenés à attendre longtemps avant d’être contactés pour répondre à une enquête correspondant à leur profil, ainsi, un manque d’engagement a été constaté.  \n\nNous avons ainsi émis l’hypothèse que la spontanéité et la curiosité de visiteurs qui seraient convertis en testeurs permettraient d’adresser ces difficultés. La réflexion que nous avons menée s’est ainsi focalisée sur la conception d’une solution offrant cette nouvelle expérience en vue de l’évaluation de produits.  \nNous avions conçu le premier « living lab », sous forme de « Bar à tests » afin de proposer un nouveau processus d’évaluation des produits et lever nos incertitudes sur l’apport d’une telle approche en termes de résultats et comment concilier ce type d’étude avec nos approches traditionnelles de recrutement des panélistes.  \nEnfin, nous avons également été confrontés au besoin de renforcer l’engagement de nos panélistes par des initiatives de gamification. Notre défi portait également sur la conception d’une méthodologie permettant d’identifier l’axe le plus pertinent à proposer au débat dans le cadre d’enquêtes qualitatives de groupe."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Difficulté principale du projet R&D** : Obtention d'un panel discriminant, consensuel et reproductible.\n  \n- **Incertitudes liées à la perception** :\n  - Impossibilité de prédire l'appréhension des produits cosmétiques par les panélistes.\n  - Influence de la perception subjective et du choix descriptif personnel des panélistes.\n\n- **Évaluation continue** :\n  - Nécessité d'évaluer la souplesse des hypothèses à chaque étape de l'étude pour obtenir des résultats exploitables.\n  - Importance de la pertinence des questionnaires pour comprendre la perception du produit par les panélistes.\n\n- **Motivation des panélistes** :\n  - Maintien de la motivation des panélistes crucial pour obtenir des résultats fiables.\n  - Considéré comme la principale difficulté de l'étude.\n  - Absence de publications sur les moyens de motivation des panélistes et leur impact sur l'obtention d'un panel efficace."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLa difficulté à laquelle nous nous confrontons dans ce projet R&D est l’obtention d’un panel discriminent, consensuel et reproductible. Il est en effet impossible de prédire l’appréhension des produits cosmétiques par les panélistes à cause de la perception subjective et au choix du descriptif très personnel. Chaque étape de notre étude nous contraint à évaluer la souplesse de nos hypothèses afin d’obtenir des résultats exploitables aux différents stades des tests de produits. Nous devons alors continuellement évaluer la pertinence de nos questionnaires pour appréhender la perception du produit par les panélistes et recueillir leurs impressions.  \nNous devons également veiller à maintenir les panélistes motivés afin d’avoir des résultats fiables. Ceci est considéré comme la principale difficulté de notre étude surtout qu’aucune publication ne s’est intéressée aux différents moyens de motivation des panélistes et leur impact sur l’obtention d’un panel discriminent, consensuel et reproductible."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Difficulté principale** : Obtention d'un panel discriminant, consensuel et reproductible pour l'évaluation des produits cosmétiques.\n  \n- **Problème de prévision** : Impossibilité de prédire l'appréhension des produits cosmétiques par les panélistes en raison de la perception subjective et du choix descriptif personnel.\n\n- **Évaluation continue** : Nécessité d'évaluer la souplesse des hypothèses à chaque étape de l'étude pour obtenir des résultats exploitables.\n\n- **Pertinence des questionnaires** : Importance de continuellement évaluer les questionnaires pour comprendre la perception des produits par les panélistes et recueillir leurs impressions.\n\n- **Choix des descripteurs** : Défi de sélectionner des descripteurs qui soient :\n  - Compréhensibles pour l'ensemble du panel.\n  - Non redondants.\n  - Capables de qualifier efficacement une crème.\n\n- **Étude expérimentale réussie** : En 2021, une étude sur les descriptifs de texture des crèmes a été menée avec succès.\n\n- **Extension de l'étude** : En 2022, l'objectif était d'étendre la caractérisation aux aspects visuels et olfactifs des produits.\n\n- **Caractérisation discriminante** : Besoin d'étudier des descripteurs qui permettent une caractérisation discriminante, consensuelle et reproductible, tout en évitant d'orienter le panel pour ne pas biaiser les appréciations.\n\n- **Travail théorique et empirique** : Nécessité de définir des échelles pour situer un échantillon par rapport aux différentes dimensions évaluées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLa difficulté à laquelle nous nous confrontons dans ce projet R&D est l’obtention d’un panel discriminent, consensuel et reproductible. Il est en effet impossible de prédire l’appréhension des produits cosmétiques par les panélistes à cause de la perception subjective et au choix du descriptif très personnel. Chaque étape de notre étude nous contraint à évaluer la souplesse de nos hypothèses afin d’obtenir des résultats exploitables aux différents stades des tests de produits. Nous devons alors continuellement évaluer la pertinence de nos questionnaires pour appréhender la perception du produit par les panélistes et recueillir leurs impressions. Nous sommes également confrontés à un défi lié au choix des descripteurs afin que ceux-ci soient parlants à l’ensemble du panel, non redondants entre eux et surtout qu’ils permettent de qualifier une crème.  \nSi nous avons pu mener une étude expérimentale qui s’est révélée être une réussite sur les descriptifs de la texture des crèmes en 2021, en 2022 nous avons souhaité étendre cette caractérisation au visuel et à l’odeur. Nous nous sommes alors proposés d’étudier les descripteurs susceptibles de véhiculer une caractérisation discriminante, consensuelle et reproductible. De la même façon que pour les descripteurs de texture, nous devons veiller à ne pas orienter notre panel afin de ne pas biaiser les appréciations d’un échantillon.  \nA ce travail de définition des descripteurs s’ajoute un travail théorique et empirique de détermination des échelles permettant de situer un échantillon par rapport aux différentes dimensions évaluées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif du programme de R&D** : \n  - Répondre à la problématique de gestion du patrimoine et de défiscalisation pour particuliers.\n  - Modèle basé sur le conseil indépendant et adapté à chaque situation.\n\n- **Complexité technique de la solution** :\n  - Intégration des avancées en économie, économétrie (analyse des décisions individuelles) et informatique.\n  - Modélisation de recommandations selon plusieurs critères.\n\n- **Incertitude scientifique majeure** :\n  - Création d'un modèle financier intégrant tous les actifs financiers et immobiliers.\n  - Difficulté à modéliser la diversité des objectifs patrimoniaux des particuliers.\n\n- **Incertitudes liées au modèle de proposition de produits d'investissement** :\n  - Problèmes rencontrés dans la modélisation de l'apprentissage automatisé.\n  - Limitations des modèles existants dans la littérature, souvent centrés sur les actifs financiers.\n\n- **Difficultés spécifiques** :\n  - Les modèles génériques ne s'appliquent pas aux actifs immobiliers, qui ont une rentabilité à long terme et une liquidité moindre.\n  - Absence de mécanisme d'amélioration automatique des résultats via des données utilisateurs collectées par des algorithmes de Machine Learning.\n\n- **Objectif de la modélisation originale** :\n  - Améliorer les connaissances existantes en utilisant l'apprentissage automatique.\n  - Extrapoler de manière fiable le profil utilisateur à partir d'informations limitées.\n  - Proposer des produits adaptés aux profils clients, favorisant l'efficience.\n\n- **Problématiques de sélection de produits** :\n  - Confrontation à des problématiques de type « multi-instances ».\n  - Plusieurs descriptions d'objets (profils investisseurs) peuvent aboutir au même résultat (classement de produit identique)."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNotre programme de R&D visait à répondre à la problématique de gestion du patrimoine et de défiscalisation à destination de particuliers. Notre modèle repose sur le conseil indépendant et adapté à chaque situation. La solution que nous avons proposée permet de modéliser des recommandations en fonction de multiples critères.  \n\nNotre solution présente une complexité technique particulière, dans la mesure où elle combine les dernières avancées de différents domaines : l’économie, l’économétrie (analyse des décisions individuelles) et de l’informatique.  \n\nL’incertitude scientifique majeure à laquelle nous avons fait face a été de créer un modèle financier capable d’intégrer tous les actifs financiers et immobiliers, et de traiter l’ensemble des objectifs patrimoniaux des particuliers dont la diversité est complexe à modéliser.  \n\nNous avons par ailleurs rencontré un ensemble d’incertitudes principalement liées au modèle de proposition de produit d'investissements.  \n\nEn effet, concernant l’apprentissage automatisé, nous avons rencontré des difficultés dans la modélisation du problème. Les modèles de la littérature posent un certain nombre de problèmes, les rendant non applicables dans le système que nous souhaitions développer. En effet, ces théories génériques sont cantonnées aux actifs financiers (définir une stratégie d’investissement financier) alors que nos solutions sont également immobilières (des actifs dont la rentabilité ne s’évaluent qu’à long terme et qui sont bien sûr beaucoup moins liquides), d’autre part elles ne permettent pas de mettre en place le mécanisme d’amélioration automatique des résultats en utilisant les données utilisateurs collectées automatiquement à travers des algorithmes d’apprentissage automatique (Machine Learning).  \n\nNous voulions ici proposer une modélisation originale et améliorer les connaissances existantes. Cette modélisation articulée sur l’apprentissage automatique doit également permettre d’extrapoler de manière fiable le profil de l'utilisateur en fonction des informations disponibles, notamment dans le cas d'utilisateurs avec peu d'informations. Cela permet de proposer des produits qui correspondent aux profils des clients (d’où la notion d’efficience).  \n\nPar ailleurs, dans le cas de la sélection de produits, nous nous sommes confrontés à des problématiques de type « multi-instances », c’est-à-dire que plusieurs descriptions d’objets (de profils investisseurs) sont associées à un même résultat (même classement de produit)."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\n- **Verrous généraux de R&D :**\n  - Manque d'études approfondies sur les phénomènes en jeu.\n  - Documentation limitée avec peu d'applications concrètes sur des véhicules.\n  - Absence de simulations à l'échelle 1 tenant compte des variations de conditions (types de sols, roues en rotation, etc.).\n  - Difficulté à généraliser les conclusions basées sur des simulations variées.\n\n- **Incertitudes spécifiques par projet :**\n  - **Projet \"Captation\" :**\n    - Manque d'informations sur la qualité du flux de refroidissement.\n    - Nécessité d'étudier la collecte et l'uniformité du flux.\n    - Impact du ventilateur et distribution de la pression dans le compartiment moteur à évaluer.\n    - Insuffisance d'informations sur l'extraction du flux de refroidissement, y compris les fuites sous la carrosserie et le flux au niveau des roues.\n\n  - **Projet \"Tandem Air Curtain\" :**\n    - Manque d'informations sur le couplage avec d'autres paramètres des véhicules (sabots, jantes, crosses, etc.).\n    - Nécessité de quantifier l'impact des artefacts autour du véhicule (flancs, sillage, roues).\n    - Absence de simulations numériques pour évaluer l'intérêt et prédire les impacts des artefacts.\n\n  - **Projet \"Lame d'air sous le capot\" :**\n    - Incertitudes sur la sensibilité des différents paramètres géométriques.\n    - Défis techniques pour le chercheur (compréhension du phénomène) et le concepteur (recommandations d'utilisation).\n    - Évaluation de l'impact des artefacts sur la consommation par les consommateurs.\n\n  - **Projet \"Cooling-Plate\" :**\n    - Nécessité de créativité pour proposer différentes géométries optimisées.\n    - Prise en compte des paramètres prédominants identifiés dans la littérature.\n    - Aléas techniques liés aux limites des outils numériques pour comprendre l'impact des paramètres et utiliser des géométries complexes."}, {"role": "assistant", "content": "**Aléas, incertitudes scientifiques, verrous technologiques**\n\nLes verrous de cette opération de R&D sont liés au manque d'études approfondies sur le sujet. Les phénomènes en jeu sont souvent documentés de manière limitée, avec peu d'applications concrètes sur des véhicules. De plus, une problématique récurrente dans la littérature actuelle est le manque de simulations réalisées sur des véhicules à l'échelle 1, en prenant en compte des variations de conditions telles que différents types de sols, la présence ou non de roues en rotation, etc. Ce manque de données rend difficile la généralisation des conclusions tirées à partir d’observation de phénomène grâce à différents types de simulation.\n\nDe plus, pour chacun des projets, des incertitudes scientifiques spécifiques à l'axe d'étude se manifestent. Par exemple, dans le projet \"Captation\", il manque des informations sur la qualité du flux de refroidissement, ce qui demanderait une étude de la collecte et de l'uniformité du flux, ainsi que l'impact du ventilateur et la distribution de la pression dans le compartiment moteur. En outre, il y a un manque d'informations concernant la partie de l'extraction du flux de refroidissement, où il serait important d'étudier l'extraction proprement dite, les fuites sous la carrosserie, mais aussi le flux au niveau du passage des roues.\n\nLe verrou de la partie \"Tandem Air Curtain\" réside dans le manque d'informations sur le couplage avec d'autres paramètres des véhicules tels que les sabots, les jantes, les crosses, etc., ainsi que la quantification de l'impact de l'artifice repéré tout autour du véhicule, y compris les flancs, le sillage, les roues, etc. De plus, il n'existe pas de simulations numériques permettant d'affirmer l'intérêt de ces artefacts ou de prédire leurs impacts.\n\nPour le projet \"Lame d'air sous le capot\", les incertitudes portent sur la sensibilité de l'action des différents paramètres géométriques. De plus, cette étude, qui explore ce phénomène sous différents angles, constitue un véritable défi technique : celui du chercheur pour mieux le comprendre, celui du concepteur pour construire des recommandations d'utilisation, ou encore celui des consommateurs pour évaluer l'impact de ces artefacts sur la consommation.\n\nEnfin, pour le projet \"Cooling-Plate\", le verrou technique réside dans la nécessité de faire preuve de créativité pour proposer différentes géométries en vue d'une optimisation, en tenant compte des paramètres prédominants identifiés dans la littérature. S'ajoutent à cela les aléas techniques liés aux limites des outils numériques pour comprendre l'impact des paramètres et utiliser des géométries complexes."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Caractère précurseur du projet** : Les verrous rencontrés sont liés à l'innovation et à la nouveauté du domaine de recherche.\n  \n- **Recherche récente** : Le domaine est encore très récent, entraînant un manque de références pour orienter les travaux et résoudre les incertitudes scientifiques.\n\n- **Absence de publications** : Il existe un manque de publications sur les applications industrielles spécifiques de l'intelligence artificielle combinée à la simulation numérique, ce qui constitue un verrou technique.\n\n- **Attention aux performances et à la sécurité** : La volonté d'application industrielle impose une vigilance sur les performances et la sécurité des solutions développées.\n\n- **Respect des lois de la physique** : Un aspect crucial du projet est l'étude d'une IA qui respecte les lois physiques, ce qui représente un défi dans le domaine de convergence entre IA et simulation numérique.\n\n- **Harmonisation des capacités** : Il est nécessaire d'harmoniser les capacités de l'intelligence artificielle avec les principes physiques, ce qui est complexe.\n\n- **Équilibre entre flexibilité et contraintes** : Un équilibre doit être atteint entre la flexibilité des algorithmes d'IA et le respect des contraintes physiques, nécessitant une recherche et un développement méticuleux."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLes verrous que nous avons rencontrés dans ce projet sont intrinsèquement liés au caractère précurseur de ce projet. Tout d'abord, ce domaine de recherche est encore très récent, ce qui signifie qu'il y a peu de références sur lesquels nous pouvons nous appuyer pour orienter nos travaux et résoudre les incertitudes scientifiques.\n\nDeuxièmement, l'absence de publications concernant les applications industrielles spécifiques de l'intelligence artificielle combinée à la simulation numérique, constitue un verrou technique. De plus, du fait de cette volonté d’application industrielle, nous devons rester attentifs aux performances et à la sécurité de nos solutions.\n\nEnfin, un des aspects cruciaux du projet est l’étude d’une IA qui respecte les lois de la physique. Dans ce domaine récent de recherche où l'IA et la simulation numérique convergent, l'harmonisation entre les capacités de l'intelligence artificielle et les principes physiques est un défi complexe. L'équilibre entre la flexibilité des algorithmes d'IA et le respect des contraintes physiques doit être atteint, ce qui nécessite une recherche et un développement méticuleux."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Système intelligent pour l'identification des visiteurs en temps réel :**\n  - Incertitude scientifique principale : recherche d'une technique de classification performante.\n  - Question clé : capacité à identifier en temps réel les visiteurs potentiels d'un site de vente en ligne.\n\n- **Système de scoring pour les sites de e-commerce basé sur l'expérience client :**\n  - Incertitude : fiabilité de l'adaptation des avis de clients français pour ajuster les offres des magasins virtuels.\n  - Observation : absence de travaux antérieurs sur la collecte et l'analyse des données relatives à ces avis.\n\n- **Système d'identification des entreprises à fort risque de fraude :**\n  - Difficulté majeure : identifier avec précision le profil des entreprises frauduleuses.\n  - Importance de la précision : éviter de fausses alertes sur des entreprises non mal intentionnées.\n  - Conséquences potentielles : erreurs de prédiction peuvent nuire financièrement et à l'image de l'utilisateur du système."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nMise en place d’un système intelligent pour l’identification en temps réel des visiteurs potentiels d’un site de vente en ligne \nLa principale incertitude scientifique que relève la mise en place de notre système réside dans la recherche d’une technique qui assure la classification souhaitée avec la performance la plus élevée possible : \nSerions-nous capables d’identifier en temps réel les visiteurs potentiels d’un site de vente en ligne ? \nMise en place d’un système de scoring pour les sites de e-commerce en se basant sur l’expérience client (UC)\nLa principale incertitude devant un tel système est de pouvoir calquer de façon fiable les avis de clients français sur ces sites afin de permettre aux magasins virtuels de maintenir ou d’améliorer leurs offres en fonction des scores obtenus.\n\nEn effet, au meilleur de notre connaissance, aucun travail ne s’est penché à la collecte et à l’analyse des données relatives à ces avis.\n\nMise en place d’un système d’identification des entreprises à fort risque de fraude\nLa difficulté majeure du système à mettre en place est d’être capable d’identifier le profil d’entreprise frauduleuse en assurant une grande précision qui tient compte des éventuelles conséquences financières et commerciales. En effet, bien que faire ressortir les entreprises potentiellement mal intentionnées soit l’objectif majeur du système (avec tout le gain financier conséquent), nous devons avoir une haute confiance en notre prédiction car le système est censé ne pas induire son utilisateur en erreur en évoquant une éventualité de fraude pour des entreprises qui n’ont pas du tout cette intention. En pratique, cette dernière situation pourrait être extrêmement néfaste pour l’utilisateur du système aussi bien sur le plan financier que sur son image auprès de ses clients."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Projet** : Mise en place d’un système intelligent pour l’identification en temps réel des visiteurs potentiels d’un site de vente en ligne.\n  - **Incertitude scientifique** : Recherche d’une technique de classification avec une performance optimale.\n    - Question clé : Serons-nous capables d’identifier en temps réel les visiteurs potentiels ?\n\n- **Projet** : Mise en place d’un système de scoring pour les sites de e-commerce basé sur l’expérience client (UC).\n  - **Incertitude** : Fiabilité des avis des consommateurs français pour ajuster les offres des magasins virtuels.\n    - Question clé : Serons-nous capables de développer un tel système basé sur l’expérience client ?\n\n- **Projet** : Mise en place d’un système d’identification des entreprises à fort risque de fraude.\n  - **Difficulté majeure** : Identifier avec précision le profil d’entreprise frauduleuse tout en évitant les erreurs de classification.\n    - Objectif : Détecter les entreprises frauduleuses pour générer des gains financiers.\n    - Risque : Éviter de faussement accuser des entreprises non frauduleuses, ce qui pourrait nuire à l'utilisateur sur les plans financier et d'image."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nMise en place d’un système intelligent pour l’identification en temps réel des visiteurs potentiels d’un site de vente en ligne \nLa principale incertitude scientifique que relève la mise en place de notre système réside dans la recherche d’une technique qui assure la classification souhaitée avec la performance la plus élevée possible : \nSerions-nous capables d’identifier en temps réel les visiteurs potentiels d’un site de vente en ligne ? \nMise en place d’un système de scoring pour les sites de e-commerce en se basant sur l’expérience client (UC)\nLa principale incertitude devant un tel système est de pouvoir calquer de façon fiable les avis des français sur ces sites afin de permettre aux magasins virtuels de maintenir ou d’améliorer leurs offres en fonction des scores obtenus : \nSerions-nous capables de mettre en place un tel système en nous basant sur l’expérience client ? \n\nMise en place d’un système d’identification des entreprises à fort risque de fraude\nLa difficulté majeure du système à mettre en place est d’être capable d’identifier le profil d’entreprise frauduleuse en assurant une grande précision qui tient compte des éventuelles conséquences financières et commerciales. En effet, bien que faire ressortir les entreprises qui veulent arnaquer soit l’objectif majeur du système (avec tout le gain financier conséquent), cet objectif n’est pas suffisant car le système est censé aussi ne pas induire son utilisateur en erreur en évoquant une éventualité de fraude pour des entreprises qui n’ont pas du tout cette intention. En pratique, cette dernière situation pourrait être extrêmement néfaste pour l’utilisateur du système aussi bien sur le plan financier que sur son image auprès de ses clients."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Système intelligent pour l'identification en temps réel des visiteurs d'un site de vente en ligne :**\n  - Incertitude scientifique : Recherche d'une technique pour assurer une classification avec une performance optimale.\n  - Question clé : Serons-nous capables d'identifier en temps réel les visiteurs potentiels ?\n\n- **Système de scoring pour les sites de e-commerce basé sur l'expérience client :**\n  - Incertitude principale : Fiabilité dans la représentation des avis des consommateurs français sur les sites.\n  - Objectif : Permettre aux magasins virtuels d'ajuster leurs offres en fonction des scores obtenus.\n  - Question clé : Serons-nous capables de mettre en place un tel système basé sur l'expérience client ?\n\n- **Système d'identification des entreprises à fort risque de fraude :**\n  - Difficulté majeure : Identifier avec précision le profil des entreprises frauduleuses tout en évitant les faux positifs.\n  - Conséquences : \n    - Nécessité d'éviter d'induire en erreur les utilisateurs en signalant des entreprises non frauduleuses.\n    - Risques financiers et d'image pour l'utilisateur si une entreprise innocente est faussement identifiée comme frauduleuse.\n  - Objectif : Détecter les entreprises malveillantes tout en préservant la confiance des utilisateurs."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nMise en place d’un système intelligent pour l’identification en temps réel des visiteurs potentiels d’un site de vente en ligne \nLa principale incertitude scientifique que relève la mise en place de notre système réside dans la recherche d’une technique qui assure la classification souhaitée avec la performance la plus élevée possible : \nSerions-nous capables d’identifier en temps réel les visiteurs potentiels d’un site de vente en ligne ? \nMise en place d’un système de scoring pour les sites de e-commerce en se basant sur l’expérience client (UC)\nLa principale incertitude devant un tel système est de pouvoir calquer de façon fiable les avis des français sur ces sites afin de permettre aux magasins virtuels de maintenir ou d’améliorer leurs offres en fonction des scores obtenus : \nSerions-nous capables de mettre en place un tel système en nous basant sur l’expérience client ? \n\nMise en place d’un système d’identification des entreprises à fort risque de fraude\nLa difficulté majeure du système à mettre en place est d’être capable d’identifier le profil d’entreprise frauduleuse en assurant une grande précision qui tient compte des éventuelles conséquences financières et commerciales. En effet, bien que faire ressortir les entreprises qui veulent arnaquer soit l’objectif majeur du système (avec tout le gain financier conséquent), cet objectif n’est pas suffisant car le système est censé aussi ne pas induire son utilisateur en erreur en évoquant une éventualité de fraude pour des entreprises qui n’ont pas du tout cette intention. En pratique, cette dernière situation pourrait être extrêmement néfaste pour l’utilisateur du système aussi bien sur le plan financier que sur son image auprès de ses clients."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Développement des modules logiciels de la console CMC :**\n  - Défis majeurs rencontrés en 2023.\n  - Nécessité de déploiements dans des environnements air gap :\n    - Installation, mise à jour et maintenance des systèmes.\n  - Objectif d'autonomie totale pour les équipes internes des clients :\n    - Besoin de recherches approfondies pour y parvenir.\n  - Scalabilité essentielle :\n    - Capacité à gérer des parcs de quelques centaines à plus de 100 000 postes.\n  - Intégration dans des parcs informatiques hétérogènes :\n    - Défi architectural et gestion des identités.\n\n- **Intégration des modules CMC dans le portail Web :**\n  - Défis liés à la définition d'interfaces claires et évolutives.\n  - Adoption d'une approche « API first » au sein du service mesh :\n    - Traitement des API comme produit autonome.\n    - Maintien de modules à responsabilité limitée avec connectivité cohérente et évolutive.\n\n- **Adoption du endpoint monitoring :**\n  - Verrous rencontrés :\n    - Uniformisation des informations collectées sur différentes plateformes.\n    - Représentation et accessibilité des informations consolidées au niveau du CMC.\n\n- **Conformité des postes :**\n  - Verrou principal : définition de la conformité et conditions d’ouverture d’un tunnel.\n  - Absence de normes ou définitions claires de la conformité :\n    - Nécessité de collecter les besoins et d’investiguer la faisabilité technique.\n  - Veille permanente requise en raison de l'évolution rapide des produits et API tiers (antivirus, pare-feu, etc.) sur différentes plateformes."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNous avons fait face à différents verrous au cours de nos travaux de l’année 2023.\n\nLe développement des différents modules logiciels de la console CMC a été confronté à plusieurs défis majeurs. Tout d'abord, il a été essentiel de permettre les déploiements dans des environnements air gap, ce qui inclut l'installation, la mise à jour et la maintenance des systèmes. De plus, un objectif crucial a été d'offrir aux équipes internes des clients une autonomie totale en matière de déploiement, de mise à jour et de maintenance, bien que cela nécessite encore des recherches approfondies. La scalabilité a également été un enjeu primordial, avec la nécessité de pouvoir gérer des parcs allant de quelques centaines à plus de 100 000 postes. Enfin, concevoir une console capable d’être intégrée et de fonctionner efficacement dans des parcs informatiques hétérogènes a représenté un défi majeur, tant sur le plan architectural que dans la gestion des identités.\n\nPar ailleurs, l'intégration des divers modules du CMC dans le portail Web a présenté des défis majeurs, notamment en ce qui concerne la définition d'interfaces claires et évolutives entre ces modules. Pour répondre à ce défi, nous avons opté pour une approche « API first » au sein du service mesh, qui consiste à traiter les API comme leur propre produit et pas seulement comme un élément d’intégration.  Cette approche permet de maintenir des modules à responsabilité limitée tout en assurant une connectivité cohérente et évolutive entre eux.\n\nNous avons également rencontré des verrous lors de l’adoption du endpoint monitoring. Les difficultés de ce projet résidaient en l’uniformisation des informations collectées sur les différentes plateformes, ainsi qu’en la représentation et l’accessibilité des informations consolidées au niveau du CMC.\n\nEnfin, lors de nos travaux sur la conformité des postes, le verrou principal a été de définir ce qu’était la conformité et sous quelles conditions l’ouverture d’un tunnel était autorisée. En effet, il n’existe pas à ce jour de norme ou de définition de la conformité et nous avons donc eu à réaliser un travail de collecte des besoins et investiguer la faisabilité technique. Par ailleurs, ce sujet a nécessité une veille permanente du fait de l’évolution rapide des produits et API tiers (antivirus, pare feu, etc.) sur les différentes plateformes."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- Identification de verrous et d'incertitudes dans le projet :\n  - Classification des mails comme spam ou non mal définie.\n  - Limites des connaissances accessibles sur les facteurs de classification.\n\n- Problème principal à résoudre :\n  - Identification des paramètres de classification des mails en tant que spam.\n  - Solutions d’envoi et de réception de mails perçues comme des \"boîtes noires\".\n\n- Outils de mesure de la délivrabilité des mails :\n  - Actuellement incomplets et présentant des limites.\n  - Incapacité à mesurer de manière fiable :\n    - La délivrabilité des mails envoyés par les principaux fournisseurs de messagerie.\n    - La réputation d’une adresse mail.\n\n- Objectif du projet :\n  - Développer un outil de mesure de la délivrabilité et de la réputation d’une adresse mail pour répondre aux incertitudes identifiées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNous avons identifié plusieurs verrous et incertitudes. Les facteurs déterminant la classification d’un mail comme spam ou non sont mal définis et représentent une réelle limite des connaissances accessibles. Le suivi à la lettre des recommandations des principaux fournisseurs de messagerie ne permet pas de s’assurer que les mails seront bel et bien livrés dans la boite de réception principale. Le premier problème à résoudre consiste donc à identifier l’ensemble des paramètres de classification des mails en tant que spam ou non alors que les solutions d’envoi et de réception de mails constituent de réelles boites noires. \n\nDeuxièmement, les outils de mesure de la délivrabilité des mails sont à ce jour incomplets. En effet, ils présentent systématiquement des limites ne permettant pas de mesurer avec fiabilité la délivrabilité des mails envoyés des principaux fournisseurs de messagerie, ni la réputation d’une adresse mail. C’est pour répondre à ces incertitudes que nous avons souhaité développer un outil de mesure de la délivrabilité et de la réputation d’une adresse mail."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Limitations Technologiques Rencontrées (2019)**:\n  - Nécessité de recherches supplémentaires en raison de divers verrous technologiques.\n\n- **Verrou Technologique Principal**:\n  - Comportement du VPN lors de la mise en veille de l’ordinateur :\n    - Automatisation de la mise en place de tunnels et sécurisation du réseau dépend de l’état de la machine.\n    - Délai de réinitialisation de la carte réseau après mise en veille empêche le VPN de connaître l’adresse IP.\n    - Risque de changement de réseau pendant la veille dû à la forte mobilité des utilisateurs VPN.\n\n- **Limitation Connexe**:\n  - Changement fréquent entre réseaux internet (Wi-Fi et Ethernet) :\n    - Éteint une carte et active une autre avec un délai, impactant la connectivité VPN.\n\n- **Difficulté de Détection de Certificat**:\n  - Temps de détection de l’absence de certificat :\n    - Peut prendre entre 15 et 20 secondes de Timeout, multiplié par le nombre de tests.\n    - Long délai pour les utilisateurs nécessitant une connexion sécurisée.\n\n- **Accès aux Données Applicatives**:\n  - Solutions de client VPN existantes limitées à la couche basse :\n    - Accès uniquement à des adresses IP, ports sources et destinations.\n    - Projet nécessite accès à des données applicatives de haute couche pour qualifier les flux.\n\n- **Authentification Machine/Utilisateur**:\n  - Verrou technologique supplémentaire :\n    - Nécessite des modifications de protocoles existants pour l’authentification.\n\n- **Limitations avec les Solutions OTP**:\n  - Mise en place de tunnel VPN dépendante de la connaissance du mot de passe :\n    - Mot de passe non généré sans connexion, nécessitant l’inclusion du tunnel VPN dans le processus de développement."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nAu cours des travaux de R&D de l’année 2019, nous avons rencontré des nombreuses limitations technologiques qui nous ont conduit à des recherches supplémentaires.  \nUn des principaux verrous technologiques rencontrés était comportement du VPN lors de la mise en veille de l’ordinateur. En effet, dans le but de rendre automatique la mise en place de tunnels et la sécurisation du réseau, il est indispensable de connaître l’état de la machine. Or, une fois mise en veille, la carte réseau nécessite un délai pour se remettre en l’état, ce qui empêche la solution VPN de connaître l’adresse IP pour utiliser l’interface (l’utilisateur peut avoir changé de réseau pendant la veille). De plus, la forte mobilité des utilisateurs VPN rend ces travaux de recherche de solution incontournables.  \nAussi, nous pouvons observer une limitation similaire due au changement fréquent de réseau internet wifi et Ethernet, qui éteint une carte et met en route une autre avec un certain délai.  \n\nUne autre difficulté que nous avons rencontrée au cours de nos travaux est le temps de détection de l’absence de certificat, qui peut prendre entre 15 et 20s de Timeout, multiplié par le nombre de tests effectués pour le détecter. Ce temps cumulé est très long pour une personne ayant besoin de se connecter à un réseau de manière sécurisé et dépendant de la nature du procédé par certificats.  \n\nDe plus, les solutions de client VPN dans la littérature sont situées très basse couche, et ne peuvent donc accéder qu’a des adresses IP, des ports sources et destination. Or, dans le cadre de notre projet ayant pour objectif d’intégrer la gestion des flux dans une solution VPN, nous avions besoin d’accéder à des données applicatives, de haute couche, pour pouvoir qualifier l’application liée au flux capté. De même, l’authentification du couple machine / utilisateur est un verrou technologique supplémentaire impliquant des modifications de protocole existant.  \n\nEnfin, pour la prise en compte des solutions de OTP, nous avons rencontré une nouvelle limitation, la mise en place de tunnel VPN nécessitant la connaissance du mot de passe, non généré sans connexion. Ainsi, il nous fallait inclure le tunnel VPN dans la boucle et développer une solution propre."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Limitations technologiques rencontrées :**\n  - Nécessité de recherches supplémentaires durant les travaux de R&D sur le projet ZTNA.\n\n- **Problématique principale :**\n  - Difficulté à établir un tunnel VPN automatiquement sur un réseau protégé par un portail captif.\n  - Impossibilité de récupérer l’adresse IP du serveur pour une connexion sécurisée.\n\n- **Développement supplémentaire :**\n  - Mise en place d'un axe de développement pour détecter les portails captifs.\n\n- **Diversité technologique :**\n  - Absence de standards pour les portails captifs entraînant une multitude de solutions.\n  - Difficulté à qualifier les différents types de portails captifs pour leur détection.\n\n- **Méthode de détection :**\n  - Nécessité d’envoyer des paquets de manière non sécurisée pendant un court laps de temps pour la détection.\n\n- **Verrous technologiques sur Micro VPN :**\n  - Solutions de client VPN existantes limitées à des opérations à très basse couche (adresses IP, ports sources et destination).\n  - Besoin d'accéder à des données applicatives de haute couche pour qualifier les applications liées aux flux captés.\n\n- **Authentification :**\n  - Verrou supplémentaire lié à l'authentification du couple machine/utilisateur nécessitant des modifications de protocoles existants.\n\n- **Intégration des modules :**\n  - Intégration des modules développés (Mode filtrant, TND, Portail Captif, Micro VPN, OTP) dans une solution VPN pour réaliser des tests complets d’intégration et de fonctionnement."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours des travaux de R&D sur le projet ZTNA, nous avons rencontré des nombreuses limitations technologiques qui nous ont conduit à des recherches supplémentaires. \n\nLa principale difficulté fût que nous nous sommes retrouvés face à la problématique de devoir monter un tunnel VPN automatiquement avec un réseau protégé par un portail captif. Ainsi, il n’était pas possible de récupérer l’adresse IP du serveur pour monter le tunnel et sa connexion de manière sécurisée au réseau. Nous avons donc dû mettre en place un axe de développement supplémentaire dans le but de détecter les portails captifs. \nDès lors, nous nous sommes heurtés à la diversité des types de technologies. En effet, il n’existe actuellement pas de standards de portails captifs, et donc de multiples solutions différentes. Ainsi, il fût donc difficile de qualifier ceux-ci pour pouvoir ensuite les détecter. \nEnfin, pour la détection il est donc nécessaire d’envoyer des paquets de manière non sécurisée quelques instants. \n\nAussi, au cours de nos travaux sur Micro VPN, nous avons rencontré de nombreux verrous technologiques. En effet, les solutions de client VPN dans la littérature sont situées à très basse couche, et ne peuvent donc accéder qu’a des adresses IP, des ports sources et destination. Or, dans le cadre de notre projet ayant pour objectif d’intégrer la gestion des flux dans une solution VPN, nous avions besoin d’accéder à des données applicatives, de haute couche, pour pouvoir qualifier l’application liée au flux capté. De même, l’authentification du couple machine / utilisateur est un verrou technologique supplémentaire impliquant des modifications de protocole existant. \n\nDe plus, nous avons dû intégrer les différents modules développés : Mode filtrant, TND, Portail Captif, Micro VPN, OTP, à une solution VPN dans le but d’effectuer des tests complets d’intégration et de fonctionnement."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques - Projet ZTNA\n\n- **Limitations Technologiques** : Rencontres fréquentes de limitations nécessitant des recherches supplémentaires durant les travaux de R&D.\n\n- **Verrou Technologique Principal** : \n  - Difficulté à différencier les paquets dans un flux IPsec.\n  - Nécessité de créer des tunnels d’accès spécifiques pour différentes applications.\n\n- **Mode Filtrant** :\n  - Problèmes de détection de portails captifs nécessitant une intervention utilisateur.\n  - Absence de standards pour les portails captifs entraînant une diversité technologique.\n  - Difficulté à qualifier et détecter les différents portails captifs.\n  - Confrontation à des verrous technologiques liés à la compatibilité du mode filtrant avec des solutions de sécurité existantes chez un partenaire.\n\n- **Gestion des Configurations VPN** :\n  - Nécessité de développer une solution pour permettre aux administrateurs de connaître les configurations VPN sur les postes informatiques.\n  - Absence de solutions existantes a conduit à la création de plusieurs scénarios pour récupérer les configurations VPN.\n  - Identification de la solution la plus opérationnelle à ce stade.\n\n- **Adaptation des Techniques d'Authentification** :\n  - Complexité d’adaptation des techniques telles que le One-Time Password à différents protocoles d’authentification.\n  - Axes de recherche concentrés sur cette problématique."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours des travaux de R&D sur le projet ZTNA, nous avons rencontré de nombreuses limitations technologiques qui nous ont conduit à des recherches supplémentaires. \n\nLe principal verrou technologique que nous avons rencontré concernant le projet de MicroVPN consistait en notre capacité à différencier les différents paquets transitant au sein d’un flux IPsec. Cette différenciation est nécessaire afin de pouvoir créer des tunnels d’accès spécifiques aux différentes applications transitant par le réseau. C’est sur ce point que nos efforts se sont concentrés. \n\nConcernant la mise en place d’un mode filtrant, nous avons rencontré des difficultés quant à la détection d’un portail captif nécessitant l’intervention de l’utilisateur pour passer outre ce portail. Dans la continuité des travaux réalisés en 2020, nous nous sommes heurtés à la diversité des types de technologies. En effet, il n’existe actuellement pas de standards de portails captifs, et donc de multiples solutions différentes. Ainsi, il fût difficile de qualifier ceux-ci pour pouvoir ensuite les détecter. Aussi, lors de la mise en place effective de ce mode filtrant chez un de nos partenaires, nous nous sommes confrontés à des verrous technologiques relatifs à la compatibilité de notre mode filtrant avec les solutions de sécurité déjà mises en place.\n\nIl nous a également paru pertinent de développer une solution permettant à un administrateur d’avoir connaissance des configurations VPN installées sur les différents postes d’un parc informatique. Puisque nous n’avons identifié aucune solution permettant de répondre à cette problématique, nous avons dû développer plusieurs scénarios permettant de récupérer les différentes configurations VPN, avant de déterminer la solution la plus opérationnelle à ce stade.\n\nNous nous sommes également heurtés à la complexité d’adapter des techniques comme le One-Time Password à différents protocoles d’authentification. Nos travaux se sont donc axés sur cette problématique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Limitations Technologiques Rencontrées :**\n  - Nécessité de recherches supplémentaires en raison de diverses limitations.\n\n- **Verrous Technologiques Principaux :**\n  - **Performance et Latence :**\n    - Problèmes liés à l'accès aux ressources réseau via un point de contrôle centralisé avec le ZTNA.\n  - **Scalabilité :**\n    - Difficultés à établir de nombreuses connexions simultanées.\n  - **Intégration des Systèmes Existants :**\n    - Intégration étroite requise avec les systèmes et infrastructures existants pour la mise en place du ZTNA.\n\n- **Verrou Technologique MicroVPN :**\n  - **Adaptation aux Appareils Mobiles :**\n    - Nécessité d'adapter la logique des MicroVPN aux appareils Android.\n    - Différences significatives dans le type et la quantité de données traitées par rapport aux postes informatiques.\n\n- **Mode Filtrant :**\n  - **Difficultés Rencontrées :**\n    - Augmentation du nombre de règles de filtrage entraînant des blocages partiels ou complets du réseau.\n    - Problèmes de contradiction entre les règles et de séquençage.\n\n- **Conception d’un Centre de Connexion :**\n  - **Édition de Configurations VPN :**\n    - Assurance que les administrateurs peuvent définir tous les paramètres d'un client VPN et les utilisateurs y ayant accès.\n\n- **Développement d'une Solution de Monitoring :**\n  - **Connaissance des Configurations VPN :**\n    - Nécessité de développer une solution pour que les administrateurs puissent connaître les configurations VPN installées sur différents postes.\n    - Absence de solutions existantes pour cette problématique.\n    - Développement d'un éditeur de configurations VPN pour définir les paramètres d'exécution et les utilisateurs autorisés.\n    - Résolution de ces problématiques essentielle pour le monitoring des postes connectés à un réseau informatique."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours des travaux de R&D sur le projet ZTNA, nous avons rencontré de nombreuses limitations technologiques qui nous ont conduit à des recherches supplémentaires. \n\nLes principaux verrous technologiques rencontrés ont porté sur la performance et la latence liées à l’accès aux ressources réseau en raison du trafic acheminé via un point de contrôle centralisé avec le ZTNA, sur la scalabilité lorsque de nombreuses connexions simultanées doivent être établies, et aussi sur l’intégration des systèmes existants en raison de l’intégration étroite avec les systèmes et les infrastructures existants que peut nécessiter la mise en place du ZTNA. \n\nUn autre verrou technologique majeur que nous avons rencontré concernant le projet de MicroVPN consistait à adapter la logique des MicroVPN aux appareils mobiles sous Android. Le type et la quantité de données traités par ce genre d’appareil étant significativement différent d’un poste informatique, il nous a fallu définir une logique permettant de répondre aux spécificités des échanges de données de ce type d’appareil mobile. \n\nConcernant la mise en place d’un mode filtrant, nous avons rencontré des difficultés relatives à l’augmentation du nombre de règles de filtrage. Cette augmentation peut en effet entrainer des blocages partiels voir complets du réseau. Des aléas de contradiction entre les règles et de séquençage de ces dernières ont par exemple été relevés.\n\nDans le cadre de la conception d’un centre de connexion permettant d’éditer des configurations VPN, nous avons dû nous assurer qu’un administrateur pouvait définir l’ensemble des paramètres d’un client VPN ainsi que les utilisateurs y ayant accès.\n\nIl nous a également paru pertinent de développer une solution permettant à un administrateur d’avoir connaissance des configurations VPN installées sur les différents postes d’un parc informatique. Puisque nous n’avons identifié aucune solution permettant de répondre à cette problématique, nous avons souhaité développer un éditeur de configurations VPN permettant de définir les paramètres d’exécutions du VPN, ainsi que les utilisateurs pouvant y avoir accès. La résolution des problématiques était nécessaire afin de réaliser nos travaux de monitoring des postes connectés à un réseau informatique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Incertitudes et Verrous Techniques** : Confrontation à divers défis techniques dans le projet de R&D.\n  \n- **Durées de Préparation des Composants** :\n  - Développement d’un système autoalimenté par des données récoltées automatiquement.\n  - Objectif : Minimiser l'intervention des utilisateurs.\n\n- **Emplacement du Consommateur** :\n  - Détermination de l’emplacement pour optimiser le moment de livraison ou de retrait.\n  - Facteurs influents : trajet vers le restaurant, mode de transport, conditions de circulation.\n  - Intégration des données de géolocalisation en temps réel et des informations météorologiques.\n  - Problèmes rencontrés : Complexité de développement d'une solution interopérable avec les moyens de géolocalisation des livreurs.\n\n- **Agrégation des Données pour Préparation de Commande** :\n  - Objectif : Envoi rapide du message de préparation à l’équipier (temps de retrait < 1 minute).\n\n- **Pondération des Critères d’Achat** :\n  - Difficulté à isoler ou corréler plusieurs critères influençant l’acte d’achat.\n  - Incertitude liée à l’ajout de nouveaux critères et recalcul des pondérations.\n\n- **Sécurité et Hygiène Alimentaire** :\n  - Gestion précise des temps de conservation des aliments et matières premières.\n  - Limitation de la méthode actuelle basée sur des déclaratifs.\n  - Impact potentiel sur la gestion des stocks en fonction de l’historique des commandes.\n\n- **Décentralisation de la Reconnaissance Vidéo** :\n  - Problèmes de reconnaissance des objets, notamment des packagings complexes.\n\n- **Interopérabilité des Équipements Physiques** :\n  - Problématique de communication entre systèmes (ex. TPE, imprimantes).\n  - Nécessité de développer des protocoles de communication sur des couches basses.\n  - Objectif : Intégration d’équipements externes en moins de 4 jours.\n\n- **Amélioration du Framework** :\n  - Collaboration avec Sensiolabs pour optimiser la gestion de la charge (chargements asynchrones).\n\n- **Détection RFID Longue Distance** :\n  - Complexité de la détection exclusive des articles dans le panier en temps réel.\n  - Dynamisme des signaux collectés dû au mouvement des produits.\n\n- **Objectif Global** :\n  - Optimisation de la chaîne de production pour augmenter le volume d’affaires et réduire les pertes de denrées périssables."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans le cadre de notre projet de R&D, nous sommes confrontés à un ensemble d’incertitudes, de verrous et de contraintes techniques. Pour y faire face, nous procédons à un ensemble de développements afin de répondre aux questions suivantes : \n\nComment déterminer les durées de préparation de chaque composant rentrant dans la préparation des produits vendus ? Nous visons ici le développement d’un système totalement autoalimenté par des données récoltées automatiquement, sans intervention des utilisateurs. \n\nPar quel moyen nous devons déterminer l’emplacement du consommateur afin de mesurer de manière optimale le moment de livraison ou de retrait de sa commande ? Ce temps est fonction du trajet à faire pour aller au restaurant, le mode de transport utilisé et des conditions de circulations ou déterminer le temps de parcours du livreur en fonction des données de géolocalisation retranscrites en temps réel. Notre technologie doit en effet pouvoir y intégrer les informations météorologiques impactant la célérité du transport. Nous avons à ce niveau rencontré des problématiques liées à la géolocalisation des livreurs au niveau des magasins. Ces livreurs utilisent pour la grande majorité leurs propres moyens de géolocalisation. Pour cela, nous nous sommes confrontés à la complexité de développement d’une solution interopérable avec l’ensemble de ces moyens. \n\nComment agréger l’ensemble des données pour déterminer le moment d’envoi du message de préparation de la commande à l’équipier ? Cela permet enfin de faciliter le retrait de la commande, (objectif : un temps de retrait inférieur à la minute). \n\nComment pondérer les critères impactant l’acte d’achat de l’utilisateur pour obtenir des préconisations justes ? Une des difficultés majeures, réside dans l’utilisation de plusieurs critères qui ont une influence isolée ou corrélée à d’autres facteurs. Enfin, un autre axe d’incertitude concerne l’ajout ultérieur de nouveaux critères pour lesquels nous devrons étudier les nouvelles corrélations et recalculer les pondérations de l’ensemble des critères. \n\nComment tenir compte des différentes contraintes liées au respect de la sécurité et l’hygiène alimentaire ? Le respect de ces contraintes a pour objectif de respecter les délais de conservations des aliments. Nous devons donc proposer une solution capable de gérer avec une très grande précision les temps de conservation des produits et la matière première (exemple fast food). La méthode basée sur les déclaratifs, utilisée actuellement, ne répond pas à nos exigences. Il est, par ailleurs, nécessaire de gérer l’impact sur les commandes à passer en fonction de l’historique. Un impact sur la façon de gérer les stocks est également envisageable. \n\nComment décentraliser de la reconnaissance vidéo sur des équipements externes ? Nous avons également rencontré des problématiques de reconnaissance des objets. Un verrou concernait notamment la capacité de notre solution à reconnaitre le packaging dont les formes sont complexes et non prévisibles. \n\nNous avons par ailleurs rencontré un problème d’architecture de communication avec les équipements physiques TPE (exemple Ingenico, Atos) sur la machine. Il s’agit ici d’une problématique d’interopérabilité entre différents systèmes. Nous devons à ce niveau lever un verrou de communication avec le système de communication de l’ensemble de nos sondes de température et différents équipements (imprimantes, affichages…). Ce point a notamment nécessité des travaux complexes de développement de protocoles de communication sur des couches basses (au niveau électronique). L’objectif étant de développer une solution permettant d’intégrer par la suite n’importe quel équipement externe en moins de 4 jours. \n\nComment améliorer notre Framework afin de gérer la charge de manière optimale ? En 2016, nous avons notamment repensé notre Framework et réalisé un travail en collaboration avec la société Sensiolabs (créateurs de Symphony) afin de permettre des chargements asynchrones.\n\nDans le cadre de la détection d’articles par RFID longue distance, la complexité rencontrée concerne la détection exclusive des articles présents dans le panier du client en temps réel. Cette complexité est liée également à l’aspect dynamique des signaux collectés, i.e. au déplacement des produits dans le magasin, la mise d’articles dans un panier, etc. \n\nLa conjugaison de l’ensemble des solutions permettra d’optimiser la chaîne de production pour augmenter le volume d’affaires de nos clients et réduire les pertes de denrées périssables."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Difficultés rencontrées** : Problèmes non résolus par l'état de l'art lors des développements.\n  \n- **Incertitude scientifique initiale** : Mécanisme de changement de réseau lorsque l'utilisateur sort de la zone couverte par le réseau du partenaire local.\n  \n- **Solution classique** : Changement de carte SIM, mais cette approche impacte négativement les utilisateurs et risque d'interrompre le service.\n  \n- **Objectif de développement** : Créer un mécanisme permettant de passer d'un IMSI à un autre sans que l'utilisateur ne s'en aperçoive, ce qui n'est pas couvert par l'état de l'art.\n  \n- **Nouvelles incertitudes scientifiques** :\n  - Problématique des zones blanches (absence de couverture réseau).\n  - Problématique des zones grises (service limité à un seul opérateur).\n  \n- **Questions soulevées** :\n  - Quand basculer vers le roaming sponsor ?\n  - À quelle fréquence vérifier la présence, l'absence ou la qualité du réseau ?\n  - Comment assurer un basculement seamless pour l'utilisateur (sans interruption des services de SMS, appels et données) ?\n  \n- **Basculement du roaming sponsor vers l'opérateur d'origine** :\n  - Coût élevé du roaming, nécessité de minimiser le temps passé sur ce réseau tout en optimisant l'expérience utilisateur.\n  \n- **Durée du basculement** : Prend une à deux minutes, ce qui peut sembler long pour l'utilisateur. Nécessité de gérer l'interruption de service durant cette période.\n  \n- **Prévention de l'hystérésis** : Éviter des basculements trop fréquents (toutes les 5 minutes) ; nécessité de s'assurer de la nécessité du basculement.\n  \n- **Verrou technologique supplémentaire** : Les cartes SIM possèdent plusieurs IMSI, soulevant la question de comment déterminer avec certitude le numéro actif à un moment donné."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nAu cours de nos développements, nous avons fait face à des difficultés qui ne sont pas adressées par l’état de l’art. \n\nUne première incertitude scientifique à laquelle nous avons été confrontés concernait le mécanisme de changement de réseau lorsque l’utilisateur n’est plus en zone couverte par le réseau du partenaire local.\n\nCette incertitude est classiquement résolue en changeant la carte SIM des utilisateurs. Cependant, cette solution n’est pas optimale car elle impacte fortement les utilisateurs qui doivent changer de carte SIM, avec le risque d’interruption de service que cela représente. Nous voulions donc trouver une solution n’impliquant pas un changement de carte SIM.\n\nNous voulions ainsi développer un mécanisme plus fluide pour les utilisateurs, c’est-à-dire un mécanisme permettant à la carte SIM de passer d’un IMSI à un autre sans que cela ne soit détectable par les utilisateurs. Cela n’est pas adressé par l’état de l’art.\n\nAu fur et à mesure de l’avancement de nos travaux, de nouvelles incertitudes scientifiques sont apparues, liées notamment à la problématique des zones blanches où il n’existe pas de couverture réseau, ou encore les zones grises où il existe un service limité à un seul opérateur :\n\nQuand basculer vers le roaming sponsor ? A quelle fréquence devons-nous vérifier la présence, l’absence ou la qualité du réseau ? Comment procéder pour que le basculement soit seamless pour l’utilisateur (ie : après le basculement, l’identifiant de l’utilisateur n’est plus le même : à partir de là, comment pouvons-nous procéder pour que le service ne connaisse pas d’interruption dans l’émission et la réception des sms, appels et données ?\n\nQuand est-ce que le système doit basculer du roaming sponsor vers l’opérateur d’origine ? le roaming coûte cher, donc il faut passer le moins de temps possible sur le réseau du roaming sponsor, tout en optimisant l’expérience pour l’utilisateur.\n\nLe basculement prend une à deux minutes ce qui peut paraitre long à l’utilisateur ; comment pouvons-nous gérer l’interruption du service à ce moment-là. En plus, au regard de la durée du basculement, nous ne pouvons pas se permettre de basculer toutes les 5 minutes pour à éviter les situations d'hystérésis, il faut donc être certain du besoin : comment s’assurer de la nécessité du basculement ?\n\nUn autre verrou technologique que nous avons rencontré concernait le fait que nos cartes SIM disposent de plusieurs IMSI, c’est-à-dire que sur chaque carte, il y a plusieurs numéros. Aussi, comment pouvions-nous déterminer avec certitude quel est le numéro actif à un instant donné ?"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n#### Hybridation des Réseaux Cellulaires Publics et Privés\n- **Incertitude Principale :** Fluidité du passage entre réseaux publics et privés au moment opportun.\n  - Exemple : Basculement de la carte SIM d’un réseau public à un réseau privé.\n- **Verrou Technique :** Choix du meilleur mécanisme pour le passage d’un cœur de réseau à l’autre.\n- **Solution Envisagée :** Utilisation des technologies de roaming pour l’hybridation.\n  - **Difficultés Identifiées :**\n    - Nécessité d’interfaces compatibles entre réseaux publics et privés.\n    - Opérateurs de réseaux privés souvent non membres de la GSMA et non raccordés à l’IPX, entraînant des problèmes d’interopérabilité.\n    - Coûts élevés pour surmonter ces difficultés.\n  - **Conclusion :** Le roaming n'est pas une solution viable.\n- **Autres Solutions Proposées :**\n  - Développement de cartes eSIM double ou mono IMSI sans interface de roaming.\n  - Mécanisme d’échange de secrets entre le cœur de réseau privé et le cœur de réseau public.\n- **Nouvelle Incertitude :** Détermination du nombre de profils IMSI à inclure dans la SIM.\n- **Complexité du Retour :** Passage du cœur de réseau public au cœur de réseau privé plus complexe.\n  - Incertitude sur la rapidité du retour au réseau privé.\n- **Service Unifié :** Incertitude sur la proposition d’un service unifié sur tous les réseaux, avec études en cours.\n\n#### ICARO – Réseau Mobile dans les Avions\n- **Verrou Technique Principal :** Faisabilité de la solution pour couvrir l’air au-dessus des océans.\n- **Questions Clés :**\n  - Comment gérer le réseau à haute altitude (10 000 mètres) ?\n  - Comment gérer les processus de transfert du réseau (Handover) ?\n- **Facteurs d’Incertitude :** \n  - Altitude de croisière des avions de ligne.\n  - Vitesse élevée des avions, entraînant des défis pour le développement."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nHybridation des réseaux cellulaires publics et privés\nLa principale incertitude scientifique à laquelle nous avons été confrontés lors de nos développements qui portaient sur l’hybridation des réseaux publics et privés concernait le passage de l’un à l’autre en toute fluidité et au moment opportun. Par exemple, si l’utilisateur capte toujours son réseau public sur le site industriel, il faut que sa carte SIM bascule tout de même sur le réseau privé : comment pouvions-nous déclencher ce basculement ? Un autre verrou technique auquel nous avons été confrontés concernait le passage d’un cœur de réseau à l’autre : quel était le meilleur mécanisme pour l’utilisation que nous voulions en faire ?\n\nComme nous l’avons déjà mentionné au paragraphe 1.1, nous avons tout d’abord envisagé d’utiliser les technologies de roaming pour réaliser l’hybridation des réseaux publics et privés. \n\nCependant cette solution est en réalité difficile à développer et à déployer car le réseau privé doit disposer des interfaces compatibles avec les réseaux publics, ce qui n’est pas systématique. En outre, les opérateurs de réseaux privés sont des sortes de boîtes noires qui ne sont pas toujours membres de la GSMA, ni raccordés à l’IPX. Cela entraîne d’importantes difficultés d’interopérabilité avec les réseaux publics. Ces difficultés sont surmontables, mais cela serait bien trop coûteux. Le roaming n’est donc pas une solution viable à notre problématique. \n\nNous avons donc proposé d’autres solutions : développement de cartes eSim double ou mono IMSI sans interface de roaming dédiées, ainsi qu’un mécanisme d’échange de secrets entre le cœur de réseau privé et notre cœur du réseau public. \nNous avons alors de nouveau été confrontés à une incertitude scientifique : combien de profils IMSI mettre dans la SIM ? \n\nEn outre, s’il est évidemment facile de passer du réseau privé au réseau public, le processus est nettement plus complexe dans le sens inverse. Ainsi, l’incertitude scientifique concernait le passage du cœur de réseau public au cœur de réseau privé : comment revenir le plus vite possible sur le réseau privé ? \n\nEnfin, la dernière incertitude scientifique portait sur comment proposer un service unifié sur l’ensemble des réseaux. Sur ce point, nous avons mené des études que nous présentons dans la suite de ce document. \nICARO – Réseau mobile dans les avions\nLors des développements liés à notre projet ICARO, le principal verrou technique auquel nous avons fait face concernait la faisabilité de la solution que nous souhaitions développer : \nComment pouvions-nous couvrir l’air au-dessus des océans ? \nComment allions-nous gérer le réseau à haute altitude (10 000 mètres) ? \nComment allions-nous gérer les processus de transfert du réseau (Handover) ? \n\nEn effet, l’altitude de croisière et la vitesse élevées des avions de ligne entrainaient d’importants facteurs d’incertitudes pour nos développements."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n#### Hybridation des Réseaux Cellulaires Publics et Privés\n\n- **Incertitude Scientifique Principale :**\n  - Fluidité du passage entre réseaux publics et privés au moment opportun.\n  - Exemple : nécessité de basculer la carte SIM d'un réseau public à un réseau privé même si le réseau public est toujours capté.\n\n- **Verrou Technique :**\n  - Détermination du meilleur mécanisme pour le passage d’un cœur de réseau à l’autre.\n\n- **Solution Envisagée :**\n  - Utilisation des technologies de roaming pour l’hybridation des réseaux.\n\n- **Difficultés Associées au Roaming :**\n  - Nécessité pour le réseau privé de disposer d'interfaces compatibles avec les réseaux publics (non systématique).\n  - Les opérateurs de réseaux privés peuvent être des \"boîtes noires\", souvent non membres de la GSMA et non raccordés à l’IPX.\n  - Problèmes d’interopérabilité avec les réseaux publics, entraînant des coûts élevés pour surmonter ces difficultés.\n  - Conclusion : le roaming n'est pas une solution viable pour notre problématique.\n\n- **Autres Solutions Proposées :**\n  - Développement de cartes SIM/eSIM double ou mono IMSI sans interface de roaming dédiée.\n  - Mise en place d’un mécanisme d’échange de secrets entre le cœur de réseau privé et le cœur de réseau public.\n\n- **Incertitudes Scientifiques Supplémentaires :**\n  - Détermination du nombre de profils IMSI à intégrer dans la SIM.\n  - Complexité du passage du cœur de réseau public au cœur de réseau privé : nécessité de revenir rapidement sur le réseau privé.\n  - Proposition d'un service unifié sur l’ensemble des réseaux."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nHybridation des réseaux cellulaires publics et privés\nLa principale incertitude scientifique à laquelle nous avons été confrontés lors de nos développements qui portaient sur l’hybridation des réseaux publics et privés concernait le passage de l’un à l’autre en toute fluidité et au moment opportun. Par exemple, si l’utilisateur capte toujours son réseau public sur le site industriel, il faut que sa carte SIM bascule tout de même sur le réseau privé : comment pouvions-nous déclencher ce basculement ? Un autre verrou technique auquel nous avons été confrontés concernait le passage d’un cœur de réseau à l’autre : quel était le meilleur mécanisme pour l’utilisation que nous voulions en faire ?\n\nNous avons tout d’abord envisagé d’utiliser les technologies de roaming pour réaliser l’hybridation des réseaux publics et privés. \n\nCependant cette solution est en réalité difficile à développer et à déployer car le réseau privé doit disposer des interfaces compatibles avec les réseaux publics, ce qui n’est pas systématique. En outre, les opérateurs de réseaux privés sont des sortes de boîtes noires qui ne sont pas toujours membres de la GSMA, ni raccordés à l’IPX. Cela entraîne d’importantes difficultés d’interopérabilité avec les réseaux publics. Ces difficultés sont surmontables, mais cela serait bien trop coûteux. Le roaming n’est donc pas une solution viable à notre problématique. \n\nNous avons donc proposé d’autres solutions : développement de cartes SIM/eSIM double ou mono IMSI sans interface de roaming dédiées, ainsi qu’un mécanisme d’échange de secrets entre le cœur de réseau privé et notre cœur du réseau public. \nNous avons alors de nouveau été confrontés à une incertitude scientifique : combien de profils IMSI mettre dans la SIM ? \n\nEn outre, s’il est évidemment facile de passer du réseau privé au réseau public, le processus est nettement plus complexe dans le sens inverse. Ainsi, l’incertitude scientifique concernait le passage du cœur de réseau public au cœur de réseau privé : comment revenir le plus vite possible sur le réseau privé ? \n\nEnfin, la dernière incertitude scientifique portait sur comment proposer un service unifié sur l’ensemble des réseaux ?"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif principal** : Améliorer l’hybridation des réseaux privés et publics.\n  \n- **Verrou principal** : \n  - Manque de suivi du réseau public, géré de manière autonome par la carte SIM.\n  - Impact sur la gestion de la facturation en ligne.\n  \n- **Solutions proposées** :\n  - Développement d'un nouveau SCP (Système de Contrôle de la Performance).\n  - Élaboration d'un nouveau guide pour le contrôle et l'acheminement des données, la gestion du contrôle d’accès, et le déploiement de la tarification en temps réel.\n\n- **Incertitude liée à l'échelle** :\n  - Passage à l’échelle industrielle des travaux de 2021.\n  - Collaboration avec Air France et Renault pour expérimenter divers scénarios d’utilisation dans des POC (Proof of Concept) à échelle réelle.\n\n- **Verrou lors du développement des cœurs de réseau IMS** :\n  - Gestion de l’itinérance selon les différents opérateurs partenaires et leurs types de réseaux.\n  - Problèmes d'interopérabilité technique entre systèmes et infrastructures réseau.\n  - Dépendance vis-à-vis de l’opérateur en cas de raccordement sans partenariat d’itinérance.\n  - Nécessité de garantir un service consistant pour différents opérateurs radios avec des interfaces variées.\n  - Gestion du service IMS dans un contexte multi-pays et multi-services.\n\n- **Verrous technologiques dans le contexte automobile et MVNO** :\n  - Problème de reprogrammation de la carte SIM, impactant les supports de terminaux.\n  - Changement d’IMSI (International Mobile Subscriber Identity) modifiant l'APN (Access Point Name) du téléphone.\n  - Développement nécessaire pour que l’APN du nouvel opérateur soit reconnu dans les systèmes Transatel.\n  - Exécution de plusieurs scénarios et tests pour comprendre les impacts sur les systèmes (DNS, flux de données).\n\n- **Travaux au sein du consortium européen New Symphonie** :\n  - Étude des futurs systèmes de constellations satellites.\n  - Problématique de la fusion des réseaux cellulaires et satellitaires.\n  - Verrou à lever : Intégration des satellites dans les normes cellulaires actuelles.\n  - Sécurité à long terme des réseaux face à la cryptographie post-quantique, en raison de leur mise en service pour une durée indéterminée.\n  - Étude des impacts de ces verrous sur les couches radios de bas niveau."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNotre premier objectif était d’améliorer l’hybridation des réseaux privés et publics. Le verrou principal auquel nous étions confrontés concernait le manque de suivi du réseau public, géré jusqu’alors de manière autonome par la carte SIM. Ce manque impacte notamment notre capacité à gérer la facturation en ligne. Le verrou à lever consistait alors à proposer un nouveau SCP, accompagné d’un nouveau guide, tous deux nécessaires au contrôle et à l’acheminement des données, à la gestion du contrôle d’accès et au déploiement de la tarification en temps réel. Une autre source d’incertitude pour notre projet résidait dans le passage à l’échelle industrielle de nos travaux de 2021. En collaboration avec Air France et Renault, nous avons dû expérimenter divers scénarios d’utilisation dans le cadre de POC à échelle réelle afin de résoudre des problématiques spécifiques.  \n\nLors du développement de nos cœurs de réseau IMS, nous nous sommes heurtés au verrou lié à la gestion de l’itinérance en fonction de nos différents opérateurs partenaires et de leurs types de réseaux. En effet, nous sommes parfois raccordés à l’opérateur en itinérance, ce qui nécessite une interopérabilité technique entre nos systèmes et l’infrastructure réseau. A contrario, nous pouvons être raccordés sans partenariat d’itinérance, ce qui crée une dépendance vis-à-vis de l’opérateur, qui met à notre disposition le niveau d’interfaces qu’il souhaite. Notre problématique était donc de garantir que le service soit consistant pour différents opérateurs radios n’ayant pas toujours les mêmes interfaces ; il nous a fallu parvenir à gérer notre service IMS dans un contexte multi pays et multi services.\n\nPlusieurs verrous technologiques ont été identifiés dans le contexte de programmation d’un contexte opérateur dans le contexte automobile et MVNO. La reprogrammation d’une carte SIM, qui est un problème connu, donne lieu à une problématique liée aux supports de terminaux. En effet, passer d’un IMSI Transatel à l’IMSI d’un autre opérateur modifie l’APN (Access Point Name) du téléphone. Il faut donc développer l’ensemble du système pour que l’APN du nouvel opérateur soit reconnu dans les systèmes Transatel. Pour comprendre les impacts sur les systèmes (DNS, flux de données…) et lever ce verrou, nous avons dû exécuter plusieurs scenarios et effectuer différents tests.\n\nTransatel a également réalisé des travaux au sein du consortium européen New Symphonie dans le but d’étudier les futurs systèmes de constellations satellites. Nous avons été confrontés au sujet innovant de la fusion des réseaux cellulaires et satellitaires. Un premier verrou à lever était : comment intégrer le satellite dans les normes cellulaires actuelles. Le second verrou portait sur la sécurité à long terme des réseaux et leur résistance à la cryptographie post quantique, du fait de leur future mise en service pour une durée indéterminée. Nous avons donc étudié les impacts de ces verrous sur les couches radios bas niveau."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Incertitude sur l'hybridation des réseaux :**\n  - Difficulté à passer de manière fluide entre réseaux publics et privés.\n  - Problèmes d’hétérogénéité des terminaux et des réseaux.\n  - Passage du réseau privé au public facile, inverse plus complexe.\n\n- **Solution développée :**\n  - Recherche régulière des réseaux privés disponibles.\n  - Rejet de la connexion par le réseau privé si l'IMSI public n'est pas reconnu.\n\n- **Mécanisme de passage :**\n  - Idéalement géré par la carte SIM.\n  - Difficulté d'implémentation d'applications sur certains terminaux (ex. : routeurs 4G/5G, terminaux Apple).\n\n- **Problématique de la carte SIM :**\n  - En mode « esclave », reçoit des informations uniquement du terminal.\n  - Échanges standardisés mais liberté d’implémentation des constructeurs, entraînant un comportement non déterministe.\n\n- **Passage réseau privé/public :**\n  - Perte de connectivité réseau signalée à la SIM, permettant une action pour basculer vers un réseau privé.\n\n- **Normalisation de la 5G :**\n  - Normes en cours d'élaboration, ajoutant une complexité aux développements.\n  - Questions sur l'architecture à déployer (cœur 5G seul, cœur 4G en parallèle, etc.).\n\n- **Intégration du roaming 5G :**\n  - Nécessité d'étudier et définir des solutions pour l'interface de roaming 5G.\n\n- **Service voix sur LTE (approche IMS) :**\n  - Hétérogénéité des raccordements (Light MVNO, Full MVNO, roaming) pose des défis d’adaptation.\n  - Absence de normalisation formelle complique la préparation d'une solution universelle.\n\n- **Hétérogénéité des terminaux :**\n  - Paramètres VoLTE spécifiques à Transatel, non reconnus par d'autres MNO.\n  - Nécessité de solutions de contournement pour assurer la compatibilité avec divers terminaux.\n\n- **Application opérateur :**\n  - Idée d'une application intégrée dans la ROM du téléphone, nécessitant des accords avec les fabricants.\n  - Solution alternative : application téléchargeable via Google Store, nécessitant un certificat de la carte SIM.\n\n- **Problèmes rencontrés :**\n  - Difficultés d'accès aux API sur certains terminaux (ex. : Samsung), mais succès sur d'autres.\n\n- **Migration HSS :**\n  - Incertitude liée à la diversité des approches et des contextes à tester.\n  - Nécessité de tester six combinaisons différentes pour choisir l'approche adéquate."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLa principale incertitude scientifique à laquelle nous avons été confrontés lors du développement de notre solution d’hybridation des réseaux publics et privés sur des réseaux réels concernait le passage de l’un à l’autre en toute fluidité et au moment opportun. Nous avons été confrontés à des problèmes d’hétérogénéité des terminaux et des réseaux. S’il était relativement facile de passer du réseau privé au réseau public, le processus est nettement plus complexe dans le sens inverse.\n\nFace à cette problématique, nous avons imaginé et développé une solution permettant de chercher régulièrement les réseaux privés existants dans la zone en question, puis, de laisser le réseau privé rejeter la tentative de connexion puisqu’il ne connait pas le IMSI public.\n\nPour bien comprendre cette problématique, il faut savoir que nous souhaitions tout d’abord un mécanisme de passage automatique géré entièrement par la carte SIM. En effet, une alternative aurait été d’implémenter la logique de passage privé/public dans une application résidante dans le terminal mais d’une part il n’est pas toujours possible de déployer une application dans un terminal (par exemple dans un routeur 4G/5G qui n’est pas un environnement ouvert permettant d’y installer des applicatifs), et d’autre part lorsque des applications sont déployables dans les terminaux (par exemple dans des smartphones), celles-ci n’ont pas toujours la possibilité d’interagir avec la carte SIM (comme par exemple dans les terminaux Apple).\n\nUne fois le besoin de développer cette solution dans la carte SIM établi, la problématique était que, dans une station mobile composée d’un terminal mobile et d’une carte SIM, la carte SIM est en mode « esclave » et ne reçoit des informations sur son environnement que de la part du terminal (ou plus précisément de la pile protocolaire de base : le baseband). Malheureusement, bien que beaucoup d’échanges entre la SIM et le baseband soient standardisés, une liberté d’implémentation est laissée aux constructeurs et le comportement des terminaux vis-à-vis des cartes SIM n’est ainsi pas déterministe. Cela a mené à la nécessité de faire des hypothèses, des prototypes et beaucoup d’essais empiriques pour aboutir à une solution.\n\nComme indiqué plus haut, le passage du réseau privé au réseau public était un problème relativement facile à résoudre. En effet dans ce cas-là, le terminal se retrouve en perte de connectivité réseau (car utilisant l’IMSI privé sur un réseau public avec lequel il n’y a pas d’accord d’itinérance), et cette perte de connectivité réseau est en général bien signalée à la SIM, qui peut ainsi agir en conséquence et décider d’initier une bascule vers un réseau privé.  \nUn deuxième aléa concernait la normalisation de la 5G. En effet, les normes de la 5G sont encore en cours d’élaboration, elles sont donc très mouvantes. Cela ajoutait une difficulté supplémentaire à nos développements, qui devaient rester suffisamment flexibles pour être adaptés à une éventuelle modification des normes. Dès lors, quelle architecture 5G allons-nous déployer ? Allons-nous vers un cœur de réseau 5G qui va gérer uniquement la radio 5G tout en gardant parallèlement notre cœur 4G actuel qui gérera la 4G ? Allons-nous garder uniquement la 5G étant donné qu’un cœur de réseau 5G peut aussi supporter la 4G ? Même si la 2ème possibilité de déploiement (5G uniquement) est valide en théorie, qu’en est-il en pratique ? Y aurait-il un intérêt de garder un réseau 4G pour qu’il soit contrôlable par un cœur 5G ? Et dans ce cas, les MNO vont-ils adopter ce déploiement ?\n\nEn outre, nous avons fait face à une incertitude scientifique qui concernait l’intégration du roaming 5G. En effet, cela requérait d’étudier et de définir des solutions sur l’interface de roaming 5G. Ainsi, comment gérer ceci sur les différents types de raccordements avec un déploiement 5G ?\n\nConcernant les travaux sur le service voix sur LTE avec une approche de type IMS, au-delà des contraintes liées à l’environnement complexe et non normalisé pour un opérateur MVNO, les incertitudes techniques ont porté d’une part sur l’hétérogénéité de nos raccordements. En effet, pour implémenter le même service, nous avons été confrontés à trois configurations différentes sur le même réseau radio (Light MVNO, Full MVNO et roaming). Faute de normalisation formelle comment préparer une solution indépendante des multiples variantes imposées par les opérateurs ? L’incertitude et le risque provenaient donc de la nécessité d’arriver à adapter le même réseau radio à des contraintes techniques différentes.\n\nD’autre part, nous avons fait face à une autre difficulté scientifique liée à l’hétérogénéité des terminaux. Effectivement, les paramètres VoLTE de notre solution sont spécifiques à Transatel (non connus par les autres MNO). Les téléphones (tels qu’iPhone, Samsung, Huaweï) ne connaissent pas Transatel donc nos paramètres IMS ne sont pas connus par les téléphones car ils n’y sont pas déployés.  \nNous avons donc dû envisager des solutions de contournement pour arriver à fournir un service qui fonctionne dans la majorité des terminaux.  \nNous avons constaté qu’il faudrait une application de type opérateur et donc qui soit mise dans la ROM du téléphone comme étant opérateur. Toutefois, cette solution nécessitait un accord avec Apple, Samsung ou Huawei et reste donc coûteuse, voire impossible sans accord avec ces géants.\n\nAfin de remédier à cette faiblesse, nous avons exploité une application opérateur qui puisse être téléchargée depuis le Google Store. Pour ce faire, il faut que l’application soit signée avec un certificat se trouvant dans la carte SIM et que la carte SIM dispose d’une application qui transmette ce certificat au terminal. Cette solution n’était pas adressée par l’état de l’art. Nous avons donc investigué le code source Android afin de comprendre et tester l’application sur la carte. Nous avons été confrontés à un problème sur les terminaux Samsung, qui bloquaient cet accès, alors que nous avons pu l’implémenter sur d’autres terminaux. Cette solution nous a donc permis d’avoir accès aux API qui nous intéressent pour ensuite mettre à jour les paramètres et nous donne ainsi une solution partielle à nos problèmes.\n\nConcernant les travaux sur la migration HSS, la principale incertitude scientifique était liée à la diversité des approches disponibles et des contextes à tester. Il a fallu tester six combinaisons différentes et étudier leurs avantages et limites afin de choisir l’approche approprié à notre contexte."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif principal** : Améliorer l’hybridation des réseaux privés et publics.\n  \n- **Verrou principal** : \n  - Manque de fonctionnalités des réseaux privés.\n  - Impact direct sur la capacité de fonctionnement dans un réseau public.\n  - Nécessité de proposer une solution peu coûteuse pour intégrer les fonctionnalités nécessaires à la transition entre réseaux.\n\n- **Problématique d'authentification** : \n  - Nécessité de résoudre l'authentification pour les réseaux disponibles.\n  - Intégration de nouvelles fonctionnalités d’authentification pour les cartes SIM.\n  - Objectif : Lever le verrou de bascule entre les types de réseaux.\n\n- **Verrous technologiques identifiés dans les réseaux 5G** :\n  - Exigences des partenaires MVNO, notamment British Telecom.\n  - Architecture des réseaux 4G insuffisante pour répondre aux exigences des réseaux 5G.\n  - Mise à jour et adaptation des fonctionnalités du réseau pour assurer les connexions de données et les sessions de mobilité.\n  - Solutions développées doivent être élégantes, simples et à coût limité.\n  - Réutilisation de l'infrastructure 5G existante pour intégrer les nouvelles fonctionnalités.\n  - Amélioration des architectures pour les services Non Voice et VoLTE Voice.\n  - Gestion de la consommation des données pour les utilisateurs migrés d’un réseau 4G vers un réseau 5G.\n\n- **Développement de la plateforme de provisionnement NewStack** :\n  - Problématique d'association entre MSISDN et carte SIM.\n  - Statut des cartes SIM dépendant du distributeur (ex : cartes SIM de BT actives par défaut).\n  - Conception d'une architecture pour résoudre cette problématique.\n\n- **Migration vers la nouvelle plateforme de provisionnement** :\n  - Processus de migration incluant le transfert de gestion des ressources et de provisionnement.\n  - Assurance du processus de facturation des abonnés.\n  \n- **Incertitude à lever** : \n  - Vérification de l’opérabilité des modifications apportées.\n  - Vérification du bon fonctionnement des processus de portabilité et de provisionnement des utilisateurs.\n  \n- **Collaboration avec British Telecom** : \n  - Accompagnement pour assurer le transfert vers leur nouvelle plateforme de provisionnement.\n  - Nécessité de plusieurs développements pour ce transfert."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNotre premier objectif était d’améliorer l’hybridation des réseaux privés et publics. Le verrou principal auquel nous étions confrontés concernait le manque de fonctionnalités des réseaux privés. Ce manque impacte alors directement leur capacité à fonctionner dans un réseau public. Le verrou à lever consistait alors à proposer une solution peu coûteuse d’intégration des fonctionnalités nécessaire à la bascule entre un réseau public et un réseau privé. \n\nUne fois ce travail réalisé, la problématique de l’authentification à l’un ou l’autre des réseaux disponibles devait être résolue. Les cartes SIM possèdent différents attributs permettant à un réseau d’identifier quelle carte souhaite s’y connecter. Nous avons donc souhaité intégrer de nouvelles fonctionnalités d’authentifications de la carte SIM. Ces fonctionnalités devaient par la suite permettre de lever le verrou de bascule d’un type de réseau à un autre. \n\nPlusieurs verrous technologiques ont aussi été identifiés dans le contexte de l’utilisation des réseaux 5G. Transatel, en tant que partenaires de nombreux opérateurs MVNO, devait répondre aux exigences de l’ensemble de ces partenaires. Par exemple, les intentions stratégiques de British Telecom devaient être satisfaites à l’issue de ces travaux. L’architecture des réseaux 4G ne répond que partiellement aux exigences des réseaux 5G. Il nous a alors fallu mettre à jour et adapter les fonctionnalités du réseau afin d’assurer les connexions de données et les sessions de mobilités des utilisateurs mobiles. Les solutions développées se devaient d’être les plus élégantes et simples possible, tout en limitant au maximum leur coût de mise en place. Pour cela, il nous a fallu réutiliser autant que possible l’infrastructure 5G déjà existante afin d’y intégrer les fonctionnalités souhaitées. Ces architectures ont également dû être améliorées pour les services Non Voice et VoLTE Voice. Dans le contexte d’un client qui migrerait d’un réseau 4G vers un réseau 5G, nous avons également dû répondre à la problématique de la gestion de la consommation des données des utilisateurs.\n\nTransatel a également réalisés de nombreux développements dans le but de concevoir une nouvelle plateforme de provisionnement, nommée NewStack. Nous avons été confrontés à une première problématique d’association du MSISDN (i.e. le numéro de téléphone mobile d'un abonné utilisé pour l'identifier de manière unique au sein du réseau de téléphonie mobile) et la carte SIM. Aussi, le statut des cartes SIM fournies aux utilisateurs dépend du distributeur de la carte SIM. En effet, les cartes SIM commandées auprès de BT ont directement le statut « Active », ce qui n’est pas le cas des autres cartes SIM pairées par Transatel. Nous avons alors souhaité concevoir une architecture permettant de répondre à cette problématique.\n\nIl nous fallait aussi être en mesure de migrer chaque abonné vers la nouvelle plateforme de provisionnement. Ce processus de migration devait alors prendre en compte le transfert de la gestion des ressources, le transfert de provisionnement, mais également assurer le process de facturation des abonnés. Une dernière incertitude à lever restait celle de l’opérabilité des modifications apportées. Nous avons donc souhaité vérifier le bon fonctionnement des processus de portabilité et de provisionnement des utilisateurs. accompagné British Telecom (BT) afin d’assurer le transfert vers leur nouvelle plateforme de provisionnement. Plusieurs développements ont été nécessaires en ce sens."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif principal** : Comprendre le capital « invisible » investi, incluant le capital humain, les marques, l’innovation, etc.\n  \n- **Importance des actifs incorporels** : \n  - Considérés comme cruciaux au 21ème siècle.\n  - Essentiels pour le calcul de la rentabilité économique et l’évaluation.\n\n- **Évaluation du capital invisible** : \n  - Sujet incontournable dans divers secteurs.\n  - Nécessité de déterminer des méthodes d’évaluation appropriées.\n\n- **Verrous identifiés** : \n  - Inconnues liées à l’approche des actifs incorporels dans la valorisation d’une banque.\n  - De nombreux investissements en capital invisible influencent la valorisation, mais ne sont pas comptabilisés comme tels.\n\n- **Objectifs spécifiques** : \n  - Quantifier les actifs incorporels.\n  - Modéliser la contribution de chaque actif à la valeur globale d’une banque.\n\n- **Période de référence pour l’investissement** : \n  - Importance d'estimer la durée des bénéfices issus des dépenses de marketing sur l'image de marque.\n  \n- **Capital de tier 1** : \n  - Contribution à la valorisation globale de la banque à évaluer.\n\n- **Dépenses en personnel** : \n  - Salaires du personnel de R&D et des fonctions à forte valeur ajoutée (ex. front-office) considérés comme des investissements.\n  \n- **Évaluation de la durée des investissements** : \n  - Recherche de la durée typique de ces investissements pour une meilleure valorisation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nNotre objectif est de mieux comprendre le capital « invisible » investi, comme le capital humain, les marques, l’innovation, etc.  \n\nNous supposons que ces actifs incorporels sont les plus importants du 21ème siècle et doivent être mieux appréhendés car, dans le cadre de notre technologie unique, ils sont extrêmement importants pour le calcul de la rentabilité économique et de l’évaluation. Ainsi, l’un des sujets qui devient aujourd’hui incontournable dans de nombreux secteurs, est de déterminer comment évaluer en conséquence ce capital invisible.  \n\nLes verrous auxquels nous sommes confrontés dans le cadre de notre projet découlent des inconnues que pose l’approche des actifs incorporels dans la valorisation d’une banque. En effet, de nombreux investissements en capital invisible contribuent à la valorisation d’une banque mais ne sont comptablement pas considérés comme des investissements. Notre objectif est d’une part de les quantifier et d’autre part de modéliser la contribution de chacun de ces postes à la valeur globale d’une banque.  \nAfin de les chiffrer, nous souhaitons tout d’abord considérer la période de référence de l’investissement. Ainsi, pour les dépenses de marketing qui contribuent à l’image de marque, nous devons estimer la durée pendant laquelle la banque bénéficie de la notoriété résultant de ces campagnes. Par ailleurs, le capital de tier 1 d’une banque contribue à sa valorisation. Nous nous sommes donc fixés pour mission d’évaluer la contribution de ces capitaux à la valorisation globale de la banque.  \nDe la même façon, les dépenses engagées au titre du personnel de R&D mais aussi aux fonctions à forte valeur ajoutée telles que le front-office contribuent à la valorisation d’une entreprise. Ainsi, ces salaires doivent être considérés comme des investissements. De plus, nous recherchons la durée typique de ces investissements. Nous souhaitons ainsi évaluer la période typique de ces différents investissements."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Indicateur de performance économique** : \n  - Actuellement, le seul indicateur reconnu et utilisé est le Mark-to-Market.\n  - D'autres outils ont été recherchés et développés, mais aucun n'a donné de résultats satisfaisants.\n\n- **Verrous identifiés dans le développement de la solution** :\n  - **Universalité de l'instrument** :\n    - Doit être utilisable dans tous les contextes géographiques, sectoriels et temporels.\n    - Doit traiter tous types de données comptables (amortissements, dépréciations, etc.).\n    - Les normes comptables varient significativement selon les pays et régions.\n    - Les secteurs d'activité présentent des niveaux de croissance et de risque différents.\n    - Variabilité des dates d'acquisition et de cession des titres, ainsi que des dates de calcul et de publication des comptes de résultat.\n    - **Harmonisation des opérations d'achat et de vente** : Cruciale pour établir une équité comptable et économique entre les sociétés.\n\n- **Événements impactant la valeur de l'investissement** :\n  - Événements corporates : distributions de dividendes, émissions ou rachats d'actions, spin-offs, etc.\n  - Ajustements et événements exceptionnels liés à la finance d'entreprise.\n  - **Intégration des événements** : \n    - Nécessaire pour garantir la justesse et la précision de l'instrument de mesure.\n    - La diversité et le nombre d'événements constituent un verrou.\n    - Traitement en temps réel requis, excluant un traitement manuel.\n    - Nécessité de prédéterminer, tester et modéliser ces événements dans un système automatique de consolidation.\n\n- **Reporting comptable** :\n  - Contraint par les lois, règlements et spécificités de chaque holding ou fonds d'investissement non cotés.\n  - Objectif : Concevoir le calcul en version économique depuis ses briques élémentaires.\n  - Implication : Développement d'un outil de calcul et de reporting inédit."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLe seul indicateur de performance économique qui est aujourd’hui reconnu et réellement utilisé est le Mark-to-Market. Face aux limites de celui-ci, nous pouvons raisonnablement penser que d’autres outils ont fait l’objet de recherches et de développements. Néanmoins, aucun de ceux-ci n’a, selon toute évidence, donné de résultats satisfaisants.\nLe développement de notre solution a ainsi soulevé d’importants verrous, listés ci-après.\nLa première difficulté réside dans l’universalité de l’instrument que nous souhaitons développer. Nous souhaitons, en effet, qu’il soit utilisable dans n’importe quel contexte géographique, sectoriel, temporel et pour tous types de données comptables (amortissements, dépréciations, etc.). Or les normes comptables varient, parfois significativement, d’un pays ou d’une région du globe à une autre. De même, chaque secteur d’activité affiche des niveaux de croissance ou de risque différents. Enfin, la variabilité des dates auxquelles les titres sont acquis et cédés, ainsi que celle des dates de calcul et de publication des comptes de résultat constituent autant d’obstacles à un traitement homogène des données. L’harmonisation des opérations d’achat et de vente est un verrou aussi difficile à lever qu’il est important, car c’est ce qui nous permettra de mettre toutes les sociétés sur un pied d’égalité comptable et économique.\n\nEntre ces étapes d’acquisition et de cession des actions, de nombreux évènements sont susceptibles d’impacter la valeur de l’investissement. Des évènements corporates, d’abord, comme les distributions de dividendes, les émissions ou rachats d’actions, les spin-offs, etc. ainsi que des ajustements et des évènements exceptionnels inhérents à la finance d’entreprises.\nPour que notre instrument de mesure soit à la fois juste et précis, il faut qu’il intègre chacun de ces évènements dans le calcul. Leur nombre et leur variété constituent un verrou dans ce projet, d’autant plus que le traitement doit être fait en temps réel et exclut donc un traitement exclusivement manuel. Pour lever celui-ci, il nous faudra prédéterminer un maximum de ces évènements, les tester et les modéliser au sein d’un système automatique de consolidation.\n\nL’ensemble du reporting comptable de type variation d’Actif Net Réévalué est en pratique contraint par les lois et règlements ou les spécificités de chaque holding ou fonds d’investissement non-coté. Le défi que nous nous proposons est donc de concevoir le calcul en version économique depuis ses briques élémentaires.\nCe travail implique le développement d’un outil de calcul et de reporting inédits."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **État de l'art et limites de l'industrie :**\n  - VIA vise à dépasser les limites de l'industrie actuelle en matière de sélection d'actions.\n\n- **Problème principal :**\n  - Utilisation de données comptables peu fiables pour la sélection d'actions.\n  - Nécessité de transformer ces données pour les rendre plus comparables et précises.\n\n- **Qualité des modèles :**\n  - La qualité d'un modèle dépend de la qualité des données utilisées.\n  - Tous les modèles de sélection d'actifs, sauf un (CROCI), reposent sur des données de fournisseurs comme Thomson-Reuters ou Bloomberg.\n\n- **Écueils des données comptables :**\n  - **Erreurs de traitement manuel :** Saisie incorrecte de certains champs par les fournisseurs.\n  - **Cosmétique comptable :** Utilisation de pratiques pour embellir les comptes par les directeurs financiers.\n  - **Différences de normes comptables :** Variabilité entre les pratiques comptables d'un pays à l'autre.\n\n- **Problèmes des bases de données publiques :**\n  - Présence de « trous » (absences d'observations), valeurs erronées et « outliers ».\n  - Ces anomalies déforment la distribution statistique et induisent des biais d'estimation.\n  - Impact négatif sur la performance simulée et la stabilité de la performance future.\n  - Techniques classiques de correction (ex. bootstrapping) ne sont pas satisfaisantes.\n\n- **Normes comptables et reporting :**\n  - Obligation légale pour les entreprises de publier leurs comptes régulièrement.\n  - Éléments discrétionnaires dans les normes comptables, laissant place à des choix subjectifs.\n  - Exemple : Différence d'amortissement entre deux compagnies aériennes sur la même activité économique.\n\n- **Comparaison entre sociétés :**\n  - Comparer des sociétés sur la base de comptes publiés engendre des incertitudes et biais.\n  - Normes de reporting comptable varient d'un pays à l'autre (ex. US GAAP vs IFRS vs Japanese GAAP).\n  - Impact direct sur les résultats des sociétés, rendant les comparaisons déraisonnables.\n\n- **Proposition de VIA AM :**\n  - Intégration d'une technique de normalisation comptable à grande échelle (plus de 3000 entreprises) comme avancée significative dans la gestion « Smart-Beta »."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nL’état de l’art existant et les limites de l’industrie que VIA se propose de dépasser sont exposés ci-après. La difficulté principale que nous avons rencontrée concernait l’utilisation de données comptables peu fiables pour la sélection d’actions et nécessité de les transformer pour les rendre plus comparables et plus précises.\n\nLa difficulté part du postulat de base que la qualité d’un modèle, aussi sophistiqué soit-il, dépend de la qualité des données avec lesquelles il est alimenté. Aujourd’hui, tous les modèles de sélection d’actifs financiers, à l’exception d’un seul (CROCI), reposent sur des données comptables collectées et distribuées par des fournisseurs de données tels que Thomson-Reuters ou Bloomberg. Ces données présentent toutefois trois écueils majeurs : les erreurs de traitement manuel faites par le fournisseur lors de la saisie de certains champs ; le degré de cosmétique utilisé par les sociétés, dont les directeurs financiers usent parfois pour habiller les comptes ; la différence de pratiques ou normes comptables d’un pays à l’autre ou d’une région à l’autre.\nLes bases de données publiques présentent régulièrement des « trous » (absences inexpliquées d’observations), des valeurs erronées, voire des « outliers ». Cela provient généralement d’une erreur de saisie ou de retranscription par le fournisseur. Ces observations déforment la distribution statistique du critère de sélection d’actifs analysé et induisent des biais d’estimation. Elles affectent donc la performance simulée – amoindrissant donc la stabilité de la performance future – de ce critère. Aucune technique statistique classique de correction d’échantillon (comme le « bootstrapping »), généralement très consommatrice de temps de calcul, ne se révèle satisfaisante pour corriger ce problème.\nPar ailleurs, la comptabilité découle d’une obligation légale imposée aux entreprises par un organe régional de contrôle ou de régulation, de publier leurs comptes à une fréquence régulière. Ces normes portent toutefois beaucoup d’éléments discrétionnaires, laissés à la libre appréciation des sociétés et de leurs directeurs financiers. Par exemple, une compagnie aérienne va choisir d’amortir ses avions sur une période de 5 ans, tandis qu’une autre va opter pour une période de 15 ans. Pour un même montant total à amortir, le montant amorti annuellement, donc le résultat net, sera différent, même si l’activité économique est strictement identique pour les deux compagnies. Cette liberté permet donc dans une certaine mesure aux sociétés d’habiller leurs comptes (l’effet « cosmétique »), afin de les présenter sous le meilleur angle possible. Ainsi, si le degré de flexibilité accordé aux directeurs financier demeure pertinent comptablement, il peut conduire à des inepties économiques importantes. La comparaison de deux sociétés sur la base des comptes qu’elles publient porte donc de facto un certain nombre d’incertitudes et de biais.\n\nEnfin, les sociétés se voient imposer des normes de « reporting comptable » différentes d’un pays à l’autre : par exemple, selon la norme US GAAP en vigueur aux États-Unis, la capitalisation de la partie « développement » de la R&D est interdite ; elle est en revanche autorisée en Europe (norme IFRS). La situation est encore différente au Japon (Japanese GAAP). La différence de traitement de ce champ comptable a elle aussi un impact direct sur le résultat de la société, qui va ressortir à un niveau plus ou moins élevé, sans que la différence de niveau reflète un réel écart d’activité ; la comparaison de deux sociétés publiant des comptes selon deux normes différentes se révèle donc là encore déraisonnable.\nL’équipe de recherche et de gestion de VIA AM considère donc que l’intégration d’une technique de normalisation comptable à grande échelle (plus de 3000 entreprises dans le monde) constituerait une avancée significative dans le secteur de la gestion « Smart-Beta »."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Problématique Principale**: Utilisation de données comptables peu fiables pour la sélection d’actions, nécessitant une transformation pour comparabilité.\n  \n- **Qualité des Modèles**: La qualité d'un modèle dépend de la qualité des données. Tous les modèles de sélection d'actifs financiers, sauf un (CROCI), s'appuient sur des données de fournisseurs comme Thomson Reuters ou Bloomberg.\n\n- **Écueils des Données Comptables**:\n  - **Erreurs de Traitement Manuel**: Saisie incorrecte de certains champs par le fournisseur.\n  - **Cosmétique Comptable**: Utilisation de techniques par les directeurs financiers pour embellir les comptes.\n  - **Normes Comptables Variées**: Différences de pratiques comptables entre pays ou régions.\n\n- **Problèmes des Bases de Données Publiques**:\n  - Présence de « trous » (absences inexpliquées d'observations) et de valeurs erronées.\n  - Existence d'« outliers » dus à des erreurs de saisie ou de retranscription.\n  - Ces anomalies déforment la distribution statistique et induisent des biais d'estimation, affectant la performance simulée et la stabilité de la performance future.\n\n- **Limitations des Techniques Statistiques**: Les méthodes classiques de correction d'échantillon, comme le « bootstrapping », ne sont pas satisfaisantes pour résoudre ces problèmes.\n\n- **Obligations Légales de Reporting**: Les entreprises doivent publier leurs comptes régulièrement, mais les normes contiennent des éléments discrétionnaires laissés à l'appréciation des sociétés.\n  - Exemple: Une compagnie aérienne peut choisir d'amortir ses avions sur 5 ou 15 ans, influençant le résultat net sans différence d'activité réelle.\n\n- **Impact des Normes de Reporting Différentes**:\n  - Normes variées entre pays (ex: US GAAP interdit la capitalisation de la R&D, tandis qu'IFRS en Europe l'autorise).\n  - Ces différences impactent directement les résultats des sociétés, rendant les comparaisons entre entreprises suivant des normes différentes déraisonnables."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLe principal problème que nous avons à résoudre est le suivant :  \n\nUtiliser des données comptables peu fiables pour la sélection d’actions et la nécessité de les transformer pour les rendre comparables  \n\nLa difficulté part du postulat de base que la qualité d’un modèle, aussi sophistiqué soit-il, dépend de la qualité des données avec lesquelles il est alimenté. Aujourd’hui, tous les modèles de sélection d’actifs financiers, à l’exception d’un seul (CROCI), reposent sur des données comptables collectées et distribuées par des fournisseurs de données tels que Thomson Reuters ou Bloomberg. Ces données présentent toutefois trois écueils majeurs :  \nles erreurs de traitement manuel faites par le fournisseur lors de la saisie de certains champs ;  \nle degré de cosmétique utilisé par les sociétés, dont les directeurs financiers usent parfois pour habiller les comptes ;  \nla différence de pratiques ou normes comptables d’un pays à l’autre ou d’une région à l’autre.  \n\nLes bases de données publiques présentent régulièrement des « trous » (absence inexpliquée d’observation), des valeurs erronées, voire des « outliers ». Cela provient généralement d’une erreur de saisie ou de retranscription par le fournisseur. Ces observations déforment la distribution statistique du critère de sélection d’actifs analysé et induisent des biais d’estimation. Elles affectent donc la performance simulée – donc amoindrissent la stabilité de la performance future – de ce critère. Aucune technique statistique classique de correction d’échantillon (comme le « bootstrapping »), généralement très consommatrice de temps de calcul, ne se révèle satisfaisante pour corriger ce problème.  \n\nPar ailleurs, la comptabilité découle d’une obligation légale imposée aux entreprises par un organe régional de contrôle ou de régulation, de publier leurs comptes à une fréquence régulière. Ces normes portent toutefois beaucoup d’éléments discrétionnaires, laissés à la libre appréciation des sociétés et de leurs directeurs financiers. Par exemple, une compagnie aérienne va choisir d’amortir ses avions sur une période de 5 ans, tandis qu’une autre va opter pour une période de 15 ans. Pour un même montant total à amortir, le montant amorti annuellement, donc le résultat net, sera différent, même si l’activité économique est strictement identique pour les deux compagnies. Cette liberté permet donc dans une certaine mesure aux sociétés d’habiller leurs comptes (l’effet « cosmétique »), afin de les présenter sous le meilleur angle possible. Ainsi, si ce degré de flexibilité accordé aux directeurs financier demeure pertinent comptablement, il peut conduire à des inepties économiques importantes. La comparaison de deux sociétés sur la base des comptes qu’elles publient porte donc de facto un certain nombre d’incertitudes et de biais.  \n\nEnfin, les sociétés se voient imposer des normes de « reporting comptable » différentes d’un pays à l’autre : par exemple, selon la norme US GAAP en vigueur aux Etats-Unis, la capitalisation de la partie « développement » de la R&D est interdite ; elle est autorisée en Europe (norme IFRS). La situation est encore différente au Japon (Japanese GAAP). La différence de traitement de ce champ comptable a elle aussi un impact direct sur le résultat de la société, qui va ressortir à un niveau plus ou moins élevé, sans que la différence de niveau reflète un réel écart d’activité ; la comparaison de deux sociétés publiant des comptes selon deux normes différentes se révèle donc là encore déraisonnable."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Connaissances Acquises**: Les travaux antérieurs ont permis de lever divers verrous et d'acquérir des connaissances utiles pour les travaux de 2018.\n  \n- **Inconvénients des Données Comptables**:\n  - Souvent incomplètes.\n  - Parfois maquillées par les directeurs financiers.\n  - Dépendantes des normes comptables en vigueur dans le pays d'édition.\n  \n- **Difficultés Générales**: Ces problèmes affectent tant les sociétés financières que non-financières.\n\n- **Application de Modèles de Normalisation**: L'application à de nouveaux secteurs engendre des verrous supplémentaires spécifiques à chaque secteur.\n\n#### Secteur des Cartes de Crédit\n- **Différences avec les Sociétés Non-Financières**: Les sociétés de cartes de crédit, sous-secteur des services financiers, investissent massivement dans la création et le renforcement de marques.\n- **Problème Comptable**: Les dépenses peuvent être inscrites au bilan ou hors bilan, créant un décalage entre les éléments comptables et la réalité économique.\n\n#### Secteur Immobilier\n- **Utilisation des Ratios Traditionnels**: Les ratios comme ROE, ROA, P/B, et PER sont couramment utilisés pour les décisions d'investissement.\n- **Absence d'Approches Alternatives**: Pas d'approches comparables sur une base économique, quel que soit le pays ou les GAAP.\n- **Inclusion du Goodwill**: \n  - Problème de standardisation entre regroupement d'entreprises et transactions d'actifs.\n  - Le goodwill n'est pas un actif opérationnel et ne contribue pas à la production de trésorerie.\n  \n- **Dettes Hors Bilan**: Non-prise en compte dans les multiples et ratios comptables, générant des incohérences.\n\n#### Divisions Financières des Grands Groupes\n- **Inclusion dans le Modèle**: Intégration des divisions financières qui fonctionnent comme des banques.\n- **Approches Concurrentes**:\n  - **CROCI**: Inclut 100% des actifs et de la dette dans les mesures de rentabilité et de valorisation économique (approche consolidée).\n  - **HOLT**: Déduit les actifs des divisions financières des obligations totales (approche déconsolidée).\n- **Problèmes des Approches**:\n  - L'approche déconsolidée dissimule les risques financiers et ignore le capital économique investi.\n  - L'approche consolidée fausse la mesure de rentabilité en incluant 100% des créances sur prêts.\n\n#### Problèmes dans le Secteur Bancaire\n- **Différences avec les Sociétés Industrielles**:\n  - La trésorerie est un actif d'exploitation.\n  - Les passifs sont principalement financiers à long terme ou remboursables à la discrétion de la banque.\n  - Les immobilisations corporelles représentent une fraction des actifs au bilan.\n  \n- **Objectif des Travaux de 2018**: Élargir le modèle de normalisation pour surmonter ces verrous.\n\n### Conclusion\n- **Difficultés d'Extension**: L'extension du modèle de normalisation aux quatre secteurs mentionnés présente des défis à surmonter par les travaux de R&D de 2018.\n- **Objectif Final**: Proposer un outil de normalisation avancé, englobant des milliers d'entreprises financières et non-financières, offrant une visibilité inédite sur les marchés."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLes travaux des exercices précédents nous ont permis de nous affranchir de différents verrous et d’acquérir des connaissances qui nous ont été particulièrement utiles dans la réalisation des travaux de 2018. En effet, les données comptables, de manière générale, présentent trois inconvénients majeurs qui constituaient autant de verrous à lever : elles sont souvent incomplètes, plus ou moins maquillées par les directeurs financiers des sociétés et dépendantes des normes comptables en vigueur dans le pays où elles sont éditées. Ces difficultés concernent autant les sociétés financières que non-financières. Les enseignements tirés des travaux des années passées nous ont donc aidés à surmonter les difficultés rencontrées dans notre projet. Néanmoins, l’application d’un modèle de normalisation comptable à des secteurs nouveaux, non-intégrés aux univers d’investissement habituellement considérés, est à l’origine de verrous supplémentaires. Ceux-ci sont propres aux secteurs en question : \n\nNous avons souhaité étendre notre modèle de normalisation comptable aux sociétés de cartes de crédit. Celles-ci constituent un sous-secteur des sociétés dites de « services financiers », elles diffèrent donc, à priori, des sociétés non financières pour lesquelles nous avons développé notre précédent outil de normalisation. Une grande partie de leur capital économique est en effet investi dans la création, l’acquisition et le renforcement de marques. Or ces dépenses peuvent être inscrites au bilan mais aussi hors bilan, ce qui crée un décalage entre les éléments comptables et la réalité économique, et constitue un réel verrou dans le développement d’un modèle pertinent de normalisation de l’information financière.\n\nNous avons ensuite souhaité intégrer les sociétés immobilières à notre univers normalisé. Les ratios et multiples comptables traditionnels tels que le ROE (Return on Equity), ROA (Return on Asset), P/B (Price-to-Book), PER (Price-to-Earnings), etc. sont de loin les indicateurs les plus utilisés dans le monde pour les décisions d'investissement dans le secteur immobilier coté. Il n’existe pas d’approches alternatives permettant de se rapprocher de la réalité économique sur une base comparable, quel que soit le pays ou GAAP (Generally Accepted Accounting Principles), ce qui représente un problème majeur pour les investisseurs. Du côté des actifs, un verrou important réside dans l’inclusion, au sein de la normalisation, du goodwill. Il s’agit, en des termes simplifiés, de la différence entre la valeur d'achat et la valeur économique d’une entreprise. D'un point de vue comptable, toutes les analyses de multiple ou de rendement ne différencient pas le regroupement d'entreprises de la transaction d'actifs (achat de propriétés ou asset deal). Cela signifie qu'il existe un problème majeur de standardisation, car le goodwill, qui provient uniquement du regroupement d'entreprises, est inclus exactement dans les mêmes retours et calculs de multiples que ceux de la transaction d'actifs. Cela peut créer un problème de comparabilité si l’objectif est d’obtenir des rendements opérationnels, tout simplement parce que le goodwill n’est pas, par définition, un actif opérationnel et ne contribue donc pas à la production de trésorerie. Un autre aléa, côté passifs cette fois-ci, réside dans la non-prise en compte des dettes hors bilan dans les multiples et autres ratios comptables, ce qui peut générer des incohérences entre éléments comptables et réalité économique.\n\nIl existe un groupe spécifique de sociétés appartenant à différents secteurs qui consacrent des moyens, des ressources et des actifs considérables aux activités de financement de leurs clients. Nous avons souhaité inclure également ces divisions financières de grands groupes à notre solution. Les sociétés concernées exploitent leurs divisions financières comme des banques, fournissant des prêts à leurs clients. On observe largement d’importants « à recevoir » du côté des actifs et une dette tout aussi importante associée aux activités de financement des clients du côté des passifs. Nos concurrents adoptent deux approches différentes. CROCI, d’abord, inclut systématiquement 100% des actifs des divisions financières en Economic Capital Invested (ECI) et 100% de la dette en Full Enterprise Value (FEV), ce qui revient à les prendre entièrement en compte à la fois dans les multiples de rentabilité (RCR, pour Real Cash Return) et de valorisation économique. Cette solution, dite « consolidée », est à l'opposé de ce que semble proposer l'autre concurrent HOLT, ou une solution « déconsolidée ». Cela signifie que tous les actifs des divisions financières sont déduits de leurs obligations totales et que le solde est inclus dans les investissements à long terme dans des entreprises associées, donc exclu de toute mesure de rentabilité et intégré dans la valeur d'entreprise uniquement dans une mesure très limitée. Les approches « consolidée » et « déconsolidée » posent toutes deux d'importants problèmes. La dernière dissimule les risques liés au véritable levier financier et ignore totalement le capital économique investi de la division financière dans la mesure de rentabilité. Tandis que la première aborde assez efficacement le problème de l'endettement financier, mais pénalise la société en incluant 100% des créances sur prêts dans l’ECI, ce qui fausse la mesure de rentabilité économique, ou RCR.\n\nEnfin, les problèmes qui nous ont amenés à analyser et à développer des solutions pour la transformation des données comptables en données économiques ne sont pas exclusifs aux sociétés industrielles. Dans le cas des sociétés bancaires, la comptabilité n’a pas non plus été conçue pour aider les investisseurs dans leurs choix. Il existe trois différences principales entre les deux groupes de sociétés, qui ont des implications importantes en termes de rentabilité et d’évaluation. Premièrement, à l’inverse des produits industriels, la trésorerie est un actif d’exploitation pour les banques. Ensuite, à la différence des produits industriels, les passifs sont principalement de nature financière uniquement s’ils sont à très long terme ou remboursables à la discrétion de la banque. Enfin, les immobilisations corporelles ne sont pas au cœur de la génération de flux de trésorerie, car elles ne représentent qu’une fraction du total des actifs de la banque au bilan. L’objectif de nos travaux, en 2018, étant d’élargir le champ d’application d’un modèle développé au préalable, chacune de ces différences constitue donc un verrou à lever.\n\nL’extension aux quatre secteurs (ou sous-secteurs) susmentionnés d’un modèle de normalisation universelle présente donc un certain nombre de difficultés que nous allons tâcher d’outrepasser à travers les travaux de R&D réalisés en 2018. Ceux-ci devraient nous permettre de proposer aux investisseurs l’outil de normalisation le plus avancé du marché, regroupant plusieurs milliers d’entreprises, à la fois financières et non-financières, et offrant ainsi une visibilité inédite sur l’état des marchés."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Verrous technologiques identifiés :**\n  - Développement d'une plateforme légère, compatible avec tous les navigateurs et peu gourmande en ressources.\n  - Hébergement des maquettes des utilisateurs avec gestion du versionnage et possibilité de revenir à des versions historiques.\n\n- **Gestion des métadonnées :**\n  - Prise en charge des différents formats de maquettes déposées par les utilisateurs.\n  - Risque de modification de la structure des données du projet dû au chargement de multiples versions par chaque utilisateur.\n\n- **Choix technologique :**\n  - Nécessité de choisir des technologies flexibles.\n  - Conception d'une infrastructure pour détecter les modifications entre deux versions et charger uniquement les changements.\n\n- **Défis à surmonter :**\n  - Chargement rapide et peu gourmand en ressources des maquettes.\n  - Automatisation des contrôles des maquettes en convertissant des règles métiers en règles applicables automatiquement.\n\n- **Flexibilité pour les utilisateurs :**\n  - Permettre aux utilisateurs de sélectionner les niveaux de contrôle qu'ils souhaitent pour les objets de vérification."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLors de la réalisation de nos travaux de recherche, nous avons été confrontés à plusieurs verrous technologiques et incertitudes scientifiques. En premier lieu, nous avons identifié un verrou quant à la capacité à développer une plateforme légère, compatible avec tous les navigateurs et peu gourmande en ressources. Il nous a fallu alors héberger les maquettes des utilisateurs, assurer leur versionnage, et permettre de remonter à une version historique si nécessaire.  \nUn autre verrou à surmonter réside dans la gestion des métadonnées des maquettes, il nous a fallu prendre en charge les différents formats des maquettes déposées par les utilisateurs. En outre le chargement de multiples versions par chaque utilisateurs participant à un projet est susceptible de modifier la structure des données du projet. Nous devons ainsi choisir des technologies flexibles et concevoir une infrastructure permettant la détection des modifications entre deux versions en vue du chargement des modifications uniquement entre ces deux maquettes. En outre, le chargement rapide et peu gourmand de ces maquettes représente un défi que nous devrons surmonter.  \nDe plus, quant à l’automatisation des contrôles des maquettes, il nous a fallu réussir à convertir des règles métiers en règles pouvant être ajoutées automatiquement à une maquette et aux fichiers associés. Nous devons en outre permettre aux utilisateurs une flexibilité dans les objets de vérification pour ne sélectionner que les niveaux de contrôles qu’ils souhaitent."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Identification des limites** : Les recherches sur l'état de l'art ont révélé des incertitudes et des expérimentations dans le projet.\n  \n- **Verrou 1 - Concentration sur les réseaux neuronaux** :\n  - La littérature se focalise sur les réseaux neuronaux pour la classification de textes et de sites internet.\n  - Ces technologies nécessitent beaucoup de données et un temps de calcul élevé.\n  - Les solutions concurrentes utilisent des réseaux neuronaux complexes (LSTM, Transformer) nécessitant des ressources de calculs industrielles (environ 500 k$).\n\n- **Verrou 2 - Dépendance linguistique** :\n  - Les algorithmes existants sont souvent dépendants de la langue de la page web.\n  - L'objectif est de développer une solution agnostique à la langue.\n\n- **Verrou 3 - Taille de l'échantillon de données** :\n  - La classification des graphes des sites nécessite un échantillon de données conséquent.\n  - Les algorithmes de Graph Learning requièrent un grand nombre de données pour un apprentissage efficace.\n\n- **Verrou 4 - Segmentation numérique du texte** :\n  - Les algorithmes existants segmentent numériquement le texte, ce qui impacte le temps d'exécution des requêtes de classification.\n\n- **Aléas dans la recherche** :\n  - Difficultés rencontrées lors de la mise en commun des graphes sémantiques et comportementaux.\n  - Problèmes liés au calcul et à la classification sur le méta-graphe résultant dans un temps raisonnable.\n\n- **Défi global** :\n  - Surmonter ces difficultés pour développer une solution performante."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNos recherches de l’état de l’art nous ont permis d’identifier de nombreuses limites qui rendaient ainsi notre entreprise dans ce projet incertaine et expérimentale.\n\nLe premier verrou auquel nous nous sommes heurtés est le fait que la littérature quant aux classifications de textes et de sites internet se concentre, depuis quelques années, sur les réseaux neuronaux. En effet, cette technologie a apporté un véritable renouveau aux algorithmes de compréhension du langage et d’apprentissages mais demande beaucoup de données et de temps de calcul. Les solutions concurrentes que nous avons consultées exploitent des réseaux de neurones extrêmement complexes (LTSM, Transformer) qui nécessitent des ressources de calculs industrielles (se chiffrant à environ 500 k$).\n\nUne seconde lacune de l’état de l’art est liée à la dépendance des algorithmes à la langue de la page web, or nous souhaitons développer une solution agnostique à la langue utilisée.\n\nUne autre limite porte sur la taille de l’échantillon de données nécessaires pour que l’algorithme de classification soit capable de catégoriser correctement les pages analysées. En effet, nous souhaitons classifier les graphes des sites obtenus et pour cela, nous devons entrainer notre algorithme sur des jeux de graphes d’entrainement. Les algorithmes de Graph Learning usuels demandant un très grand nombre de données pour apprendre.\n\nDe plus, un verrou technologique était posé par le mode de fonctionnement des algorithmes existants, qui procèdent à une segmentation numérique du texte, impactant ainsi le temps d’exécution des requêtes de classification.\n\nEnfin, au cours de nos travaux de recherche et au fur et à mesure de nos choix et expérimentations technologiques, nous nous sommes heurtés à de nombreux aléas concernant la mise en commun des graphes sémantiques et comportementaux, et principalement le calcul et la classification sur le méta-graphe résultant dans un temps raisonnable.\n\nNotre défi était ainsi de surmonter ces difficultés et de développer une solution performante."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Limites identifiées dans l'état de l'art :**\n  - Concentration sur les réseaux neuronaux pour la classification de textes et sites internet.\n  - Les réseaux neuronaux (LSTM, Transformer) nécessitent de grandes quantités de données et des temps de calcul importants.\n  - Coût des ressources de calculs industriels estimé à environ 500 k$.\n\n- **Dépendance linguistique :**\n  - Les algorithmes existants sont souvent dépendants de la langue de la page web.\n  - Objectif de développer une solution agnostique à la langue.\n\n- **Taille de l'échantillon de données :**\n  - Nécessité d'un grand nombre de données pour entraîner l'algorithme de classification.\n  - Besoin de jeux de graphes d'entraînement pour classifier les graphes des sites.\n\n- **Mode de fonctionnement des algorithmes :**\n  - Segmentation numérique du texte impactant le temps d'exécution des requêtes de classification.\n\n- **Aléas rencontrés :**\n  - Difficultés dans la mise en commun des graphes sémantiques et comportementaux.\n  - Problèmes de calcul et classification sur le méta-graphe dans un temps raisonnable.\n\n- **Défis pour le développement de la solution prédictive :**\n  - Modélisation des phénomènes régissant les ventes et leur fiabilité pour des projections futures.\n  - Traitement des séries temporelles : \n    - Interprétation des tendances.\n    - Saison et cyclicité des phénomènes à modéliser pour assurer des prédictions de qualité.\n\n- **Solutions existantes :**\n  - Solutions pour prédire le nombre de visites sur un site.\n  - Solutions pour prédire le stock.\n  - Aucune solution ne combine ces deux aspects en adressant l'ensemble de la chaîne de valeur.\n\n- **Proposition de développement :**\n  - Développement expérimental d'une solution répondant aux besoins identifiés.\n  - Contraintes à respecter : performance, agnosticisme linguistique, compréhension et faible consommation de ressources."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNos recherches de l’état de l’art nous ont permis d’identifier de nombreuses limites qui rendaient ainsi notre entreprise dans ce projet incertaine et expérimentale. Le premier verrou auquel nous nous sommes heurtés est le fait que la littérature quant aux classifications de textes et de sites internet se concentre, depuis quelques années, sur les réseaux neuronaux. En effet, cette technologie a apporté un véritable renouveau aux algorithmes de compréhension du langage et d’apprentissage mais exige beaucoup de données et de temps de calcul. Les solutions concurrentes que nous avons consultées exploitent des réseaux de neurones extrêmement complexes (LTSM, Transformer) qui nécessitent des ressources de calculs industrielles (se chiffrant à environ 500 k$). Une seconde lacune de l’état de l’art est liée à la dépendance des algorithmes à la langue de la page web, or nous souhaitons développer une solution agnostique à la langue utilisée. Une autre limite porte sur la taille de l’échantillon de données nécessaires pour que l’algorithme de classification soit capable de catégoriser correctement les pages analysées. En effet, nous souhaitons classifier les graphes des sites obtenus et pour cela, nous devons entrainer notre algorithme sur des jeux de graphes d’entrainement. Les algorithmes de Graph Learning usuels demandant un très grand nombre de données pour apprendre. De plus, un verrou technologique était posé par le mode de fonctionnement des algorithmes existants, qui procèdent à une segmentation numérique du texte, impactant ainsi le temps d’exécution des requêtes de classification. Enfin, au cours de nos travaux de recherche et au fur et à mesure de nos choix et expérimentations technologiques, nous nous sommes heurtés à de nombreux aléas concernant la mise en commun des graphes sémantiques et comportementaux, et principalement le calcul et la classification sur le méta-graphe résultant dans un temps raisonnable. Notre défi était ainsi de surmonter ces difficultés et de développer une solution performante. Pour le développement de notre solution prédictive, les défis résident dans la modélisation des phénomènes régissant les ventes et leur fiabilité en vue d’une projection sur le futur. En particulier, le traitement des séries temporelles pose des défis que la littérature scientifique ne manque pas de relever : l’interprétation des tendances, la saisonnalité des phénomènes, leur cyclicité sont autant de paramètres à modéliser pour assurer une prédiction de qualité. Nous avons pu identifier des solutions permettant de réaliser des prédictions du nombre de visites sur un site d’une part et d’autres solutions proposant des prédictions du stock, mais aucune solution ne combine ces deux aspects en adressant la totalité de la chaine de valeur. Face à cette lacune de l’état de l’art, nous nous proposons de développer expérimentalement une solution répondant à ce besoin, toujours avec les contraintes énoncées plus haut : performant, agnostique à la langue, compréhensible et nécessitant peu de ressources."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif de R&D**: Concevoir une solution de traduction de sites web en ligne, assurant un référencement conforme aux moteurs de recherche.\n\n#### Difficultés rencontrées en 2018\n\n1. **Verrou 1: Connecteur de traduction**\n   - Développement d'un connecteur pour l'outil de traduction Weglot à chaque technologie de site web.\n   - Conditions à vérifier:\n     - Diffusion du contenu côté serveur.\n     - Diffusion des pages via une URL unique et dédiée.\n   - **Limites identifiées**:\n     - Gestion des URL variable selon les technologies de développement.\n     - Accès aux serveurs non permis pour certaines technologies.\n   - **Solution trouvée**: Conception d'une solution combinant un proxy inverse, un CDN, et une fonction générant aléatoirement des adresses IP.\n\n2. **Verrou 2: Détection du contenu à traduire**\n   - Points bloquants:\n     - Origine du contenu (sources multiples).\n     - Découpage du contenu pour la traduction.\n   - **Défis spécifiques**:\n     - Détection du contenu dynamique non présent dans le code source HTML.\n     - Gestion des droits d'accès via iframes, limitant l'accès et la modification.\n     - Problème de récursivité avec les JSON lors de l'envoi de données vers l'API Weglot.\n     - Complexité du découpage due à des erreurs de syntaxe ou de ponctuation, entraînant des traductions erronées.\n\n3. **Verrou 3: Amélioration des performances**\n   - Objectif: Réduire le temps de réponse du service de traduction et augmenter la capacité de charge.\n   - **Pistes d'amélioration explorées**:\n     - Changement de l'architecture d'une table de la base de données.\n     - Parallélisation de l'exécution des tâches.\n     - Combinaison de bases ORM et de requêtes SQL selon le type d'opérations.\n   - **Changement de paradigme**: Transition d'une architecture monolithique vers des micro-services."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’objectif de nos travaux de R&D était de concevoir une solution de traduction des sites web déjà en ligne assurant un référencement des pages conforme aux préconisations des moteurs de recherche.\n\nEn 2018, nous avons été confrontés à trois principales difficultés.\n\nLe premier verrou rencontré était lié au développement d’un connecteur de notre outil de traduction Weglot à chaque technologie de site web utilisée, sans impacter la performance du référencement des pages traduites. Pour cela, nous devions assurer la vérification de deux conditions : la diffusion du contenu côté serveur et la diffusion des pages via une URL unique et dédiée. Or, deux limites ont été identifiées, rendant ainsi irréalisable notre hypothèse de solution vérifiant les deux conditions. La première limite est liée à la gestion des URL qui, selon les technologies de développement de sites employées par nos utilisateurs, n’est pas toujours possible. La seconde limite était rencontrée pour les technologies qui ne permettent pas l’accès aux serveurs. Finalement, nous avons surmonté cette difficulté grâce à la conception d’une solution couplant un proxy inverse et un CDN ainsi qu’une fonction permettant de générer aléatoirement des adresses IP.\n\nLe second verrou rencontré était lié à la détection du contenu à traduire. Là encore, nous avons été exposés à deux points bloquants : l’origine du contenu et son découpage en vue de la traduction. En effet, les sources du contenu affiché sur le site étant multiples, nous avons dû développer des solutions permettant de détecter l’intégralité du contenu, en particulier le contenu dynamique, non présent dans le code source HTML. Nous avons dû gérer les droits via les iframes qui, à partir des navigateurs, bloquent l’accès et la modification du contenu. Pour les JSON, nous avons également rencontré un problème lié à la récursivité en lien avec l’envoi des données vers l’API Weglot. Le découpage du contenu à traduire était également complexe en raison d’erreurs de syntaxe ou de ponctuation isolée qui sont alors à l’origine d’erreurs de détection du contenu résultant en des traductions erronées.\n\nEnfin, le troisième défi était lié à l’amélioration des performances de nos outils : API et plateforme. Nous étions à la recherche de pistes d’amélioration pouvant réduire de temps de réponse de notre service de traduction et permettre une augmentation de la capacité de charge de notre outil. Nous nous sommes alors tournés vers une association de trois pistes d’amélioration couplées à un changement de paradigme de notre infrastructure (micro-services se substituant à un monolithe). Les trois pistes d’amélioration explorées étaient : un changement de l’architecture d’une table de la base, la parallélisation de l’exécution des tâches et enfin le recours à la combinaison de bases ORM et de requêtes SQL selon le type d’opérations à traiter."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Contexte de R&D** : Confrontation à des incertitudes techniques et verrous technologiques.\n\n#### 1. Détection du Contenu Texte\n- **Objectif** : Détecter l'intégralité du corpus tout en distinguant :\n  - Texte affiché\n  - Texte pour description (référencement)\n  - Code sous-jacent du site\n- **Gestion des balises** :\n  - Ignorer certaines balises pour faciliter l'édition\n  - Ajouter de nouvelles balises pour changer le style d'affichage selon la langue\n\n- **Conséquences d'une mauvaise détection** :\n  - Traduction incomplète, nuisant à la crédibilité du client et perte potentielle de clients pour les sites marchands.\n  - Traduction erronée de code, pouvant entraîner un crash du site (ex : mots réservés en programmation).\n  - Mélange de langues nuisant à la lisibilité.\n\n#### 2. Détection des Contenus Dynamiques\n- **Problématique** : Reconnaissance des contenus dans des scripts dynamiques (Javascript, requêtes Ajax).\n- **Difficultés rencontrées** :\n  - Identifier les parties textuelles dans un script dynamique.\n  - Traitement des éléments dynamiques impossible à gérer en raison de la charge sur les serveurs.\n\n- **Solutions envisagées** :\n  - Permettre à l'utilisateur d'indiquer quels scripts dynamiques doivent être traduits.\n  - Développer un algorithme pour extraire le contenu texte d'un script donné.\n\n#### 3. Intégration Serveur\n- **Objectif** : Concevoir un outil d'intégration serveur adaptable à tous les sites, indépendamment du langage de programmation.\n- **Défi principal** : Hétérogénéité des technologies utilisées pour chaque site.\n- **Recherche nécessaire** :\n  - Identifier des points d'accès pour intégrer WeGlot au niveau serveur.\n  - Accéder aux parties \"bas niveau\" des technologies (ex : changement des URL).\n\n- **Importance de l'intégration serveur** :\n  - Prise en charge de l'architecture multilingue.\n  - Réinjection de contenus traduits dans le code source avec des URL uniques.\n  - Génération de méta-balises pour le référencement multilingue.\n\n#### 4. Risques Associés\n- **Mauvaise exécution de l'intégration** :\n  - Critique pour la visibilité du site.\n  - Risque de pénalisation par les moteurs de recherche en cas d'absence d'indexation des pages (considérée comme duplication).\n- **Nécessité de scinder les entités selon la langue** pour une bonne indexation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans le cadre de nos travaux de R&D, nous avons été confrontés à un ensemble d’incertitudes techniques et de verrous technologiques. \n\nLa première difficulté rencontrée est liée à la détection du contenu texte des pages. Notre solution doit détecter l’intégralité du corpus et en même temps savoir distinguer le texte affiché, le texte utilisé pour la description de la page (pour le référencement) et le code qui sous-tend le site, tout en gérant les balises spécifiques au style d’affichage des contenus (permettre de les ignorer pour faciliter l’édition ou d’en ajouter de nouvelles afin de changer le style d’affichage d’une langue à l’autre). Cependant, une mauvaise détection du texte va entrainer les problèmes suivants : \n\nFournir une traduction incomplète du site du client, ceci nuirait à sa crédibilité et risquerait de lui faire perdre des clients potentiels s’il s’agit d’un site marchand ;  \nTraduire du texte qui est en fait du code (la confusion qui ici parait insensée est beaucoup plus fréquente que nous le pensons : la plupart des mots réservés dans les langages de programmation étant des mots courants en anglais) : une simple erreur de détection pourrait ici conduire à un crash du site ;  \nUn mélange de différentes langues dans la même page peut être très nuisible en termes de lisibilité du site.\n\nA cette difficulté de détection du contenu dans le code source, s’ajoute la reconnaissance et la détection des contenus présents dans des scripts dynamiques (Javascript ou requête Ajax – asynchrone) car ces derniers ne sont pas présentés de la même manière que le contenu intégré dans le code source d’une page web. Cela renforce les problèmes évoqués plus haut.  \nBien qu’il soit possible de détecter le code Javascript, nous faisons face à deux principales difficultés : \n\nReconnaître au sein d’un script dynamique les parties textuelles ;  \nLa reconnaissance et le traitement de tous les éléments dynamiques d’une page est impossible à traiter en termes de charge et de consommation de ressources pour nos serveurs. Nous devons donc arriver à développer d’une part, un moyen de permettre à l’utilisateur d’indiquer quels scripts dynamiques sont à traduire par WeGlot et d’autre part, un algorithme permettant de trouver dans un script donné le contenu texte.\n\nUne autre complication à surmonter dans ce projet de R&D est la conception d’un outil permettant une intégration serveur. En effet, nous souhaitons nous adapter à tous les sites, quel que soit le langage de programmation utilisé. Pour cela, nous devons proposer une solution interopérable avec l’ensemble des systèmes. \n\nLa difficulté principale vient de l’hétérogénéité des développements et technologies utilisées pour la mise en place de chaque site. Nous devons à ce niveau chercher des points d’accès à ces technologies au niveau du serveur où “brancher” WeGlot. D’autre part, il est souvent nécessaire d’accéder à des parties “bas niveau” de ces technologies, touchant à l’architecture du site, comme le changement des URL. Cette dualité de niveaux d’intervention, requiert un temps important de recherche pour chacune des technologies afin de comprendre : \n\nOù insérer le code WeGlot ;  \nComment intervenir sur les parties bas niveau.\n\nL’intégration serveur permettrait ainsi de pouvoir prendre en charge l’architecture multilingue du site afin de réinjecter des contenus traduits dans le code source, sous des URL uniques et dédiées, et d’assurer une génération de méta-balises pour le référencement du site. Les balises SEO sont en effet des éléments indispensables afin de pouvoir référencer un site dans plusieurs langues et de pouvoir optimiser ce dernier selon les bonnes pratiques des moteurs de recherche.\n\nUne mauvaise exécution de cette étape est critique en termes de visibilité du site puisque les moteurs de recherche considèrent une absence d’indexation des pages comme une duplication de la même page et pénalisent ainsi cette pratique, interdite en référencement. La difficulté sera ici aussi de scinder toutes les entités selon la langue."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Contexte du projet Yakhin :**\n  - Complexité de conception des couches dynamiques.\n  - Nécessité de développements expérimentaux sur tous les modules du prototype.\n\n- **Difficultés techniques identifiées :**\n  1. **Exposition dynamique de Web Services (WS) :**\n     - Limitations des applications classiques basées sur fichiers plats.\n     - Besoin de structurer un nouveau modèle d’exposition de WS dynamique.\n     - Objectif : Modéliser une couche d'abstraction générique et paramétrable pour la communication avec d'autres services.\n     - Problème de performance lié à l'interprétation des requêtes dynamiques.\n     - Nécessité de mécanismes de cache pour maintenir les performances.\n\n  2. **Persistance dynamique des données :**\n     - Recherche de solutions de sauvegarde/restauration adaptées aux modèles métiers inconnus des utilisateurs.\n     - Conception d'une couche de persistance compatible avec divers types de bases de données (relationnelles et NoSQL).\n     - Développement d'un driver générique capable de détecter le dialecte SQL selon la base de données utilisée.\n     - Minimisation des modifications de code pour éviter des cycles de développement longs.\n     - Intégration de plugins via un serveur centralisé pour supporter plusieurs versions de SGBD sans redéploiement.\n\n  3. **Moteur de rendu :**\n     - Automatisation de la description et de l’implémentation des écrans via un moteur de rendu.\n     - Extraction dynamique des propriétés des objets métiers pour générer des vues cohérentes.\n     - Défi de maintenir un degré de personnalisation élevé malgré la complexité des modèles de données.\n     - Nécessité de mécanismes automatiques pour mettre à jour les informations affichées lors de la saisie par l'utilisateur.\n     - Étude de solutions pour permettre à l'utilisateur de définir des règles de dépendance entre champs, potentiellement via un langage formel.\n\n  4. **Auto-génération de l’Application Factory Yakhin :**\n     - Développement d'une architecture souple et dynamique, peu dépendante des langages existants.\n     - Création d'un système réentrant pour générer Yakhin selon les spécificités des clients.\n     - Objectif d'un runtime léger et dynamique, avec une base de données hébergeant le méta-modèle de l'application.\n     - Intégration de l'option de téléchargement de plugins pour réduire l'obsolescence de la solution."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nFace aux limites relevées dans l’état de l’art et devant la complexité de conception des couches dynamiques sur l’application Yakhin, les chercheurs sont contraints de mener des développements expérimentaux sur l’ensemble des modules constituant le prototype. \n\nAu cours de la réalisation de ce projet, l’équipe de WEMANITY a identifié plusieurs difficultés techniques majeures relatives aux limitations de l’état de l’art. Ces difficultés se centraient autour de quatre axes principaux : \nExposition de points d’entrée et de sortie dynamiques sur Yakhin \nLes applications classiques, pas assez évolutives, sont généralement codées sur des fichiers plats et enregistrées sur un disque dur. Plusieurs frameworks (CXF ou Access) existent aujourd’hui pour l’exposition des Web services à partir d’un descripteur « .XML », « .Java » ou « .properties » ou pour la génération de requêtes paramétrables à partir d’un modèle de données de BD définit encore une fois en XML. Cependant, quand il s’agit d’une exposition dynamique de WS à partir d’une BD, ces frameworks deviennent vite obsolètes. L’enjeu ici est de pouvoir structurer un nouveau modèle d’exposition de WS dynamique tout en gardant des performances similaires à celle des frameworks statiques. \n\nLa difficulté majeure sur ce point consiste à modéliser une couche d'abstraction générique, paramétrable et souple pour pouvoir communiquer avec des services offerts par d'autres applications. D'un point de vue technique, l'administrateur de l'application doit avoir la possibilité de définir un mapping entre les objets du modèle de son application, et ceux définis dans le contrat du service à consommer (WSDL ou REST-like). \n\nUn autre problème technique rencontré concerne la performance qui pourra être offerte par la solution. Le fait d'exploiter une définition de service dynamique augmente le coût d'interprétation des requêtes reçues, et impacte directement les performances de Yakhin. Les chercheurs devront alors réfléchir à des mécanismes de cache permettant de limiter les dégradations de performance consécutives à ce modèle. \nPersistance dynamiques des données \nLes chercheurs doivent également trouver de nouvelles solutions de sauvegarde et restauration de données permettant de mettre en place des outils de persistance dynamiques et répondant aux spécificités des modèles métiers des futures utilisateurs. Sachant que ces modèles ne sont pas connus au moment de la conception de l’application Yakhin, ils doivent alors être génériques pour couvrir un maximum de modèles exposés par les utilisateurs. \n\nDans ce cadre, les chercheurs doivent imaginer une couche de persistance qui s’adapte à tous les types de BD (Relationnelles ou Big Data) que l’utilisateur serait en mesure d’utiliser. Gérer les BD NoSQL et également les BD relationnelles rajoute une couche de complexité à la conception de l’Application Factory Yakhin. Les chercheurs doivent, en effet, concevoir un driver générique avec un haut degré d’abstraction permettant de manipuler différents drivers spécifiques aux sources de base de données utilisées. Le driver visé doit être en mesure de détecter le dialecte SQL utilisé en fonction de la nature de la base de données de l’utilisateur. Un simple paramétrage du driver devrait suffire à l’utilisateur pour faire migrer le traitement des données selon le dialecte approprié à la base de données.\n\nEn outre, les chercheurs visent la création d’une solution qui nécessite le moins possible de modifications de codes, ce afin de s'affranchir des cycles de développement / déploiement, y compris en interne. L'utilisation généralisée de « plugins » développés autour d’une API très générique et stable contraints les chercheurs à imaginer un système de téléchargement de plugins externes à partir d'un serveur centralisé (tel qu'un repository de paquets). Les plugins offrant le support de plusieurs versions de SGBD pourraient ainsi être uploadés sur le serveur, et intégrés dynamiquement dans les instances d'applications existantes, sans nécessiter de re-déploiement bas niveau. \nMoteur de Rendu \nLa description des rendus se fait traditionnellement via des fichiers XML statiques définissant les positions des différents composants d’une vue. Les chercheurs visent ici la création d’un moteur de rendu permettant d’automatiser le processus de description et d’implémentation des écrans de l’application générée. Le moteur devrait pouvoir tirer à partir de la Base de données les propriétés de l’objet métier de l’utilisateur et en constituer une vue de composants cohérents. Les données ainsi que la structure même des vues doivent être conçues de façon dynamique et automatisée afin de répondre aux exigences d’un grands nombre d’entreprises. \n\nLa difficulté technologique principale liée au développement de ce moteur réside dans la capacité à conserver un degré de personnalisation idéal pour les utilisateurs finaux, et ce quelle que soit la complexité du modèle de données de l'écran en question. Il sera possible à terme de définir un modèle de données complexe par écran, ce modèle étant une agrégation d'un ou plusieurs objets métiers. Cet enjeu a fait émerger une contrainte technique importante quant à la cohérence des informations affichées : une grappe d'objets métiers issus d'une source de données persistante, ou d'un service externe, représente à l'instant t un tout fonctionnellement cohérent. Cette cohérence a vocation à être altérée par l'utilisateur lors de la saisie d'informations dans l'écran, et il revient à la solution Yakhin de proposer des mécanismes automatiques de mise à jour dans l'écran des informations corrélées. \n\nLe caractère extrêmement variable des règles de dépendance inter-objets relève en fait de la création d'algorithmes métier dans la couche présentation. Etant donné, la fréquence de ce besoin il n'est pas envisageable de contraindre l'utilisateur à devoir uploader un plugin implémentant ces algorithmes, et ce pour plusieurs raisons : le recours au code serait trop important, et l’exécution de ces algorithmes ne pourrait être faite que coté serveur afin d’éviter la dégradation des performances. Dans ce cadre, les chercheurs devront étudier plusieurs options : offrir la possibilité pour l'utilisateur d'uploader du code javascript, et / ou mettre en place des fonctionnalités permettant de définir des règles de dépendances entre les champs très évoluées. Cette dernière alternative requiert le développement d’un langage formel. \nAuto-génération de l’Application Factory Yakhin \nÉtant conscient de l’évolution accélérée des technologies autour des langages de programmation, les chercheurs de WEMANITY souhaitent relever un défi hautement complexe visant la création d’une architecture suffisamment souple avec un minimum de dépendance vers les langages existants. La solution visée par l’équipe R&D est de développer à moyen terme un système réentrant où Yakhin serait elle-même générée selon les spécificités de chaque client.\n\nL’idée est de concevoir un système récursif qui permettrait d’avoir un runtime extrêmement léger et totalement dynamique. L’application serait alors constituée d’une BD et d’un runtime qui permettrait de créer une seconde BD hébergeant le méta model de l’application dans lequel l’utilisateur pourrait pousser ces données. \n\nAdjoint à l’option de téléchargement de plugins, la solution présenterait alors un degré d'obsolescence extrêmement faible."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Défis technologiques du projet :**\n  - Deux aspects principaux : \n    - Chatbot (interface d’échange)\n    - Système de recommandation sous-jacent\n\n- **Difficultés liées au chatbot :**\n  - Conception d'un outil répondant à plusieurs contraintes :\n    - Compréhension du langage naturel\n    - Compréhension du contexte de la conversation\n    - Guidage du flux de la conversation pour cohérence\n    - Enregistrement des échanges avec les utilisateurs\n  - Problématiques à résoudre :\n    - Traitement du langage naturel\n    - Développement de graphes conversationnels\n    - Conception d'une infrastructure pour l'enregistrement des informations utilisateur\n  - Contraintes de performance :\n    - Temps de réponse de l'outil\n    - Connectivité avec le système de recommandation\n    - Capacité à traiter de grands volumes de données\n\n- **Problématique du système de recommandation :**\n  - Résolution d'un problème multi-objectifs sous contraintes\n  - Objectif : recommander un événement pertinent en quasi temps réel\n  - Limite de l'état de l'art :\n    - Publications existantes se concentrent sur un seul objectif\n    - Difficulté à modéliser des problèmes du monde réel\n  - Proposition d'une approche par pondération pour satisfaire de nombreuses contraintes\n  - Incertitudes à lever sur la mise en œuvre des résultats de recherche\n\n- **Identification des goûts des utilisateurs :**\n  - Nécessité de développer une méthode pour appréhender les préférences des utilisateurs\n  - Affinement du profil utilisateur à chaque itération\n\n- **Incertitudes technologiques :**\n  - Choix technologiques pour le développement du prototype\n  - Mise en œuvre de protocoles d’évaluation et de comparaison des performances des solutions développées"}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nLes défis technologiques que nous devons relever dans le cadre de ce projet portent sur les deux aspects que nous étudions : le chatbot, en tant qu’interface d’échange, et le système de recommandation sous-jacent.\n\nConcernant le chatbot, la principale difficulté que nous devons adresser est liée à la conception d’un outil répondant à nos nombreuses contraintes :  \nComprendre le langage naturel ;  \nComprendre le contexte de la conversation ;  \nGuider le flux de la conversation pour rester cohérent ;  \nEnregistrer les échanges avec les différents utilisateurs.  \nCes contraintes se traduisent par la résolution de problématiques ayant trait au traitement du langage naturel, le développement de graphes conversationnels, et la conception d’une infrastructure capable d’enregistrer des informations liées à un utilisateur afin d’inscrire une conversation dans un contexte.  \nA ces contraintes d’ordre fonctionnel, s’ajoutent des contraintes fondamentales de performance, liées au temps de réponse de l’outil, sa capacité à être connecté à un système de recommandation et sa capacité à traiter de grands volumes de données.\n\nLa problématique liée au développement du système de recommandation s’apparente quant à elle à la résolution d’un problème multi-objectifs sous contraintes.  \nEn effet, nous ambitionnons de recommander à un utilisateur un évènement pertinent dans un délai de temps raisonnable (quasi temps réel).  \nLa limite de l’état de l’art que nous souhaitions dépasser dans le cadre de nos travaux de recherche théorique portait sur la combinaison de multiples objectifs en vue de la résolution de notre problématique de recommandation.  \nEn effet, les publications rencontrées dans notre étude bibliographique ne poursuivent qu’un seul objectif et permettent, à ce titre, difficilement de modéliser les problèmes du monde réel.  \nNous souhaitons, face à ces limites, considérer l’introduction d’une approche par pondération, plus en phase avec le grand nombre de contraintes que nous devons satisfaire.  \nSi ce problème a pu être modélisé dans le cadre de nos travaux théoriques, des incertitudes restent à lever quant à la mise en œuvre des résultats de nos recherches.\n\nPar ailleurs, pour générer les suggestions pertinentes d’évènements aux collaborateurs, nous devrons identifier une méthode permettant d’appréhender leurs goûts et affiner le profil d’utilisateur qui se dresse à chaque itération.\n\nEnfin, une incertitude porte sur les choix technologiques réalisés dans le cadre du développement de ce prototype dans notre démarche expérimentale, nous devrons ainsi mettre en œuvre des protocoles d’évaluation et de comparaison des performances des solutions développées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n#### Constitution du corpus de travail\n- **Défis rencontrés** : Diversité et volume du corpus limitent l'apprentissage de l'outil.\n- **Importance de la base de données** : Doit être riche pour concevoir une solution performante.\n- **Robustesse face aux accents** : Outil doit fonctionner efficacement dans toutes les régions, en tenant compte des chaînes locales de France Télévisions.\n- **Échantillon pour évaluation** : Une partie de la base de données est dédiée à l'évaluation des outils de reconnaissance vocale (STT).\n- **Métadonnées** : Détermination des métadonnées a posé des verrous, nécessitant des solutions pour améliorer leur recueil.\n- **Architecture des transcriptions** : Doit refléter la typologie des extraits étudiés.\n\n#### Définition d’une transcription de référence\n- **Difficulté de la transcription de référence** : Nécessité de confronter les transcriptions des outils STT.\n- **Absence de solution automatique** : Annotation manuelle d'extraits pour établir une transcription de référence.\n- **Automatisation souhaitée** : Étude de la faisabilité d'une solution combinant plusieurs transcriptions imparfaites pour fiabiliser les résultats.\n- **Prototype interne** : Développement d'un outil complexe pour générer des transcriptions plus fiables que les solutions STT individuelles.\n- **Complexité du prototype** : Actuellement trop complexe pour une utilisation à grande échelle.\n\n#### Métriques d’évaluation\n- **Incertitudes liées aux métriques** : Complexité de caractérisation des transcriptions et diversité des enjeux contextuels.\n- **Contexte d'utilisation** : Différentes priorités selon le contexte (ex. sous-titrage vs. reconnaissance des locuteurs).\n- **Définition des métriques** : Nécessité de créer des métriques pour évaluer la qualité des transcriptions.\n- **Métriques reconnues par la communauté** : Taux d'erreur sur les mots reconnus (WER) et capacité à reconnaître les entités nommées (NER).\n- **Caractéristiques spécifiques** : Évaluation de la reconnaissance des locuteurs dans des dialogues ou débats.\n\n#### Expérimentation des outils STT\n- **Protocole d’expérimentation** : Nécessité de définir un protocole pour évaluer les outils STT existants.\n- **Implémentation des métriques** : Application des métriques d'évaluation dans le cadre des expérimentations.\n- **Nuancer les analyses** : Interprétation des résultats selon le type de contenu soumis aux outils STT.\n- **Importance du contexte** : Différentes implications des performances selon le type de contenu (texte lu vs. débat interactif).\n- **Objectif du protocole** : Réflexion sur les défis et prototypage de cadres d’expérimentation."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nConstitution du corpus de travail\nLes défis attachés à la constitution du corpus de travail étaient nombreux.\nLa diversité et le volume de celui-ci constituaient des contraintes fortes dans la mesure où notre futur outil sera limité dans son apprentissage si les extraits d’entrainement ne lui permettent pas de rencontrer une grande diversité de contenus et notre outil ne pourrait être suffisamment entrainé si le volume de données disponibles est trop faible.\nAinsi, la constitution de notre base de données média doit être suffisamment riche pour nous permettre de concevoir une solution performante. En particulier, nous souhaitons que notre futur outil soit robuste face aux accents régionaux, le réseau France Télévisions étant constitué d’une multitude de chaines locales, notre outil doit offrir le même niveau de performance quelle que soit la région considérée.\nDe plus, un échantillon de cette base de données est destiné à l’évaluation des outils STT du marché. Cet échantillon doit permettre d’évaluer l’ensemble des caractéristiques que nous définissons dans la suite de nos travaux.\nPar ailleurs, la détermination des métadonnées associées aux contenus a également été à l’origine de verrous. Nous avons ainsi envisagé différentes pistes de solutions qui nous permettraient d’améliorer le recueil des métadonnées associées aux extraits compilés dans notre base de données. Nous avons notamment eu à concevoir l’architecture des transcriptions afin de rendre compte de la typologie des extraits étudiés.\nDéfinition d’une transcription de référence\nL’une des difficultés que nous avons eu à adresser portait sur la détermination d’une transcription de référence permettant de confronter les transcriptions issues des outils à évaluer.\nSi nous disposions déjà d’une solution automatique performante de transcription, notre projet n’aurait pas de sens. Face à l’inexistence d’une telle solution, nous avons dû procéder à l’annotation manuelle de certains extraits afin de disposer d’une transcription de référence à laquelle comparer les données de sortie des outils STT.\nLe travail d’annotation manuelle étant fastidieux, nous avons souhaité automatiser cette solution.\nNous avons ainsi étudié la faisabilité du développement d’une solution de combinaison de plusieurs transcriptions imparfaites issues des outils STT du marché en vue de la fiabilisation de la transcription de sortie.\nNous avons ainsi prototypé un outil interne au fonctionnement complexe en vue de la génération de transcriptions plus fiables que les offreurs de solutions STT pris isolément. Une première étape d’enrichissement NLP des textes est suivie d’un processus régi par des règles complexes en vue de la fusion/comparaison des hypothèses de transcriptions générées par chaque outil évalué.\nLe développement de ce prototype nous permet d’avancer dans notre exploration du sujet des STT, mais son fonctionnement est aujourd’hui trop complexe pour envisager une utilisation à large échelle de ce principe.\nMétriques d’évaluation\nLes incertitudes liées à la définition de métriques d’évaluation découlent de la complexité de caractérisation d’une transcription ainsi que la diversité des enjeux selon le contexte d’utilisation de celle-ci.\nEn effet, si le contexte d’utilisation est le sous-titrage d’une vidéo, l’intérêt de l’identification du locuteur est moins important que celui de la qualité de la reconnaissance des mots énoncés.\nAinsi, notre défi est de définir des différentes métriques qui nous permettront d’évaluer la qualité d’une transcription.\nDe plus, les métriques admises par la communauté scientifique des médias (réunie dans l’EBU) se restreignent à une évaluation du taux d’erreur sur les mots reconnus (WER – Word Error Rate) ou encore la capacité de la solution à reconnaitre les entités nommées (NER – Name Entity Error Rate) énoncées par rapport à une liste de noms propres qui lui a été soumise.\nEn particulier, les caractéristiques telles que la capacité de l’outil à reconnaitre le locuteur en cas de dialogue ou encore en cas de débat lorsque plusieurs personnes peuvent se couper la parole et/ou parler simultanément. Nous devions ainsi définir des métriques capables de rendre compte de ce type de performances.\nExpérimentation des outils STT\nEnfin, la dernière complexité à laquelle nous avons été confrontés était le besoin de définir une protocole d’expérimentation pour évaluer les outils STT existants. Nous devions ainsi implémenter les métriques d’évaluation développées mais aussi nuancer les analyses des outils STT selon le contexte d’utilisation visé pour l’outil.\nUn autre défi était l’interprétation des résultats d’évaluation selon le type de contenus soumis aux solutions STT : de « mauvaises » performances en analyse d’un texte lu sont plus « graves » que des performances médiocres sur la transcription d’un débat avec plusieurs interlocuteurs qui interagissent alors qu’il y a des bruits d’environnement plus ou moins présents.\nAinsi, l’objectif de notre protocole d’évaluation était de réfléchir à tous ces défis et prototyper des cadres d’expérimentation."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Identification des points de blocage** : L'étude de la littérature a révélé plusieurs obstacles à la transformation des organisations.\n\n- **Premier verrou : Processus génériques**  \n  - Incapacité à détecter les vraies priorités et à adresser les problèmes réels.  \n  - Rythme effréné des organisations exacerbe cette limitation.  \n  - **Approche proposée** :  \n    - Alignement entre stratégie et opérationnel à l'échelle de l'organisation.  \n    - Évolution vers un modèle expérimental (test & learn).  \n    - Diagnostic amélioré des valeurs de l'entreprise.\n\n- **Deuxième verrou : Initiatives ultra-localisées**  \n  - Les initiatives de transformation agile sont souvent limitées à des environnements logiciels, entraînant des échecs.  \n  - **Hypothèse** : Échecs évitables en prenant en compte l'ensemble de la chaîne de valeur.  \n  - **Objectif** : Définir un modèle d'opération offrant une vision globale de la transformation agile.\n\n- **Troisième verrou : Décorrélation du top management**  \n  - Le top management est déconnecté des problématiques de terrain et des silos entre unités.  \n  - Mauvaise communication et incarnation de la transformation par le top management.  \n  - **Étude expérimentale** : Évaluer le rôle du management de proximité comme intermédiaire et identifier les missions à confier pour un leadership adapté à la transformation agile.\n\n- **Quatrième verrou : Inconnue liée au digital**  \n  - Introduction du digital et émergence de nouveaux métiers créent des incertitudes.  \n  - **Défi** : Concevoir des modèles de formation pour développer les compétences nécessaires, favorisant autonomie et collaboration.  \n  - Objectif : Pérenniser la transformation au-delà des interventions extérieures.\n\n- **Défi imprévu : Impact de la pandémie de COVID-19**  \n  - Contexte de travail à distance imposé par les confinements a ajouté des difficultés inédites.  \n  - **Verrous rencontrés** :  \n    - Inconnues liées à la situation soudaine.  \n    - Adaptation nécessaire au changement sans alternatives.  \n    - Résistance au changement plus difficile à détecter à distance."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nL’étude de la littérature que nous avons menée nous a permis d’identifier de nombreux points de blocage à l’origine de l’échec des transformations des organisations.\n\nUn premier verrou que nous avons pu constater était l’incapacité de processus génériques à détecter les vraies priorités des transformations et adresser les vrais problèmes, notamment en raison du rythme effréné auquel sont soumises les organisations. Pour dépasser cette limite, nous nous sommes proposé une double approche consistant d’une part en un alignement entre la stratégie et l’opérationnel à l’échelle de l’organisation en cours de transformation à travers l’évolution vers un modèle plus expérimental (test & learn) et d’autre part visant à mieux diagnostiquer la situation dans laquelle se trouve l’entreprise d’un point de vue valeurs.\n\nUn second verrou connexe à cette problématique est posé par le caractère ultra-localisé des initiatives de transformation agile des organisations, bien souvent dans des environnements logiciels, conduisant à leur échec. Nous supposons que ces échecs pourraient être évités en prenant en compte l’ensemble de la chaine de valeur. Nous visons ainsi la définition d’un modèle d’opération qui permettrait d’avoir une vision globale de la transformation agile de l’organisation.\n\nUn troisième verrou auquel nous avons été confrontés découle de la décorrélation du top management par rapport aux problématiques de terrain, mais aussi le silotage présent entre les différentes unités. En effet, le top management étant décorrélé du terrain, il est en mauvaise posture pour communiquer concrètement autour de la transformation et l’incarner. Notre étude expérimentale vise à évaluer la capacité du management de proximité à endosser un rôle d’intermédiaire entre ces deux niveaux et identifier les missions à confier pour tendre vers un modèle de leadership s’inscrivant dans le cadre d’une transformation agile.\n\nQuatrièmement, à l’échelle individuelle, nous avons été confrontés à une inconnue posée par l’introduction du digital dans l’ensemble des métiers ainsi que l’émergence de nouveaux métiers. Le défi à relever dans le cadre de cet axe de travail est de concevoir des modèles permettant de former les collaborateurs aux nouvelles compétences de demain impliquant un fort niveau d’autonomie et de collaboration afin de pérenniser la démarche de transformation au-delà de l’intervention de cabinets extérieurs.\n\nEnfin, un défi imprévu s’est ajouté en 2020 dans le contexte de la pandémie de COVID-19 et des nouveaux modes de travail que les confinements successifs ont imposés. Nous avons ainsi dû adresser les difficultés inédites posées par le travail à distance dans le contexte des transformations agiles engagées au sein d’organisations. Les verrous ici rencontrés découlent des inconnues de la situation qui s’est soudain imposée, ne laissant aucune alternative que l’adaptation au changement. Par ailleurs, le changement à distance est encore plus dur à conduire dans la mesure où certains signaux visibles de résistance n’apparaissent pas à distance."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Identification des points de blocage** : L'étude de la littérature a révélé plusieurs obstacles à la transformation des organisations.\n\n- **Premier verrou** : \n  - Incapacité des processus génériques à détecter les priorités réelles et à résoudre les problèmes critiques.\n  - Rythme rapide des organisations entravant la transformation.\n  - **Approche proposée** :\n    - Alignement entre stratégie et opérationnel.\n    - Évolution vers un modèle expérimental (test & learn).\n    - Diagnostic approfondi des valeurs de l'entreprise.\n\n- **Second verrou** :\n  - Initiatives de transformation agile souvent ultra-localisées, principalement dans des environnements logiciels, entraînant des échecs.\n  - Proposition d'une approche prenant en compte l'ensemble de la chaîne de valeur.\n  - Objectif : Définir un modèle opérationnel offrant une vision globale de la transformation agile.\n\n- **Conduite du changement** :\n  - Composante historique des plans de transformation, mais impact limité.\n  - Mauvaise compréhension des décideurs sur la discipline.\n  - Association Change Management, Communication & Formation perçue comme un verrou.\n  - Objectif : Créer un parcours d'accompagnement durable pour intégrer la transformation dans l'ADN de l'organisation.\n\n- **Décorrélation du top management** :\n  - Top management déconnecté des problématiques de terrain et silotage entre unités.\n  - Difficulté à communiquer et incarner la transformation.\n  - Étude expérimentale pour évaluer le rôle du management de proximité comme intermédiaire.\n  - Identification des missions à confier pour un modèle de leadership agile.\n\n- **Défis individuels** :\n  - Introduction du digital et émergence de nouveaux métiers.\n  - Nécessité de concevoir des modèles de formation pour développer des compétences d'autonomie et de collaboration.\n  - Objectif : Pérenniser la transformation au-delà des interventions extérieures.\n\n- **Communautés de pratiques** :\n  - Difficulté des organisations à évaluer l'intérêt et les retombées des communautés de pratiques.\n  - Travaux visant à réduire les incertitudes et à implémenter ces communautés comme outils de partage des connaissances.\n  - Contexte : Formation des collaborateurs et montée en compétences."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’étude de la littérature que nous avons menée nous a permis d’identifier de nombreux points de blocage à l’origine de l’échec des transformations des organisations.  \n\nUn premier verrou que nous avons pu constater était l’incapacité de processus génériques à détecter les vraies priorités des transformations et adresser les vrais problèmes, notamment en raison du rythme effréné auquel sont soumises les organisations. Pour dépasser cette limite, nous nous sommes proposé une double approche consistant d’une part en un alignement entre la stratégie et l’opérationnel à l’échelle de l’organisation en cours de transformation à travers l’évolution vers un modèle plus expérimental (test & learn) et d’autre part visant à mieux diagnostiquer la situation dans laquelle se trouve l’entreprise d’un point de vue valeurs.  \n\nUn second verrou connexe à cette problématique est posé par le caractère ultra-localisé des initiatives de transformation agile des organisations, bien souvent dans des environnements logiciels, conduisant à leur échec. Nous supposons que ces échecs pourraient être évités en prenant en compte l’ensemble de la chaine de valeur. Nous visons ainsi la définition d’un modèle d’opération qui permettrait d’avoir une vision globale de la transformation agile de l’organisation.  \n\nLa conduite du changement est une composante historique des plans de transformation. Néanmoins son impact reste minime, et la bonne compréhension des décideurs vis-à-vis de la discipline reste limitée. L’association Change Management, Communication & Formation est un raccourci et un verrou dans la pensée des décideurs pour embarquer au plus tôt les initiatives de changement, et ainsi maximiser les chances d’une transformation pérenne. L’objectif de nos développements expérimentaux était de bâtir un parcours d’accompagnement durable de la transformation afin d’inscrire celle-ci dans l’ADN de l’organisation.  \n\nUn autre verrou auquel nous avons été confrontés découle de la décorrélation du top management par rapport aux problématiques de terrain, mais aussi le silotage présent entre les différentes unités. En effet, le top management étant décorrélé du terrain, il est en mauvaise posture pour communiquer concrètement autour de la transformation et l’incarner. Notre étude expérimentale vise à évaluer la capacité du management de proximité à endosser un rôle d’intermédiaire entre ces deux niveaux et identifier les missions à confier pour tendre vers un modèle de leadership s’inscrivant dans le cadre d’une transformation agile.  \n\nCinquièmement, à l’échelle individuelle, nous avons été confrontés à une inconnue posée par l’introduction du digital dans l’ensemble des métiers ainsi que l’émergence de nouveaux métiers. Le défi à relever dans le cadre de cet axe de travail est de concevoir des modèles permettant de former les collaborateurs aux nouvelles compétences de demain impliquant un fort niveau d’autonomie et de collaboration afin de pérenniser la démarche de transformation au-delà de l’intervention de cabinets extérieurs.  \n\nEnfin, nous avons abordé un nouvel axe de travail posé par la difficulté des organisations à apprécier l’intérêt et les retombées potentielles de la mise en place de communautés de pratiques. Nos travaux visent ainsi à dissiper les incertitudes autour de ce système et à les implémenter en tant qu’outil de partage des connaissances dans un contexte de formation des collaborateurs et de montée en compétences."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques et Verrous Technologiques\n\n- **Identification des points de blocage :**\n  - Échec des transformations des organisations lié à des processus génériques incapables de détecter les vraies priorités et problèmes.\n  \n- **Premier verrou :**\n  - Incapacité des processus à s'adapter au rythme des organisations.\n  - Proposition d'une double approche :\n    - Alignement entre stratégie et opérationnel via un modèle expérimental (test & learn).\n    - Diagnostic approfondi de la situation de l’entreprise en termes de valeurs.\n\n- **Second verrou :**\n  - Caractère ultra-localisé des initiatives de transformation agile, souvent limitées aux environnements logiciels.\n  - Supposition que l'échec pourrait être évité en prenant en compte l'ensemble de la chaîne de valeur.\n  - Objectif : définir un modèle d’opération pour une vision globale de la transformation agile.\n\n- **Méthodologie d'alignement multiniveau :**\n  - Définition d'une méthodologie pour aligner les initiatives locales et globales.\n  - But : inscrire la transformation agile dans la durée pour maximiser les retombées.\n\n- **Conduite du changement :**\n  - Impact historique limité des plans de transformation.\n  - Compréhension insuffisante des décideurs sur la discipline du changement.\n  - Association Change Management, Communication & Formation perçue comme un verrou.\n  - Objectif : créer un parcours d’accompagnement durable pour intégrer la transformation dans l’ADN de l’organisation.\n\n- **Stratégie d’optimisation des portefeuilles de projets :**\n  - Priorisation des initiatives et identification des éléments facilitant leur réussite.\n  - Élaboration d’un référentiel pour les partenariats et relations avec les fournisseurs afin d’éviter les freins externes à l’agilité.\n\n- **Décorrélation du top management :**\n  - Top management éloigné des problématiques de terrain et silotage entre unités.\n  - Étude expérimentale pour évaluer le rôle du management de proximité comme intermédiaire.\n  - Identification des missions à confier pour un modèle de leadership en transformation agile.\n\n- **Défis liés à l'introduction du digital :**\n  - Inconnue sur l'impact du digital et l'émergence de nouveaux métiers.\n  - Conception de modèles de formation pour développer des compétences d’autonomie et de collaboration.\n\n- **Communautés de pratiques :**\n  - Difficulté des organisations à évaluer l'intérêt et les retombées de ces communautés.\n  - Objectif : dissiper les incertitudes et implanter ces communautés comme outils de partage des connaissances et de formation des collaborateurs."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’étude de la littérature que nous avons menée nous a permis d’identifier de nombreux points de blocage à l’origine de l’échec des transformations des organisations.\n\nUn premier verrou que nous avons pu constater était l’incapacité de processus génériques à détecter les vraies priorités des transformations et adresser les vrais problèmes, notamment en raison du rythme effréné auquel sont soumises les organisations. Pour dépasser cette limite, nous nous sommes proposé une double approche consistant d’une part en un alignement entre la stratégie et l’opérationnel à l’échelle de l’organisation en cours de transformation à travers l’évolution vers un modèle plus expérimental (test & learn) et d’autre part visant à mieux diagnostiquer la situation dans laquelle se trouve l’entreprise d’un point de vue valeurs.\n\nUn second verrou connexe à cette problématique est posé par le caractère ultra-localisé des initiatives de transformation agile des organisations, bien souvent dans des environnements logiciels, conduisant à leur échec. Nous supposons que ces échecs pourraient être évités en prenant en compte l’ensemble de la chaine de valeur. Nous visons ainsi la définition d’un modèle d’opération qui permettrait d’avoir une vision globale de la transformation agile de l’organisation.\n\nToujours calquée sur cette dualité entre les initiatives à l’échelle locale et celles conduites à l’échelle globale de l’organisation, nous nous sommes attelés à la définition d’une méthodologie permettant l’alignement multiniveau de ces démarches. Ces travaux servent en outre notre souhait d’inscrire dans la durée la transformation agile des organisations afin que celles-ci puissent réussissent leurs transformations et puissent en tirer les retombées espérées.  \nLa conduite du changement est une composante historique des plans de transformation. Néanmoins son impact reste minime, et la bonne compréhension des décideurs vis-à-vis de la discipline reste limitée. L’association Change Management, Communication & Formation est un raccourci et un verrou dans la pensée des décideurs pour embarquer au plus tôt les initiatives de changement, et ainsi maximiser les chances d’une transformation pérenne. L’objectif de nos développements expérimentaux était de bâtir un parcours d’accompagnement durable de la transformation afin d’inscrire celle-ci dans l’ADN de l’organisation.  \nUn autre axe sur lequel nous nous sommes penchés pour faciliter les transformations et l’adhésion des équipes à cette démarche était la définition d’une stratégie d’optimisation des portefeuilles de projets internes. En effet, l’un des leviers que nous avons pu actionner dans notre modélisation était la priorisation de la conduite des initiatives et la détermination des éléments facilitant la réussite de celles-ci. Un travail similaire a été réalisé en vue de la définition d’un référentiel pour les partenariats et les relations entre l’organisation et ses fournisseurs pour s’assurer que les freins à l’agilité ne viennent pas de l’extérieur bloquer les rouages d’une vision de « delivery ».\n\nUn autre verrou auquel nous avons été confrontés découle de la décorrélation du top management par rapport aux problématiques de terrain, mais aussi le silotage présent entre les différentes unités. En effet, le top management étant décorrélé du terrain, il est en mauvaise posture pour communiquer concrètement autour de la transformation et l’incarner. Notre étude expérimentale vise à évaluer la capacité du management de proximité à endosser un rôle d’intermédiaire entre ces deux niveaux et identifier les missions à confier pour tendre vers un modèle de leadership s’inscrivant dans le cadre d’une transformation agile.\n\nEn outre, à l’échelle individuelle, nous avons été confrontés à une inconnue posée par l’introduction du digital dans l’ensemble des métiers ainsi que l’émergence de nouveaux métiers. Le défi à relever dans le cadre de cet axe de travail est de concevoir des modèles permettant de former les collaborateurs aux nouvelles compétences de demain impliquant un fort niveau d’autonomie et de collaboration afin de pérenniser la démarche de transformation au-delà de l’intervention de cabinets extérieurs.\n\nEnfin, nous avons abordé un nouvel axe de travail posé par la difficulté des organisations à apprécier l’intérêt et les retombées potentielles de la mise en place de communautés de pratiques. Nos travaux visent ainsi à dissiper les incertitudes autour de ce système et à les implémenter en tant qu’outil de partage des connaissances dans un contexte de formation des collaborateurs et de montée en compétences."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Objectif du projet** : Concevoir un modèle de détection précoce de pathologies (glaucome, cataracte, diabète rétinopathique, DMLA) à partir de photos de fond d'œil en ophtalmologie.\n\n- **Incertitude scientifique principale** : Taille réduite des ensembles de données utilisés (ORIGA et IDRID).\n\n- **Défi majeur** : Entraîner un modèle avec un petit volume de données disponibles.\n\n- **Solution adoptée** :\n  - Utilisation de la méthode de **Transfer Learning** avec des modèles de reconnaissance d’images (VGG16, ResNet50).\n  - Suppression des couches fully-connected pour obtenir les **feature maps** comme sorties.\n\n- **Problème des données annotées** : Petit ensemble de données annotées (100 images par maladie).\n\n- **Approche d'apprentissage actif** :\n  - Étiquetage automatique du dataset.\n  - L'algorithme interroge l’ophtalmologiste pour améliorer la classification lorsque nécessaire.\n  - Génération de nouvelles données étiquetées pour ré-entraîner le modèle.\n\n- **Autre aléa** : Intégration des caractéristiques médicales dans les modèles de Deep Learning.\n  - Les modèles de Deep Learning sont des \"boîtes noires\" ; les attributs détectés dans les jeux de données sont inconnus.\n\n- **Proposition de solution** :\n  - Développement d'une approche de détection des pathologies oculaires intégrant les connaissances métier.\n  - Modèle multibranche basé sur des réseaux de neurones convolutifs (FCN).\n  - Combinaison de **domain knowledge** et **non-domain knowledge** dans le modèle."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nL’objectif de nos travaux était de concevoir un modèle de détection de pathologies précoces (glaucome, cataracte, diabète rétinopathique et DMLA) à partir des photos de fond d’œil acquises dans les cabinets d’ophtalmologie. \n\nLa principale incertitude scientifique à laquelle nous avons été confrontés lors de nos développements est liée à la taille réduite des deux ensembles de données utilisés notamment ORIGA et IDRID. \n\nN’ayant pas de grands ensembles de données, le défi majeur consistait à entrainer un modèle avec seulement les données dont nous disposons. Afin de remédier à ce problème, nous avons employé la méthode de Transfer Learning basée sur des modèles de reconnaissance d’images (VGG16, ResNet50) pour entrainer le modèle CNN et supprimer les couches fully-connected (entièrement connectées) afin d’obtenir les features maps en tant que sorties. \n\nDe plus, comme nous n’avons qu’un petit ensemble de données annotées (100 images par maladie), nous avons employé l’apprentissage actif (active learning) afin d’étiqueter automatiquement notre dataset. Dans l’apprentissage actif, l’algorithme est capable d’interroger de manière interactive l’ophtalmologiste lorsque le modèle n’est pas capable de faire une bonne classification par lui-même. Cette méthode génère ainsi de nouvelles données étiquetées pour ré-entrainer notre modèle.\n\nUn autre aléa réside dans le besoin d’intégrer les caractéristiques inhérentes au domaine médical dans les modèles du Deep learning. En effet, ces connaissances, entrent en jeu lors d’un diagnostic par un expert, toutefois les modèles de Deep Learning sont des boites noires et les attributs qui ont été détectés entre les différents jeux de données exploités pour l’apprentissage sont inconnus.  \nFace à ces limites, nous avons proposé de concevoir une approche de détection des pathologies oculaires dont la prise en compte des connaissances métier soit certaine. L’approche est basée sur un modèle multibranches basés sur les réseaux neurones convolutifs (FCN) et une combinaison de features de type domain et non-domain knowledge."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, Incertitudes Scientifiques, Verrous Technologiques\n\n- **Défis du projet** : Détermination et réduction de l'empreinte carbone des activités numériques confrontés à plusieurs défis technologiques majeurs.\n  \n- **Limitations des moniteurs de consommation d'énergie** :\n  - Les « smart meters » améliorent la surveillance de la consommation électrique.\n  - Incapacité à distinguer la consommation par appareil ou application spécifique.\n  - Cette limitation empêche une compréhension détaillée des contributeurs à la consommation d'énergie, surtout dans un contexte de numérisation croissante.\n  - Incertitude sur l'impact réel des activités en ligne sur la consommation totale d'énergie.\n\n- **Extensions de navigateur et applications de suivi** :\n  - Avancées dans la mesure de la consommation énergétique liée aux navigateurs web.\n  - Surveillance précise des ressources système (mémoire vive, temps processeur, bande passante).\n  - Limitations :\n    - Concentriques uniquement sur les actions dans le navigateur.\n    - Négligent la consommation d'autres applications.\n    - Dépendance à la coopération des utilisateurs, entraînant des lacunes dans la collecte de données.\n    - Préoccupations de confidentialité concernant la collecte et le traitement des données.\n\n- **Modèles de calcul de l'équivalent carbone** :\n  - Estiment l'empreinte environnementale des activités en ligne en tenant compte de divers facteurs.\n  - Dépendent souvent d'hypothèses et de données généralisées, entraînant des approximations et incertitudes.\n  - Précision liée à la qualité des données disponibles, nécessitant une amélioration continue des méthodologies.\n  - Complexité de mesurer la consommation CPU au niveau individuel en raison de la diversité des activités numériques et des spécificités matérielles.\n  - Difficulté à mesurer de manière exhaustive et précise les activités réparties entre le navigateur et d'autres applications.\n\n- **Préoccupations de confidentialité** :\n  - Anonymisation des données et extensions de navigateur indépendantes présentent des défis complexes.\n  - Méthodes d'anonymisation peuvent compromettre la spécificité des informations agrégées, réduisant la précision des analyses.\n  - Extensions indépendantes renforcent la protection des données, mais limitent la mise à jour dynamique des catégories.\n  - Analyse des données côté serveur offre une vue exhaustive, mais soulève des inquiétudes quant à la confidentialité, nécessitant une gestion rigoureuse pour éviter des compromis."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nNotre projet de détermination et de réduction de l'empreinte carbone des activités numériques rencontre plusieurs défis et verrous technologiques majeurs, liés à l'état actuel de la technologie. Ces obstacles sont cruciaux à surmonter pour atteindre nos objectifs ambitieux.\n\nLes moniteurs de consommation d'énergie, comme les « smart meters », ont considérablement amélioré la surveillance de la consommation électrique des ménages. Cependant, ils présentent des limitations importantes. Leur incapacité à distinguer la consommation énergétique par appareil ou application spécifique constitue un verrou technologique majeur. Cette limitation empêche une compréhension détaillée des contributeurs précis à la consommation d'énergie, notamment dans un contexte de numérisation croissante où les activités en ligne jouent un rôle significatif. Actuellement, les méthodes existantes ne permettent pas de détailler l'empreinte énergétique des activités en ligne spécifiques, ajoutant à cela une incertitude sur l'impact réel de ces activités sur la consommation totale d'énergie.\n\nLes extensions de navigateur et les applications de suivi représentent une avancée notable en mesurant la consommation énergétique liée à l'utilisation des navigateurs web. Elles permettent une surveillance précise des ressources système, comme la mémoire vive, le temps processeur et la bande passante réseau. Cependant, ces technologies se concentrent uniquement sur les actions effectuées dans le navigateur, négligeant la consommation d'autres applications. De plus, la dépendance à la coopération des utilisateurs peut entraîner des lacunes dans la collecte de données, limitant ainsi leur efficacité globale. La confidentialité des utilisateurs pose également un défi, car la collecte et le traitement des données doivent être équilibrés avec le respect de la vie privée.\n\nLes modèles sophistiqués de calcul de l'équivalent carbone prennent en compte divers facteurs pour estimer l'empreinte environnementale des activités en ligne. Cependant, ces modèles reposent souvent sur des hypothèses et des données généralisées, ce qui peut entraîner des approximations et des incertitudes. La précision de ces modèles dépend fortement de la qualité des données disponibles, nécessitant une amélioration continue des méthodologies. La diversité des activités numériques et des spécificités matérielles complique la mesure précise de la consommation CPU au niveau individuel. De plus, les activités réparties entre le navigateur et d'autres applications sont difficiles à mesurer de manière exhaustive et précise.\n\nEnfin, la confidentialité est une préoccupation centrale dans la collecte et l'analyse des données d'empreinte carbone des utilisateurs. Les solutions actuelles, telles que l'anonymisation des données et les extensions de navigateur indépendantes, présentent des défis complexes. Les méthodes d'anonymisation peuvent compromettre la spécificité des informations agrégées, réduisant la précision des analyses. Les extensions de navigateur indépendantes renforcent la protection des données, mais limitent la capacité de mise à jour dynamique des catégories, affectant ainsi la flexibilité. L'analyse des données côté serveur, bien qu'offrant une vue exhaustive, soulève des inquiétudes quant à la confidentialité, nécessitant une gestion rigoureuse pour éviter tout compromis."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "### Aléas, incertitudes scientifiques, verrous technologiques\n\n- **Contexte du projet R&D** : Confrontation à un ensemble d’incertitudes, verrous et contraintes techniques.\n  \n- **Questions clés abordées** :\n  - **Durées de préparation** : Détermination des durées de préparation de chaque composant pour les produits vendus.\n  - **Emplacement du consommateur** : Identification du meilleur moyen pour déterminer l’emplacement du consommateur afin d’optimiser le moment de livraison ou de retrait de commande.\n    - Facteurs influents : \n      - Trajet vers le restaurant\n      - Mode de transport utilisé\n      - Conditions de circulation\n      - Données de géolocalisation en temps réel\n      - Informations météorologiques impactant le transport.\n  - **Agréger les données** : Développement d'une méthode pour agréger les données afin de déterminer le moment d’envoi du message de préparation de commande à l’équipier.\n    - Objectif : Faciliter le retrait de commande en visant un temps de retrait inférieur à une minute.\n  - **Pondération des critères d'achat** : Prise en compte des différents critères influençant l’acte d’achat pour obtenir des préconisations précises.\n    - Difficulté : Interaction entre critères isolés et corrélés.\n  - **Sécurité et hygiène alimentaire** : Intégration des contraintes de sécurité et d’hygiène alimentaire pour respecter les délais de conservation des aliments.\n  - **Conservation des aliments** : Recherche de la meilleure méthode de conservation des aliments dans un casier connecté.\n  - **Température optimale** : Mise en place d’un système de communication entre le casier, les modules de gestion de commandes en ligne et la base de données des produits pour maintenir les préparations à la température optimale de consommation.\n\n- **Objectif global** : Optimisation de la chaîne de production pour :\n  - Augmenter le volume d’affaires des clients.\n  - Réduire les pertes de denrées périssables."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques  \nDans le cadre de notre projet de R&D, nous avons confrontés un ensemble d’incertitudes, de verrous et de contraintes techniques. Pour y faire face, nous avons procédé un ensemble de développements afin de répondre aux questions suivantes :  \n\nComment déterminer les durées de préparation de chaque composant rentrant dans la préparation des produits vendus ?  \nPar quel moyen nous devons déterminer l’emplacement du consommateur afin de mesurer de manière optimale le moment de livraison ou retrait de sa commande ? Ce temps est fonction du trajet à faire pour aller au restaurant, le mode de transport utilisé et des conditions de circulations ou déterminer le temps de parcours du livreur en fonction des données de géolocalisation retranscrites en temps réel. Notre technologie doit en effet pouvoir y intégrer les informations météorologiques impactant la célérité du transport.  \nComment agréger l’ensemble des données pour déterminer le moment d’envoi du message de préparation de la commande à l’équipier ? cela permet enfin de faciliter le retrait de la commande, (objectif : un temps de retrait inférieur à la minute).  \nComment pondérer les critères impactant l’acte d’achat de l’utilisateur pour obtenir des préconisations justes ? Une des difficultés majeures, réside dans l’utilisation de plusieurs critères qui ont une influence isolée ou corrélée à d’autres facteurs.  \nComment tenir compte des différentes contraintes liées au respect de la sécurité et l’hygiène alimentaire ? Le respect de ces contraintes a pour objectif de respecter les délais de conservations des aliments.  \nQuel est la meilleure manière de conservation des aliments dans un casier connecté ?  \nComment maintenir les préparations à la température optimale de consommation ? Pour cela, il est nécessaire de mettre en place un système de communication entre le casier, les modules de gestion de commandes en ligne et la base de données des produits pour aller chercher les valeurs de températures nécessaires au maintien du produit.  \n\nLa conjugaison de l’ensemble des solutions permettra d’optimiser la chaîne de production pour augmenter le volume d’affaires de nos clients et réduire les pertes de denrées périssables."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Problématique de détection de fraude :**\n  - Gestion d'une grande quantité de données en constante croissance.\n  - Évolution continue des modèles d'intelligence artificielle pour améliorer les performances.\n  \n- **Limitations du modèle initial :**\n  - Le modèle développé ne répondait pas aux besoins de traitement des volumes de données.\n  - Nécessité de revoir les algorithmes de détection de fraude et de développer des modèles plus adaptés.\n\n- **Transformation des données :**\n  - Les données récoltées ne sont pas exploitables directement.\n  - Un processus de transformation est nécessaire avant traitement par les modèles.\n  - Problème de temps de transformation : plus d'une heure pour traiter les données d'une semaine, trop lent pour les besoins.\n\n- **Détection des achats frauduleux :**\n  - Les achats frauduleux représentent une infime partie des transactions.\n  - Déséquilibre important entre données d'achats frauduleux et non-frauduleux.\n  - Impact sur le modèle : surpondération des données non-frauduleuses.\n  - Recherche de méthodes pour équilibrer les deux catégories de données.\n\n- **Détection des anomalies :**\n  - Nécessité d'alerter en temps réel sur les anomalies des transactions.\n  - Modèles basés sur des données historiques, ce qui rend les calculs des courbes prévisionnelles longs.\n  - Verrou dans l'alerte en temps réel des anomalies.\n  - Test de modèles performants avec une quantité de données limitée.\n\n- **Faux positifs dans la détection d'anomalies :**\n  - Problème majeur lié à la quantité de faux positifs générés par les modèles.\n  - Mise en place d'une démarche expérimentale pour comparer différents modèles de prédiction des ventes afin de résoudre ce problème."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLa problématique de la détection de fraude implique la gestion d’une large quantité de données, qui plus est en constante croissance. Aussi, les modèles d’intelligence artificielle sont en constante évolution afin d’améliorer leurs performances. Le modèle que nous avions initialement développé pour détecter les éventuelles fraudes ne permettait pas de répondre à ces deux constats. Nous avons été très vite confrontés à des problématiques de traitement du volume de données, qui nous ont obligées à revoir nos algorithmes de détection de fraude, et donc à développer des modèles plus adaptés à nos problématiques.\n\nAussi, les données récoltées ne sont pas directement exploitables en l’état. Un processus de transformation de la donnée brute est alors nécessaire avant qu’elle puisse être traitée par nos modèles. Or, nous avons été confrontés à des problématiques de temps de transformation de la donnée. Il nous fallait en effet plus d’une heure afin de transformer les données récoltées en une semaine, ce qui était largement trop lent par rapport à nos besoins.\n\nNotre solution doit être capable de détecter les achats frauduleux. Néanmoins, ces derniers ne représentent qu’une infime partie des achats réalisés. Il existe donc un déséquilibre important entre la quantité de données liée à des achats frauduleux et celle liée à des achats non-frauduleux. Ce déséquilibre impacte de manière considérable notre modèle puisqu’il accorde une plus grande importance aux données d’achat non frauduleuses qu’aux données frauduleuses. Nous avons donc cherché à résoudre cette problématique en testant différentes méthodes nous permettant d’équilibrer les deux catégories de données.\n\nNous avons également été confrontés à des problématiques relatives à notre outil de détection des anomalies sur les transactions effectuées. Notre solution se devait de renvoyer des alertes lorsqu’une anomalie est relevée en temps réel. Or, nos modèles se basent sur les données acquises au cours des mois précédents afin d’analyser si la trajectoire des ventes diverge fortement de la trajectoire attendue. Les calculs des courbes prévisionnelles pouvaient donc être extrêmement longs et cela représentait un véritable verrou dans notre volonté d’alerte en temps réel des anomalies. Il nous a donc fallu tester les modèles les plus performants lorsque nous limitions la quantité de données qu’il leur était fournie.\n\nUn second verrou majeur lié à la problématique de détection des anomalies était la quantité de faux positifs renvoyés par nos modèles de détection d’anomalie. Nous avons alors mis en place une démarche expérimentale visant à comparer différents modèles de prédiction des ventes afin de résoudre ce problème."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction de la section 'Aléas, Incertitudes et Verrous' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger une synthèse claire et précise à partir des bullet points que tu reçois en input. Le texte doit être bien structuré, mettre en évidence les incertitudes techniques, les risques identifiés, ainsi que les verrous technologiques dans le cadre du projet. Tu dois utiliser un langage technique et rigoureux, en respectant la précision scientifique. Il est important d'inclure des détails pertinents, d'expliquer les défis auxquels le projet fait face, et de souligner les points de blocage ou d'incertitude, tout en maintenant un ton professionnel et fluide."}, {"role": "user", "content": "- **Évolutions technologiques** :\n  - Introduction de nouveaux modèles économiques basés sur les microtransactions dans les jeux en ligne.\n  - Augmentation des fraudes par carte de crédit en ligne.\n\n- **Difficultés rencontrées par les marchands de jeux en ligne** :\n  - Application des techniques standard de détection de fraudes compliquée par le manque d'informations personnelles sur les clients.\n  - Nécessité d'une classification en temps réel des transactions.\n\n- **Caractéristiques spécifiques des transactions de jeux en ligne** :\n  - Traitement de microtransactions à faible volume.\n  - Exigence de traitement instantané en temps réel.\n\n- **Nécessité d'une approche automatisée** :\n  - Importance d'une fouille de données automatisée pour traiter les transactions frauduleuses.\n\n- **Concept d'anonymat dans l'industrie des jeux** :\n  - Manque d'informations personnelles (noms, adresses) dans les environnements de jeu en ligne.\n  - Complexité accrue de l'application des techniques standards due à l'absence de données personnelles et à la courte historique des transactions des joueurs.\n\n- **Données disponibles** :\n  - Bien que les marchands disposent de données détaillées sur le comportement en jeu et les transactions antérieures, la détection de fraudes reste difficile.\n  - Rareté des exemples positifs complique l'identification des comportements frauduleux.\n\n- **Conséquences des contraintes** :\n  - Augmentation de la propension aux faux positifs dans les systèmes de détection.\n  - Nécessité de revoir les algorithmes de détection de fraude et d'anomalies.\n  - Développement de modèles plus adaptés aux problématiques spécifiques rencontrées."}, {"role": "assistant", "content": "Aléas, incertitudes scientifiques, verrous technologiques\nLes évolutions technologiques, telles que les achats de jeux en ligne et les jeux massivement multijoueurs en ligne, ont introduit de nouveaux modèles économiques basés sur les microtransactions. Cependant, cela a également entraîné une augmentation des fraudes par carte de crédit en ligne. Les marchands de jeux en ligne rencontrent des difficultés à appliquer les techniques standard de détection de fraudes en raison du manque d'informations personnelles sur leurs clients et de la nécessité d'une classification en temps réel [33]. Contrairement à d'autres détaillants en ligne, les marchands de jeux en ligne doivent traiter des microtransactions à faible volume et fournir instantanément en temps réel. Ainsi, une approche automatisée de fouille de données est nécessaire pour traiter les transactions frauduleuses par carte de crédit. Par ailleurs, l'industrie elle-même, par le biais de plateformes de jeux comme Instant Gaming et d'autres services de streaming similaires, est partiellement définie par le concept d'anonymat, nous ne disposons pas d’informations personnelles telles que les véritables noms et adresses personnels dans un environnement de jeu en ligne. Ce manque de données personnelles, en plus de la transaction courte historique des joueurs, rend aussi complexe l'application des techniques standards.\n\nEnfin, même si les marchands de jeux bénéficient de données détaillées sur le comportement en jeu et les transactions antérieures, ce qui permet d'utiliser ces données pour identifier les modèles de comportement légitimes et frauduleux, le problème de détection de fraudes ou de rares événements, est intrinsèquement difficile en raison de la rareté des exemples positifs [34]. Cela peut entraîner une propension accrue aux faux positifs.\n\nCes contraintes nous ont très vite confrontés à des problématiques technologiques dans notre contexte opérationnel, qui nous ont obligées à revoir nos algorithmes de détection de fraude et d’anomalies, et donc à développer des modèles plus adaptés à nos problématiques."}]}
