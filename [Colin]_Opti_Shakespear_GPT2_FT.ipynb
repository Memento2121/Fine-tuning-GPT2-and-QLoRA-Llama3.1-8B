{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNOSJRLLXNQE",
        "outputId": "4e8c9eab-be5d-4f23-f8d0-c986f2a97680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Fine-tuning-GPT2' already exists and is not an empty directory.\n",
            "/content/Fine-tuning-GPT2\n"
          ]
        }
      ],
      "source": [
        "# If you are running this notebook on Google Colab run this cell to clone the repository\n",
        "!git clone https://github.com/Memento2121/Fine-tuning-GPT2-and-QLoRA-Llama3.1-8B.git\n",
        "%cd Fine-tuning-GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "43_P4s7FXQno"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtU5XqYqXyy2",
        "outputId": "925c17df-ccc6-47b5-f030-d20361ea7be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ],
      "source": [
        "# dataset is a text file of shakespear text\n",
        "\n",
        "with open('input.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "print(data[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZwFoVNBio0cj"
      },
      "outputs": [],
      "source": [
        "seed_val = 42\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OEKeKfciBkx",
        "outputId": "ea226899-59e9-49ca-935c-e2f4e1c82d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split into 372 chunks and saved to output_chunks.csv.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "def split_text_into_chunks(input_file, output_csv, chunk_size=3000):\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Split text into chunks of specified character size\n",
        "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    # Write chunks to a CSV file\n",
        "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['chunk'])  # Write header\n",
        "        for chunk in chunks:\n",
        "            writer.writerow([chunk])\n",
        "\n",
        "    print(f\"Split into {len(chunks)} chunks and saved to {output_csv}.\")\n",
        "\n",
        "# Example usage\n",
        "split_text_into_chunks('input.txt', 'output_chunks.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdQVAh5qiUA8",
        "outputId": "1bfe7e18-5b81-4285-c40e-b6833dbd4b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                 chunk\n",
            "0    First Citizen:\\nBefore we proceed any further,...\n",
            "1    ever\\nAppear in your impediment. For the deart...\n",
            "2    eart, to the seat o' the brain;\\nAnd, through ...\n",
            "3    usands of these quarter'd slaves, as high\\nAs ...\n",
            "4    eads on at noon: but I do wonder\\nHis insolenc...\n",
            "..                                                 ...\n",
            "367  e a vassal of him.\\n\\nPROSPERO:\\nSo, slave; he...\n",
            "368   thyself\\nUpon this island as a spy, to win it...\n",
            "369  elier than I meant you should.\\n\\nGONZALO:\\nTh...\n",
            "370  \\nANTONIO:\\nO, widow Dido! ay, widow Dido.\\n\\n...\n",
            "371  cts?\\n\\nANTONIO:\\nNone, man; all idle: whores ...\n",
            "\n",
            "[372 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "# load into a data frame\n",
        "df = pd.read_csv ('output_chunks.csv')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtTC4IiOkfdF",
        "outputId": "9a14fdee-f894-43b0-f797-286be16c23e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the GPT tokenizer.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9S2WgGxi067",
        "outputId": "0b0fd3f0-1ab3-4329-c767-a2bf3e7861b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "834\n"
          ]
        }
      ],
      "source": [
        "data = df.chunk.copy()\n",
        "\n",
        "for x in data:\n",
        "    tokens = tokenizer.tokenize(x)\n",
        "    print(len(tokens))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QZ8sCLQkeB_",
        "outputId": "596fd172-580b-45a3-a4c5-54beeccd8f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The beginning of sequence token <|startoftext|> token has the id 50257\n",
            "The end of sequence token <|endoftext|> has the id 50256\n",
            "The padding token <|pad|> has the id 50258\n"
          ]
        }
      ],
      "source": [
        "#print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
        "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
        "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
        "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MJPFFyGAk0k1"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 2\n",
        "epochs = 5\n",
        "learning_rate = 5e-4\n",
        "warmup_steps_factor = 0.1\n",
        "epsilon = 1e-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Tw1pwLBRk1dF"
      },
      "outputs": [],
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, max_length):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pLb5dSul8zK",
        "outputId": "42aca763-6785-4241-9b94-a0037249dda3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "context size : 1024\n",
            "Training set size: 334\n",
            "Validation set size: 38\n"
          ]
        }
      ],
      "source": [
        "block_size = GPT2Config.from_pretrained('gpt2').n_positions\n",
        "print(f\"context size : {block_size}\")\n",
        "\n",
        "dataset = GPT2Dataset(data, tokenizer, max_length=block_size)\n",
        "\n",
        "# Define the split ratio\n",
        "train_ratio = 0.9\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Optional: create DataLoaders for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ksM-ZQJy2OgP"
      },
      "outputs": [],
      "source": [
        "warmup_steps = int(warmup_steps_factor*len(train_loader)*epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE52T-DjoS91",
        "outputId": "d3b8f284-7aac-49c4-dfa1-0a6942d07f5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Model\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UojDu6O3jd3F",
        "outputId": "b98c7a99-64aa-4ffd-ec2e-4e5ae9f17c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 124441344\n",
            "Total parameters: 124441344\n",
            "Percentage of trainable parameters: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Define function to print trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable parameters: {trainable_params}\")\n",
        "    print(f\"Total parameters: {total_params}\")\n",
        "    print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n",
        "\n",
        "# Print trainable parameters\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM0UeMDkpfuM",
        "outputId": "560bf7f9-abdf-493b-9761-1cef3b71d24f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training steps: 835\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "print(f\"Total training steps: {total_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Dw-73iZDpA",
        "outputId": "d45ecbfa-1492-404b-c564-d47985b0d98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1 took 37.24 seconds / Norm : nan / Total Norm : nan\n",
            "Batch 2 took 0.10 seconds / Norm : nan / Total Norm : nan\n",
            "Batch 3 took 0.11 seconds / Norm : nan / Total Norm : nan\n",
            "Batch 4 took 0.11 seconds / Norm : inf / Total Norm : nan\n",
            "Batch 5 took 0.11 seconds / Norm : inf / Total Norm : nan\n",
            "Batch 6 took 0.10 seconds / Norm : inf / Total Norm : nan\n",
            "Batch 7 took 0.11 seconds / Norm : inf / Total Norm : nan\n",
            "Batch 8 took 0.21 seconds / Norm : 236634.4688 / Total Norm : 0.0020\n",
            "Batch 9 took 0.15 seconds / Norm : 202215.2344 / Total Norm : 0.0020\n",
            "Batch 10 took 0.15 seconds / Norm : 194916.9531 / Total Norm : 0.0020\n",
            "Average time per batch: 0.1266 seconds\n",
            "Tokens per second : 16171.16576732044\n",
            "Estimated total training time: 1.76 minutes\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import time\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "model = torch.compile(model)\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Function to measure time per batch\n",
        "def measure_time_per_batch(dataloader, num_batches=10):\n",
        "    model.train()\n",
        "    torch.cuda.synchronize()\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        if i == 1:\n",
        "            start_time = time.time()\n",
        "        t0 = time.time()\n",
        "        if i >= num_batches:\n",
        "            break\n",
        "        input_seq = batch[0].to(device)\n",
        "        target_seq = batch[0].to(device)\n",
        "        mask_seq = batch[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "          outputs = model(input_seq, labels=target_seq, attention_mask = mask_seq)\n",
        "          loss = outputs.loss\n",
        "        scaler.scale(loss).backward()\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        torch.cuda.synchronize()\n",
        "        # Calculate gradient norm\n",
        "        total_norm = 0.0\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                param_norm = p.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        total_norm = total_norm ** 0.5\n",
        "        t1 = time.time()\n",
        "        print(f\"Batch {i+1} took {t1-t0:.2f} seconds / Norm : {norm:.4f} / Total Norm : {total_norm:.4f}\")\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "    avg_time_per_batch = (end_time - start_time) / (num_batches - 1) # don't count the first iteration\n",
        "    return avg_time_per_batch\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Measure the average time per batch\n",
        "avg_time_per_batch = measure_time_per_batch(train_loader)\n",
        "print(f\"Average time per batch: {avg_time_per_batch:.4f} seconds\")\n",
        "\n",
        "print(f\"Tokens per second : {block_size*batch_size/avg_time_per_batch}\")\n",
        "\n",
        "# Calculate total training time\n",
        "total_iterations = len(train_loader) * epochs\n",
        "estimated_total_time = total_iterations * avg_time_per_batch\n",
        "print(f\"Estimated total training time: {estimated_total_time / 60:.2f} minutes\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW_jOi9vdQFQ",
        "outputId": "d7ba0885-d40c-4095-c07f-bd9a58fcd46e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Iter 166 Loss: 2.8665401935577393\n",
            "0:  bipartisanlove,\n",
            "'Twas when 't thou wast a slave to a king.\n",
            "\n",
            "MOMEO:\n",
            "I may I may but tell thee that they are gone.\n",
            "\n",
            "MOMEO:\n",
            "Nay, let me know this, and we will all come.\n",
            "Thou art now the king, and I'll not talk to him.\n",
            "\n",
            "MOMEO:\n",
            "So, sir, to-day thou shalt sit alone,\n",
            "And not talk to him or do any thing;\n",
            "Because he is the king.\n",
            "My son's king; 'tis the king:\n",
            "He's the queen.\n",
            "\n",
            "MOMEO:\n",
            "Look, we are all kings.\n",
            "\n",
            "MOMEO:\n",
            "I will tell him thy king is king.\n",
            "\n",
            "MOMEO:\n",
            "I'll make him king again.\n",
            "\n",
            "MOMEO:\n",
            "Ay, my lord.\n",
            "\n",
            "MOMEO:\n",
            "'Tis not, but I'll bring\n",
            "Epoch 0 Iter 166 Validation Loss: 3.0906066643564323\n",
            "Epoch 0 Total Loss: 3.5616165606561534\n",
            "Epoch 1 Iter 166 Loss: 2.5959529876708984\n",
            "0:  increasing of all,\n",
            "That now comes upon the king's head,\n",
            "And I thus conclude by him himself.\n",
            "\n",
            "WARWICK:\n",
            "Hath it in me yet, like a crown'd,\n",
            "To make the realm prosperous again?\n",
            "\n",
            "KING RICHARD II:\n",
            "But how now!\n",
            "\n",
            "RICHARD:\n",
            "To the Duke of Warwick.\n",
            "\n",
            "KING RICHARD II:\n",
            "What will he have?\n",
            "\n",
            "RICHARD:\n",
            "To the Earl of Warwick, Duke of York,\n",
            "And all the Duke of Surrey;\n",
            "Who shall be Prince of York and all the Duke of York.\n",
            "\n",
            "KING RICHARD II:\n",
            "Now, what says Warwick?\n",
            "\n",
            "RICHARD:\n",
            "How now!\n",
            "\n",
            "KING RICHARD II:\n",
            "Well, how now!\n",
            "\n",
            "RICHARD:\n",
            "Who comes at thee?\n",
            "\n",
            "RICHARD:\n",
            "What! what?\n",
            "\n",
            "KING RICHARD II:\n",
            "\n",
            "Epoch 1 Iter 166 Validation Loss: 2.985097144779406\n",
            "Epoch 1 Total Loss: 2.8532164296704137\n",
            "Epoch 2 Iter 166 Loss: 2.2735342979431152\n",
            "0: day:\n",
            "O, we have such a warrant, so do we:\n",
            "O, what is the matter with our lady?\n",
            "\n",
            "LADY CAPULET:\n",
            "My lord, it is,\n",
            "Even in a warrant for an eye-counsellor.\n",
            "\n",
            "HASTINGS:\n",
            "If you do not come straight from our lady,\n",
            "That we shall not find her within the county,\n",
            "And, as we can, tell her that there are no dogs,\n",
            "No hounds, no pugs, no sheep, no hounds,\n",
            "No fowl, no sheep-hoop, no fowl-sheep,\n",
            "No eel, no pash-sheep, no falcon-sheep, no jest,\n",
            "No lamb-shearing, no calf-shearing, no lamb-shearing,\n",
            "No sheep-shearing, no lamb-shearing, no lamb-shearing,\n",
            "A woollen lamb-\n",
            "Epoch 2 Iter 166 Validation Loss: 3.0216241886741235\n",
            "Epoch 2 Total Loss: 2.4826474104098932\n",
            "Epoch 3 Iter 166 Loss: 2.2573204040527344\n",
            "0:  Hange the law:\n",
            "But, my lord, the most grievous sin I can conceive\n",
            "Beguile me from heaven and hell!\n",
            "For I have no other choice but to obey\n",
            "My sovereign's will. Behold, as I hear,\n",
            "The man whom your heavenly grace is,\n",
            "Beseech you, my soul, to continue\n",
            "A sinful precedent till we meet again,\n",
            "That is, to that end we may all suffer\n",
            "Our very loss, so we may be avenged:\n",
            "For this is a kind of redemption,\n",
            "That men like to cheques and take their joys\n",
            "In loss of loss of joy,\n",
            "When they see that they have deserved't!\n",
            "\n",
            "GLOUCESTER:\n",
            "My gracious lord, the grace of heaven,\n",
            "Condemns the execution of this offence,\n",
            "And makes no apology for it:\n",
            "It's not my fault; for it came from my soul\n",
            "To redeem me.\n",
            "\n",
            "\n",
            "Epoch 3 Iter 166 Validation Loss: 3.123616783242477\n",
            "Epoch 3 Total Loss: 2.1410969845549075\n",
            "Epoch 4 Iter 166 Loss: 1.7141039371490479\n",
            "0:  foods,\n",
            "I thank you.\n",
            "\n",
            "First Servant:\n",
            "The duke's coming.\n",
            "\n",
            "Third Servant:\n",
            "And he's coming.\n",
            "\n",
            "First Servant:\n",
            "And he's coming.\n",
            "\n",
            "Third Servant:\n",
            "'Tis like they'll meet once more.\n",
            "\n",
            "Second Servant:\n",
            "'Twas never before I saw before.\n",
            "\n",
            "Third Servant:\n",
            "If he be so near, it follows\n",
            "That he shall not stay at the palace.\n",
            "\n",
            "Second Servant:\n",
            "It follows, sir, that he should leave the duke.\n",
            "\n",
            "Third Servant:\n",
            "Why, the duke will not be in presence of the nobles,\n",
            "Unless he offer some form of revenge.\n",
            "\n",
            "First Servant:\n",
            "Why, the duke will not be in presence of the nobles.\n",
            "\n",
            "Third Servant:\n",
            "The duke will not be in presence of the nobles;\n",
            "Unless he demand that his lordships\n",
            "Epoch 4 Iter 166 Validation Loss: 3.2540064736416467\n",
            "Epoch 4 Total Loss: 1.8604062938404655\n",
            "Training took 2.41 minutes\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune the model\n",
        "\n",
        "do_train = True\n",
        "t0 = time.time()\n",
        "if do_train:\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      total_val_loss = 0\n",
        "      for i, batch in enumerate(train_loader):\n",
        "          input_seq = batch[0].to(device)\n",
        "          target_seq = batch[0].to(device)\n",
        "          mask_seq = batch[1].to(device)\n",
        "          optimizer.zero_grad()\n",
        "          with torch.cuda.amp.autocast():\n",
        "            outputs = model(input_seq, labels=target_seq, attention_mask = mask_seq)\n",
        "            loss = outputs.loss\n",
        "          scaler.scale(loss).backward()\n",
        "          norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "          scheduler.step()\n",
        "          total_loss += loss.item()\n",
        "          if (i+1) % int((1/epochs)*total_steps) == 0 and i > 0:\n",
        "              model.eval()\n",
        "              print(f\"Epoch {epoch} Iter {i} Loss: {loss.item()}\")\n",
        "              sample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,\n",
        "                                    top_k=50,\n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95,\n",
        "                                    num_return_sequences=1,\n",
        "                                    pad_token_id=tokenizer.pad_token_id,\n",
        "                                    eos_token_id=tokenizer.eos_token_id,\n",
        "                                )\n",
        "              for k, sample_output in enumerate(sample_outputs):\n",
        "                    print(\"{}: {}\".format(k, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "              total_val_loss = 0\n",
        "              for j, batch in enumerate(val_loader):\n",
        "                  input_seq = batch[0].to(device)\n",
        "                  target_seq = batch[0].to(device)\n",
        "                  mask_seq = batch[1].to(device)\n",
        "                  with torch.no_grad():\n",
        "                      outputs = model(input_seq, labels=target_seq, attention_mask = mask_seq)\n",
        "                  loss = outputs.loss\n",
        "                  total_val_loss += loss.item()\n",
        "              print(f\"Epoch {epoch} Iter {i} Validation Loss: {total_val_loss/(j+1)}\")\n",
        "              model.train()\n",
        "      print(f\"Epoch {epoch} Total Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "print(f\"Training took {(time.time()-t0)/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anlOziJ9y4XS",
        "outputId": "8dadf1b1-aab3-4f7f-d0ca-258b8e93be30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time we were called on to join the ranks of the Irish.  The Irish who had seen the new Republic and the\n",
            "Irish who had been forced to flee the country on this last trip for freedom before the war were now now so well trained\n",
            "that the country had the capability of producing the finest Irish warriors and soldiers.  It was the same way that the\n",
            "Irish took on French and Italians during the Civil War. They had to fight for the Irish only on the front lines.  All\n",
            "the way up the Iron Curtain they used the Irish for their own personal ambitions.  The Irish's war service and their war\n",
            "and war effort could not have been more glorious than it is today.  It is no secret that the Irish were the strongest\n",
            "and most disciplined and loyal troops in all the British Empire's wars of empire.  The whole of Britain's War of\n",
            "Independence began as an Irish war and ended with an Irish rebellion and the blood of the Irish people\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "model_name = 'gpt2'\n",
        "model2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer2 = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "model2.eval()\n",
        "\n",
        "model2.to(device)\n",
        "\n",
        "prompt = \"Once upon a time\"\n",
        "\n",
        "#input_ids = tokenizer2.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "tokenizer2.pad_token = tokenizer2.eos_token\n",
        "\n",
        "encoded_input = tokenizer2(prompt, return_tensors='pt', padding=True, truncation=True)\n",
        "input_ids = encoded_input['input_ids'].to(device)\n",
        "attention_mask = encoded_input['attention_mask'].to(device)\n",
        "\n",
        "output_sequences = model2.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=200,  # Adjust the max_length as needed\n",
        "    num_return_sequences=1,  # Number of sequences to return\n",
        "    temperature=1.0,  # Adjust the temperature for diversity\n",
        "    top_k=50,  # Use top_k sampling\n",
        "    top_p=0.95,  # Use top_p sampling\n",
        "    do_sample=True,  # Enable sampling to generate diverse sequences\n",
        "    pad_token_id=tokenizer2.eos_token_id,\n",
        ")\n",
        "\n",
        "generated_text = tokenizer2.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "wrapped_text = textwrap.fill(generated_text, width=120)\n",
        "\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HUsVM55LdC5",
        "outputId": "617bc26b-1843-4115-bb75-e1891b5d3587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: Once upon a time of great war?\n",
            "\n",
            "First Messenger:\n",
            "Ay, my lord.\n",
            "\n",
            "Second Messenger:\n",
            "By the holy Paulina of Padua,\n",
            "An offer which you have kindly declined,\n",
            "If it be honourable for your good cause.\n",
            "\n",
            "Third Messenger:\n",
            "Hark, what noise canst thou make?\n",
            "\n",
            "Second Messenger:\n",
            "My lord,\n",
            "Ay, a murmur, a murmur, a murmur, a murmur, a murmur;\n",
            "A murmur indeed, a murmur, and a very murmur.\n",
            "\n",
            "Third Messenger:\n",
            "O, that the King of Naples did solicit you hither!\n",
            "\n",
            "PAULINA:\n",
            "O, I am sent with greetings from Lord Angelo.\n",
            "\n",
            "Second Messenger:\n",
            "A murmur, a murmur, and a very murmur.\n",
            "\n",
            "Third Messenger:\n",
            "Come hither, my liege. My lord,\n",
            "I do beseech you to pardon me, and am come\n",
            "To seek advice in this quarrel.\n",
            "\n",
            "PAULINA:\n",
            "And that's to help, my lord.\n",
            "\n",
            "Third Messenger:\n",
            "Hark you, come hither. I would my very soul were mad!\n",
            "\n",
            "PAULINA:\n",
            "O, my lord!\n",
            "\n",
            "Third Messenger:\n",
            "A murmur, a murmur, an affray.\n",
            "\n",
            "PAULINA:\n",
            "O, my Lord Angelo!\n",
            "\n",
            "Second Messenger:\n",
            "A\n",
            "\n",
            "\n",
            "1: Once upon a time,\n",
            "To lay hold of thy sovereign and his princely son:\n",
            "In this my son, like a poor distressed mother,\n",
            "As well acquainted with the sorrows of men,\n",
            "Is come to make me a father.\n",
            "\n",
            "GLOUCESTER:\n",
            "You knew not of the marriage.\n",
            "\n",
            "BUSHY:\n",
            "Why, no; I know it well.\n",
            "\n",
            "GLOUCESTER:\n",
            "But that I would have married,\n",
            "Without marrying such a crown, I thought fit;\n",
            "That I should have left thee the title,\n",
            "Which I had in my marriage usurp'd;\n",
            "With whom, being a stranger, not nobleman\n",
            "But in consequence of the loss of the queen,\n",
            "And her brother Gloucester, Duke of Hereford,\n",
            "To me that brought thee here; who,\n",
            "Having laid me on and put me down to be king,\n",
            "Had been my loving father.\n",
            "\n",
            "HASTINGS:\n",
            "Were it not so?\n",
            "\n",
            "BUSHY:\n",
            "Ay, and yet I'll be king once: by your grace I swear\n",
            "I shall never marry again, though God forbid;\n",
            "Inhabit no less than this land,\n",
            "Which in being gracious, we will never have.\n",
            "\n",
            "GLOUCESTER:\n",
            "What! wouldst thou have us there?\n",
            "\n",
            "BUSHY:\n",
            "Ay; and we are poor Lancaster,\n",
            "Being but as poor as ourselves\n",
            "\n",
            "\n",
            "2: Once upon a time,\n",
            "Upon a time you shall forget all: but, as I said, 'tis as easy\n",
            "As you remember a goodly meal.\n",
            "\n",
            "KING RICHARD III:\n",
            "O, but, Warwick, will you forget me?\n",
            "\n",
            "WARWICK:\n",
            "Ay, Warwick, ay, or else I will kill you all.\n",
            "3 KING HENRY VI\n",
            "\n",
            "RIVERS:\n",
            "My liege, I'll have some private chat with Lord Northumberland.\n",
            "Lord Northumberland?\n",
            "\n",
            "RIVERS:\n",
            "Ay, indeed; at such a rate will I woo him.\n",
            "\n",
            "WARWICK:\n",
            "'Tis full time since our coronation: and I\n",
            "have not forgot myself much: why, I prithee,\n",
            "pray, pardon me: that I did forget myself\n",
            "much before our coronation: for which I think you are\n",
            "wicked, I would not forget myself much: but this\n",
            "is a very pitiful time to be rid of you all.\n",
            "\n",
            "KING RICHARD III:\n",
            "The fitter hath lost his wits and made his wits\n",
            "greater: the worse is that he was never so\n",
            "great, but was a lesser, Duke of Gloucester,\n",
            "and I am sure of it. Why, Warwick, take my boots off,\n",
            "my boots off, my boots on: make peace, and swear fealty to me.\n",
            "\n",
            "RIVERS\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"Once upon a time\"\n",
        "\n",
        "generated = tokenizer.encode(prompt)\n",
        "generated = torch.tensor(generated).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated,\n",
        "                                do_sample=True,\n",
        "                                top_k=50,\n",
        "                                max_length = 300,\n",
        "                                top_p=0.95,\n",
        "                                num_return_sequences=3,\n",
        "                                pad_token_id=tokenizer.pad_token_id,\n",
        "                                eos_token_id=tokenizer.eos_token_id,\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
