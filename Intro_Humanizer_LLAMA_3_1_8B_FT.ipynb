{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlylnk33evSGgjjS+PpXV8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08ab50dea78e4bc78d05b5bc04d8ffa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b3ac3bd62aa454284de2ede3aa394cd",
              "IPY_MODEL_d1ee93270d4f466f94d2f38c7785dd33",
              "IPY_MODEL_6c3746effbc1445296457b199e1512c1"
            ],
            "layout": "IPY_MODEL_723ae66e550d43c7a1e1e7b2fbc462ab"
          }
        },
        "6b3ac3bd62aa454284de2ede3aa394cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4e230ced0ac4efa85b22c4de9f0ce5d",
            "placeholder": "​",
            "style": "IPY_MODEL_465feddb4321447fbfebcd94b34daa7d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d1ee93270d4f466f94d2f38c7785dd33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cd7bcb4e718482d8e16e2908593c030",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_725b8f1e22b647bd991f4b3c2837b45f",
            "value": 4
          }
        },
        "6c3746effbc1445296457b199e1512c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_152fe98691e54ea2a8eeee01f7bc6e2e",
            "placeholder": "​",
            "style": "IPY_MODEL_5cb1712932ba418ab5ae19fdb7ffcafe",
            "value": " 4/4 [00:13&lt;00:00,  2.99s/it]"
          }
        },
        "723ae66e550d43c7a1e1e7b2fbc462ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e230ced0ac4efa85b22c4de9f0ce5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "465feddb4321447fbfebcd94b34daa7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cd7bcb4e718482d8e16e2908593c030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "725b8f1e22b647bd991f4b3c2837b45f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "152fe98691e54ea2a8eeee01f7bc6e2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cb1712932ba418ab5ae19fdb7ffcafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# If you are running this notebook on Google Colab run this cell to clone the repository\n",
        "!git clone https://github.com/Memento2121/Fine-tuning-GPT2-and-QLoRA-Llama3.1-8B.git\n",
        "%cd Fine-tuning-GPT2-and-QLoRA-Llama3.1-8B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsDgCWlv0M1p",
        "outputId": "f52e8668-6401-4366-9b0b-9b4b219d86ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Fine-tuning-GPT2-and-QLoRA-Llama3.1-8B' already exists and is not an empty directory.\n",
            "/content/Fine-tuning-GPT2-and-QLoRA-Llama3.1-8B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "!pip install peft trl\n",
        "!pip install -U bitsandbytes\n",
        "!pip install --upgrade transformers\n",
        "\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM, setup_chat_format"
      ],
      "metadata": {
        "id": "43_P4s7FXQno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4498be30-e48c-40a2-8109-1453e453cd1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.9.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.21.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.10.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_val = 42\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed(seed_val)"
      ],
      "metadata": {
        "id": "xAdIwOTqEGjz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "\n",
        "api_token = \"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\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "max_seq_length = 2048\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_auth_token=api_token, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=api_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "08ab50dea78e4bc78d05b5bc04d8ffa8",
            "6b3ac3bd62aa454284de2ede3aa394cd",
            "d1ee93270d4f466f94d2f38c7785dd33",
            "6c3746effbc1445296457b199e1512c1",
            "723ae66e550d43c7a1e1e7b2fbc462ab",
            "e4e230ced0ac4efa85b22c4de9f0ce5d",
            "465feddb4321447fbfebcd94b34daa7d",
            "7cd7bcb4e718482d8e16e2908593c030",
            "725b8f1e22b647bd991f4b3c2837b45f",
            "152fe98691e54ea2a8eeee01f7bc6e2e",
            "5cb1712932ba418ab5ae19fdb7ffcafe"
          ]
        },
        "id": "GtTC4IiOkfdF",
        "outputId": "e4279419-ae34-45b2-efb2-c1755747c068"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08ab50dea78e4bc78d05b5bc04d8ffa8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "print(tokenizer.chat_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Tpl3Wya_Fr",
        "outputId": "d8b0c91b-6dd7-4b1b-d80c-59fa4a346179"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
            "' + message['content'] + '<|im_end|>' + '\n",
            "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
            "' }}{% endif %}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# load jsonl dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"output.jsonl\", split = 'train')\n",
        "\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(dataset) * split_ratio)\n",
        "\n",
        "train_dataset = dataset.select(range(split_index))\n",
        "val_dataset = dataset.select(range(split_index, len(dataset)))\n",
        "\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n",
        "\n",
        "print(train_dataset.column_names)\n",
        "\n",
        "\n",
        "print(train_dataset[0])\n",
        "print(val_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_qB72Gm0Y5X",
        "outputId": "a47a8146-1ade-43eb-c0be-7a1909063059"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 20\n",
            "Validation set size: 5\n",
            "['messages']\n",
            "{'messages': [{'role': 'system', 'content': \"Tu es un assistant qui aide à réécrire les introductions de rapport de présentation d'entreprises générées par l'IA pour les rendre plus humaines, chaleureuses et engageantes. Chaque texte doit refléter un ton personnel et authentique, comme s'il était écrit par une personne réelle travaillant dans l'entreprise. Utilise les exemples fournis d'introductions générées par l'IA et rédigées par des humains pour identifier et appliquer les nuances de style, de ton et de structure qui rendent un texte plus personnel et proche du lecteur.\"}, {'role': 'user', 'content': \"Akeneo a été fondée en 2013 avec l'objectif de fournir aux entreprises un outil pour gérer et valoriser efficacement leurs produits, considérés comme une ressource essentielle. Dans des secteurs comme les produits culturels ou la grande distribution, les catalogues peuvent comprendre des millions de produits répartis en centaines de familles (Musique, Livres, Films, Jeux Vidéos, etc.), avec une volumétrie en constante augmentation (+10 000 produits par mois pour un vendeur de produits culturels en moyenne). Dans le domaine de la mode, les marques doivent renouveler leurs collections au moins deux fois par an, en s’assurant que les informations sur les nouveaux produits (photos, tailles, couleurs, matières, descriptions marketing, etc.) soient continuellement enrichies. Akeneo a ainsi développé une plateforme spécialement conçue pour les équipes marketing, permettant d'intégrer et de gérer de manière optimale les données produits d'une entreprise. L'équipe d'Akeneo, forte de plus de 10 ans d'expérience dans le domaine du e-commerce, a constaté que de nombreuses entreprises, petites et grandes, sont confrontées à des systèmes complexes ou obsolètes pour la gestion de leurs données produits. Pour répondre à ce besoin, Akeneo s'est donné pour mission d'améliorer ces systèmes de gestion de données. Afin de mener à bien ses projets de R&D et d'innovation, Akeneo a recruté des experts reconnus et des data-scientists qualifiés, et prévoit de continuer à renforcer son équipe pour atteindre plus de 100 collaborateurs d'ici janvier 2018.\"}, {'role': 'assistant', 'content': 'Nous avons fondé Akeneo en 2013 avec la conviction que le produit est la ressource la plus importante d’une entreprise et qu’il fallait un outil aux marchands pour les aider à gérer et valoriser leurs ressources. En effet, dans le monde des produits culturels ou de la grande distribution, il n’est pas rare de trouver des catalogues de plusieurs millions de produits répartis en plusieurs centaines de familles (Musique, Livres, Films, Jeux Vidéos, etc.). La volumétrie de ces catalogues ne cesse d’augmenter (+10 000 produits par mois en moyenne pour un vendeur de produits culturels). Dans le monde de la mode, les collections de produits sont renouvelées au moins deux fois par an (collections) avec une nécessité pour les marques de perpétuellement maintenir un processus d’enrichissement des informations relatives à ces nouveaux produits (photos, tailles, couleurs, matières, descriptions marketing, etc.). Nous avons donc créé une plateforme, pensée pour les équipes marketing, afin d’intégrer et gérer de manière optimale les milliards de données constituants les bases de données des produits d’une entreprise. Les personnes composant notre équipe évoluent dans le monde du e-commerce depuis plus de 10 ans. Au contact des petites et grandes marques, que nous avons pu accompagner dans le développement de leur activité e-commerce, nous avons réalisé qu’elles étaient toutes confrontées à des systèmes complexes ou archaïques pour gérer leurs données produits. Nous nous sommes donc confié la mission de mettre de l’ordre dans leurs système de gestion de données produit. Pour nous permettre d’aboutir dans nos projets de R&D et d’innovation, nous avons recrutés des experts reconnus dans le domaine, ainsi que des data-scientists très compétents. Nous prévoyons encore d’embaucher des personnes afin de soutenir l’effort de R&D et d’innovation. Nous pensons que nos effectifs vont atteindre plus de 100 personnes en janvier 2018.'}]}\n",
            "{'messages': [{'role': 'system', 'content': \"Tu es un assistant qui aide à réécrire les introductions de rapport de présentation d'entreprises générées par l'IA pour les rendre plus humaines, chaleureuses et engageantes. Chaque texte doit refléter un ton personnel et authentique, comme s'il était écrit par une personne réelle travaillant dans l'entreprise. Utilise les exemples fournis d'introductions générées par l'IA et rédigées par des humains pour identifier et appliquer les nuances de style, de ton et de structure qui rendent un texte plus personnel et proche du lecteur.\"}, {'role': 'user', 'content': \"Fondé en 2004 par Chafik GASMI, CHAFIK STUDIO est un studio de création polyvalent qui englobe divers domaines tels que l'architecture, l'architecture d'intérieur, le design, le retail design et la direction artistique. Le studio est reconnu pour ses réalisations novatrices, fonctionnelles et intemporelles, alliant simplicité et séduction. CHAFIK.STUDIO a collaboré avec des marques prestigieuses comme Lancôme, Baccarat, Fendi, Sephora, entre autres. L'objectif du studio est de placer l'art au centre de chaque expérience, grâce à une équipe pluridisciplinaire aux compétences variées et complémentaires. Le studio est à l'origine de nombreuses innovations uniques qui ont rencontré un grand succès. Parmi ces réalisations, on compte le premier concept de magasin Sephora sur les Champs-Élysées, le premier musée d'art moderne du monde arabe, le premier hôtel Baccarat, ainsi que la nouvelle expérience de vente au détail pour Lancôme. Le studio a également conçu des œuvres emblématiques comme la chaise Square à Times Square et le fauteuil Ying à Matignon.\"}, {'role': 'assistant', 'content': \"CHAFIK STUDIO a été fondé en 2004 par Chafik GASMI en tant que studio de création polyvalent englobant les domaines de l'architecture, de l'architecture d'intérieur, du design, du retail design et de la direction artistique. Il est renommé pour ses réalisations novatrices, fonctionnelles, épurées, séduisantes et intemporelles. CHAFIK.STUDIO a collaboré avec des marques prestigieuses telles que Lancôme, Baccarat, Fendi, Sephora, et bien d'autres. L'objectif de Chafik Studio est de placer l'art au cœur de toutes les expériences grâce à une équipe pluridisciplinaire aux compétences complémentaires. Le studio est à l'origine de nombreuses innovations uniques et couronnées de succès, notamment le premier concept de magasin Sephora sur les Champs-Élysées, le premier musée d'art moderne du monde arabe, le premier hôtel Baccarat, la nouvelle expérience de vente au détail de Lancôme, la chaise Square à Times Square, le fauteuil Ying à Matignon, et bien plus encore.\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_dataset[0]['messages'][0]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO8-jMk45u_7",
        "outputId": "1a776cbb-aea5-4472-bc69-703090417075"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tu es un assistant qui aide à réécrire les introductions de rapport de présentation d'entreprises générées par l'IA pour les rendre plus humaines, chaleureuses et engageantes. Chaque texte doit refléter un ton personnel et authentique, comme s'il était écrit par une personne réelle travaillant dans l'entreprise. Utilise les exemples fournis d'introductions générées par l'IA et rédigées par des humains pour identifier et appliquer les nuances de style, de ton et de structure qui rendent un texte plus personnel et proche du lecteur.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_dataset[0]['messages'][1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nohlEj4v2ugw",
        "outputId": "0efbdd93-ddcc-4ee2-baa4-9d80cf6b3c8f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fondé en 2004 par Chafik GASMI, CHAFIK STUDIO est un studio de création polyvalent qui englobe divers domaines tels que l'architecture, l'architecture d'intérieur, le design, le retail design et la direction artistique. Le studio est reconnu pour ses réalisations novatrices, fonctionnelles et intemporelles, alliant simplicité et séduction. CHAFIK.STUDIO a collaboré avec des marques prestigieuses comme Lancôme, Baccarat, Fendi, Sephora, entre autres. L'objectif du studio est de placer l'art au centre de chaque expérience, grâce à une équipe pluridisciplinaire aux compétences variées et complémentaires. Le studio est à l'origine de nombreuses innovations uniques qui ont rencontré un grand succès. Parmi ces réalisations, on compte le premier concept de magasin Sephora sur les Champs-Élysées, le premier musée d'art moderne du monde arabe, le premier hôtel Baccarat, ainsi que la nouvelle expérience de vente au détail pour Lancôme. Le studio a également conçu des œuvres emblématiques comme la chaise Square à Times Square et le fauteuil Ying à Matignon.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE52T-DjoS91",
        "outputId": "ac102976-02ff-4b23-84d9-d98ae9d26934"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128258, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128258, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "tN9XW_yszxeo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "l3LtvvV_z6Ct"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=\"all-linear\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6fbj5fx0CIQ",
        "outputId": "a5d6db5a-4c45-4d24-cea0-ab8e31798623"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 20971520 || all params: 4561588224 || trainable%: 0.45974162879634795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir='./Intro_Humanizer_Weights',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=5,\n",
        "    logging_steps=5,  # Log every 100 steps\n",
        "    learning_rate=5e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    torch_compile=True,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=5,\n",
        "    logging_dir='./logs',\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    max_grad_norm=1.0,\n",
        "    max_steps=80,\n",
        "    max_seq_length = max_seq_length\n",
        ")\n",
        "\n",
        "if sft_config.gradient_checkpointing :\n",
        "  model.gradient_checkpointing_enable()\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QkHKGkShelm8",
        "outputId": "f543605a-135f-4d67-da7c-2e6183779e30"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "The speedups for torchdynamo mostly come wih GPU Ampere or higher and which is not detected here.\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:366: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [80/80 07:28, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.815000</td>\n",
              "      <td>2.003617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.926400</td>\n",
              "      <td>1.839483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.653100</td>\n",
              "      <td>1.521506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.357500</td>\n",
              "      <td>1.331915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.221400</td>\n",
              "      <td>1.164214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.134200</td>\n",
              "      <td>1.046950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.992400</td>\n",
              "      <td>0.973866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.047900</td>\n",
              "      <td>0.951375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.845200</td>\n",
              "      <td>0.943676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.934700</td>\n",
              "      <td>0.943993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.854800</td>\n",
              "      <td>0.931147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.842900</td>\n",
              "      <td>0.931777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.779600</td>\n",
              "      <td>0.938413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.763900</td>\n",
              "      <td>0.942891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.819600</td>\n",
              "      <td>0.943019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.752900</td>\n",
              "      <td>0.942948</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66c467eb-2bcdfc0647cc4725025f05d5;a4899aa3-59e3-4af4-b023-e77def5ebe9a)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3.1-8B is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B - will assume that the vocabulary was not modified.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=80, training_loss=1.1088514298200607, metrics={'train_runtime': 463.4845, 'train_samples_per_second': 0.173, 'train_steps_per_second': 0.173, 'total_flos': 3074903909597184.0, 'train_loss': 1.1088514298200607, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [\n",
        "    {\"role\": \"user\", \"content\": \"What's the weather like?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"It's sunny and warm.\"}\n",
        "]\n",
        "\n",
        "# Example of processing the conversation\n",
        "formatted_input = tokenizer.apply_chat_template(conversation)\n",
        "print(tokenizer.decode(formatted_input))"
      ],
      "metadata": {
        "id": "rblv7Z5PfqYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00c3d39-15f3-4f67-bf09-374200d9da7d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|>user\n",
            "What's the weather like?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "It's sunny and warm.<|im_end|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def remove_assistant_part(entry):\n",
        "    # Filter out the assistant's response from the messages\n",
        "    messages = [msg for msg in entry[\"messages\"] if msg[\"role\"] != \"assistant\"]\n",
        "    return messages\n",
        "\n",
        "text = val_dataset[0]\n",
        "entry = remove_assistant_part(text)\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(entry, tokenize = True, add_generation_prompt=True)\n",
        "generated = torch.tensor(prompt).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated,\n",
        "                                max_new_tokens = 350,\n",
        "                                eos_token_id=tokenizer.eos_token_id,\n",
        "                                pad_token_id=tokenizer.pad_token_id,\n",
        "                                do_sample=True,\n",
        "                                top_k=50,\n",
        "                                top_p=0.95,\n",
        "                                temperature=1,\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))\n",
        "\n",
        "expected_output = text[\"messages\"][-1][\"content\"]\n",
        "print(f\"Expected: {expected_output}\")"
      ],
      "metadata": {
        "id": "YDEfNRJ5cV0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "837c175b-3bcd-44df-ec62-53f45b0a8d8d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: <|im_start|>system\n",
            "Tu es un assistant qui aide à réécrire les introductions de rapport de présentation d'entreprises générées par l'IA pour les rendre plus humaines, chaleureuses et engageantes. Chaque texte doit refléter un ton personnel et authentique, comme s'il était écrit par une personne réelle travaillant dans l'entreprise. Utilise les exemples fournis d'introductions générées par l'IA et rédigées par des humains pour identifier et appliquer les nuances de style, de ton et de structure qui rendent un texte plus personnel et proche du lecteur.<|im_end|>\n",
            "<|im_start|>user\n",
            "Fondé en 2004 par Chafik GASMI, CHAFIK STUDIO est un studio de création polyvalent qui englobe divers domaines tels que l'architecture, l'architecture d'intérieur, le design, le retail design et la direction artistique. Le studio est reconnu pour ses réalisations novatrices, fonctionnelles et intemporelles, alliant simplicité et séduction. CHAFIK.STUDIO a collaboré avec des marques prestigieuses comme Lancôme, Baccarat, Fendi, Sephora, entre autres. L'objectif du studio est de placer l'art au centre de chaque expérience, grâce à une équipe pluridisciplinaire aux compétences variées et complémentaires. Le studio est à l'origine de nombreuses innovations uniques qui ont rencontré un grand succès. Parmi ces réalisations, on compte le premier concept de magasin Sephora sur les Champs-Élysées, le premier musée d'art moderne du monde arabe, le premier hôtel Baccarat, ainsi que la nouvelle expérience de vente au détail pour Lancôme. Le studio a également conçu des œuvres emblématiques comme la chaise Square à Times Square et le fauteuil Ying à Matignon.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Fondé en 2004 par Chafik GASMI, CHAFIK STUDIO est un studio de création polyvalent qui regroupe les disciplines de l’architecture, de l’architecture d’intérieur, du design, du retail design et de la direction artistique. Le studio est réputé pour ses réalisations novatrices, intemporelles et fonctionnelles, à la fois simples et séduisantes. CHAFIK.STUDIO a travaillé avec de grandes marques comme Lancôme, Baccarat, Fendi ou encore Sephora, entre autres. Notre but est d’immerger l’art au cœur de l’expérience client en recrutant une équipe pluridisciplinaire, forte de compétences variées et complémentaires. Grâce à cette réflexion transdisciplinaire, nous sommes la source d’innovations uniques qui deviennent des succès. Parmi ses réalisations, Chafik Studio a ainsi signé le premier concept de magasin Sephora sur les Champs-Élysées, le premier musée d’art moderne du monde arabe, le premier hôtel Baccarat, ainsi qu’une toute nouvelle expérience de vente au détail pour Lancôme. Nous sommes également à l’origine d’œuvres emblématiques telles que la chaise Square sur Times Square et le fauteuil Ying à Matignon. En effet, les créations de Chafik Studio représentent une approche moderne de l’architecture, en s’intéressant aux usages, à la création d’espaces et à l’apport d’\n",
            "\n",
            "\n",
            "Expected: CHAFIK STUDIO a été fondé en 2004 par Chafik GASMI en tant que studio de création polyvalent englobant les domaines de l'architecture, de l'architecture d'intérieur, du design, du retail design et de la direction artistique. Il est renommé pour ses réalisations novatrices, fonctionnelles, épurées, séduisantes et intemporelles. CHAFIK.STUDIO a collaboré avec des marques prestigieuses telles que Lancôme, Baccarat, Fendi, Sephora, et bien d'autres. L'objectif de Chafik Studio est de placer l'art au cœur de toutes les expériences grâce à une équipe pluridisciplinaire aux compétences complémentaires. Le studio est à l'origine de nombreuses innovations uniques et couronnées de succès, notamment le premier concept de magasin Sephora sur les Champs-Élysées, le premier musée d'art moderne du monde arabe, le premier hôtel Baccarat, la nouvelle expérience de vente au détail de Lancôme, la chaise Square à Times Square, le fauteuil Ying à Matignon, et bien plus encore.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def remove_assistant_part(entry):\n",
        "    # Filter out the assistant's response from the messages\n",
        "    messages = [msg for msg in entry[\"messages\"] if msg[\"role\"] != \"assistant\"]\n",
        "    return messages\n",
        "\n",
        "text = val_dataset[1]\n",
        "entry = remove_assistant_part(text)\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(entry, tokenize = True, add_generation_prompt=True)\n",
        "generated = torch.tensor(prompt).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated,\n",
        "                                max_new_tokens = 350,\n",
        "                                eos_token_id=tokenizer.eos_token_id,\n",
        "                                pad_token_id=tokenizer.pad_token_id,\n",
        "                                do_sample=True,\n",
        "                                top_k=50,\n",
        "                                top_p=0.95,\n",
        "                                temperature=1,\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=False)))\n",
        "\n",
        "expected_output = text[\"messages\"][-1][\"content\"]\n",
        "print(f\"Expected: {expected_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27VyzCWvI_Gi",
        "outputId": "e468df33-d60f-4ef8-9b51-f7af3f237cb4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: <|im_start|>system\n",
            "Tu es un assistant qui aide à réécrire les introductions de rapport de présentation d'entreprises générées par l'IA pour les rendre plus humaines, chaleureuses et engageantes. Chaque texte doit refléter un ton personnel et authentique, comme s'il était écrit par une personne réelle travaillant dans l'entreprise. Utilise les exemples fournis d'introductions générées par l'IA et rédigées par des humains pour identifier et appliquer les nuances de style, de ton et de structure qui rendent un texte plus personnel et proche du lecteur.<|im_end|>\n",
            "<|im_start|>user\n",
            "Née du rapprochement de Commercy Soudure et de Cimlec Industrie en 2013, Commercy Robotique est devenue le pôle d'expertise robotique du Groupe Gorgé, une ETI familiale française fondée en 1990, reconnue pour son innovation et son orientation à l'export dans le domaine des technologies de pointe. Commercy Robotique intègre des robots pour une large gamme de clients, des PME aux grandes industries, couvrant divers secteurs. Nos équipes conçoivent et réalisent des solutions robotisées sur mesure pour des processus industriels variés tels que le soudage, la manutention, la manipulation et le coupage. Fort de ses ingénieurs et experts spécialisés dans l'équipement robotique, le soudage et l'intégration robotique, Commercy Robotique développe des solutions innovantes adaptées aux besoins spécifiques de chaque client. Avec près de 40 ans d'expertise en robotique multi-process, Commercy Robotique a servi plus de 750 clients dans des secteurs d'activités diversifiés.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Née du rapprochement de Commercy Soudure et de Cimlec Industrie en 2013, Commercy Robotique est le pôle d’expertise robotique du Groupe Gorgé. C’est une ETI familiale française fondée en 1990, reconnue pour son innovation et son orientation à l’export dans le domaine des technologies de pointe. Commercy Robotique intègre des robots pour une large gamme de clients, des PME aux grandes industries, dans tous les secteurs. Nos équipes conçoivent et réalisent des solutions robotisées sur mesure pour des processus industriels variés : le soudage, la manutention, la manipulation et le coupage. Fort de nos ingénieurs et experts spécialisés dans l’équipement robotique, le soudage et l’intégration robotique, Commercy Robotique développe des solutions innovantes adaptées aux besoins spécifiques de chaque client. Avec près de 40 ans d’expertise en robotique multi-process, Commercy Robotique a servi plus de 750 clients dans des secteurs d’activités variés. Le savoir-faire de notre équipe ingénieur permet de démontrer aux industriels que l’intelligence artificielle n’est pas seulement une révolution dans l’univers du traitement d’images, mais également un outil précieux dans la mise en place d’usines capables de réécrire l’histoire des process. Ces nouvelles techniques technologiques nous ouvrent les portes d’un monde où la réalité virtuelle et les robots sont des parten\n",
            "\n",
            "\n",
            "Expected: Issue du rapprochement de Commercy Soudure et de Cimlec Industrie en 2013, Commercy Robotique devient le pôle d’expertise robotique du Groupe Gorgé, une ETI familiale française, innovante et exportatrice créée en 1990. Les activités du Groupe s’inscrivent dans les produits et services de haute-technologie. Commercy Robotique intègre des robots pour les industriels comme les PME, de tous secteurs. Nos équipes conçoivent et réalisent des solutions robotisées adaptées à chaque process industriel : soudage, manutention, manipulation, coupage... Commercy Robotique s’appuie sur ses ingénieurs et experts des métiers de l'équipement robot, du soudage et de l'intégration robotique pour étudier et réaliser des solutions innovantes adaptées aux besoins de ses clients. Avec près de 40 ans d'expertise en robotique multi-process, nous intervenons pour plus de 750 clients, tous secteurs d’activités confondus.\n"
          ]
        }
      ]
    }
  ]
}