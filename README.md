# Fine-tuning-GPT2

The goal is to finetune GPT-2 on a Shakespear text with a single L4 GPU (Google Colab).

Normal_version : Training time = 4 min 47 secs
Validation Loss: 3.02
Training Loss: 2.80

Opti_version : Training time = 2 min 25 secs 
Validation Loss: 3.25
Training Loss: 1.86
(!slight overfitting!)
