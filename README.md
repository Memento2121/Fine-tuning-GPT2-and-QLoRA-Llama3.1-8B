# Fine-tuning-GPT2 + QLoRA Llama 3.1 8B

The goal is to finetune GPT-2 (124M) and QLoRA finetune Llama 3.1 (8B) on a Shakespear text with a single L4 GPU (Google Colab).

Results : QLoRA finetune Llama 3.1 (8B) is way better