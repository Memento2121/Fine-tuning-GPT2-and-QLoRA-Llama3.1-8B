{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- L'état de l'art examine les aspects liés au risque opérationnel, particulièrement dans les établissements financiers.\n\n- Le risque opérationnel est défini par le Comité de Bâle comme le risque de pertes dues à des carences ou des défauts des procédures, du personnel, des systèmes internes ou à des événements extérieurs. Cette définition inclut le risque juridique, mais exclut les risques stratégiques et de réputation [1].\n\n- Le risque opérationnel couvre un large éventail de facteurs classés en sept catégories principales, comme illustré dans la Figure 1 1.\n\n- L'identification des facteurs de risque opérationnel doit être accompagnée de méthodes d'évaluation quantitative, mais la pondération des risques est complexe en raison de leur nature souvent diffuse et difficilement quantifiable [2].\n\n- La cartographie du risque est essentielle pour analyser les processus opérationnels, identifier les sources d'exposition, et créer des référentiels spécifiques à chaque entreprise, en tenant compte de la fréquence des incidents et de l'ampleur des pertes [2].\n\n- Le domaine du risque opérationnel évolue constamment, influencé par les pratiques numériques telles que le cloud computing, la cybercriminalité, et le développement des pratiques mobiles, qui introduisent de nouveaux risques nécessitant des ajustements des outils de mesure [3].\n\n- Une fois les référentiels de risque définis, l'évaluation quantitative commence, basée sur des données internes et externes. Le benchmark sectoriel est utilisé pour pallier les lacunes des données internes [4].\n\n- Les accords de Bâle proposent trois approches pour calculer les besoins en fonds propres en fonction des risques :\n  - Approche « indicateur de base » : utilisation d'une formule simple et uniforme.\n  - Approche « standard » : différenciation des métiers avec des coefficients spécifiques et une prise en compte plus approfondie de l'historique.\n  - Approche « mesures avancées » : pour les établissements capables de mesurer eux-mêmes leurs risques avec des calculs complexes pour minimiser l'immobilisation de fonds [5].\n\n- La mise en œuvre de ces méthodes de calcul peut nécessiter des techniques statistiques avancées, telles que la méthode des moments ou la simulation de Monte-Carlo, impliquant l'approche « fréquence/sévérité », des estimations bayésiennes et des analyses de scenarii, ce qui souligne la complexité des outils nécessaires [5] [6].\n\n- Les démarches proactives en gestion du risque opérationnel visent à optimiser les activités pour réduire l'exposition aux risques, en plus de les identifier et de les mesurer. La Figure 1 2 montre un exemple de dispositif de suivi des risques opérationnels [7].\n\n- Les Key Risk Indicators (KRI) sont des paramètres essentiels pour évaluer et suivre la performance en matière de risque opérationnel. Ils fournissent une vision pertinente de la situation, permettent de fixer des objectifs quantitatifs, et de suivre les progrès réalisés [8].\n\n- Le suivi des indicateurs sur le long terme se fait via des tableaux de bord et des outils d'exploration des données, adaptés aux différents départements avec des niveaux de détails spécifiques, permettant des analyses fines [8].\n\n- Des moyens d'action à court terme sont nécessaires pour détecter les crises, émettre des alertes et traiter les problèmes. Les données collectées, y compris en situation de crise, doivent enrichir continuellement la base de connaissances [8].\n\n- La gestion du risque opérationnel est de plus en plus complexe, notamment avec l'intégration de données volumineuses, la contrainte du temps réel et la nécessité de disponibilité des systèmes [2]."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nDans cet état de l’art, nous avons parcouru dans un premier temps les différents aspects liés au risque opérationnel, en particulier dans le contexte des établissements financiers. \n\nLa première étape de toute démarche de gestion du risque opérationnel consiste à identifier les facteurs de risque dans l’ensemble des activités de la structure étudiée. \n\nLe Comité de Bâle  sur le contrôle bancaire définit (section V.A.644) le risque opérationnel comme suit : « risque de pertes résultant de carences ou de défauts attribuables à des procédures, personnels et systèmes internes ou à des événements extérieurs. La définition inclut le risque juridique, mais exclut les risques stratégique et de réputation » [1]. Cette définition recouvre un nombre important de facteurs qui sont classés dans les sept catégories présentées dans la Figure 1 1.\n\n \nFigure 1 1 Les différents types de risque opérationnel identifiés par le Comité de Bâle  \n\nL’identification des différents facteurs de risque doit être accompagnée de la spécification des méthodes d’évaluation quantitative. La définition de la pondération à associer à chaque élément qui contribue à un facteur de risque est une tâche délicate. D’après Fimarkets [2], « le sujet présente un aspect diffus et non quantifié qui le rend difficile à appréhender ». La « cartographie du risque » est une étape primordiale dans toute politique de gestion du risque. Elle consiste à effectuer une analyse des processus opérationnels en relation avec les différentes catégories de risque afin d’identifier les sources d’exposition et leur importance relative (dépendant, par exemple, de la fréquence des incidents ou de l’importance des pertes subies à chaque occurrence) [2]. Elle aboutit à la constitution de référentiels propres à chaque entreprise.\n\nLe risque opérationnel est un domaine en perpétuelle évolution, non seulement pour des applications au domaine financier, mais aussi dans tous les secteurs d’activité. Les changements réguliers des pratiques et des systèmes mis en œuvre imposent de fréquentes révisions des outils de mesure et de supervision du risque. C’est ainsi qu’il est possible de tenir compte des nouvelles formes de risque et d’équilibrer les poids accordés à chaque facteur. Aujourd’hui, les pratiques numériques sont une source particulièrement importante de changement [3] : le cloud computing, la cybercriminalité ou encore le développement des pratiques mobiles créent de nouveaux risques et rendent nécessaire la création de nouveaux moyens d’action.\n\nAvez-vous réalisé des recherches sur les risques opérationnels que vous avez considérés dans vos travaux ? Pouvez-vous le cas échant, présenter les principales avancées par rapport à cet état de l’art ? \n\nDans un deuxième temps, nous avons analysé les procédures et les techniques utilisées dans le cadre de la supervision actuelle. \n\nUne fois que des référentiels de risque et des processus ont été définis, l’étape d’évaluation quantitative des risques peut débuter. Elle vise à mesurer précisément l’importance relative de chaque facteur identifié et à calculer les pertes qu’il peut engendrer. Pour cela, elle utilise les données collectées dans le cadre des activités de l’entreprise et en particulier celles qui sont relatives aux incidents, mais aussi des données externes afin de combler les manques des données collectées en interne. En effet, toutes les sociétés ne subissent pas les effets de tous les risques identifiés ; elles ne disposent donc pas de données permettant de modéliser l’importance de tous les facteurs et doivent utiliser les informations d’autres sociétés qui ont été confrontées aux problèmes qu’elles n’ont pas eus. On parle pour cette démarche de « benchmark sectoriel » [4]. Il est aussi nécessaire de définir des niveaux d’exposition acceptable pour chaque type de risque [2].\n\nLes accords de Bâle ont fourni, pour les activités financières, un cadre général de calcul des besoins en fonds propres (fonds qui permettent de couvrir les pertes envisagées en cas d’incident) en fonction des risques. Il est constitué de trois approches [5] : \n\n-\tUne approche « indicateur de base » : elle utilise une formule simple et uniforme pour toutes les sociétés. \n-\tUne « approche standard » : dans cette approche huit « lignes de métier » sont distinguées et des coefficients différents sont utilisés pour chacune d’elles. De plus, la profondeur de l’historique considéré est plus importante. \n-\tUne approche de « mesures avancées » : cette approche est réservée à des établissements qui vérifient un nombre important de critères. Ces critères montrent qu’ils sont en mesure de définir et de mesurer eux-mêmes le risque auquel ils sont exposés. Les calculs peuvent, pour ces structures, être beaucoup plus complexes et imposer des analyses beaucoup plus fines que pour les deux autres approches (afin de calculer les besoins en fonds propres le plus précisément possibles et minimiser l’immobilisation de fonds). \n\nLa mise en œuvre de ces méthodes de calcul peut imposer l’application de techniques statistiques avancées. Gamonet [6] recense les différentes approches usuelles, qu’elles relèvent de l’approche standard ou des mesures avancées. Dans ce cadre, des outils tels que la « méthode des moments » ou la simulation de Monte-Carlo interviennent dans l’implémentation d’approches variées (approche « fréquence/sévérité », estimations bayésiennes, analyses de scenarii… [5]) et mettent en avant la complexité des outils nécessaires. \n\nCes différents éléments indiquent que les niveaux de spécialisation et de connaissance nécessaires à la maîtrise de ces approches sont élevés. La mise en œuvre de ces techniques n’est donc pas accessible à toutes les structures car elle exige des moyens trop importants ou impose des délais trop longs pour nombre d’entre elles.\n\nEnfin, nous avons analysé les méthodes de supervision des processus pour la minimisation du risque opérationnel. \n\nLe risque opérationnel est abordé dans le cadre de démarches « proactives » qui n’ont pas seulement pour objet l’identification et la mesure des risques, mais aussi l’optimisation des activités pour diminuer l’exposition. La Figure 1 2 présente une vue globale de ce type de démarches. \n\n \nFigure 1 2. Exemple de dispositif de suivi des risques opérationnels - Cas de la Société Générale [7]\nAfin de mettre en œuvre une stratégie de pilotage des activités améliorant les performances sur le plan du risque opérationnel, les sociétés doivent mettre en place des indicateurs de risque clé (Key Risk Indicator - KRI) [8]. Il s’agit de paramètres calculés à partir des données créées par les activités opérationnelles. En agrégeant ces données, les KRI fournissent une vision pertinente de la situation, permettent de fixer des objectifs quantitatifs et de suivre les évolutions engendrées par les efforts mis en œuvre.\n\nL’amélioration de la performance passe par le suivi des évolutions à long terme des procédures internes. Pour cela, les principaux indicateurs sont restitués en direction des responsables de la gestion des risques opérationnels au travers de tableaux de bord et d’outils offrant des fonctionnalités d’exploration des données [8]. Pour offrir une vision complète, les systèmes mis en place doivent offrir des vues spécifiques à chaque département tout en présentant des vues transversales et agrégées. Différents niveaux de détails doivent permettre des analyses fines, en particulier grâce à la production de statistiques à la demande.\n\nMais il est aussi important de mettre en place des moyens d’action à court terme afin de détecter les situations de crise, d’assurer l’émission d’alertes et de fournir des outils de traitement des problèmes rencontrés [8]. Les données collectées durant le fonctionnement normal des activités ou dans les situations de crise doivent être collectées afin d’enrichir la base de connaissances. \n\nPlusieurs éléments contribuent à accentuer la complexité de ces tâches [2]. Ainsi, il devient aujourd’hui nécessaire d’intégrer des données volumineuses au sein des procédures ; la contrainte de temps réel est de plus en plus importante et est accompagnée d’impératifs de disponibilité des composantes applicatives. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Magento, solution open source, propose des produits configurables permettant de gérer les variations de produits avec des attributs comme la couleur et la taille, mais avec une flexibilité limitée et des performances faibles [1].\n- Hybris, solution propriétaire, utilise une approche multi-niveau pour la modélisation des variantes, mais présente des problèmes de redondance de données et nécessite des outils spécifiques pour la gestion [2].\n- Des recherches suggèrent l'utilisation de la décomposition et de la hiérarchisation pour améliorer la gestion des produits, avec des modèles basés sur des axes fonctionnels, techniques et structurels [3][4].\n- D'autres études explorent la corrélation entre les variations de design au sein des familles de produits et leurs performances pour déterminer une plateforme commune [5].\n- La modélisation des produits nécessite une adaptation pour répondre aux besoins spécifiques, en s'inspirant des travaux existants.\n- Les bases de données relationnelles sont rigides, nécessitant des modifications lourdes pour des changements de structure, ce qui est déconseillé en production [6][7][8].\n- Le modèle EAV (Entity-Attribute-Value) offre une flexibilité accrue pour la gestion des produits, mais avec des limites telles que l'augmentation du nombre de tables et la dégradation des performances [9][10][11].\n- Les bases relationnelles modernes, comme MySQL 5.7 et PostgreSQL, permettent désormais la gestion de données sans schéma via des champs JSON, mais présentent des défis en termes de performance d'indexation [12].\n- Les bases de données NoSQL, comme MongoDB, offrent une grande flexibilité et scalabilité, mais sont encore récentes et présentent des limites en termes de stabilité et d'adoption [13][14][15][16].\n- ElasticSearch, moteur de recherche orienté document, est performant pour la recherche en temps réel et la gestion de grands volumes de données, mais nécessite un mapping précis des index [17][18][19].\n- Le développement d'une solution globale unique regroupant les avantages de MySQL, MongoDB, et ElasticSearch est nécessaire pour dépasser les limites des solutions actuelles.\n- La nouvelle solution repose sur le principe de séparation commande-requête pour améliorer les performances sans altérer le comportement du système [20][21][22].\n- Les entités référentielles utilisent des attributs stockés en JSON pour garantir des performances élevées, bien que des défis subsistent concernant la modification de la structure [23][24][25].\n- Deux approches de validation (à l'écriture et à la lecture) ont été envisagées pour garantir la cohérence des données, chacune ayant ses avantages et inconvénients.\n- La validation à l'écriture garantit la cohérence mais est coûteuse en termes de temps et de ressources, tandis que la validation à la lecture est rapide mais impacte les performances de lecture.\n- L'essor du cloud computing et des technologies de conteneurisation comme Docker et Kubernetes permet d'abstraire l'infrastructure, mais ces solutions sont encore expérimentales à grande échelle [26][27].\n- Les solutions d'allocation dynamique des ressources, bien que largement étudiées, ne répondent pas directement aux besoins spécifiques de l'entreprise pour offrir un service optimal à ses clients [28].\n- La transformation numérique, via des pratiques comme le DevOps, est essentielle pour assurer des solutions pérennes et réduire la pénibilité du travail des équipes sur le long terme."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nDans cet état de l’art, nous avons parcouru dans un premier temps les différents aspects liés au risque opérationnel, en particulier dans le contexte des établissements financiers. \n\nLa première étape de toute démarche de gestion du risque opérationnel consiste à identifier les facteurs de risque dans l’ensemble des activités de la structure étudiée. \n\nLe Comité de Bâle  sur le contrôle bancaire définit (section V.A.644) le risque opérationnel comme suit : « risque de pertes résultant de carences ou de défauts attribuables à des procédures, personnels et systèmes internes ou à des événements extérieurs. La définition inclut le risque juridique, mais exclut les risques stratégique et de réputation » [1]. Cette définition recouvre un nombre important de facteurs qui sont classés dans les sept catégories présentées dans la Figure 1 1.\n\n \nFigure 1 1 Les différents types de risque opérationnel identifiés par le Comité de Bâle  \n\nL’identification des différents facteurs de risque doit être accompagnée de la spécification des méthodes d’évaluation quantitative. La définition de la pondération à associer à chaque élément qui contribue à un facteur de risque est une tâche délicate. D’après Fimarkets [2], « le sujet présente un aspect diffus et non quantifié qui le rend difficile à appréhender ». La « cartographie du risque » est une étape primordiale dans toute politique de gestion du risque. Elle consiste à effectuer une analyse des processus opérationnels en relation avec les différentes catégories de risque afin d’identifier les sources d’exposition et leur importance relative (dépendant, par exemple, de la fréquence des incidents ou de l’importance des pertes subies à chaque occurrence) [2]. Elle aboutit à la constitution de référentiels propres à chaque entreprise.\n\nLe risque opérationnel est un domaine en perpétuelle évolution, non seulement pour des applications au domaine financier, mais aussi dans tous les secteurs d’activité. Les changements réguliers des pratiques et des systèmes mis en œuvre imposent de fréquentes révisions des outils de mesure et de supervision du risque. C’est ainsi qu’il est possible de tenir compte des nouvelles formes de risque et d’équilibrer les poids accordés à chaque facteur. Aujourd’hui, les pratiques numériques sont une source particulièrement importante de changement [3] : le cloud computing, la cybercriminalité ou encore le développement des pratiques mobiles créent de nouveaux risques et rendent nécessaire la création de nouveaux moyens d’action.\n\nAvez-vous réalisé des recherches sur les risques opérationnels que vous avez considérés dans vos travaux ? Pouvez-vous le cas échant, présenter les principales avancées par rapport à cet état de l’art ? \n\nDans un deuxième temps, nous avons analysé les procédures et les techniques utilisées dans le cadre de la supervision actuelle. \n\nUne fois que des référentiels de risque et des processus ont été définis, l’étape d’évaluation quantitative des risques peut débuter. Elle vise à mesurer précisément l’importance relative de chaque facteur identifié et à calculer les pertes qu’il peut engendrer. Pour cela, elle utilise les données collectées dans le cadre des activités de l’entreprise et en particulier celles qui sont relatives aux incidents, mais aussi des données externes afin de combler les manques des données collectées en interne. En effet, toutes les sociétés ne subissent pas les effets de tous les risques identifiés ; elles ne disposent donc pas de données permettant de modéliser l’importance de tous les facteurs et doivent utiliser les informations d’autres sociétés qui ont été confrontées aux problèmes qu’elles n’ont pas eus. On parle pour cette démarche de « benchmark sectoriel » [4]. Il est aussi nécessaire de définir des niveaux d’exposition acceptable pour chaque type de risque [2].\n\nLes accords de Bâle ont fourni, pour les activités financières, un cadre général de calcul des besoins en fonds propres (fonds qui permettent de couvrir les pertes envisagées en cas d’incident) en fonction des risques. Il est constitué de trois approches [5] : \n\n-\tUne approche « indicateur de base » : elle utilise une formule simple et uniforme pour toutes les sociétés. \n-\tUne « approche standard » : dans cette approche huit « lignes de métier » sont distinguées et des coefficients différents sont utilisés pour chacune d’elles. De plus, la profondeur de l’historique considéré est plus importante. \n-\tUne approche de « mesures avancées » : cette approche est réservée à des établissements qui vérifient un nombre important de critères. Ces critères montrent qu’ils sont en mesure de définir et de mesurer eux-mêmes le risque auquel ils sont exposés. Les calculs peuvent, pour ces structures, être beaucoup plus complexes et imposer des analyses beaucoup plus fines que pour les deux autres approches (afin de calculer les besoins en fonds propres le plus précisément possibles et minimiser l’immobilisation de fonds). \n\nLa mise en œuvre de ces méthodes de calcul peut imposer l’application de techniques statistiques avancées. Gamonet [6] recense les différentes approches usuelles, qu’elles relèvent de l’approche standard ou des mesures avancées. Dans ce cadre, des outils tels que la « méthode des moments » ou la simulation de Monte-Carlo interviennent dans l’implémentation d’approches variées (approche « fréquence/sévérité », estimations bayésiennes, analyses de scenarii… [5]) et mettent en avant la complexité des outils nécessaires. \n\nCes différents éléments indiquent que les niveaux de spécialisation et de connaissance nécessaires à la maîtrise de ces approches sont élevés. La mise en œuvre de ces techniques n’est donc pas accessible à toutes les structures car elle exige des moyens trop importants ou impose des délais trop longs pour nombre d’entre elles.\n\nEnfin, nous avons analysé les méthodes de supervision des processus pour la minimisation du risque opérationnel. \n\nLe risque opérationnel est abordé dans le cadre de démarches « proactives » qui n’ont pas seulement pour objet l’identification et la mesure des risques, mais aussi l’optimisation des activités pour diminuer l’exposition. La Figure 1 2 présente une vue globale de ce type de démarches. \n\n \nFigure 1 2. Exemple de dispositif de suivi des risques opérationnels - Cas de la Société Générale [7]\nAfin de mettre en œuvre une stratégie de pilotage des activités améliorant les performances sur le plan du risque opérationnel, les sociétés doivent mettre en place des indicateurs de risque clé (Key Risk Indicator - KRI) [8]. Il s’agit de paramètres calculés à partir des données créées par les activités opérationnelles. En agrégeant ces données, les KRI fournissent une vision pertinente de la situation, permettent de fixer des objectifs quantitatifs et de suivre les évolutions engendrées par les efforts mis en œuvre.\n\nL’amélioration de la performance passe par le suivi des évolutions à long terme des procédures internes. Pour cela, les principaux indicateurs sont restitués en direction des responsables de la gestion des risques opérationnels au travers de tableaux de bord et d’outils offrant des fonctionnalités d’exploration des données [8]. Pour offrir une vision complète, les systèmes mis en place doivent offrir des vues spécifiques à chaque département tout en présentant des vues transversales et agrégées. Différents niveaux de détails doivent permettre des analyses fines, en particulier grâce à la production de statistiques à la demande.\n\nMais il est aussi important de mettre en place des moyens d’action à court terme afin de détecter les situations de crise, d’assurer l’émission d’alertes et de fournir des outils de traitement des problèmes rencontrés [8]. Les données collectées durant le fonctionnement normal des activités ou dans les situations de crise doivent être collectées afin d’enrichir la base de connaissances. \n\nPlusieurs éléments contribuent à accentuer la complexité de ces tâches [2]. Ainsi, il devient aujourd’hui nécessaire d’intégrer des données volumineuses au sein des procédures ; la contrainte de temps réel est de plus en plus importante et est accompagnée d’impératifs de disponibilité des composantes applicatives. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Les techniques de recherche et de filtrage booléennes actuelles ne sont pas suffisantes pour l'appariement complexe candidat-emploi [1].\n- Les systèmes de recommandation sont adoptés pour comprendre les exigences des postes et résoudre le problème du débordement d'informations.\n\n- Les Job Recommender Systems (JRS) sont classés en quatre catégories : filtrage collaboratif, filtrage basé sur le contenu, approches basées sur les connaissances, et systèmes hybrides [2].\n- Zheng et Al-Otaibi ont résumé les avantages et inconvénients des JRS existants, avec des exemples spécifiques comme Prospect pour le filtrage basé sur le contenu (CBR), CASPER pour le filtrage collaboratif (CFR), et Proactive pour la recommandation basée sur les connaissances (KBR) [3][4][5][6][7].\n- Les systèmes hybrides combinent plusieurs techniques pour améliorer les performances, souvent en combinant le filtrage collaboratif avec d'autres approches pour surmonter les limitations individuelles.\n\n- CASPER utilise le filtrage collaboratif automatisé (ACF) en deux étapes : trouver des utilisateurs similaires, puis recommander les emplois qu'ils ont appréciés [6].\n- CASPER présente des limites, notamment la rareté du filtrage collaboratif due au faible chevauchement des utilisateurs sur les mêmes offres et la qualité variable des résultats de recherche. Le manque de description détaillée sur la détection des domaines connexes dans l'article est également noté.\n- Farber et al. ont développé un système hybride combinant filtrage basé sur le contenu et collaboratif pour surmonter la rareté des données de notation, illustré par un modèle d'aspect latent [8].\n\n- Malinowski et al. distinguent deux types de systèmes de recommandation d'emploi : CV-recommender pour les recruteurs et job recommender pour les demandeurs d'emploi [1].\n- Le système collecte des données de profil via des formulaires Web détaillés (données démographiques, éducation, expérience professionnelle, compétences linguistiques et informatiques, prix et publications). Les CV sont utilisés pour faciliter le jugement humain.\n\n- Les grandes entreprises comme IBM et HP ont développé des systèmes pour filtrer les CV, tels que PROSPECT, qui utilise un modèle CRF pour segmenter et étiqueter les CV [5][9].\n- Ces systèmes aident les entreprises à sélectionner les candidats, mais ne sont pas conçus pour aider les demandeurs d'emploi à trouver des postes adaptés. Ils utilisent des ontologies pour améliorer l'extraction d'informations et découvrir de nouveaux termes via un module d'apprentissage adaptatif.\n\n- Lu et al. ont utilisé l'analyse sémantique latente (LSA) pour calculer les similitudes entre emplois et candidats, mais en se basant uniquement sur les facteurs « intérêt » et « éducation » [12].\n- Xing et al. ont utilisé des modèles de pertinence structurés (SRM) pour apparier CV et emplois [13].\n- Drigas et al. ont proposé un système expert utilisant des règles Neuro-Fuzzy pour évaluer les correspondances entre profils d'utilisateurs et offres d'emploi, nécessitant un entraînement préalable du réseau [14].\n- Daramola et al. ont proposé un outil basé sur la logique floue pour le recrutement en ligne, utilisant une métrique de distance floue pour classer les profils des candidats en fonction de leur éligibilité [15].\n- Yao et al. ont développé un système hybride intégrant des relations basées sur le contenu et sur l'interaction, utilisant l'analyse sémantique latente pour traiter les données non structurées [12].\n\n- Les systèmes de recommandation d'emploi actuels ont des limitations :\n  - Ils traitent principalement les données structurées, alors que les CV et descriptions de poste sont souvent non structurés.\n  - Les modules d'extraction d'informations sont conçus pour les recruteurs, pas pour les demandeurs d'emploi.\n  - La granularité des champs d'information est souvent insuffisante, nécessitant une amélioration pour des recommandations de meilleure qualité."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nD’après la littérature, les techniques de recherche et de filtrage booléennes actuelles ne peuvent satisfaire la complexité de l'exigence d'appariement candidat-emploi [1]. Les chercheurs espèrent que le système pourra comprendre les exigences du poste, déterminer quelles exigences sont obligatoires et lesquelles sont facultatives, mais préférables. Ils ont donc adopté la technique des systèmes de recommandation pour résoudre le problème du débordement d'informations. Les systèmes de recommandation sont largement acceptés dans divers domaines pour suggérer des produits, des services et des informations aux clients latents.\n\n1.2.1\tSystèmes de recommandation\nLa recherche d'emploi, qui a fait l'objet de certains sites Web commerciaux de recherche d'emploi et d'articles de recherche, n'est pas un nouveau sujet de recherche d'informations. Habituellement, les chercheurs les appelaient « Job Recommender Systems » (JRS), car la plupart d'entre eux utilisaient des technologies issues de systèmes de recommandation. Wei et al. ont classé les systèmes de recommandation en quatre catégories [2]: filtrage collaboratif, filtrage base sur le contenu, approches basées sur les connaissances et hybrides. \n\nCertaines de ces techniques avaient été appliquées au JRS, Zheng et al. [3] et Al-Otaibi et al. [4] ont résumé les catégories de plates-formes de recrutement en ligne existantes et énuméré les avantages et les inconvénients des approches techniques dans différents JRS. Les catégories comprennent :\n-\tRecommandation basée sur le contenu (CBR). Le principe d'une recommandation basée sur le contenu est de suggérer des éléments qui ont des informations de contenu similaires aux utilisateurs correspondants, comme Prospect [5].\n-\tRecommandation sur le filtrage collaboratif (CFR). La recommandation de filtrage collaboratif trouve des utilisateurs similaires qui ont le même goût que l'utilisateur cible et recommande des éléments en fonction de ce que les utilisateurs similaires apprécient, comme CASPER [6].\n-\tRecommandation basée sur les connaissances (KBR). Dans la recommandation basée sur les connaissances, des règles et des modèles obtenus à partir de la connaissance fonctionnelle de la façon dont un élément spécifique répond aux exigences d'un utilisateur particulier, sont utilisés pour recommander des éléments, comme Proactive [7].\n-\tLes systèmes de recommandation hybrides combinent deux techniques de recommandation ou plus pour obtenir de meilleures performances et surmonter les inconvénients de chaque technique individuelle. Habituellement, le filtrage collaboratif est combiné à une autre technique pour tenter d'éviter le problème de montée en puissance.\n\n1.2.2\tSystèmes de recommandation d'emploi\nRafter et al. ont commencé à utiliser le filtrage collaboratif automatise (ACF) dans leur système de recommandation d'emploi, « CASPER » [6]. Dans le système, les profils utilisateur sont obtenus à partir des journaux du serveur, qui incluent : les données de révision, les données de temps de lecture et les données d'activité. Tous ces facteurs sont considérés comme une mesure de la pertinence parmi les utilisateurs. Le système recommande des travaux en deux étapes : tout d'abord, le système trouve un ensemble d'utilisateurs liés à l'utilisateur cible ; deuxièmement, les emplois qui ont plu aux utilisateurs concernés seront recommandés à l'utilisateur cible. Le système utilise une stratégie de filtrage collaborative basée sur des clusters. La similitude entre les utilisateurs est basée sur le nombre d'emplois qu'ils ont tous deux examinés ou appliqués.\n\nCASPER permet également aux utilisateurs de rechercher des emplois par une requête qui est une combinaison de certains champs : comme l'emplacement, le salaire, la compétence, etc. Le système utilise une telle requête pour trouver des travaux, et les travaux renvoyés sont classés avec l'algorithme de filtrage collaboratif. Dans leur article, les auteurs ne donnent pas de description détaillée sur la façon de détecter les domaines connexes dont ils ont besoin et comment transférer la description de poste semi-structurée vers les données structurées.\n\nLes pénuries de filtrage collaboratif : Premièrement, étant donné que le nombre de résultats de recherche est énorme et que les résultats sont triés de manière aléatoire, la probabilité que deux utilisateurs similaires examinent les mêmes emplois est faible, ce qui pose le problème de la rareté du filtrage collaboratif. Les auteurs ont également remarqué le problème de parcimonie causé par peu de profils d'utilisateurs, ils essaient donc d'utiliser une solution basée sur des clusters pour résoudre ce problème. Deuxièmement, comme les offres d'emploi recommandées proviennent des résultats de recherche d'autres utilisateurs, la qualité des résultats de la recherche actuelle étant faible, la qualité de la recommandation ne peut pas être élevée.\n\nFarber et al. [8] ont présenté un système de recommandation base sur une approche hybride. Le système intègre deux méthodes, le filtrage base sur le contenu et le filtrage collaboratif, qui tente de surmonter le problème de la rareté des données de notation en exploitant un modèle combiné. Dans le système, la source de données est des CV synthétiques. Le modèle d'aspect latent est illustre dans la Figure 1 ci-dessous.\n\n \n\nFigure 1 : Modèle d’aspect latent\n\n\nDans Malinowski et al. [1], ils classent les systemes de recommandation d'emploi en deux catégories, CV-recommender, qui recommande les CV au recruteur, et le job recommender, qui recommande des emplois aux demandeurs d'emploi. Le système collecte les données de profil des utilisateurs en leur demandant de saisir leurs profils dans l'interface basée sur le formulaire Web champ par champ. Les données d'entrée collectées sont :\n-\tDonnées démographiques (par exemple : date de naissance, coordonnées) ;\n-\tDonnées sur l'éducation (par exemple : cours scolaires, notes, université, type de diplôme, examens universitaires intermédiaires et finaux, études de troisième cycle) ;\n-\tExpérience professionnelle (par exemple :  nom de l'entreprise, type d'emploi, groupe industriel, domaine professionnel) ;\n-\tCompétences linguistiques (par exemple : langue, niveau de connaissance) ;\n-\tCompétences informatiques (par exemple, type de compétences, niveau de connaissances) ;\n-\tPrix, bourses, publications et autres.\n\nLe système a également demandé aux utilisateurs de télécharger leurs CV, mais les CV étaient uniquement pour faciliter le jugement humain. Dans l’étude de Malinowski, le modèle d’aspect latent est un modèle statistique qui doit être formé avant d’être appliqué à la recommandation. Le système utilise les résultats de recherche des utilisateurs comme données de formation pour entrainer le modèle a des fins de recommandation, de sorte que le système a besoin d'une formation de longue durée pour chaque utilisateur.\n\n1.2.3\tExtraction d'informations dans le systeme de recommandation de travaux\nLes grandes entreprises informatiques ont rencontré le même problème de débordement d'informations lorsqu'elles recevaient plusieurs CV pour une offre d'emploi. Le recruteur devait filtrer toutes les candidatures manuellement, mais cette tâche était également fastidieuse et longue. Pour cette raison, ces entreprises ont essayé de créer des systèmes pour aider à filtrer les CV.\n\nAmit et al. a IBM ont présenté un système, « PROSPECT » [5], pour aider à présélectionner les candidats pour les emplois. Le système utilise un chercheur de CV pour extraire les informations des CV, qui utilisent un modèle de champ aléatoire conditionnel (CRF) pour segmenter et étiqueter les CV. Le modèle CRF utilisait trois types de fonctionnalités, à savoir : Lexique, Visuel et Entité nommée. L'article a comparé certains algorithmes pour classer les candidats, ces méthodes incluent : Okapi BM25, KL, Lucene Scoring et Lucene Scoring + SkillBoost.\n\nHP a également construit un système pour résoudre le problème similaire, qui est présenté dans l'article de Gonzalez et al. [9]. Le système accorde également beaucoup d'attention à l'extraction d'informations.\n\nLes dictionnaires utilisés pour baliser les entités doivent être mis à jour régulièrement car de nouveaux termes apparaissent toujours. Un module d'apprentissage adaptatif permet donc d'atteindre deux objectifs : utiliser des données sémantiques pour améliorer l'extraction d'informations et découvrir de nouveaux termes.\nUne ontologie orientée domaine est utilisée pour représenter les connaissances, les règles d'inférence sont définies en fonction de la base de connaissances de l'ontologie. Lorsqu'un terme détecté est trouvé, le système effectuera une recherche dans une base de connaissances externe, telle que DBpedia. Les curriculum vitae sont également classés dans différentes catégories comme « Technologie Web » et « Aucune technologie Web » par le classificateur naïf Bayes. L'entreprise peut affecter les employés appropries aux postes requis avec le système.\n\nL'objectif principal des systèmes construits par IBM et HP est d'aider les entreprises a sélectionner de bons candidats, mais ne peuvent pas aider les demandeurs d'emploi à trouver des emplois appropriés.\n\nYu et al. [10] ont utilisé un cadre IE en cascade pour obtenir les informations détaillées des curriculum vitae. Dans un premier temps, le modèle de modélisation de Markov cache (HMM) est utilisé pour segmenter le CV en blocs consécutifs. Sur la base du résultat, un modèle SVM est utilisé pour obtenir les informations détaillées dans le certain bloc, les informations comprennent : nom, adresse, éducation, etc.\n\nCelik Duygua et Elci Atilla ont proposé un Rsum Parser (ORP) [11] base sur une ontologie, qui utilise l'ontologie pour assister le processus d'extraction d'informations. Le système traite un résumé dans les étapes suivantes : conversion du fichier de résumé en texte brut, séparation du texte en quelques segments, utilisation de la base de connaissances de l'ontologie pour trouver les concepts dans les phrases, normalisation de tous les termes et classification des phrases pour obtenir les termes.\n\nToutefois, les informations personnelles que le système a récupérées comme le nom et les adresses ne sont pas les informations qui intéressent les recruteurs. Les recruteurs veulent des informations relatives à l'offre d'emploi et peuvent les aider à juger de la compétences des candidats.\n\n1.2.4\tAlgorithmes de correspondance dans les systèmes de recommandation d'emploi\nLu et al. [12] ont utilisé l'analyse sémantique latente (LSA) pour calculer les similitudes entre les emplois et les candidats, mais ils n'ont sélectionné que deux facteurs « intérêt » et « éducation » pour comparer les candidats. \n\nXing et al. [13] ont utilisé des modèles de pertinence structures (SRM) pour faire correspondre les curriculum vitae et les emplois.\n\nDrigas et al. [14] ont presenté un système expert pour faire correspondre les emplois et les demandeurs d'emploi, et pour recommander les chômeurs aux postes. Le système expert a utilisé des règles Neuro-Fuzzy pour évaluer la correspondance entre les profils d'utilisateurs et les offres d'emploi. Le système utilise une matrice de relations pour représenter la relation floue entre ces spécialités. Le système a besoin des données d'entrainement pour entrainer ce réseau Neuro-Fuzzy. Les données de curriculum vitae et les données sur les offres d'emploi ont été entrées manuellement dans le système.\n\nDaramola et al. [15] ont également proposé un outil de système expert base sur la logique floue (FES) pour le recrutement de personnel en ligne. Dans l'article, les auteurs ont supposé que les informations étaient déjà collectées. Le système utilise une métrique de distance floue pour classer les profils des candidats dans l'ordre de leur éligibilité à l'emploi.\n\nYao et al. [12] ont également présenté un système de recommandation hybride qui intégrait une relation basée sur le contenu et basée sur l'interaction. Dans la partie basée sur le contenu, les relations entre emploi-emploi, emploi-demandeur d'emploi et demandeur d'emploi-demandeur d'emploi peuvent être identifiées par leur similitude de profils. Deux approches sont utilisées pour calculer les similitudes. Pour les données structurées, comme l’âge et le sexe, les valeurs de somme de poids seront renvoyées ; pour les données non structurées, comme la similitude entre l'emploi et le profil utilisateur, l'analyse sémantique latente est appliquée dans le système.\n\nEn résumé, il existe certains problèmes dans les systèmes de recommandation de travaux précédents :\n-\tLa plupart des systèmes ne peuvent traiter que les données structurées des curriculum vitae et des descriptions de poste, mais en réalité les deux sont dans des formats non structurés, tels que des fichiers texte ou des pages Web HTML.\n-\tLes systèmes dotés de modules d'extraction d'informations sont conçus pour que les recruteurs sélectionnent les candidats, et non pour que les demandeurs d'emploi sélectionnent des emplois.\n-\tDans les systèmes, les champs d'information correspondant aux curriculum vitae et aux descriptions de poste sont grossiers. Pour améliorer la qualité de la recommandation, nous devons améliorer la granularité des champs d'information."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- **Topic identification** : Identification de sujets (Topic identification) consiste à regrouper des documents similaires en clusters et à attribuer des étiquettes à ces clusters. Ces étiquettes peuvent ensuite être utilisées pour des tâches d'analyse supplémentaires [1].\n- Avec l’essor des médias sociaux, l’identification de sujets est couramment utilisée pour l'extraction de mots-clés sensibles dans l’opinion publique [2].\n- L’application de la sémantique de documents et des règles d'association de texte pour le regroupement et l'identification des sujets est explorée dans [3], tandis que le clustering pour générer des résumés de texte est discuté dans [4].\n- Le clustering appliqué à un flux de documents pour incorporer des propriétés temporelles et regrouper des articles de presse est analysé dans [5][6].\n- L’utilisation de modèles sémantiques, comme l’indexation sémantique latente, est une approche populaire pour l’identification des sujets, surmontant les problèmes de synonymes [7].\n- Les algorithmes de clustering, tels que K-means et ses variantes, sont largement utilisés pour le regroupement de documents, malgré des limites liées à l'initialisation et au nombre de clusters [8].\n- Les algorithmes de classification hiérarchique, bien que sans ces limites, dépendent des critères d'arrêt pour la qualité des clusters [9].\n- Le calcul de la similarité entre documents est crucial pour le clustering ; des mesures autres que la similarité cosinus et euclidienne sont utilisées pour prendre en compte les styles d'écriture variés [10].\n\n- **Algorithmes utilisés en NLP** : Pour traiter les mots de plusieurs documents, le clustering K-means est utilisé, où K centroïdes sont assignés aléatoirement, puis ajustés jusqu'à stabilisation pour identifier des thèmes [11].\n- DBscan, un autre algorithme, crée des clusters basés sur des zones de forte densité, mais il nécessite une distance limite définie entre mots, ce qui peut poser problème en termes de fiabilité des résultats.\n- Les algorithmes de clustering non supervisés, comme LDA (Latent Dirichlet Allocation), assignent des thèmes aux mots en fonction de distributions probabilistes, mais sont complexes à évaluer et requièrent un tuning délicat [12][13][14][15].\n- Doc2Vec, un modèle de réseau de neurones, prédit la probabilité d’occurrence d’un mot en tenant compte du contexte et de la phrase entière, apprenant ainsi des représentations de phrases [16].\n- BERT (Bidirectional Encoder Representations from Transformers), un modèle NLP avancé, représente les mots selon leur contexte précédent et suivant, et est utilisé pour des tâches comme l’analyse des sentiments et la détection de sujets [17].\n- Ces algorithmes ont été explorés pour identifier les technologies et sujets évoqués dans la littérature.\n\n- **Méthodes d’apprentissage automatisé** : L'apprentissage automatique, composante de l'intelligence artificielle, est utilisé pour exécuter des tâches complexes comme l’identification d’objets ou la détection de maladies [18].\n- L'apprentissage peut être supervisé (données étiquetées), non supervisé (structure sous-jacente recherchée), ou par renforcement (récompenses pour chaque action) [19][20][21].\n- Le Machine Learning repose sur deux piliers : les données d'entraînement et les algorithmes d'apprentissage.\n\n- **Algorithmes supervisés** : Parmi les méthodes supervisées, on trouve le boosting, les SVM (Support Vector Machines), le k-NN (k plus proches voisins), et les arbres de décision.\n\n- **Algorithmes non supervisés** : Pour les données non étiquetées, des méthodes comme K-means et les réseaux neuronaux, dont LSTM et Prophet pour la prédiction de séries temporelles, sont efficaces.\n- LSTM se base sur les valeurs récentes pour réduire la variance due à la saisonnalité, tandis que Prophet modélise les tendances saisonnières comme un système additif.\n\n- **Méthodes de clustering** : HDBSCAN, un algorithme de clustering, étend DBSCAN en créant une classification hiérarchique, utile pour des clusters de formes et densités variées, ou contenant du bruit [22].\n- Le Node2Vec model, utilisé pour l’embedding de graphes, permet de représenter des graphes sous une forme calculable, avec diverses stratégies comme la marche de graphe [23].\n\n- **Évaluations et métriques** : Pour comparer des modèles, plusieurs métriques sont utilisées, telles que la MAE (Mean Absolute Error), qui mesure la moyenne des erreurs absolues entre observations, et le coefficient de silhouette, qui évalue la qualité des clusters [24][25][26][27].\n- La MAE est plus simple et plus interprétable que la RMSE, tandis que le coefficient de silhouette varie entre -1 et 1, un score élevé indiquant une bonne assignation des points aux clusters.\n- D’autres métriques incluent l’indice de Jaccard pour la similarité entre ensembles et le f1-score pour évaluer la performance des algorithmes de machine learning.\n\n- **Évolution et diffusion des technologies** : Le cycle de vie des technologies, selon Gartner, comprend cinq phases allant de l'Innovation Trigger au Plateau of Productivity [28][29].\n- Le modèle de diffusion de Bass décrit l’adoption des nouvelles technologies par la population, avec des coefficients représentant l'influence interne et externe [30][31].\n- Le Shifted Gompertz Model, dérivé du modèle de Bass, introduit une distribution aléatoire pour l'adoption de nouveaux produits [32].\n\n- **Prédiction du succès des start-ups** : Les PME sont cruciales pour l'innovation et la croissance économique, mais 90 % des start-ups échouent dans leur première année [33][34][35].\n- Des modèles prédictifs basés sur l’apprentissage automatique, tels que XGBoost et Random Forest, sont utilisés pour anticiper le succès des start-ups, en se concentrant sur des variables non financières comme le management et les diplômes [36][37].\n- Cependant, ces modèles sont limités par le manque de données temporelles et la difficulté d'obtenir des données non financières détaillées."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nD’après la littérature, les techniques de recherche et de filtrage booléennes actuelles ne peuvent satisfaire la complexité de l'exigence d'appariement candidat-emploi [1]. Les chercheurs espèrent que le système pourra comprendre les exigences du poste, déterminer quelles exigences sont obligatoires et lesquelles sont facultatives, mais préférables. Ils ont donc adopté la technique des systèmes de recommandation pour résoudre le problème du débordement d'informations. Les systèmes de recommandation sont largement acceptés dans divers domaines pour suggérer des produits, des services et des informations aux clients latents.\n\n1.2.1\tSystèmes de recommandation\nLa recherche d'emploi, qui a fait l'objet de certains sites Web commerciaux de recherche d'emploi et d'articles de recherche, n'est pas un nouveau sujet de recherche d'informations. Habituellement, les chercheurs les appelaient « Job Recommender Systems » (JRS), car la plupart d'entre eux utilisaient des technologies issues de systèmes de recommandation. Wei et al. ont classé les systèmes de recommandation en quatre catégories [2]: filtrage collaboratif, filtrage base sur le contenu, approches basées sur les connaissances et hybrides. \n\nCertaines de ces techniques avaient été appliquées au JRS, Zheng et al. [3] et Al-Otaibi et al. [4] ont résumé les catégories de plates-formes de recrutement en ligne existantes et énuméré les avantages et les inconvénients des approches techniques dans différents JRS. Les catégories comprennent :\n-\tRecommandation basée sur le contenu (CBR). Le principe d'une recommandation basée sur le contenu est de suggérer des éléments qui ont des informations de contenu similaires aux utilisateurs correspondants, comme Prospect [5].\n-\tRecommandation sur le filtrage collaboratif (CFR). La recommandation de filtrage collaboratif trouve des utilisateurs similaires qui ont le même goût que l'utilisateur cible et recommande des éléments en fonction de ce que les utilisateurs similaires apprécient, comme CASPER [6].\n-\tRecommandation basée sur les connaissances (KBR). Dans la recommandation basée sur les connaissances, des règles et des modèles obtenus à partir de la connaissance fonctionnelle de la façon dont un élément spécifique répond aux exigences d'un utilisateur particulier, sont utilisés pour recommander des éléments, comme Proactive [7].\n-\tLes systèmes de recommandation hybrides combinent deux techniques de recommandation ou plus pour obtenir de meilleures performances et surmonter les inconvénients de chaque technique individuelle. Habituellement, le filtrage collaboratif est combiné à une autre technique pour tenter d'éviter le problème de montée en puissance.\n\n1.2.2\tSystèmes de recommandation d'emploi\nRafter et al. ont commencé à utiliser le filtrage collaboratif automatise (ACF) dans leur système de recommandation d'emploi, « CASPER » [6]. Dans le système, les profils utilisateur sont obtenus à partir des journaux du serveur, qui incluent : les données de révision, les données de temps de lecture et les données d'activité. Tous ces facteurs sont considérés comme une mesure de la pertinence parmi les utilisateurs. Le système recommande des travaux en deux étapes : tout d'abord, le système trouve un ensemble d'utilisateurs liés à l'utilisateur cible ; deuxièmement, les emplois qui ont plu aux utilisateurs concernés seront recommandés à l'utilisateur cible. Le système utilise une stratégie de filtrage collaborative basée sur des clusters. La similitude entre les utilisateurs est basée sur le nombre d'emplois qu'ils ont tous deux examinés ou appliqués.\n\nCASPER permet également aux utilisateurs de rechercher des emplois par une requête qui est une combinaison de certains champs : comme l'emplacement, le salaire, la compétence, etc. Le système utilise une telle requête pour trouver des travaux, et les travaux renvoyés sont classés avec l'algorithme de filtrage collaboratif. Dans leur article, les auteurs ne donnent pas de description détaillée sur la façon de détecter les domaines connexes dont ils ont besoin et comment transférer la description de poste semi-structurée vers les données structurées.\n\nLes pénuries de filtrage collaboratif : Premièrement, étant donné que le nombre de résultats de recherche est énorme et que les résultats sont triés de manière aléatoire, la probabilité que deux utilisateurs similaires examinent les mêmes emplois est faible, ce qui pose le problème de la rareté du filtrage collaboratif. Les auteurs ont également remarqué le problème de parcimonie causé par peu de profils d'utilisateurs, ils essaient donc d'utiliser une solution basée sur des clusters pour résoudre ce problème. Deuxièmement, comme les offres d'emploi recommandées proviennent des résultats de recherche d'autres utilisateurs, la qualité des résultats de la recherche actuelle étant faible, la qualité de la recommandation ne peut pas être élevée.\n\nFarber et al. [8] ont présenté un système de recommandation base sur une approche hybride. Le système intègre deux méthodes, le filtrage base sur le contenu et le filtrage collaboratif, qui tente de surmonter le problème de la rareté des données de notation en exploitant un modèle combiné. Dans le système, la source de données est des CV synthétiques. Le modèle d'aspect latent est illustre dans la Figure 1 ci-dessous.\n\n \n\nFigure 1 : Modèle d’aspect latent\n\n\nDans Malinowski et al. [1], ils classent les systemes de recommandation d'emploi en deux catégories, CV-recommender, qui recommande les CV au recruteur, et le job recommender, qui recommande des emplois aux demandeurs d'emploi. Le système collecte les données de profil des utilisateurs en leur demandant de saisir leurs profils dans l'interface basée sur le formulaire Web champ par champ. Les données d'entrée collectées sont :\n-\tDonnées démographiques (par exemple : date de naissance, coordonnées) ;\n-\tDonnées sur l'éducation (par exemple : cours scolaires, notes, université, type de diplôme, examens universitaires intermédiaires et finaux, études de troisième cycle) ;\n-\tExpérience professionnelle (par exemple :  nom de l'entreprise, type d'emploi, groupe industriel, domaine professionnel) ;\n-\tCompétences linguistiques (par exemple : langue, niveau de connaissance) ;\n-\tCompétences informatiques (par exemple, type de compétences, niveau de connaissances) ;\n-\tPrix, bourses, publications et autres.\n\nLe système a également demandé aux utilisateurs de télécharger leurs CV, mais les CV étaient uniquement pour faciliter le jugement humain. Dans l’étude de Malinowski, le modèle d’aspect latent est un modèle statistique qui doit être formé avant d’être appliqué à la recommandation. Le système utilise les résultats de recherche des utilisateurs comme données de formation pour entrainer le modèle a des fins de recommandation, de sorte que le système a besoin d'une formation de longue durée pour chaque utilisateur.\n\n1.2.3\tExtraction d'informations dans le systeme de recommandation de travaux\nLes grandes entreprises informatiques ont rencontré le même problème de débordement d'informations lorsqu'elles recevaient plusieurs CV pour une offre d'emploi. Le recruteur devait filtrer toutes les candidatures manuellement, mais cette tâche était également fastidieuse et longue. Pour cette raison, ces entreprises ont essayé de créer des systèmes pour aider à filtrer les CV.\n\nAmit et al. a IBM ont présenté un système, « PROSPECT » [5], pour aider à présélectionner les candidats pour les emplois. Le système utilise un chercheur de CV pour extraire les informations des CV, qui utilisent un modèle de champ aléatoire conditionnel (CRF) pour segmenter et étiqueter les CV. Le modèle CRF utilisait trois types de fonctionnalités, à savoir : Lexique, Visuel et Entité nommée. L'article a comparé certains algorithmes pour classer les candidats, ces méthodes incluent : Okapi BM25, KL, Lucene Scoring et Lucene Scoring + SkillBoost.\n\nHP a également construit un système pour résoudre le problème similaire, qui est présenté dans l'article de Gonzalez et al. [9]. Le système accorde également beaucoup d'attention à l'extraction d'informations.\n\nLes dictionnaires utilisés pour baliser les entités doivent être mis à jour régulièrement car de nouveaux termes apparaissent toujours. Un module d'apprentissage adaptatif permet donc d'atteindre deux objectifs : utiliser des données sémantiques pour améliorer l'extraction d'informations et découvrir de nouveaux termes.\nUne ontologie orientée domaine est utilisée pour représenter les connaissances, les règles d'inférence sont définies en fonction de la base de connaissances de l'ontologie. Lorsqu'un terme détecté est trouvé, le système effectuera une recherche dans une base de connaissances externe, telle que DBpedia. Les curriculum vitae sont également classés dans différentes catégories comme « Technologie Web » et « Aucune technologie Web » par le classificateur naïf Bayes. L'entreprise peut affecter les employés appropries aux postes requis avec le système.\n\nL'objectif principal des systèmes construits par IBM et HP est d'aider les entreprises a sélectionner de bons candidats, mais ne peuvent pas aider les demandeurs d'emploi à trouver des emplois appropriés.\n\nYu et al. [10] ont utilisé un cadre IE en cascade pour obtenir les informations détaillées des curriculum vitae. Dans un premier temps, le modèle de modélisation de Markov cache (HMM) est utilisé pour segmenter le CV en blocs consécutifs. Sur la base du résultat, un modèle SVM est utilisé pour obtenir les informations détaillées dans le certain bloc, les informations comprennent : nom, adresse, éducation, etc.\n\nCelik Duygua et Elci Atilla ont proposé un Rsum Parser (ORP) [11] base sur une ontologie, qui utilise l'ontologie pour assister le processus d'extraction d'informations. Le système traite un résumé dans les étapes suivantes : conversion du fichier de résumé en texte brut, séparation du texte en quelques segments, utilisation de la base de connaissances de l'ontologie pour trouver les concepts dans les phrases, normalisation de tous les termes et classification des phrases pour obtenir les termes.\n\nToutefois, les informations personnelles que le système a récupérées comme le nom et les adresses ne sont pas les informations qui intéressent les recruteurs. Les recruteurs veulent des informations relatives à l'offre d'emploi et peuvent les aider à juger de la compétences des candidats.\n\n1.2.4\tAlgorithmes de correspondance dans les systèmes de recommandation d'emploi\nLu et al. [12] ont utilisé l'analyse sémantique latente (LSA) pour calculer les similitudes entre les emplois et les candidats, mais ils n'ont sélectionné que deux facteurs « intérêt » et « éducation » pour comparer les candidats. \n\nXing et al. [13] ont utilisé des modèles de pertinence structures (SRM) pour faire correspondre les curriculum vitae et les emplois.\n\nDrigas et al. [14] ont presenté un système expert pour faire correspondre les emplois et les demandeurs d'emploi, et pour recommander les chômeurs aux postes. Le système expert a utilisé des règles Neuro-Fuzzy pour évaluer la correspondance entre les profils d'utilisateurs et les offres d'emploi. Le système utilise une matrice de relations pour représenter la relation floue entre ces spécialités. Le système a besoin des données d'entrainement pour entrainer ce réseau Neuro-Fuzzy. Les données de curriculum vitae et les données sur les offres d'emploi ont été entrées manuellement dans le système.\n\nDaramola et al. [15] ont également proposé un outil de système expert base sur la logique floue (FES) pour le recrutement de personnel en ligne. Dans l'article, les auteurs ont supposé que les informations étaient déjà collectées. Le système utilise une métrique de distance floue pour classer les profils des candidats dans l'ordre de leur éligibilité à l'emploi.\n\nYao et al. [12] ont également présenté un système de recommandation hybride qui intégrait une relation basée sur le contenu et basée sur l'interaction. Dans la partie basée sur le contenu, les relations entre emploi-emploi, emploi-demandeur d'emploi et demandeur d'emploi-demandeur d'emploi peuvent être identifiées par leur similitude de profils. Deux approches sont utilisées pour calculer les similitudes. Pour les données structurées, comme l’âge et le sexe, les valeurs de somme de poids seront renvoyées ; pour les données non structurées, comme la similitude entre l'emploi et le profil utilisateur, l'analyse sémantique latente est appliquée dans le système.\n\nEn résumé, il existe certains problèmes dans les systèmes de recommandation de travaux précédents :\n-\tLa plupart des systèmes ne peuvent traiter que les données structurées des curriculum vitae et des descriptions de poste, mais en réalité les deux sont dans des formats non structurés, tels que des fichiers texte ou des pages Web HTML.\n-\tLes systèmes dotés de modules d'extraction d'informations sont conçus pour que les recruteurs sélectionnent les candidats, et non pour que les demandeurs d'emploi sélectionnent des emplois.\n-\tDans les systèmes, les champs d'information correspondant aux curriculum vitae et aux descriptions de poste sont grossiers. Pour améliorer la qualité de la recommandation, nous devons améliorer la granularité des champs d'information."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- **Gamme Andritz** : Andritz propose une large gamme d'équipements de déshydratation des boues, incluant centrifugeuses, filtres à bande, et filtres presses à plateaux. D'autres technologies comme les lits de séchage et filtres sous vide sont marginales ou obsolètes.\n- **Presse à vis** : Historiquement marginale, la presse à vis a regagné de l'intérêt pour sa faible consommation d'énergie. Andritz, fort de son expérience dans le secteur du papier, a développé la presse à vis ContiPress pour le traitement des boues urbaines et industrielles.\n- **Limites des presses à vis existantes** : Les presses à vis sur le marché présentent des limites, telles que l'arrêt de l'alimentation de boue lorsque la pression dépasse un certain seuil, entraînant une perte de productivité. De plus, l'arrêt de l'alimentation pendant le lavage compromet l'efficacité de l'appareil.\n- **Innovations Andritz** : Pour pallier ces limitations, Andritz a conçu un système d'automatisation qui ajuste la vitesse de la vis en fonction de la pression d'entrée, assurant un débit constant de boue. De plus, un nouveau mode de lavage permet de nettoyer la presse sans interrompre la production.\n- **Conception spécifique pour boues organiques** : La presse à vis développée par Andritz a été adaptée spécifiquement pour traiter les boues organiques, avec des modifications visant à améliorer la force axiale de poussée et la pression intérieure supportées par la boue.\n- **Déshydratation des boues de papeterie** : En 2017, Andritz a étudié la déshydratation des boues de papeterie à faible taux de fibres, un domaine où peu de concurrents sont présents. La presse à vis est avantageuse par rapport aux filtres à bande et centrifugeuses en raison de sa faible consommation en eau et en énergie.\n- **Campagne d'essais 2017** : Les essais ont montré qu'un meilleur drainage dans la première zone de la presse augmenterait la capacité et réduirait l'eau transportée dans la zone de pression. Des modifications ont été envisagées, telles que la création d'un double filet et de déflecteurs.\n- **Double filet vs double moteur** : Contrairement à la technologie concurrente de double moteur, le double filet proposé par Andritz éviterait les problèmes d'étanchéité et d'ajustement mécanique, tout en restant dans un volume fermé avec des paniers immobiles.\n- **Objectifs de performance** : Le développement visait une augmentation de 10 % du débit et de la siccité sans perte de taux de capture. La validation des hypothèses a nécessité des tests expérimentaux pour comparer les résultats avec les prévisions."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nDans sa gamme, Andritz propose une très large gamme d’équipements utilisés en déshydratation des boues, tels que les centrifugeuses, les filtres à bande et les filtres presses à plateaux.\nIl existe d’autres outils sur le marché mais leur utilisation est marginale ou obsolète : lits de séchage, filtres sous vide, presses à piston avec drains flexibles… \n\nLes presses à vis ont quant à elles été l’un des premiers appareils développés pour traiter les boues, tels que l’ancien Taster. Il y a dix ans, l’utilisation de la presse à vis était très marginale, mais la volonté de réduire les consommations d’électricité dans les stations d’épuration a été le déclic qui a provoqué un regain d’intérêt pour cet outil de filtration. La presse à vis est en effet la moins énergivore parmi les appareils de déshydratation des boues.\n\nAndritz a une compétence reconnue dans la conception et la construction de la presse à vis de par ses activités dans les usines de production de pâtes à papier. Cet outil est très utilisé pour déshydrater les pulpes de papier et est particulièrement adapté pour traiter des produits très fibreux.\n\nPour le marché de l’environnement, Andritz a développé une gamme de presses à vis ContiPress (1990-2000) pour le traitement de boues urbaines et industrielles. Ces appareils étaient déjà une évolution des machines utilisées pour le Pulp & Paper. Cependant, ils ne sont pas adaptés au marché des boues municipales, boues très organiques, en raison de leur coût (taille de machine très conséquente).\nNous avons donc décidé de développer une gamme spécifique pour le traitement de boues organiques.\n\nLa presse à vis est un outil mécanique très simple en comparaison de la centrifugeuse (appareil le plus répandu), ce qui a pour avantage une maintenance aisée pouvant être réalisée par un personnel non spécialisé, à l’inverse du second appareil. \n\nAvant le process de déshydratation, la boue est tout d’abord mélangée à des polymères dans une cuve de floculation alimentée par les pompes (cf. Figure 2.1).\n\n \nFigure 2.1 : Alimentation de la presse à vis\nLa boue floculée est ensuite injectée dans la presse à vis, au sein d’un tambour cylindrique (grille perforée) dans lequel se trouve une vis avec âme conique (cf. Figure 2.2).\n\nLa boue est déshydratée de manière continue : convoyée lentement à l’intérieur du tambour, la boue subit une pression de déshydratation croissante du fait de la forme conique de l’âme de la vis et du volume de plus en plus réduit du tambour. La boue est ainsi comprimée contre la face intérieure de la grille perforée et le filtrat passe à travers les trous de la grille. La phase solide (cake) est évacuée à l’extrémité du tambour.\n\n        \nFigure 2.2 : Presse à vis\nAu démarrage de ce programme de recherche et développement, le point de différenciation que nous souhaitions atteindre par rapport aux technologies connues (exemples : Huber, Ishigaki/Emo, Acat, Fkc, Adequatec, etc.) était de n’avoir aucun moment de rupture de production que ce soit en mode de fonctionnement automatique ou pendant le lavage. En effet, après expertise des presses à vis existantes, nous avons constaté les situations suivantes :\n-\tLe débit d’alimentation de boue est réduit, voire arrêté, dès que la pression d’entrée augmente au-delà d’un certain seuil. Or, la capacité de la presse à vis est censée être garantie durant la production. L’obligation de performance sur la capacité du matériel n’est donc pas respectée vis-à-vis du client ;\n-\tL’alimentation de boue est arrêtée durant l’étape de lavage, stoppant donc la production. Or, les presses à vis nécessitent une quinzaine de minutes pour retrouver un régime stabilisé et durant ce laps de temps, elles ne sont pas efficientes. Il y a donc une perte de productivité non négligeable.\n\nIl est à noter également que la fréquence de lavage dépend du degré de colmatage et que le pouvoir colmatant d’une boue ne se définit qu’à la mise en route de l’appareil. Ainsi, la capacité donnée par nos concurrents est faussée.\n\nAfin de pallier ces limites, nous avons conçu un système d’automatisation qui ajuste la vitesse de la vis convoyeuse en fonction de la pression d’entrée, ce qui permet de conserver un débit d’alimentation de boue constant.\nD’autre part, nous avons développé de nouveaux modes de fonctionnement du lavage du tambour, de manière à ce que celui-ci puisse être nettoyé sans recourir à un arrêt de la machine et donc de la production.\n\nNotre approche du problème se distingue donc très nettement de celle de nos concurrents et s’est accompagnée d’aléas techniques dont les solutions nous appartiennent.\n\nPour concevoir la presse à vis, nous nous sommes appuyés sur nos acquis internes dans le Pulp & Paper et sur différentes publications et brevets, dont certaines références sont listées ci-après. Nous avons également analysé les solutions proposées par nos concurrents, telles que celles présentées en Annexe 2.1 – Presses à vis concurrentes.\n\nAfin d’améliorer le dimensionnement de notre outil presse à vis, nous avons procédé à de nombreuses modifications dans la conception en étudiant en particulier la force axiale de poussée de la machine et la pression intérieure supportées par la boue. \n\nPar ailleurs, en 2017, nous nous sommes intéressés à la déshydratation des boues de papeterie à faible taux de fibre. \nAujourd'hui, les presses à vis utilisées en papeterie permettent d'obtenir de forte siccité mais elles ne fonctionnent qu'avec un fort taux de fibre. Lorsque ce dernier diminue, le rendement de la machine est très faible. Peu de concurrents se sont ainsi positionnés sur ce marché, d’autant plus que la variation de la qualité des boues de papeterie nécessite une conception robuste de l’outil presse à vis. \nPour traiter ces boues, les filtres à bande sont généralement préférés. Cependant, cette technologie nécessite une surveillance accrue de l'installation pour ne pas endommager les toiles, et consomme de l'eau de lavage en continu, qui est réintroduite dans le process de dépollution. Les centrifugeuses sont également utilisées dans cette application mais sont consommatrices de puissance électrique.\nLa presse à vis possède l'avantage d'avoir une consommation en eau faible (1 lavage par heure) pour une puissance installée (3 kW) faible. \nAfin de pouvoir proposer une solution alternative et efficiente sur le marché, nous avons donc décidé de lancer une campagne d’essais pour évaluer les performances de notre outil presse à vis sur les boues de papeterie, et apporter les améliorations techniques nécessaires à notre prototype.\n\nLes résultats des campagnes d’essais menées en 2017 ont permis de mettre en évidence le fait qu’un meilleur drainage dans la première zone, partie égouttage gravitaire, permettrait d’accroitre la capacité et de diminuer la quantité d’eau transportée dans la zone de pression de la machine. Pour ce faire, nous avons envisagé en 2018 et en 2019 différentes hypothèses, notamment en ce qui concerne la conception de la vis :\n-\tCréation d’un double filet.\n-\tCréation de déflecteurs.\n\nSuite à notre analyse des différentes solutions existantes, nous avons pu observer qu’aucun de nos concurrents n’utilisait de double filet. En effet, les systèmes généralement utilisés sont basés sur un double moteur permettant d’accélérer la vitesse de rotation en entrée afin d’augmenter le drainage. Cependant, notre hypothèse permettrait de surpasser cette technologie à différents niveaux. Outre son prix particulièrement onéreux et les difficultés inerrantes à sa mise en œuvre, le double moteur présente également des problématiques d’étanchéité. La conception en double moteur est basée sur une vis en rotation dont la première partie du panier est accélérée mécaniquement par la mise en rotation de celui-ci tandis que la seconde partie reste fixe. Cela pose donc différents problèmes d’ajustement d’un point de vue mécanique. \nEn opposition, la solution de double filet que nous souhaitons proposer n’est pas sujette à ce problème car nous restons dans un volume fermé avec des paniers immobiles.\n\nNous avons notamment étudié la conception de la vis convoyeuse utilisée dans l’application huile d’olive pour la partie malaxage. \nNotre challenge était d’obtenir pour notre outil presse à vis une augmentation du débit de 10 % et une augmentation de la siccité sans diminution du taux de capture.\nLa validation de l’une ou plusieurs de ces hypothèses a nécessité la caractérisation et la comparaison des résultats obtenus expérimentalement vis-à-vis de nos prévisions. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- **Vibrations et réducteurs** : Les systèmes fermés sans dissipation ni sollicitation extérieure subissent des oscillations propres, tandis qu'un système soumis à une excitation extérieure subit des oscillations forcées pouvant entraîner la résonance, potentiellement destructrice [1].\n- Pour les nouvelles décanteuses Q-série, il était crucial d'identifier les fréquences propres du réducteur pour éviter les résonances. Étant donné la complexité du système rotor + réducteur, des simulations numériques ont été utilisées, complétées par des études expérimentales.\n- Un autre axe de recherche portait sur les réducteurs, s'inspirant du pont différentiel automobile, pour concevoir un réducteur différentiel léger et court capable de supporter des vitesses supérieures à 3500 g. Cette conception permettrait aussi d'utiliser des moteurs plus petits, réduisant ainsi la consommation d'énergie.\n\n- **Problématique de moussage** : Depuis 2015, des problèmes d'émulsion ont été observés dans les centrifugeuses dédiées à l'huile de palme, ce qui a conduit à réexaminer la conception de la vis convoyeuse et de la chambre de distribution. En 2016, ces développements ont inspiré des solutions pour réduire la mousse dans les applications sanitaires, bien que la formation de mousse diffère de celle des émulsions.\n- La mousse est un milieu complexe où un liquide ou solide est mêlé à du gaz, et sa stabilité est affectée par le drainage, le mûrissement, et la coalescence [4]. Les méthodes existantes pour évaluer la mousse, comme celles de Ross-Miles et Bikerman, ne sont pas adaptées à l'étude de la mousse en conditions dynamiques, ce qui a nécessité le développement d'un protocole spécifique pour minimiser la formation de mousse.\n- Des modifications ont été apportées à la conception des sections d'entrée des pipettes pour minimiser l'introduction d'air et réduire la mousse lors du prélèvement de fluides.\n\n- **Lean Duplex Bowl** : Les anciens bols de centrifugeuse étaient fabriqués en acier inoxydable 316 L, couramment utilisé pour sa résistance à la corrosion et sa haute ductilité, mais la volonté de réduire l'épaisseur des bols a conduit à explorer l'acier Lean Duplex, présentant des propriétés mécaniques supérieures [7].\n- L'acier Lean Duplex, avec une structure austéno-ferritique, offre une haute limite d'élasticité tout en conservant une bonne ductilité, et est utilisé dans des secteurs exigeants comme l'industrie pétrolière. Le défi était d'introduire ce matériau dans les applications alimentaires et chimiques.\n- La coulée par centrifugation, essentielle pour la réalisation de corps creux cylindriques, offre des qualités physiques et mécaniques optimisées grâce à la force centrifuge, permettant d'obtenir des structures plus compactes et homogènes que par coulée gravitaire [10][11][12]."}, {"role": "assistant", "content": "1.1\tÉtat de l’art\n1.1.1\tVibrations - Réducteur\nDans le cadre du développement des nouvelles décanteuses Q-série, nous nous sommes intéressés aux vibrations des éléments rotatifs.\n\nDe façon générale, un système fermé, (sans dissipation et sans sollicitation extérieure), ne peut être le siège que d’oscillations libres, appelées également oscillations propres. Ces vibrations sont dues à un transfert entre énergie potentielle et énergie cinétique [1].\nUn système soumis à une excitation extérieure est quant à lui le siège d’oscillations forcées, ou entretenues. En effet, lorsque le système est faiblement amorti, et que la fréquence d’excitation est égale ou voisine d’une des fréquences propres du système, l’amplitude de la réponse vibratoire tend vers l’infini. On dit que le système est en résonance, cet état peut conduire à une dégradation rapide des propriétés du système pouvant aller jusqu’à la ruine [1].\n\nNotre défi était alors d’identifier les fréquences propres du réducteur et éviter de le solliciter à une fréquence proche. Le système mécanique qu’est l’ensemble rotor + réducteur étant complexe, la modélisation afin de procéder à un calcul analytique est impossible. Nous nous sommes ainsi appuyés sur des simulations numériques, notamment sur la résolution des équations d’équilibre en continu et sur la méthode par éléments finis. Ces développements analytiques nécessitent cependant d’être complétés par une étude expérimentale, car les vibrations mécaniques en milieu continu sont difficilement modélisables et les équations d’équilibre mécanique ne suffisent pas à quantifier les phénomènes qui se produisent.\n\n\nUn autre axe important dans nos travaux portait sur les réducteurs en eux-mêmes, utilisés pour la transmission du mouvement de rotation entre le moteur et le bol de la décanteuse.\nUn réducteur est un organe mécanique équipé d’engrenages permettant de faire varier une vitesse ou un couple entre deux éléments qui se trouvent en entrée et en sortie de ce réducteur. Le réducteur permet ainsi d’adapter le couple et la vitesse de rotation d’un arbre d’entrée par rapport au couple et à la vitesse de rotation d’un arbre de sortie.\n\nDans le cadre de nos travaux, nous nous sommes inspirés d’un organe mécanique, le pont différentiel, présent sur les voitures qui assure la possibilité de deux roues montées sur un même arbre (roues avant ou roues arrière) de tourner à des vitesses différentes. Le cas se pose en particulier dans les virages où les deux roues parcourent nécessairement des distances différentes et doivent donc tourner à des vitesses différentes. Notre conception consistera à concevoir un réducteur différentiel permettant d’avoir en entrée deux vitesses et une vitesse en sortie.\nNotre défi est alors de développer un réducteur léger, court et pouvant atteindre des vitesses hautes : plus de 3500 g (au lieu de 3000 g).\nA ce jour, des centrifugeuses peuvent atteindre ces vitesses. Cependant, ce sont des machines avec de grandes queues à 3500 g, or l’encombrement disponible dans notre centrifugeuse rend cette solution inenvisageable. De plus, la distance du palier pourrait être raccourcie mais le bâti ne serait alors plus standard.\n\nPar ailleurs, le fait d’avoir deux arbres d’entrée et de travailler de façon différentielle entre les deux arbres, permet de mettre des moteurs plus petits en termes de puissance. Ainsi, les pertes d’énergies sont moindres et la centrifugeuse est moins énergivore.\n1.1.2\tMoussage\nNous rencontrons depuis 2015 des problématiques d’émulsion qui impactent la séparation de l’eau et de l’huile sur les centrifugeuses dédiées à l’huile de palme. Nos travaux de recherche nous avaient conduit à réétudier totalement la conception de la vis convoyeuse et de la chambre de distribution.\n\nEn 2016, nous avons souhaité nous inspirer de ces développements pour réduire la mousse dans les applications sanitaires. Cependant, la formation de mousse a des origines bien différentes de la formation d’émulsion.\nEn effet, l’émulsion est définie comme une dispersion de gouttelettes d’un liquide dans une phase continue d’un autre liquide, avec lequel il est pratiquement non miscible. La mousse est quant à elle présentée comme un milieu complexe constitué d’un matériau liquide ou solide intimement mêlé à du gaz.\nNous avons ainsi dû explorer ce sujet et développer des solutions nouvelles pour adresser ce problème qui impacte les performances de nos machines.\n\nDans la publication [4], Rio s’intéresse à la question de la stabilité des mousses liquides. L’auteur avance notamment que celles-ci ont tendance à se déstabiliser de trois façons :\n-\tEn drainant : une séparation de phase a lieu par gravité, le liquide ayant tendance à s’écouler entre les bulles et à s’accumuler en bas de la mousse ;\n-\tEn mûrissant : les petites bulles se vidant dans les plus grandes ;\n-\tEn coalesçant : l’amincissement des films lié au drainage amenant à la rupture de ceux-ci et à la fusion des bulles.\nBien que cette publication présente les phénomènes physico-chimiques régissant la stabilité des mousses, l’étude ne s’intéresse qu’à leur stabilité statique, tandis que nous nous intéressons à la formation de cette mousse au sein des pipettes de prélèvements, qui contiennent des fluides à très grande vitesse.\n\nDans la publication [5], Hearn présente différentes grandeurs pertinentes dans l’étude de la stabilité des mousses. Pour le développement de notre protocole d’étude de la stabilité des mousses contenues dans le lait de soja, nous avons inclus des mesures de volume de mousse et de volume de liquide en fonction du temps pour un échantillon prélevé.\nEnfin, Salager et Choplin [6] présentent trois méthodes d’étude des mousses : la méthode Ross-Miles, la méthode Bikerman ainsi qu’une méthode mixte.\nLa méthode de Ross-Miles ne répond pas à notre problématique puisqu’il s’agit d’une méthode statique, et que celle-ci renseigne davantage sur la moussabilité que sur la stabilité des mousses. En effet, la hauteur de mousse dans la colonne a tendance à persister, ce qui dégrade la précision des résultats.\nLa méthode Bikerman, quant à elle, s’intéresse à l’intégralité du processus de formation puis de dégradation de la mousse. Cette expérience nécessite un débit de gaz constant comme paramètre d’entrée à l’expérience, or dans notre cas, la formation de mousse n’est pas désirée, la mesure du débit exact d’air entrant dans le liquide est alors impossible à déterminer, d’autant plus qu’il est variable.\nEnfin, la méthode mixte s’appuie elle aussi sur un débit de gaz en entrée d’un volume de liquide dans une colonne et la mesure de la persistance (ou stabilité) s’effectue grâce à la dégradation de la mousse en haut du tube alors que celle qui se trouve à l’aplomb du liquide reste stable.\nCes méthodes d’étude de mousse n’étant pas adaptées à notre application, nous avons ainsi développé un protocole spécifique pour évaluer la formation de mousse.\nSuite à cette étude, nous avons modifié substantiellement les sections d’entrée de la pipette pour introduire un minimum d’air et éviter la création de mousse sur les produits moussants.\n1.1.3\tLean Duplex Bowl\nL’ancienne conception de bol de la centrifugeuse était réalisée en acier inoxydable austénitique 316 L. Cette nuance d’acier est très employée pour les équipements de l’industrie alimentaire (réservoirs, tubes, pompes), l’industrie navale, l’industrie du papier ainsi que l’industrie pétrolière et gazière.\nCe matériau est principalement choisi pour ses caractéristiques, qui rendent toutes les applications citées possibles :\n-\tTrès bonne résistance à la corrosion en milieu acide et corrosif ;\n-\tTrès bonne résistance à la corrosion intergranulaire, même après soudage ;\n-\tExcellente soudabilité ;\n-\tHaute ductilité…\nD’autres propriétés mécaniques ainsi que la composition chimique précise sont détaillées en Annexe 1 – Caractéristiques de l’acier inox 316 L.\n\nCe matériau convient à nos conceptions et nous permet de réaliser des bols de centrifugeuses répondant aux exigences du cahier des charges. Toutefois, notre souhait de développer un bol optimisé, notamment du point de vue de l’épaisseur, nous a amenés à considérer un autre matériau, présentant des propriétés mécaniques supérieures, afin de pouvoir affiner les parois du bol.\n\nLa publication [7] propose une classification des aciers inoxydables (cf. Figure 1.1).\nNous avons confronté les propriétés mécaniques, notamment la résistance, des aciers austénitiques (dont le 316 L fait partie) et des aciers duplex.\n \nFigure 1.1: Classification des aciers inoxydables [7]\nNous nous sommes ainsi penchés sur l’inox de classe Lean Duplex. L’appellation duplex tient au fait que ces aciers ont une structure austéno-ferritique : la structure biphasée de l’alliage permet d’obtenir des limites d’élasticités élevées tout en conservant une bonne ductilité (cf. Annexe 2 – Caractéristiques des aciers Lean Duplex).\nLes Lean Duplex inox étaient principalement utilisés dans des secteurs où la résistance à la corrosion de l’acier est essentielle, comme dans l’industrie pétrolière. Ils sont néanmoins peu à peu arrivés dans le bâtiment et les travaux publics. Notre défi est d’introduire ce matériau dans notre secteur, notamment en nous assurant de sa compatibilité avec les applications exigeantes (alimentaire, chimique…).\n\n\nLa coulée par centrifugation est un procédé incontournable dans la réalisation des corps creux cylindriques sans noyaux. Elle consiste à couler le métal liquide dans un moule animé d’un mouvement de rotation autour de son axe principal. L’alliage fondu est soumis à une force centrifuge tendant à l’éloigner de l’axe de rotation et à le plaquer contre les parois du moule [10].\n\nLes pièces obtenues par centrifugation possèdent de très bonnes qualités physiques et mécaniques. La force centrifuge exercée pendant toute la solidification de l’alliage permet de limiter les retassures et d’obtenir un alliage à structure plus fine, plus compacte et plus homogène, que dans la coulée par gravité. Un autre avantage cité par Hajjaji [11] réside dans le remplissage homogène des zones peu épaisses et des formes extérieures en relief, en particulier les brides et les collerettes sur le tube à couler.\nLa centrifugation se fait dans une coquille à axe vertical, à une vitesse de l’ordre de 600 tr/min. Le métal liquide est ainsi projeté, grâce à la force centrifuge, contre la paroi interne de la coquille où il se solidifie. L‘épaisseur de paroi est fonction du dosage et est de ce fait très variable. Mathieu et Macé [12] présentent d’ailleurs un schéma explicatif des différentes étapes (cf. Figure 1.2).\n\n \nFigure 1.2: Schéma explicatif des différentes étapes de la coulée par centrifugation [12]"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- **Absence de publications récentes** : Lors du début de ce programme, aucune publication récente n’a été trouvée concernant une centrifugeuse capable de fonctionner à 3500 g avec un débit de 80 m³/h. Seul un concurrent, Siebtechnik, a développé une centrifugeuse atteignant 3500 g, mais avec un débit maximal de 20 m³/h.\n- **Défis techniques** : Le projet nécessitait de nouveaux dispositifs pour atteindre les performances souhaitées, avec des défis techniques liés à la tenue des roulements et à l’étanchéité des paliers, influencés par les conditions de fonctionnement (vitesse, température, propreté, etc.) et la conception de la machine.\n- **Limites des roulements** : Les premiers essais ont montré que les roulements classiques atteignaient leur limite à 3500 g, nécessitant le développement de solutions innovantes pour réduire la taille de la centrifugeuse tout en maintenant ses performances de séparation, et en diminuant la consommation d’énergie.\n- **Fonctionnement des roulements** : Les roulements permettent la rotation rapide d’un arbre avec un minimum de frottement, supportant des charges axiales et radiales. Ils sont constitués de deux bagues, d’éléments roulants (billes ou rouleaux), et d’une cage pour maintenir l’équilibre entre les éléments roulants.\n- **Types de roulements** : Les roulements à rouleaux ont une plus forte capacité de charge par rapport aux roulements à billes, en raison de leur contact linéaire avec la piste des bagues, contre un contact ponctuel pour les billes.\n- **Roulements cylindriques** : Pour les calculs à 3500 g, des roulements cylindriques à simple rangée (type NU) ont été utilisés initialement, mais ils ont été remplacés par des roulements à double rangée (type NN) pour supporter des charges radiales et des vitesses plus élevées dans un encombrement restreint.\n- **Lubrification des roulements** : La lubrification est essentielle pour protéger les roulements contre l’usure et la corrosion, réduire les frottements, évacuer la chaleur, et empêcher l’intrusion d’impuretés. Deux types de lubrifiants sont utilisés : la graisse, qui est plus économique mais moins performante, et l’huile, qui permet une meilleure dissipation thermique mais nécessite une étanchéité stricte.\n- **Durée de vie des roulements** : La durée de vie d’un roulement est déterminée empiriquement, dépendant des conditions d’utilisation. La fatigue du roulement due aux contraintes répétées est un facteur clé, pouvant causer des défauts comme l’écaillage et le grippage, réduisant la longévité du roulement.\n- **Vitesse de rotation des roulements** : La vitesse de rotation est limitée par des contraintes mécaniques et thermiques, et dépend de divers facteurs comme le type de roulement, la charge, et le mode de lubrification.\n- **Étanchéité des roulements** : Les joints d’étanchéité sont nécessaires pour retenir le lubrifiant et empêcher la pénétration d’impuretés. Ils doivent résister aux hautes températures et aux pressions de fonctionnement, tout en minimisant les frottements.\n- **Étanchéité des paliers** : En collaboration avec SKF, un nouveau système d’étanchéité en polytétrafluoroéthylène (PTFE) a été développé pour résoudre les problèmes d’usure rapide et de fuites des bagues d’étanchéité des paliers dans les centrifugeuses. Le PTFE est choisi pour sa haute résistance à la température, son excellente résistance chimique, et son faible coefficient de frottement."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nLorsque nous avons débuté ce programme de recherche, nous n’avons trouvé aucune publication récente sur le développement d’une centrifugeuse fonctionnant à une accélération de 3500 g avec une capacité de débit de 80 m3/h. \n\nUn de nos concurrents, Siebtechnik, a développé une centrifugeuse pouvant aller jusqu’à 3500 g mais avec un débit traité maximal de 20 m3/h de suspension.\n\nNous avons donc conclu qu’aucune solution technique n’avait encore jamais été testée pour atteindre nos performances souhaitées et que celles-ci requéraient de nouveaux dispositifs dans le processus de centrifugation.\n\nDes problématiques de tenue des roulements et d’étanchéité des paliers rentrent en ligne de compte dans le développement de la nouvelle centrifugeuse. Elles sont dépendantes d’une part des conditions de fonctionnement du roulement (vitesse, température, propreté, charges, vibrations) et d’autre part, de la vitesse et conception de la machine (bol, palier,…).\n\nNos premiers essais ont montré que la limite des roulements était atteinte à 3500 g. Aucune solution technologique n’était transposable sur notre machine. Le challenge technique nous le rappelons était double : réduire la taille de la centrifugeuse (diamètre du bol) et conserver les mêmes performances de séparation dans le but de diminuer la consommation en énergie. Ce projet converge ainsi vers la proposition d’une évolution technologique dans le processus de centrifugation.\n\nDans la littérature, les publications s’attachent seulement à décrire les roulements existants et les causes possibles de leur détérioration. \n\no\tRoulement\n\nUn roulement permet une rotation rapide d’un arbre avec un minimum de frottement tout en assurant son positionnement, et permet de supporter des charges axiales ou radiales importantes.\n\nIl existe différents types de roulement (cf. Figure 2). Chaque roulement est constitué de deux bagues (intérieure et extérieure), d’éléments roulants permettant la rotation relative des bagues tels que des billes ou des rouleaux, et d’une cage permettant de maintenir une équidistance entre les éléments roulants.\n\n \nFigure 2 : Principaux types de roulements et butées (catalogue général SKF).\n\nLe choix du roulement se fait en fonction de la conception de la machine. En effet, un roulement à rouleaux possède notamment une plus forte capacité de charge par rapport à un roulement à billes. Cela est dû au type de contact entre le roulement et la piste des bagues (cf. Figure 3). Le roulement à bille a un contact ponctuel avec son chemin et génère donc de faibles frottements. Le roulement à rouleaux a quant à lui un contact linéaire avec la piste et présente ainsi une capacité de charge environ trois fois plus importante avec un frottement environ dix fois plus élevé, par rapport à un roulement à bille de mêmes dimensions.\n\n \nFigure 3 : Contacts entre l’élément roulant (bille et rouleau) et la piste.\n\nA ce jour, ce sont des rouleaux cylindriques qui sont employés dans les centrifugeuses car ils peuvent supporter des charges radiales plus élevées que les billes, leur surface de contact étant plus élevée avec les pistes. De plus, ils conviennent parfaitement pour des applications à vitesses de rotation élevées.\n\nNous avons utilisé au cours de nos premiers calculs de dimensionnement à 3500 g, un roulement industriel classique, à rouleaux cylindriques à simple rangée de type NU (cf. Figure 4 a). Cependant, il s’est avéré que la limite des roulements était atteinte. L’ampleur des contraintes subies était trop importante. Nous avons alors décidé de les remplacer par de nouveaux types de roulement, à double rangée de rouleaux de type NN (cf. Figure 4 b), qui ont une haute précision de fabrication. Ces roulements, solidaires de la bague intérieure à l’inverse des roulements de type NU, ont une capacité de charge radiale et de vitesse encore plus élevées dans un encombrement relativement restreint. C’est la première fois que ce type de roulement est utilisé dans une centrifugeuse car habituellement, ces roulements sont montés sur des machines-outils et nous n’avions aucune certitude sur leur tenue pour notre application.\n\t\n(a)       (b) \nFigure 4 : Roulement à rouleaux cylindriques : (a) à simple rangée NU, (b) à double rangée NN.\n\no\tLubrification du roulement\n\nLa lubrification est essentielle pour le bon fonctionnement du roulement. Elle protège les éléments du roulement contre l’usure et la corrosion, et réduit les frottements dans les zones de contact. Cela permet d’augmenter la durée de vie du roulement.\n\nD’autre part, le lubrifiant, en étant en circulation, évacue efficacement la chaleur produite par frottement, ce qui permet d’éviter toute surchauffe du roulement notamment durant des applications à haute vitesse.\nLe lubrifiant empêche également toute impureté et tout corps étranger de pénétrer dans le roulement. Il prolonge ainsi sa longévité en maintenant les pièces dans un bon état de propreté.\n\nLe choix du mode de lubrification influe sur les performances du roulement et dépend des conditions de fonctionnement. Deux solutions sont possibles :\n-\tLa graisse : c’est le lubrifiant le plus économique. La configuration du logement et l’étanchéité sont faciles à réaliser. Cependant, par rapport à l’huile, son coefficient de frottement est plus élevé, l’évacuation thermique est plus faible, et les contrôles de quantité et d’état sont plus difficiles.\n-\tL’huile : ce lubrifiant a une bonne pénétration dans le roulement et permet une vitesse de rotation limite plus élevée que celle de la graisse. La chaleur peut être évacuée facilement avec une circulation forcée. Le remplacement complet de l’huile est facile, de même pour l’évacuation de corps étrangers. Les contrôles de niveau et de l’état sont aisés. Les inconvénients sont que les fuites sont fréquentes si les conditions d’utilisation ne sont pas parfaites. L’étanchéité est donc nécessaire pour retenir le lubrifiant dans le volume occupé par les roulements et pour éviter toute contamination externe (eau, boue, impuretés…).\n\n\n\n\n\no\tDurée de vie d’un roulement\n\nDe façon générale, la durée de vie d’un roulement correspond à la période durant laquelle il fonctionne correctement et assure de manière satisfaisante les fonctions désirées. \n\nLorsqu’un roulement fonctionne sous charge, le chemin de roulement des bagues et les éléments roulants subissent des contraintes répétées qui entraîne un phénomène de fatigue. Celui-ci peut alors engendrer un écaillage, à savoir, fissuration et arrachement de fragments de métal (cf. Figure 5). La fatigue d’un roulement est l’un des facteurs déterminants pour la durée de vie de celui-ci. De plus, la présence de particules étrangères dans le roulement favorise l’écaillage : laminées entre les corps roulants et les chemins, elles créent des contraintes très localisées et très élevées qui amorcent plus rapidement l’écaillage du roulement. Ces contraintes sont d’autant plus fortes que le niveau de charge du roulement est élevé. \n\nUn roulement cesse également de fonctionner correctement sous l’effet d’un grippage, qui est un transfert de matière par échauffement altérant l'état de surface du matériau. \n\nD’autres défauts peuvent aussi réduire la longévité du roulement, tels qu’une détérioration du lubrifiant, une perte d’étanchéité, ou encore une augmentation des vibrations.\n\nL’ensemble de ces détériorations entraînent une destruction du roulement.  \n\n \nFigure 5 : Défauts de roulement : écaillage et grippage.\n\nLa durée de vie d’un roulement est déterminée de manière empirique car elle dépend des conditions d’utilisation. On peut l’estimer à partir d’un calcul statistique basé sur des probabilités. A titre indicatif, si un roulement est calculé pour avoir une durée de vie de 5000 heures avec un taux de fiabilité de 90 %, il y a : \n-\t10 % de chances que le roulement n’atteigne pas les 5000 heures de fonctionnement\n-\t3 % de chances qu’il ne fonctionne seulement 2500 heures\n-\t50 % de chances qu’il tienne plus de 10000 heures.\n\no\tVitesse de rotation d’un roulement \n\nLa vitesse de rotation est limitée par des contraintes mécaniques et thermiques. En raison du frottement des éléments roulants sur les chemins, la température d’un roulement en rotation tend à augmenter avec la vitesse. Au-delà d’une vitesse limite, le roulement ne tourne plus de manière continue et le phénomène de grippage se produit avec un dégagement excessif de chaleur. La vitesse limite d’un roulement varie selon différents facteurs tels que le type de roulement, sa taille, la charge, le lubrifiant utilisé, ou le mode de dissipation de la chaleur.\n\no\tEtanchéité d’un roulement\n\nPour retenir le lubrifiant dans le roulement et pour empêcher toute pénétration d’eau, de poussières ou  autres corps étrangers néfastes au bon fonctionnement, il est nécessaire d’avoir des joints d’étanchéité. Afin d’assurer au roulement la meilleure durée de vie, il convient donc de rechercher un joint approprié pour chaque application, en tenant compte notamment du lubrifiant utilisé. Le joint doit pouvoir résister aux hautes températures et pressions de fonctionnement, et avoir de faibles frottements en rotation. Cela requiert une amélioration continue de la qualité des joints utilisés dans les centrifugeuses.\n\no\tEtanchéité d’un palier\n\nDans une centrifugeuse, la bague d’étanchéité du palier est soumise à d’importants frottements qui engendrent une usure rapide et des fuites. Usuellement, ce sont des bagues en caoutchouc synthétique à base de nitrile, d’acrylique, de silicone ou de fluor qui sont employées. En partenariat avec SKF, nous avons élaboré et validé en 2013 un nouveau système d’étanchéité en polytétrafluoroéthylène PTFE (Ecoflon) afin d’apporter une solution à ce problème. Ce matériau se distingue des autres polymères thermoplastiques par sa haute tenue en température (205 °C), son excellente résistance chimique et surtout, ses propriétés de glissement grâce à un coefficient de frottement extrêmement faible."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- **Principe de la centrifugeuse** : La centrifugeuse accélère la décantation des particules plus lourdes que l'eau à l'aide de la force centrifuge. L'accélération de la boue dépend de la vitesse de rotation du bol et de son rayon, ainsi que du diamètre, de la longueur de la section cylindrique, et de l'angle du cône, définis selon les exigences de l'application.\n- **Problématiques techniques** : Le développement de la centrifugeuse implique de gérer les défis liés à la tenue des roulements et à l'étanchéité des paliers, influencés par la vitesse, la température, la propreté, et les charges. Les roulements permettent la rotation rapide des composants en supportant des charges axiales et radiales importantes.\n- **Matériaux des roulements** : Les roulements sont fabriqués principalement en acier ou en céramique, ce dernier étant plus léger et réduisant les phénomènes de glissement. Toutefois, les roulements en céramique sont 7 à 8 fois plus chers que ceux en acier.\n- **Lubrification** : La lubrification est cruciale pour protéger les roulements contre l'usure, la corrosion, et les impuretés, et pour réduire les frottements. Elle joue également un rôle essentiel dans l'évacuation de la chaleur produite par le frottement, particulièrement dans les applications à haute vitesse.\n- **Développement expérimental** : Pour réduire la consommation énergétique des centrifugeuses, il a été nécessaire de revoir la conception de la vis convoyeuse et de développer un palier lisse spécifique, capable de fonctionner immergé dans la boue, ce qui constitue une innovation majeure.\n- **Optimisation énergétique** : Pour réduire la consommation d'énergie de 30 %, plusieurs stratégies ont été explorées :\n  - Réduction du diamètre de sortie liquide pour diminuer la puissance hydraulique consommée, avec une conception revue incluant les nouvelles plaquettes TurboWISE.\n  - Augmentation de la vitesse de rotation pour atteindre 3500 à 4000 g, accompagnée d'une réduction de la distance entre les paliers, d'une amélioration de l'étanchéité, et du développement d'un système de lubrification air-huile.\n  - Diminution des masses en rotation, notamment du bol et de la vis convoyeuse, pour réduire la consommation énergétique. Une nouvelle géométrie de vis avec un double cône HP a été développée pour éviter le by-pass de boue liquide.\n  - Étude approfondie de la trajectoire du fluide dans un distributeur totalement submergé, identifiant une pression d'alimentation élevée qui pourrait être une cause majeure de performances moyennes. Le distributeur, qui joue un rôle crucial dans l'accélération du produit pour une séparation efficace, a été au centre de cette analyse.\n- **Manque de littérature récente** : Peu de publications récentes traitent des améliorations potentielles pour les centrifugeuses. Le projet s'est donc appuyé principalement sur des publications internes et des brevets."}, {"role": "assistant", "content": "1.1\tÉtat de l’art\nLe principe d’une centrifugeuse est basé sur la décantation naturelle d’un produit contenant des particules plus lourdes que l’eau. La force centrifuge permet d’accélérer ce phénomène.\n\nDans la centrifugeuse, la boue à déshydrater est introduite dans un bol en rotation à grande vitesse avec une solution de polymères (Cf. Figure 1). L'accélération que subit la boue dépend d'un grand nombre de facteurs dont notamment la vitesse de rotation du bol et son rayon. Les particules solides sont décantées puis comprimées en périphérie sur les parois du bol sous l’effet de la force centrifuge. Le diamètre, la longueur de la section cylindrique et l'angle du cône sont définis selon les exigences spécifiques de l'application.\n\nUne vis sans fin tourne à une vitesse légèrement différente de celle du bol et évacue la phase solide du côté opposé au liquide (centrat) à l’extrémité conique du bol. Cette vitesse différentielle détermine le temps de rétention des particules solides dans le bol et donc, la siccité du sédiment de sortie. Ce type de procédé permet ainsi une déshydratation en continu des boues. Le centrat est quant à lui évacué par débordement de l’autre côté de la machine.\n\n \nFigure 1 : Fonctionnement d'une centrifugeuse\nLors du développement d’une centrifugeuse, des problématiques de tenue des roulements et d’étanchéité des paliers rentrent en ligne de compte. Elles sont dépendantes d’une part des conditions de fonctionnement du roulement (vitesse, température, propreté, charges des composants en rotation) et d’autre part, de la conception de la machine (bol, vis, palier, etc.).\n\nUn roulement permet une rotation rapide d’un arbre avec un minimum de frottement tout en assurant son positionnement, et permet de supporter des charges axiales ou radiales importantes.\nIl existe différents types de roulement (Cf. Figure 2). Chaque roulement est constitué de deux bagues (intérieure et extérieure), d’éléments roulants permettant la rotation relative des bagues tels que des billes ou des rouleaux, et d’une cage permettant de maintenir une équidistance entre les éléments roulants. Le choix du roulement se fait en fonction de la conception de la machine.\n\n \nFigure 2 : Principaux types de roulements et butées (catalogue général SKF)\nLes matériaux employés pour la fabrication du roulement dépendent de l'application pour laquelle il est conçu, mais il doit être généralement très résistant à la compression. C'est pourquoi les matériaux les plus utilisés sont l'acier et la céramique. Les roulements en céramique présentent l’avantage d’être plus légers que leurs analogues en acier, ce qui leur permet d’être mis en rotation plus facilement et d’éviter ainsi les phénomènes de glissement lorsque la charge imposée au roulement est faible. En contrepartie, ils sont en moyenne 7 à 8 fois plus chers dans le cadre de notre application.\n\nD’autre part, la lubrification est essentielle pour le bon fonctionnement du roulement. Elle protège les éléments du roulement contre l’usure, la corrosion et la pénétration de tout corps étranger, et réduit les frottements dans les zones de contact. Cela permet d’augmenter la durée de vie du roulement.\nDe plus, le lubrifiant, en étant en circulation, évacue efficacement la chaleur produite par frottement, ce qui permet d’éviter toute surchauffe du roulement notamment durant des applications à haute vitesse.\n\nLes verrous technologiques auxquels nous avons été confrontés pour diminuer la consommation énergétique relèvent du développement expérimental. Nous avons dû revoir entièrement la conception des pièces majeures, telle que la vis convoyeuse, selon des conditions particulières de fonctionnement. Le développement d’un palier lisse spécifique, qui puisse être immergé dans la boue, a constitué un point novateur de nos travaux.\n\nNous avons également été confrontés à des contraintes mécaniques suite à l’intégration de ces nouveaux éléments au sein de la centrifugeuse et à la variabilité des performances, la boue n’étant plus traitée de la même façon que dans une machine standard.\n\nLa consommation énergétique des centrifugeuses est une problématique d’actualité qui va dans le sens du développement durable. ANDRITZ a mis en place dans le cadre de son pôle R&D un programme visant la réduction de la puissance consommée de ses machines.  \n\nPour atteindre l’objectif ambitieux de 30 %, nous avons étudié plusieurs axes de recherche :\n\n•\tLa réduction du diamètre de sortie liquide\nLa puissance hydraulique consommée d’une centrifugeuse est proportionnelle au carré des diamètres des sorties du produit. Diminuer le diamètre de sortie liquide permettrait donc d’avoir un gain de puissance. Nous avons entièrement revu la conception de la vis d’extraction et de la chambre d’alimentation, avec notamment l’intégration de nouvelles plaquettes appelées TurboWISE. Diminuer le diamètre de sortie du solide permettrait un gain supplémentaire. Une étude a également été menée dans le cas d’une centrifugeuse ayant une vis convoyeuse totalement submergée et une pompe contrôlant la pression en entrée.\n\nL’augmentation des performances étant le second axe de priorité de notre projet, nous avons également travaillé sur les points suivants :\n\n•\tL’augmentation de la vitesse de rotation : roulements et système de lubrification\nEn nous appuyant sur les résultats d’un programme de recherche parallèle, nous souhaitons atteindre 3500 voire 4000 g en réduisant la distance entre les paliers. Cette accélération permettra d’améliorer les performances de séparation équivalentes pour une centrifugeuse de taille réduite et moins énergivore. \nL’étanchéité du roulement de butée de vis a été améliorée et un nouveau système de lubrification air-huile a été développé.\n\n•\tLa modification des caractéristiques mécaniques des éléments en rotation : le bol et la vis convoyeuse\nOn imagine facilement que diminuer les masses en rotation dans la centrifugeuse permet de limiter sa consommation énergétique. Nous avons donc diminué l’épaisseur des éléments en rotation que sont le bol et la vis convoyeuse. En parallèle, pour éviter le phénomène de by-pass de boue liquide vers la sortie solide, censée évacuer un produit déshydraté, nous avons développé et testé une nouvelle géométrie pour la vis convoyeuse avec un double cône HP.\n\n•\tL’étude de la trajectoire de fluide à l’intérieur du distributeur complétement submergé dans la boue\nContrairement à une centrifugeuse habituelle, la vis convoyeuse est totalement submergée. Cette nouvelle conception induit un fonctionnement complétement différent par rapport aux machines existantes sur le marché. Lors des tests de performances de nos prototypes, nous avons constaté un phénomène de pression d’alimentation élevée que nous n’avions jamais eu à appréhender jusqu’à présent. Cette pression élevée en alimentation serait une des causes majeures des performances moyennes atteintes. Nous avons donc cherché à déterminer son origine et à caractériser l’influence du distributeur sur les trajectoires de fluide et la pression de rotation. Nous supposons en effet que le fait que le distributeur soit totalement submergé impacte considérablement son efficacité. Or, son rôle est d’accélérer le produit en rotation, processus déterminant pour la bonne séparation de la boue. Les travaux de recherche menés dans le cadre de cette étude ont donc pour objectif d’acquérir une meilleure connaissance des phénomènes d’écoulement et de pression de rotation à l’intérieur du distributeur afin d’améliorer substantiellement la conception de notre centrifugeuse et les performances de séparation.\nAu cours de ce projet de recherche, nous avons constaté qu’il existait peu de publications récentes abordant les possibles améliorations. Pour orienter nos travaux, nous nous sommes ainsi appuyés essentiellement sur des publications internes et des brevets, dont certaines références sont listées ci-après."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- **Extraction de l'huile d'olive** : Contrairement à d'autres huiles végétales, l'huile d'olive ne nécessite aucun raffinage ou transformation chimique. Le processus d'extraction comprend deux étapes majeures : le broyage des olives et la séparation des phases solides et liquides.\n- **Broyage des olives** : Les olives sont d'abord nettoyées puis broyées à l'aide de broyeurs à meule, à disques dentés ou à marteaux, produisant une pâte d'olive. Cette pâte est ensuite malaxée sous température contrôlée pour homogénéisation et coalescence des gouttelettes d'huile.\n- **Malaxage et coalescence** : Le malaxage favorise la coalescence des microgouttelettes d'huile, facilitant leur extraction. Pour produire de l'huile d'olive extra vierge, la température de malaxage ne doit pas dépasser 27-30 °C.\n- **Centrifugation horizontale à trois phases** : La pâte malaxée est centrifugée pour séparer l'huile, les margines (phase aqueuse), et les grignons (phase solide). Ce processus nécessite l'ajout d'une grande quantité d'eau, augmentant le volume de coproduits à éliminer.\n- **Décantation et centrifugation verticale** : Pour séparer l'huile de l'eau, une décantation gravitationnelle est souvent utilisée, mais elle est longue et peut affecter la qualité de l'huile. La centrifugeuse verticale est préférée pour accélérer la séparation des phases huile/eau.\n- **Centrifugeuses horizontales à deux phases** : Les technologies récentes ont permis de développer des centrifugeuses horizontales à deux phases, qui extraient directement l'huile de la pâte malaxée sans nécessiter d'étapes supplémentaires.\n- **Défis technologiques** : La vis de convoyage est critique pour réduire le taux d'huile d'olive dans les grignons en sortie machine. Les principaux concurrents atteignent des taux de 1,5 à 2,5 %, mais l'objectif est d'atteindre un taux inférieur à 1,5 %.\n- **Innovation de la vis sticks** : Le concept de « vis sticks » vise à maintenir un cisaillement suffisant tout en réduisant le mixage pour stabiliser le fonctionnement de la centrifugeuse dans le temps. Différentes configurations de vis avec des éléments saillants ont été étudiées.\n- **Exemples de brevets concurrents** : Westfalia et Flottweg ont développé des centrifugeuses avec des éléments protubérants pour améliorer le mixage et la séparation, mais l'impact de ces éléments sur les performances dans l'application huile d'olive reste insuffisamment documenté.\n- **Nouvel axe de recherche** : La conception de la « vis sticks » vise à explorer l'impact des éléments saillants sur la stabilité et les performances de séparation, avec pour objectif un fonctionnement plus stable et un meilleur contrôle du mixage du produit."}, {"role": "assistant", "content": "1.1\tÉtat de l’art\nObtenue à partir du fruit de l’olivier, l’huile d’olive ne requiert aucune étape de raffinage ou de transformation chimique contrairement aux autres huiles végétales. Dans le processus d’extraction, nous pouvons distinguer deux étapes majeures : le broyage des olives (trituration) et la séparation des différentes phases. \n\nLes olives sont tout d’abord nettoyées dans un système de laveuse-effeuilleuse qui va les débarrasser de la terre, des feuilles, tiges, cailloux et autres impuretés. Elles sont ensuite broyées afin que les cellules détruites puissent libérer leur contenu. Le broyage peut être réalisé par un broyeur à meule, un broyeur à disques dentées ou un broyeur à marteaux (cf. Figure 1).\n\n \nFigure 1 : Broyeur à marteaux\n\nA ce stade du procédé, une pâte d’olive est obtenue. Elle va être malaxée afin d’être homogénéisée (cf. Figure 2) : la pâte est remuée sous une température contrôlée et durant une durée déterminée selon le type d’olive et leur maturité. Pour produire une huile d’olive extra vierge, la température ne doit pas excéder 27-30 °C.\n\nOutre le rôle d’homogénéisation de la pâte, le malaxage permet la coalescence des gouttes d’huile (cf. Figure 3). Les microgouttelettes d’huile, qui viennent d’être libérées de leurs lipovacuoles cellulaires, vont se regrouper pour former des gouttes de plus grande taille qui seront plus faciles à extraire de la pâte.\n\n \nFigure 2 : Malaxage de la pâte d’olive\n\n \nFigure 3 : Image MEB de coalescence de gouttelettes d’huile\n\nLe malaxage terminé, la pâte d’olive est ensuite centrifugée horizontalement afin de séparer les phases solides et liquides (cf. Figure 4).\n\n \nFigure 4 : Pâte malaxée et centrifugeuse horizontale\n\nLes centrifugeuses horizontales à trois phases ont été les premières à être développées.\nElles permettent de séparer selon les densités :\n•\tL’huile (ρ ~ 0,916 kg/dm3) : phase huileuse dite « légère »\n•\tLes margines (ρ ~ 1,015 à 1,086 kg/dm3) : phase aqueuse dite « lourde »\n•\tLes grignons (ρ ~ 1,2 kg/dm3) : phase solide contenant les restes des noyaux, la peau et la pulpe des olives dépourvue de son huile.\n\nPour que le système fonctionne, il est nécessaire d’ajouter une grande quantité d’eau en amont de la séparation. Or, celle-ci va se mélanger aux margines et donc augmenter le volume de coproduits à éliminer. A la sortie de la centrifugeuse, trois produits sont obtenus : des grignons très humides d’un côté, une phase huileuse avec un peu d’eau et une phase aqueuse avec un peu d’huile.\n\nPour séparer l’huile et l’eau, une décantation gravitationnelle peut être mise en place : les phases liquides sont placées dans des cuves en inox et l’eau de végétation est ensuite éliminée après repos pour ne conserver que l’huile d’olive.\nCependant, le temps de séparation des phases peut être relativement long, allant de quelques heures à quelques jours. Or, le temps de contact entre l’eau et l’huile a une influence négative sur la qualité finale de l’huile d’olive. De plus, les systèmes de trituration métalliques augmentent l’émulsion huile/eau et cette émulsion est trop stable pour être séparée par simple décantation à échelle industrielle.\n\nAfin d’accélérer la séparation des phases huile/eau, la centrifugeuse verticale est préférée. La vitesse de rotation de la machine va rompre l’émulsion et favoriser la séparation.\n\nLes avancées technologiques ont permis de développer aujourd’hui des centrifugeuses horizontales à deux phases. L’intérêt majeur de ce type de système est qu’aucune étape supplémentaire n’est requise après centrifugation : lorsque l’appareil est bien réglé, l’huile d’olive est directement extraite de la pâte malaxée, la phase solide étant composée des grignons et de l’eau de végétation.\n\nLes verrous technologiques auxquels nous avons été confrontés relèvent du développement expérimental et sont issus de l’application pratique des phénomènes se passant à l’intérieur de la centrifugeuse : la vis de convoyage est l’élément critique du procédé de séparation pour obtenir un taux d’huile d’olive faible dans les grignons en sortie machine.\n\nA ce jour, nos principaux concurrents (GEA Westfalia Separator, Alfa Laval, Flottweg, Hiller) sont parvenus à atteindre un taux d’huile d’olive dans les grignons de 1,5 – 2,5 %, les clients exigeant un taux inférieur à 2,5 %. \n\nPour concevoir la vis de notre centrifugeuse, nous nous sommes appuyés sur différentes publications et nous avons étudié les brevets déposés par nos concurrents, dont certaines références sont listées ci-après. Notre challenge est de proposer une évolution technologique de la vis pour atteindre un taux d’huile d’olive dans les grignons en sortie machine inférieur à 1,5 %.\n\nConcernant notre nouveau concept de « vis sticks », nous souhaitons conserver le cisaillement du produit tout en le mixant moins, de façon à ce qu’il ne soit pas trop retenu au sein de la centrifugeuse et ainsi, fiabiliser le fonctionnement de la machine dans le temps. Pour cela, nous avons étudié différentes configurations de vis existantes présentant en plus des pales, d’autres éléments saillants pour déstabiliser la couche solide, tels que des plots. \n\nLa centrifugeuse décrite dans le brevet WO02089992 déposé par Westfalia, fait appel à un procédé mettant en jeu une vis et des éléments cylindriques qui émergent sur le pourtour du tambour (cf. Figure 5).\nCes doigts permettent un brassage différent de la pâte d’olive à l’intérieur de la centrifugeuse mais nécessitent d’avoir des trous dans les filets pour que le bol puisse tourner.\n \nFigure 5 : Centrifugeuse du brevet WO0208992\n\nLe concept breveté par Flottweg sous la référence EP2586533 décrit une centrifugeuse à vis à bol plein composée d’un tambour rotatif et d’une vis sans fin. Celle-ci présente par ailleurs à sa surface des éléments en couteaux (cf. Figure 6) disposés dans la voie de transport, de sorte que lors du fonctionnement de la centrifugeuse, ils soient immergés dans le produit à séparer. Nous notons que les doigts sont inclinés afin qu’il n’y ait pas de bourrage.\n\n  \nFigure 6 : Centrifugeuse du brevet EP2586533 et détail de la  disposition des couteaux\n\nCes deux brevets illustrent des exemples de centrifugeuses présentant une zone de mixage non « traditionnelle », en ce sens que les bols et vis ici décrits présentent des éléments protubérants en plus des pales. \nAu cours de notre étude bibliographique, nous n’avons pas trouvé d’informations suffisantes sur l’impact de ces éléments protubérants sur les conditions de convoyage du produit et les performances de séparation dans l’application huile d’olive.  \nUn nouvel axe de recherche de ce programme a donc été de concevoir une « vis sticks » pour soulever ces inconnues. Notre but est de tendre vers un fonctionnement plus stable de la machine dans le temps grâce à un mixage moins important du produit."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- **Dégrillage des eaux usées** : Le dégrillage est une étape de prétraitement essentielle dans l'épuration des eaux usées, éliminant les matières volumineuses susceptibles d'endommager les équipements en aval. Les eaux usées passent à travers un dégrilleur installé en tête de station, composé d'une grille et d'un système de relevage pour évacuer les déchets accumulés.\n- **Types de dégrilleurs** : Il existe plusieurs modèles de dégrilleurs, notamment :\n  - **Dégrilleur courbe** : Utilisé pour des canaux de moins de 2 m de profondeur, nettoyé par un peigne monté sur un bras rotatif.\n  - **Dégrilleur vertical** : Adapté aux canaux plus profonds (jusqu'à 10 m), nettoyé par un racleur à dents articulé.\n- **Espacement des barreaux** : Les dégrilleurs se différencient par l'espacement de leurs barreaux :\n  - Dégrilleurs grossiers : Espacement de 10 à 40 mm.\n  - Dégrilleurs à tôles perforées : Diamètre de maille de 3 à 10 mm.\n  - Dégrilleurs fins : Diamètre de maille de 1 à 3 mm.\n- **Limites des dégrilleurs à barreaux** : Ces modèles ne permettent pas de gérer une grande quantité de déchets à haute fréquence et ne proposent pas de plateforme modulaire. Le dégrillage par l'amont pousse les déchets dans la maille, rendant l'entretien complexe.\n- **Innovation dans la conception des dents** : La révision de la conception des dents dans notre dégrilleur Aqualine, inspirée par le brevet EP 0480075 A1, permet une maintenance facilitée sans modification majeure de la structure.\n- **Décharge des déchets** : En 2017, nous avons optimisé la décharge des déchets en modifiant la zone de décharge et repositionnant la brosse de plaques perforées pour éviter la rétention des déchets et maintenir l'efficacité du dégrilleur.\n- **Systèmes membranaires** : Les bioréacteurs à membrane nécessitent un dégrillage plus fin pour protéger les membranes des filasses, qui sont difficiles à extraire manuellement.\n- **Technologie Aquadrum** : Le dégrilleur fin Aquadrum utilise un tambour incliné avec une maille de 1 mm pour un dégrillage efficace. Les déchets sont convoyés par une spire solidaire du tambour, assurant un transport continu sans dénaturation.\n- **Performances d'Aquadrum** : En 2013, notre prototype Aquadrum a atteint un taux de capture de 99 % avec une maille de 1 mm et de 97 % avec une maille de 2 mm, surpassant les performances de nos concurrents.\n- **Choix technologiques** : Les innovations incluent une perforation cylindrique du tambour, une conception cylindrique pour une zone de filtration libre de déchets, et un convoyage intégré au tambour pour éviter les bourrages.\n- **Problèmes rencontrés en 2017** : Une destruction majeure du châssis d'un Aquadrum 1000 sur site a révélé une fragilité structurelle, conduisant à une nouvelle conception du tambour et du châssis pour améliorer la robustesse.\n- **Avancée de l'état de l'art** : Le développement de notre dégrilleur fin de forte capacité basé sur une technologie de transport hydraulique a permis de faire progresser l'état de l'art, comblant un vide bibliographique et répondant aux besoins du marché."}, {"role": "assistant", "content": "1.1\tÉtat de l’art\nL’épuration des eaux usées commence par un prétraitement appelé dégrillage. Cette opération mécanique consiste à débarrasser les eaux usées des matières volumineuses et des déchets susceptibles de gêner les traitements ultérieurs, voire d’endommager les organes mécaniques des équipements installés en aval. \n\nLes eaux usées sont filtrées au travers d’un dégrilleur installé dans un canal en tête de station. La pièce principale de cet équipement est une grille composée de barres transversales plus ou moins espacées et de forme particulière, selon une pente déterminée.\n\nLes déchets, stoppés par la grille, sont évacués mécaniquement à l’aide d’un dispositif de relevage et d’éjection automatique : un peigne remonte le long de la grille emmenant avec lui les déchets qui vont se déverser dans un réceptacle placé derrière la grille et être compactés.\n\nLa vitesse de passage à travers la grille doit être suffisante pour que les matières s’accumulent sur la grille, sans provoquer une perte de charge trop importante, ni entraîner un colmatage en profondeur des barreaux ou un départ des matières avec le flot.\n\nIl existe différents modèles de dégrilleurs :\n•\tLe dégrilleur courbe : installé dans des canaux de profondeurs inférieures à 2 m, il filtre des eaux qui ne sont pas beaucoup chargées. Le dégrilleur fonctionne par asservissement et le nettoyage de la grille est réalisé grâce à un peigne monté sur un bras tournant autour d’un axe horizontal. Les déchets sont éjectés dans un panier de récupération (cf. Figure 2).\n\n \nFigure 2 : Dégrilleur courbe\n\n•\tLe dégrilleur vertical : adapté pour des canaux allant jusqu’à 10 m de profondeur, cet appareil est composé d’une grille inclinée qui filtre généralement des eaux chargées de matières solides difficiles à séparer. \nLe nettoyage de la grille se fait à l’aide d’un racleur composé de dents de manière à bien passer entre les barreaux. La rangée de dents remonte le long de la grille grâce à un système de chaîne s’articulant dans des rails de guidage. \n\nLes dégrilleurs se différencient également selon l’écartement des barreaux de la grille :\n•\tDégrilleur grossier à barreau, avec un écartement allant de 10 à 40 mm ;\n•\tDégrilleur à tôles perforées, avec un diamètre de maille allant de 3 à 10 mm ;\n•\tDégrilleur fin, avec un diamètre de maille allant de 1 à 3 mm.\n\nLes technologies existantes avant le projet Aqualine étaient essentiellement les dégrilleurs à barreaux. Cependant, ces derniers ne permettent pas de remonter une grande quantité de déchets à une grande fréquence et aucun ne propose un concept de plateforme modulaire. De plus, le dégrillage par l’amont pousse les déchets dans la maille. Notre dégrilleur avec dents, Aquaguard, est une alternative à ces appareils mais son inconvénient majeur est qu’il nécessite 100 % de la largeur sur un côté pour les opérations de maintenance : le changement d'une dent nécessite l'intervention sur 2 axes avec beaucoup d'efforts physiques (20' par axe).\n\nAfin de faciliter et de permettre la maintenance du système avec dent dans la largeur du châssis, nous avons revu la conception des dents. Pour cela, nous avons pu notamment prendre connaissance du brevet de Bormet, référencé EP 0480075 A1 et tombé dans le domaine public en 2012, lors du développement de notre nouveau dégrilleur Aqualine.\n\nEn 2017, nos travaux ont consisté à améliorer la décharge des déchets de notre machine sans modification de la partie avale.\nNos concurrents ont modifié la forme de leurs plaques pour avoir un brossage efficace au détriment du relèvement des déchets.\nAprès analyse des zones non lavées sur notre dégrilleur, nous nous sommes orientés vers une solution différente. Nous avons étudié une nouvelle conception de la zone de décharge pour éviter toute rétention de déchets et nous avons modifié le positionnement de la brosse de plaques perforées, initialement située en bas de la machine en-dessous du système d’entraînement et désormais située sur le haut de la machine, entrainant une redéfinition de la conception de la structure machine.\nNotre solution permet de conserver la capacité de relevage des déchets de notre appareil et évite ainsi une maintenance opérationnelle trop importante en amont de la machine due à la non remontée des déchets.\n\n\nDepuis quelques années, un nouveau procédé, appelé système membranaire, existe sur le marché pour les stations d’épurations. \n\nLa technologie des bioréacteurs à membrane repose sur le procédé de boues activées. La matière organique contenue dans un effluent entrant est dégradée par des micro-organismes contenus dans un réservoir. Pour séparer l’eau à traiter et la biomasse active à forte charge, il est nécessaire de mettre en place un système d’ultrafiltration. \n\nCe système nécessite un dégrillage plus fin que les stations classiques. En effet par construction, ces membranes ne doivent pas recevoir de fillasses car il est quasiment impossible de les extraire des membranes une fois qu’elles en sont prisonnières. Le décolmatage est manuel et très difficile.\n\nCe système se présente sous forme de casier réunissant quelques centaines de membranes dans lesquelles les filasses s’accumulent (cf. Figure 3).\n\n(a)     (b)  \nFigure 3 : Membrane : (a) propre, (b) encrassée.\n\nLe marché des dégrilleurs fins est extrêmement restreint. Certains fabricants proposent ce type de machines mais à de très faibles débits. Le principal concurrent que nous avons identifié sur le marché de niche des dégrilleurs fins de forte capacité est la société allemande Huber. Il s’avère qu’elle a opté pour une technologie d’évacuation des solides par outil mécanique. Cependant, avec des tambours hors sol et deux motorisations, les dégrilleurs dans le canal rencontrent des problèmes de remontée des déchets. Les tambours possèdent en effet deux étapes distinctes réalisées par la mise en place de plusieurs machines existantes :\n•\tUne pour la filtration\n•\tUne pour la remontée des déchets\n•\tUne pompe pour la remontée des filtrats chargés.\n\nPour proposer aux clients une solution plus efficiente, nous avons décidé de concevoir un produit ayant recours à une technologie hydraulique, tout en améliorant le débit traversier par rapport aux concurrents.\n\nLa Figure 4 présente le principe de fonctionnement d’Aquadrum.\n\n \nFigure 4 : Fonctionnement d’Aquadrum.\n\nL’eau du canal est filtrée à travers un tambour incliné possédant une maille de 1 mm de diamètre. Les particules supérieures à la maille sont retenues coté amont.\nLa mise en rotation du tambour entraîne la remontée des déchets grâce à la spire convoyeuse jusqu’à la sortie de l’appareil, appelée zone d’égouttage des déchets. La spire étant solidaire du tambour, le déchet n’est pas dénaturé par le convoyage. En sortie d’Aquadrum, le déchet est soit convoyé et transféré par pompe, soit compressé.\nPour assurer une filtration continue, une rampe de lavage nettoie séquentiellement le média de l’extérieur vers l’intérieur. Les eaux de lavage et particules sont récupérées dans une goulotte et celle-ci les convoie grâce au débit de lavage en dehors de la zone de filtration. \n\nLes travaux relatifs à la conception de notre dégrilleur fin Aquadrum de forte capacité ont été substantiels et chronophages. \nAprès plusieurs années de développement, nous sommes parvenus en 2013 à un prototype fonctionnel en regard des performances techniques fixées. Alors que la concurrence atteignait seulement un taux de capture de 92 %, notre taux de capture réel a été évalué à 99 % lorsque la plus petite maille de tambour, perforations de Ø1mm, a été testée ; et à 97 % lorsque le tambour avait des perforations de Ø2mm, pour un débit inférieur à 1000 m3/h. Ces mesures quantitatives de l'efficacité de dégrillage (SCR) ont été effectuées par l'institut UK Water Industry Research - UKWIR). \n\nCes performances s’expliquent par plusieurs choix technologiques :\n•\tLa perforation de notre tambour, basée sur des orifices cylindriques, permet un dégrillage très fin et performant en deux dimensions, alors que nos concurrents sur ce marché utilisent des machines à perforations fendues (tambour conçu en fil soudée appelé wedge wire).\n•\tHabituellement, les dégrilleurs ont une conception plane (écran de filtration). La réalisation d'un dégrilleur cylindrique a permis d'obtenir une zone de filtration complétement libre de tout déchets à chaque rotation. Le rendement maximal de la machine est donc préservé quel que soit la charge entrante.\n•\tLe système de convoyage des déchets fait partie intégrante du tambour (spire soudée à l'intérieur de la tôle perforée du tambour). Cela permet d’assurer une évacuation continue des déchets, d'éviter tout bourrage dans la zone de convoyage et d'éviter tout risque d’extrusion ou de délitage du produit pendant son convoyage. La capacité hydraulique et la fiabilité de l’équipement sont donc augmentées.\n•\tEn comparaison d'autres outils de dégrillage, l'efficacité de l'Aquadrum n’est pas liée à des pièces mobiles qui s'usent telles que des brosses ou des racleurs. Le taux de capture est donc optimal à tout moment.\n\nCependant, en 2017, nous avons rencontré une destruction majeure du châssis de notre dégrilleur Aquadrum 1000 en fonctionnement sur un site client. Bien que les conditions d'alimentation préconisées par ANDRITZ n’eussent pas été complètement respectées (charge très importante dans le dégrilleur), cela a permis de mettre en évidence une fragilité de la structure de la machine. \nAfin de pallier ce problème, une nouvelle conception du tambour et du châssis a été étudiée et éprouvée lors de nouveaux essais sur des installations pilotes.\n\nAu cours de ce programme de recherche, il n’existait pas de bibliographie complémentaire à nos recherches de 2013-2014 à même de nous servir de base dans la conception de nos dégrilleurs. \n\nAvec la conception de notre dégrilleur fin de forte capacité basé sur une technologie de transport hydraulique, nous avons pu faire avancer l’état de l’art."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- **Maintenance prédictive :** \n  - La maintenance prédictive consiste en une maintenance préventive déclenchée par un événement prédéterminé (ex : autodiagnostic, capteur, etc.) ou l’analyse de l’évolution de paramètres significatifs de la dégradation et de la performance.\n  - Cette approche permet de planifier les interventions, maximiser l’utilisation des entités, et réduire le nombre de maintenances correctives.\n  - L’objectif est d'estimer la limite de bon fonctionnement d’un système et de corriger les défaillances avant qu'elles ne provoquent des pannes.\n  - Les principaux enjeux incluent : identifier les paramètres clés, structurer les données, établir des seuils de bon fonctionnement, détecter les dérives, modéliser les pannes, et utiliser des systèmes intelligents auto-apprenants.\n  - Bien que largement utilisée dans l'industrie, la maintenance prédictive reste limitée à certains domaines spécifiques.\n  - Prytz et al. [8] soulignent les défis de l'implémentation dans l'industrie automobile, tels que les limites des capteurs et la complexité des systèmes dynamiques comme les véhicules.\n  - Les capteurs existants sont spécifiques à un type de défaillance (ex : acoustiques pour défauts mécaniques, thermiques pour anomalies de température, vibratoires pour problèmes d’alignement).\n  - Un défi majeur réside dans l'identification des données pertinentes (Small Data) au sein de grandes masses de données (Big Data) et leur utilisation intelligente (Smart Data).\n  - Prytz et al. [8] et Prytz [9] proposent l'utilisation du machine learning (apprentissage non supervisé et supervisé) pour révéler des relations entre paramètres et initialiser des modèles auto-apprenants.\n  - Des entreprises comme SNCF [10] et Valéo [11] explorent activement la maintenance prédictive avec les avancées du Big Data et des capteurs modernes.\n  - Selon une étude de McKinsey [12], la maintenance prédictive pourrait permettre aux entreprises d’économiser 630 milliards de dollars d’ici 2025.\n\n- **Modélisation de la fiabilité :** \n  - La fiabilité est évaluée dès la phase de conception préliminaire et mise à jour tout au long du développement pour garantir la satisfaction des exigences.\n  - La MTBF (Mean Time Between Failure) des équipements est calculée à partir des taux de défaillance des composants, en utilisant des données clients, des standards (MIL-HDBK-217F, NPRD95), et des outils de prévision.\n  - Les lois de probabilité couramment utilisées pour modéliser la fiabilité incluent : \n    - **Weibull** : Modélise les différentes phases de vie d'une pièce, très polyvalente.\n    - **Rayleigh** : Utilisée pour les mesures en ingénierie des communications.\n    - **Exponentielle** : Cas particulier de Weibull, absence de mémoire, utile pour modéliser la durée de vie.\n    - **Log-normale** : Flexible, utile pour des données symétriques ou asymétriques.\n    - **Petite valeur extrême** : Modèle utile pour les défaillances causées par le premier échec dans une série.\n    - **Normale** : Bien que courante, moins utilisée pour les durées de vie à cause de sa queue à gauche infinie.\n  - Ces distributions sont comparées dans [13].\n\n- **Modélisation de l’évolution des dégradations :** \n  - Trois types de modèles décrivent l’évolution des dégradations : \n    - **Fiabiliste** : Utilise le taux de défaillance pour représenter l’état de dégradation, particulièrement pour les composants actifs.\n    - **Physique** : Basé sur la connaissance physique du mécanisme de dégradation, utile pour les composants passifs.\n    - **Statistique** : Applique une loi mathématique aux données d’inspection, modèle particulièrement utilisé dans le rapport [16].\n  - Des outils d’adaptation des modèles au contexte industriel sont étudiés par K. Simola et M. Pulkkinnen [17], incluant l'estimation des erreurs de mesure basées sur des modèles lognormal et logit.\n  - Nous avons utilisé ces recherches pour modéliser la fiabilité de nouveaux systèmes, développer des abaques, et prendre en compte les erreurs de mesure dans les modèles de dégradation."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tLa maintenance prédictive\nLa maintenance prédictive, ou préventive conditionnelle consiste en une opération de maintenance préventive subordonnée à un type d’événement prédéterminé [1] (autodiagnostic, information d’un capteur, mesure, etc.) ou à l’analyse de l’évolution surveillée de paramètres significatifs de la dégradation et de la baisse de performance d’une entité.\nCela permet de « retarder » et de planifier les interventions. Cet événement permet de fixer un seuil d’alarme avant un seuil d’admissibilité. Le principal intérêt d’une telle stratégie est de pouvoir utiliser les entités au maximum de leur possibilité mais aussi de diminuer le nombre d’opérations de maintenance corrective. Contrairement à la maintenance corrective, les activités de la maintenance prédictive sont principalement basées sur des activités de surveillance.\n\nL’enjeu de la maintenance prédictive ou prévisionnelle est d’estimer au plus juste la limite de bon fonctionnement d’un système afin de procéder aux correctifs nécessaires avant qu’il ne tombe en panne. Le défi de cette science est de savoir relever les paramètres d’intérêt, dont les variations sont significatives d’un dysfonctionnement au-delà d’un certain seuil [2].\n\nLes enjeux de cette nouvelle forme de maintenance des équipements sont multiples [3] :\n\tIdentifier les paramètres à relever ;\n\tStocker et structurer les grandes masses de données acquises ;\n\tEtablir des seuils de référence de bon fonctionnement ;\n\tReconnaitre une dérive significative ;\n\tIdentifier les paramètres influant indépendamment ou de manière couplée sur le fonctionnement d’un composant d’intérêt ;\n\tModéliser les schémas de panne ;\n\tAméliorer les analyses à l’aide de systèmes intelligents auto-apprenants.\n\nSi l’analyse prédictive est utilisée depuis longtemps dans l’industrie sur les machines de production, comme en témoignent les articles [4 – 7], son utilisation reste cependant restreinte à un champ limité de domaines.\n\nEn effet, Prytz et al. [8] évoquent les difficultés à implémenter la maintenance prédictive dans l’industrie automobile du fait des limites des capteurs et des efforts nécessaires pour l’analyse des données recueillies. Le principal obstacle cité par les auteurs est l’impossibilité de concevoir des algorithmes de diagnostic capables de détecter différents types de défaillances qui seraient applicables à une large palette de configurations de véhicules dont l’utilisation est singulière à chacun. Une modélisation unique des véhicules devient alors impossible, d’autant plus que la complexité d’un système dynamique, tel qu’un véhicule, complique cette étude.\n\nPar ailleurs, de nombreux types de défaillances peuvent survenir dans un véhicule et les capteurs existants ne permettent d’en détecter qu’un seul type chacun [2].\nLes capteurs acoustiques, par exemple, permettent de détecter des défauts mécaniques, des problèmes électriques ou des fuites grâce aux ultrasons. Les capteurs thermiques, quant à eux, mesurent les variations de température pour déceler des anomalies. Enfin, les capteurs vibratoires servent à repérer les problèmes mécaniques tels qu’un mauvais serrage de vis ou le mauvais alignement d’un composant.\n\nUn autre verrou considérable, encore non adressé, réside dans les relations qui existent entre de multiples grandeurs mesurables dans un véhicule. En effet, si le Big Data permet la récupération et le stockage d’une quantité indicible d’informations, la réelle difficulté réside dans l’identification des données pertinentes parmi la masse de données superflue, c’est ce que l’on appelle la Small Data.\nUne fois ces données pertinentes identifiées, il convient également de les utiliser de manière intelligente, de façon à obtenir les résultats escomptés, c’est ce que l’on appelle la Smart Data.\nPrytz et al. [8] fournissent une première méthode de machine learning par apprentissage non supervisé visant à révéler des relations intéressantes entre des paramètres afin d’interpréter les données acquises par les capteurs. Prytz [9] souligne également l’intérêt du machine learning pour initialiser les modèles et les rendre auto-apprenants, mais en faisant appel à un apprentissage supervisé. Les données manipulées dans cet article proviennent d’une part de bases de données agrégées et d’autre part de données acquises en temps réel grâce à des capteurs disposés à bord du véhicule.\nSi ces deux articles [8, 9] fournissent les premières pistes de recherche explorées dans leurs laboratoires sur des systèmes prototypes, les autres acteurs du domaine gardent secrets leurs avancements. Avec les avènements du Big Data et les évolutions des capteurs capables de détecter des niveaux de vibrations très subtils et étendus sur de larges spectres de gammes, de nombreuses entreprises se lancent dans des projets de R&D visant à concevoir des méthodes de maintenance prédictive. Nous pouvons notamment citer la SNCF [10] et Valéo [11] qui abordent ces problématiques dès 2017.\nEnfin, une étude menée par le cabinet McKinsey [12] estime que la maintenance prédictive permettra aux entreprises d’économiser 630 milliards de dollars d’ici 2025.\n1.2.2\tModélisation de la fiabilité \nL'évaluation de la fiabilité constitue le principal outil de soutien à l'effort d'ingénierie de la fiabilité. Elle intervient au début de la phase de conception préliminaire et sera ensuite mise à jour en permanence pendant tout le développement afin d'évaluer la satisfaction des exigences de fiabilité.\n\nLa MTBF (Mean Time Between Failure) des équipements électriques et mécaniques est évaluée sur leur CPL (Component Part List). Nous estimons le taux de défaillance de chaque composant utilisant : \n•\tLes données du client, \n•\t[MIL-HDBK-217F not. 2] pour les composants électriques,\n•\t[NPRD95] pour les pieces non électriques,\n•\tDes outils de prévision dédicacés (évaluation des contraintes ou analyse mécanique).\n\nCe sont ces outils de prévision, l’évaluation de la fiabilité de ces données ainsi que des estimations des erreurs de mesures qui constituent une grande partie de nos travaux de recherche. \n\nA partir du taux de défaillance des équipements (λ), nous pouvons évaluer la MTBF du système par : \n \n\nPour un équipement, le lien entre le taux de défaillance λ et la MTBF est : \n\n \n\nPour la modélisation des données de fiabilité, les lois de probabilité sont couramment utilisées, notamment : \n•\tLa distribution de Weibull est une des distributions les plus couramment utilisées pour la modélisation de durées de vie. Cette distribution est facile à interpréter et est extrêmement polyvalente. Grâce à la valeur estimée de son paramètre de forme, β, vous pouvez modéliser les caractéristiques des différentes phases de la durée de vie d’’une pièce.\n•\tLorsque la distribution de Weibull a un paramètre de forme de 2, elle est identique à la distribution de Rayleigh. Cette distribution est souvent utilisée pour analyser les mesures dans le domaine de l'ingénierie des communications, pour décrire le bruit en sortie de certains récepteurs de transmission. Cette distribution est aussi couramment utilisée pour l’analyse des durées de vie de soupapes de dépression.\n•\tLa distribution exponentielle est une distribution simple avec un seul paramètre et est couramment utilisée pour la modélisation de durées de vie. Cette distribution est, en fait, un cas particulier de la distribution de Weibull (distribution de Weibull avec β = 1). Une propriété importante de la distribution exponentielle, est son absence de mémoire, ainsi, la durée de vie restante d'un composant est indépendante de son âge actuel.\n•\tLa distribution log‐normale est une distribution flexible étroitement liée à la distribution normale. Cette distribution est particulièrement utile pour la modélisation de données à peu près symétriques ou asymétriques vers la droite. Comme la distribution de Weibull, la distribution log‐normale peut prendre des allures très différentes en fonction de son paramètre de forme.\n•\tLa distribution de la plus petite valeur extrême est une distribution limite du minimum d'un très grand nombre d'observations aléatoires issues d’une même distribution arbitraire. La théorie des valeurs extrêmes est un modèle utile dans les situations où de nombreux phénomènes identiques et indépendants peuvent conduire à une défaillance et c’est la première défaillance qui déterminera la durée de vie de l’ensemble du système.\n•\tLa distribution normale : Cette distribution, symétrique et en forme de cloche, est très largement utilisée dans les statistiques. Les applications industrielles génèrent souvent des données normalement distribuées. Historiquement, la distribution normale n'a pas été aussi couramment utilisée pour analyser les durées de vie que d'autres distributions, sa queue à gauche s'étend vers l'infini négatif, ce qui pourrait conduire à des estimations (erronées) de durées de vie négatives.\n\nCes lois de distributions sont étudiées et comparées dans [13].\n1.2.3\tModélisation de l’évolution de dégradations \nOn distingue trois types de modèles pour décrire l’évolution des dégradations (Tableau 1).\n1.\tLe premier est fiabiliste. L’état de dégradation y est représenté par le taux de défaillance. L’état général du composant est décrit par le risque de défaillance à chaque instant. Cette modélisation est particulièrement utilisée (dans la fiabilité des structures) pour les composants actifs. Ils se construisent à partir d’une analyse de durées de vie, lorsque des données de défaillance sont disponibles. [14]\nLes deuxièmes et troisièmes modèles décrivent la dégradation réelle (physique) du composant. Ils concernent essentiellement les composants passifs. Ces modèles nécessitent l’identification d’un mécanisme physique responsable de la dégradation.\n2.\tLa seconde modélisation est physique. Elle s’obtient à partir de la connaissance physique du mécanisme de dégradation. Soit on a une loi physique, soit on estime les paramètres de cette loi physique à partir des données d’essai ou de données de retour d’expérience. [15]\n3.\tLe troisième modèle est statistique. On cherche à faire correspondre une loi mathématique aux données obtenues lors des inspections. Le présent rapport concerne particulièrement ce type de modélisation. [16]\n\n \nTableau 1 : Méthodes de modélisation des dégradations\nDes outils d’adaptation de ces modèles au contexte industriel sont étudiés dans l’article de K. SIMOLA et M. PULKKINEN [17]. Une estimation de l’erreur de mesure à partir de modèles de répartition de taille de défauts est proposée, ainsi qu’un modèle de PDD (Probabilité De Détection d’un défaut). \nL’estimation de l’erreur présentée est basée sur le modèle de taille de défaut. Deux modèles sont étudiés : le modèle lognormal et le modèle logit.\nSi a est la profondeur du défaut, â la profondeur mesurée, et d l’épaisseur du tube, on évalue :\n           \n \nCes modèles permettent une approximation de l’erreur due à l’instrument de mesure. Ils nécessitent cependant un échantillon étalon de référence. De plus le modèle logit n’est pas simple à utiliser mathématiquement, car on ne connaît pas l’espérance, ni l’écart-type.\nParticulièrement adaptées à notre cas, nous avons fait fort utilisation de ces recherches. En effet, n’ayant pas de modèles adaptés aux matériaux ou au contraintes innovantes rencontrés, nous avons dû mobiliser cet état de l’art, ainsi que de nombreuses campagnes de mesures dans le but de modéliser la fiabilité de ces systèmes et développer des abaques complètes. \nAussi, nous avons développé une méthodologie de prise en compte des erreurs de mesures lors de la création d’un modèle de dégradation.  "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• La maintenance prédictive consiste en une maintenance préventive conditionnelle, déclenchée par un événement prédéterminé (autodiagnostic, capteur, mesure) ou l'analyse de paramètres significatifs de dégradation [1].\n• Elle permet de retarder et planifier les interventions, fixant un seuil d'alarme avant le seuil d'admissibilité, maximisant ainsi l'utilisation des entités tout en réduisant la maintenance corrective.\n• Contrairement à la maintenance corrective, la maintenance prédictive repose principalement sur des activités de surveillance.\n• L'objectif est d'estimer la limite de bon fonctionnement d'un système et d'intervenir avant la panne, en identifiant les paramètres pertinents indiquant un dysfonctionnement au-delà d'un certain seuil [2].\n• Les enjeux incluent l'identification des paramètres à relever, le stockage et la structuration des données, l'établissement de seuils de référence, la reconnaissance des dérives, la modélisation des schémas de panne, et l'amélioration des analyses avec des systèmes intelligents auto-apprenants [3].\n• Bien que l'analyse prédictive soit utilisée dans l'industrie, son application dans l'automobile présente des obstacles tels que les limitations des capteurs et la complexité des systèmes dynamiques, empêchant la création d'algorithmes de diagnostic universels [4-7].\n• Les capteurs existants ne peuvent détecter qu'un seul type de défaillance, par exemple, les capteurs acoustiques pour les défauts mécaniques, les capteurs thermiques pour les anomalies de température, et les capteurs vibratoires pour les problèmes de serrage ou d'alignement [2].\n• Un autre obstacle est la complexité des relations entre les paramètres mesurables dans un véhicule, nécessitant des méthodes de machine learning pour les interpréter. Deux approches existent : apprentissage non supervisé pour révéler des relations intéressantes [8], et apprentissage supervisé pour initialiser et améliorer les modèles [9].\n• Ces méthodes sont encore à l'état de recherche, et les avancées des entreprises restent largement confidentielles. Toutefois, des projets de R&D, notamment à la SNCF [10] et Valéo [11], explorent des méthodes de maintenance prédictive avec l'essor du Big Data et l'amélioration des capteurs.\n\n• La modélisation de l'usure est essentielle pour prédire la durée de vie des pièces et le comportement des systèmes industriels complexes.\n• Damien Herisson [12] souligne que les connaissances actuelles en tribologie sont insuffisantes pour développer des lois de comportement complexe.\n• Meng et Ludema [13] ont recensé 182 lois d'usure publiées entre 1957 et 1992, mais ces lois présentent des limites, notamment la non prise en compte des matériaux et la complexité des paramètres nécessitant de nombreux essais.\n• Dans notre étude R&D, nous utilisons la loi d'Archard [14] et la loi énergétique [15-17], reconnues pour leur pertinence, mais leur application reste difficile en raison de la complexité des phénomènes en jeu [18-19].\n• Une difficulté non résolue est la traduction de ces lois en formules applicables aux situations spécifiques étudiées, notamment en raison du caractère non-linéaire et stochastique des phénomènes [20].\n• Pour les bandes carbone des pantographes, une autre difficulté réside dans la variation du contact entre les éléments, rendant ces lois inapplicables et nécessitant des solutions originales.\n• D'autres approches, comme les lois empiriques locales ou la modélisation thermodynamique de l'irréversibilité, sont inadaptées à notre situation car nous ne pouvons accéder aux données en local à chaque sollicitation [21-24].\n• La thèse de Hana Jerbi [25] propose une approche qualitative basée sur le modèle de Godet [26], mais elle ne permet pas une estimation chiffrée de l'usure.\n\n• Le système d'embrayage assure la liaison entre le groupe motopropulseur et la transmission, permettant une transmission progressive du couple sans à-coups, évitant ainsi l'inconfort et les ruptures de la chaîne cinématique [27].\n• Un embrayage est composé d'un disque de friction et d'un mécanisme (couvercle, diaphragme, plateau) fixé au volant moteur, actionné par une butée déplacée par une fourchette.\n• Une thèse [28] a étudié les phénomènes vibratoires à l'origine du crissement de l'embrayage, mais sans considérer la dégradation des organes du système.\n\n• Les bandes en carbone des pantographes assurent la transmission électrique et la lubrification à sec entre l'archet et la ligne dans la chaîne de traction des équipements ferroviaires [29].\n\n• Les brosses de contact, constituées de lames métalliques planes et flexibles, sont utilisées pour la transmission d'informations ponctuelles dans les systèmes ferroviaires en établissant un contact électrique avec une pièce disposée dans l'axe de la voie [30-31]."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tSmart Maintenance\nLa maintenance prédictive, ou préventive conditionnelle consiste en une opération de maintenance préventive subordonnée à un type d'événement prédéterminé [1] (autodiagnostic, information d'un capteur, mesure, etc.) ou à l'analyse de l'évolution surveillée de paramètres significatifs de la dégradation et de la baisse de performance d'une entité.\nCela permet de « retarder » et de planifier les interventions. Cet événement permet de fixer un seuil d'alarme avant un seuil d'admissibilité. Le principal intérêt d'une telle stratégie est de pouvoir utiliser les entités au maximum de leur possibilité mais aussi de diminuer le nombre des opérations de maintenance corrective. Contrairement à la maintenance corrective, les activités de la maintenance prédictive sont principalement basées sur des activités de surveillance.\n\nL’enjeu de la maintenance prédictive ou prévisionnelle est d’estimer au plus juste la limite de bon fonctionnement d’un système afin de procéder aux correctifs nécessaires avant qu’il ne tombe en panne. Le défi de cette science est de savoir relever les paramètres d’intérêt, dont les variations sont significatives d’un dysfonctionnement au-delà d’un certain seuil [2].\n\nLes enjeux de cette nouvelle forme de maintenance des équipements sont multiples [3] :\n\tIdentifier les paramètres à relever ;\n\tStocker et structurer les grandes masses de données acquises ;\n\tEtablir des seuils de référence de bon fonctionnement ;\n\tReconnaitre une dérive significative ;\n\tSavoir quels paramètres agissent indépendamment ou de manière couplée sur le fonctionnement d’un composant d’intérêt ;\n\tModéliser les schémas de panne ;\n\tAméliorer les analyses à l’aide de systèmes intelligents auto-apprenants.\n\nSi l’analyse prédictive est utilisée depuis longtemps dans l’industrie sur les machines de production, comme en témoignent les articles [4 – 7], son utilisation dans le secteur automobile pose encore des verrous non adressés à ce jour.\nEn effet, [8] évoque les difficultés à implémenter la maintenance prédictive dans l’industrie automobile du fait des limites des capteurs et des efforts nécessaires pour l’analyse des données recueillies. Le principal obstacle cité par les auteurs est l’impossibilité de concevoir des algorithmes de diagnostic capables de détecter différents types de fautes qui seraient applicables à une large palette de configurations de véhicules dont l’utilisation est singulière à chacun. Une modélisation unique des véhicules devient alors impossible, d’autant plus que la complexité d’un système dynamique, tel qu’un véhicule, complique cette étude.\n\nPar ailleurs, de nombreux types de défaillances peuvent survenir dans un véhicule et les capteurs existants ne permettent d’en détecter qu’un seul type chacun [2].\nLes capteurs acoustiques, par exemple, permettent de détecter des défauts mécaniques, des problèmes électriques ou des fuites grâce aux ultrasons. Les capteurs thermiques, quant à eux, mesurent les variations de température pour déceler des anomalies. Enfin, les capteurs vibratoires servent à repérer les problèmes mécaniques tels qu’un mauvais serrage de vis ou le mauvais alignement d’un composant.\n\nUn autre verrou considérable, encore non adressé, réside dans les relations qui existent entre de multiples grandeurs mesurables dans un véhicule. L’article [8] fournit une première méthode de machine learning par apprentissage non supervisé visant à révéler des relations intéressantes entre des paramètres afin d’interpréter les données acquises par les capteurs. La publication [9] souligne également l’intérêt du machine learning pour initialiser les modèles et les rendre auto-apprenants, mais en faisant appel à un apprentissage supervisé. Les données manipulées dans cet article proviennent d’une part de bases de données agrégées et d’autre part de données acquises en temps réel grâce à des capteurs disposés à bord du véhicule.\nSi ces deux articles [8, 9] fournissent les premières pistes de recherche explorées dans leurs laboratoires sur des systèmes prototypes, les autres acteurs du domaine gardent secrets leurs avancements. Avec les avènements du Big Data et les évolutions des capteurs capables de détecter des niveaux de vibrations très subtils et étendus sur de larges spectres de gammes, de nombreuses entreprises se lancent dans des projets de R&D visant à concevoir des méthodes de maintenance prédictive. Nous pouvons citer la SNCF [10] et Valéo [11] qui abordent ces problématiques en 2017.\n1.2.2\tModélisation de l’usure\nLes problématiques de durabilité et de fiabilité des structures sont à l’origine d’une nécessité d’étudier les phénomènes de dégradation afin de pouvoir prédire les comportements et estimer la durée de vie des pièces. La complexité des systèmes industriels et les enjeux associés au fonctionnement des différentes pièces posent ainsi le cadre pour les besoins de modélisation de l’usure.\n\nDans sa thèse sur les méthodes de dimensionnement à l’usure de systèmes de transmission, Damien Herisson [12] mentionne que « les connaissances actuelles en tribologie ne permettent pas de développer des lois de comportement complexe. »\n\nDans l’ouvrage [13], Meng et Ludema font l’inventaire des lois d’usure ayant été développées entre 1957 et 1992 et publiées dans les revues scientifiques ’Wear’ et ’Wear of Materials’. L’inventaire est effectué sur 5466 publications desquelles ils tirent 182 lois pour la modélisation de l’usure.\nCertaines de ces lois sont dérivées de la mécanique des milieux continus, les limites que nous avons identifiées pour l’utilisation de ces lois pour nos problématiques de définition de lois d’usure dans les contextes étudiés sont multiples.\nEn effet, une première limite porte sur la non prise en compte des matériaux mis en jeu dans de nombreux modèles.\nUne seconde limite tient à la complexité de certaines lois qui nécessitent, pour établir des ordres de grandeur des paramètres mis en jeu, de nombreux essais.\n\nDans le cadre de notre étude R&D, pour adresser les problématiques d’usure du système d’embrayage notamment, nous avons choisi de retenir la loi d’Archard [14] et la loi énergétique [15 – 17]. Ces deux lois ont été largement reconnues comme les plus pertinentes pour modéliser un certain nombre de phénomènes. Elles ont par ailleurs fait l’objet d’analyses particulières dans divers contextes visant à confronter la capacité de chacune à rendre compte de l’usure induite par les phénomènes mis en jeu [18, 19].\n\nLa difficulté qui reste non adressée dans la littérature porte sur la traduction de ces lois d’usure en formules pour les situations que nous souhaitons étudier. En effet, Pierre-Etienne Gautier mentionne dans son article [20] les problèmes de modélisation mécanique dans le domaine ferroviaire, notamment en raison du caractère non-linéaire et stochastique des phénomènes.\nDans le cas des bandes carbone des pantographes que nous étudions, une difficulté supplémentaire, qui rend ces deux lois inapplicables est liée à la variation du contact ente les éléments mis en jeu. Nous devrons ainsi concevoir des solutions originales qui nous permettront d’atteindre nos objectifs de modélisation de l’usure.\n\nD’autres approches existent, s’appuyant sur des formulations locales de lois empiriques développées dans un cadre expérimental. Ces lois ne sont pas pertinentes dans notre situation puisque nous ne pouvons accéder aux données en local à chaque sollicitation des organes.\nL’une de ces approches, phénoménologique, s’appuie sur des mesures expérimentales pour donner lieu à une relation entre la quantité de matière enlevée de la surface de contact après usure et une variable dépendante du chargement appliqué [21, 22].\nUne autre approche consiste à traduire thermodynamiquement le principe d’irréversibilité de la transformation en formulant les modes de dissipation de l’énergie introduite dans le système et tient compte de phénomènes mécaniques aussi complexes que l’usure [23, 24].\n\nEnfin, dans sa thèse, Hana Jerbi [25] présente une approche qualitative de l’usure, s’appuyant sur le modèle de Godet [26]. Si cette approche présente un intérêt pour illustrer le phénomène par lequel deux corps en contact donnent lieu à un troisième corps, situé à l’interface des deux et constitué des débris des deux corps en contact, son aspect qualitatif ne nous permet pas d’estimer l’usure de manière chiffrée.\n1.2.3\tSystème d’embrayage\nLe système d’embrayage, avec la boite de vitesses et les arbres de transmission sont les trois organes constitutifs de la transmission d’un véhicule, celle-ci assurant une fonction d’acheminement du couple depuis le groupe motopropulseur jusqu’aux roues.\n\nL’embrayage assure la liaison entre le groupe motopropulseur et la transmission. La mise en charge de l’accouplement se fait de manière progressive via sa transmission par adhérence qui permet d’éviter les à-coups, sources d’inconfort et de la rupture des éléments de la chaîne cinématique [27].\n\nUn embrayage est constitué d’un disque de friction et d’un mécanisme. Ce dernier se compose lui-même d’un couvercle, d’un diaphragme et d’un plateau. L’ensemble est fixé au volant moteur. Le mécanisme est actionné par la butée, déplacée par la fourchette. L’ensemble mécanique est représenté sur la Figure 1.\n \nFigure 1 : Vue éclatée d'un embrayage [source : 28]\nDans le cadre de nos recherches, nous avons identifié un travail de thèse [28], qui s’intéresse de manière spécifique aux phénomènes à l’origine du crissement de l’embrayage d’une voiture.\nCe travail ne s’intéresse cependant qu’aux phénomènes vibratoires à l’origine de la défaillance qui se retranscrit par le bruit de crissement. En particulier, la dégradation des différents organes du système d’embrayage n’est pas considérée.\n1.2.4\tBande de pantographe\nLes bandes en carbone (cf. Figure 2) que nous souhaitons étudier sont montées sur un archet qui, avec le pantographe est impliqué dans la chaîne de traction de certains équipements ferroviaires (trains, tramways).\n\nEn effet, la chaîne de traction comprend [29] :\n\tla prise de courant par pantographe avec archet muni de bandes de carbone ;\n\tle parafoudre ;\n\tle disjoncteur principal (DJ) qui met l’ensemble des équipements sous tension, mais s’ouvre automatiquement en cas de court-circuit pour protéger le reste des composants ;\n\tle convertisseur statique (CVS) qui peut produire, à partir du 750 V= de la ligne de contact, des tensions alternatives triphasées ou continues, d’amplitude variable ;\n\tles moteurs de traction ;\n\tles balais de retour du courant de ligne et de mise au potentiel des rails des véhicules, chacune de ces fonctions étant assurée par des dispositifs distincts logés dans les roues.\n\n \nFigure 2 : Schématisation de la bande de pantographe [source : catalogue MERSEN Réf 3-54201]\nLa bande carbone présente sur cet équipement assure à la fois la transmission grâce au caractère conducteur électrique du matériau et la lubrification à sec du frottement entre l’archet sur lequel elle est montée et la ligne.\n1.2.5\tBrosse de contact\nLes brosses de contact sont des dispositifs de répétition des signaux de chemins de fer à bord des machines. Elles sont principalement utilisées pour la transmission d’informations ponctuelles [30].\nLes brosses sont fixées sous la machine et établissent un contact électrique avec une pièce dite « crocodile » (cf. Figure 3 (a)) disposée dans l’axe de la voie, ce contact complétant des circuits électriques dont la fermeture déclenche le fonctionnement d’appareils appropriés.\n\nDans ces dispositifs, la brosse de machine est constituée de lames métalliques planes et flexibles (cf. Figure 3 (b)), renforcées par des contre-lames et dont la partie rectiligne inférieure vient frotter au passage sur le crocodile [31].\n  \nFigure 3 : (a) crocodile sur ligne | (b) brosse de contact à lames"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Les équipements deviennent de plus en plus complexes, impliquant de nombreux composants avec des durées de vie variables, ce qui nécessite une stratégie de maintenance pour une utilisation optimale [1].\n• La Commission Electrotechnique Internationale définit la maintenance comme un ensemble d’actions techniques et administratives visant à maintenir ou rétablir une entité dans un état spécifié ou des conditions sûres de fonctionnement (disponibilité, fiabilité, maintenabilité, sécurité) [1].\n\n• La maintenance préventive est une opération effectuée avant la détection d'une défaillance à des intervalles prédéterminés ou selon des critères prescrits, visant à réduire la probabilité de défaillance ou la dégradation du fonctionnement [2].\n• Objectifs de la maintenance préventive [3] :\n  - Réduire les coûts de défaillance ;\n  - Augmenter la fiabilité des machines ;\n  - Améliorer la disponibilité des ateliers de production ;\n  - Augmenter la durée de vie efficace des machines ;\n  - Améliorer l'ordonnancement des travaux ;\n  - Faciliter la gestion des stocks ;\n  - Assurer la sécurité.\n\n• La maintenance corrective est une opération effectuée après la détection d'une défaillance pour remettre une entité en état de fonctionnement, au moins provisoirement [1].\n• Les étapes de la maintenance corrective [4] :\n  - Test : comparaison avec une référence ;\n  - Détection : identification de la défaillance ;\n  - Localisation : identification des éléments défaillants ;\n  - Diagnostic : analyse des causes de la défaillance ;\n  - Dépannage/réparation : remise en état ;\n  - Contrôle : vérification du bon fonctionnement ;\n  - Amélioration éventuelle ;\n  - Historique : enregistrement de l'intervention.\n• Deux types de maintenance corrective [2] :\n  - Curative : rétablissement permanent ;\n  - Palliative : solution provisoire.\n\n• La maintenance prédictive, ou préventive conditionnelle, est une opération de maintenance préventive basée sur un événement prédéterminé ou l'analyse de l'évolution des paramètres significatifs [2].\n• Elle permet de retarder et de planifier les interventions, en fixant un seuil d'alarme avant un seuil d'admissibilité, optimisant l'utilisation des entités et réduisant la maintenance corrective.\n• La maintenance prédictive repose principalement sur des activités de surveillance, visant à estimer la limite de bon fonctionnement d’un système et à intervenir avant la panne [5].\n• Enjeux de la maintenance prédictive [6] :\n  - Identifier les paramètres à relever ;\n  - Stocker et structurer les données ;\n  - Établir des seuils de référence ;\n  - Reconnaître les dérives significatives ;\n  - Comprendre l'impact des paramètres indépendants et couplés ;\n  - Modéliser les schémas de panne ;\n  - Utiliser des systèmes intelligents auto-apprenants.\n• Les limites de la maintenance prédictive dans l’industrie automobile incluent les limites des capteurs et la difficulté de concevoir des algorithmes universels de diagnostic pour diverses configurations de véhicules [11].\n• Les capteurs actuels ne peuvent détecter qu'un type de défaillance chacun (acoustique pour défauts mécaniques, thermique pour anomalies de température, vibratoire pour problèmes de serrage ou d'alignement) [5].\n• Un verrou non résolu est la complexité des relations entre les paramètres mesurables dans un véhicule, nécessitant des méthodes de machine learning pour les interpréter [11, 12].\n• SNCF [13] et Valéo [14] explorent des méthodes de maintenance prédictive avec l'essor du Big Data et l'amélioration des capteurs.\n\n• Le protocole CAN (Control Area Network) est un protocole de communication série développé par Bosch pour l’industrie automobile, permettant la communication en temps réel avec un haut niveau de fiabilité [15].\n• Il existe trois bus CAN dans un véhicule, chacun à un débit différent :\n  - Bus rapide pour la sécurité (freinage, ABS, airbags) ;\n  - Bus à vitesse moyenne pour le moteur (commandes et capteurs) ;\n  - Bus lent pour les accessoires (lampes, moteurs d’asservissement) [16].\n• Les constructeurs automobiles sont méfiants à l'égard des équipements connectés aux véhicules, chacun utilisant des moyens de connexion et méthodes de codage spécifiques [17].\n\n• La gestion des données des véhicules est un obstacle majeur pour le développement de services en temps réel connectés aux véhicules [18].\n• Les deux défis principaux de la gestion des données sont [19] :\n  - Le stockage des données : les solutions traditionnelles basées sur serveur ne répondent plus aux besoins actuels ;\n  - Le traitement des données : les architectures traditionnelles ne sont pas adaptées pour le traitement de grandes quantités de données, posant des défis en termes de temps et de bande passante.\n• Les systèmes de gestion des données de véhicules (VDMS) ne répondent pas encore pleinement aux besoins actuels, bien que des solutions telles que celle utilisée pour un véhicule spatial offrent des perspectives [20].\n• Toyota se concentre sur le développement de systèmes pour la Smart Mobility, incluant VDMS, TDMS (Traffic Data Management System) et EDMS (Energy Data Management System) [21].\n• Un système VDMS visant à fournir des données de véhicules en circulation aux constructeurs, basé sur un relevé des états des ECU et une communication sans fil, présente des opportunités mais reste limité aux constructeurs spécifiques [22].\n\n• Tan Do [23] analyse le système de freinage en ligne droite pour un véhicule léger, sans inclure les plaquettes de frein, mais offre un modèle pour estimer l'influence des paramètres lors du freinage.\n• Ayadi [24] étudie la dynamique de dégradation des systèmes industriels pour déterminer les meilleures opérations de maintenance préventive, mais ne traite pas spécifiquement de l'usure des plaquettes de frein.\n• Crowder & Lawless [25] s'intéressent aux mécanismes d'usure pour développer des politiques de maintenance prédictive, mais n'abordent pas spécifiquement les systèmes de freinage ou les plaquettes de frein.\n• La thèse de Belhocine [29] étudie le comportement thermomécanique des disques de frein pour prédire leur tenue en fatigue, mais ne traite pas de la maintenance des plaquettes de frein.\n• Une méthode de caractérisation morphologique de l’usure des freins [30] s'intéresse aux plaquettes de frein, mais repose sur une méthode topographique nécessitant des photos haute définition, ce qui n'est pas adapté à nos besoins.\n• Un guide d'aide à l'estimation et à la validation de la fiabilité automobile [31] analyse la fiabilité des plaquettes de frein, mais ne prend en compte que la pression exercée sur elles, sans examiner d'autres facteurs tels que la distance de freinage.\n• De nombreuses études sur le crissement des freins [32-35] et l'usure des disques de freins [36-38] n'étudient pas les plaquettes de frein en détail.\n• Des études dans l'industrie ferroviaire sur l'usure et la maintenance des systèmes [39-41] ont enrichi notre compréhension théorique de la modélisation des contacts et de l'usure.\n\n• Les systèmes télématiques pour véhicules connectés se multiplient, mais aucune solution n'existe encore pour l'estimation de l'usure des équipements automobiles [42-44].\n• Le boitier Xee [42] offre des fonctionnalités de localisation GPS et d'alerte en cas de problème, tandis que le boitier Drust [43] propose des solutions pour diverses industries mais sans inclure la maintenance prédictive pour les freins.\n• Le boitier D-Car [44] développé avec Dunasys permet l'enregistrement, la transmission et le pilotage de données pour réseaux CAN, et peut être utilisé dans divers modes de transport, incluant des fonctionnalités de communication à distance par GPRS."}, {"role": "assistant", "content": "2.2\tÉtat de l’art\n2.2.1\tMaintenance des équipements\nLes équipements étant de plus en plus complexes font intervenir de nombreux composants dont la durée de vie peut être variable. Une stratégie de maintenance s’impose alors pour assurer une utilisation optimale des équipements. Cette stratégie relève de l’ensemble des décisions qui conduisent à définir les politiques de maintenance à adopter (présentées ci-après) et l’organisation structurelle associée pour atteindre les objectifs fixés.\nLa Commission Electrotechnique Internationale définit la maintenance comme un ensemble d’actions techniques et administratives destinées à maintenir (maintenance préventive) ou à rétablir (maintenance corrective) une entité dans un état spécifié ou dans des conditions données de sûreté de fonctionnement (disponibilité, fiabilité, maintenabilité et sécurité) lui permettant d’accomplir une fonction requise [1].\n.2.2.1.1\tMaintenance préventive\nMonchy [2] définit la maintenance préventive comme une opération de maintenance effectuée avant la détection d'une défaillance d'une entité, à des intervalles prédéterminés ou selon des critères prescrits (suite à l'analyse de l'évolution surveillée de paramètres significatifs) et destinée à réduire la probabilité de défaillance d'une entité ou la dégradation du fonctionnement d'un service rendu.\nL'intervention préventive sert à améliorer l'état de l'élément. Par conséquent, seules les défaillances progressives sont prises en compte ici. Une politique de maintenance préventive a pour objectifs [3] :\n-\tRéduire les coûts de défaillance ;\n-\tAugmenter la fiabilité d'une machine ;\n-\tAméliorer la disponibilité de l'atelier de production ;\n-\tAugmenter la durée de vie efficace d'une machine ;\n-\tAméliorer l'ordonnancement des travaux ;\n-\tFaciliter la gestion des stocks ;\n-\tAssurer la sécurité, etc.\n.2.2.1.2\tMaintenance corrective\nLa Commission Electrotechnique Internationale [1] définit la maintenance corrective comme une opération de maintenance ou dépannage effectuée après la détection d'une défaillance et destinée à remettre une entité dans un état lui permettant d'accomplir une fonction requise, au moins provisoirement.\nSes actions s'effectuent par étapes [4] :\n-\tTest : comparaison des mesures avec une référence ;\n-\tDétection : déceler l'apparition d'une défaillance ;\n-\tLocalisation : les éléments par lesquels la défaillance se manifeste ;\n-\tDiagnostic : analyse des causes de la défaillance ;\n-\tDépannage, réparation : remise en état (avec ou sans modification) ;\n-\tContrôle : contrôle du bon fonctionnement ;\n-\tAmélioration éventuelle : éviter la réapparition de la panne ;\n-\tHistorique : mise en mémoire de l'intervention pour une exploitation ultérieure.\n\nOn distingue deux types de maintenances correctives [2] :\n-\tLa maintenance corrective curative, qui permet de rétablir un matériel ou une entité dans un état spécifié ou de lui permettre d'accomplir une fonction requise. Le résultat des activités réalisées doit présenter un caractère permanent.\n-\tLa maintenance corrective palliative, qui permet à un bien d'accomplir provisoirement tout ou partie d'une fonction requise.\n.2.2.1.3\tMaintenance prédictive / Smart Maintenance\nLa maintenance prédictive, ou préventive conditionnelle consiste en une opération de maintenance préventive subordonnée à un type d'événement prédéterminé [2] (autodiagnostic, information d'un capteur, mesure, etc.) ou à l'analyse de l'évolution surveillée de paramètres significatifs de la dégradation et de la baisse de performance d'une entité.\nCela permet de « retarder » et de planifier les interventions. Cet événement permet de fixer un seuil d'alarme avant un seuil d'admissibilité. Le principal intérêt d'une telle stratégie est de pouvoir utiliser les entités au maximum de leur possibilité mais aussi de diminuer le nombre des opérations de maintenance corrective. Contrairement à la maintenance corrective, les activités de la maintenance prédictive sont principalement basées sur des activités de surveillance.\n\nL’enjeu de la maintenance prédictive ou prévisionnelle est d’estimer au plus juste la limite de bon fonctionnement d’un système afin de procéder aux correctifs nécessaires avant qu’il ne tombe en panne. Le défi de cette science est de savoir relever les paramètres d’intérêt, dont les variations sont significatives d’un dysfonctionnement au-delà d’un certain seuil [5].\n\nLes enjeux de cette nouvelle forme de maintenance des équipements sont multiples [6] :\n-\tIdentifier les paramètres à relever ;\n-\tStocker et structurer les grandes masses de données acquises ;\n-\tEtablir des seuils de référence de bon fonctionnement ;\n-\tReconnaitre une dérive significative ;\n-\tSavoir quels paramètres agissent indépendamment ou de manière couplée sur le fonctionnement d’un composant d’intérêt ;\n-\tModéliser les schémas de panne ;\n-\tAméliorer les analyses à l’aide de systèmes intelligents auto-apprenants.\n\nSi l’analyse prédictive est utilisée depuis longtemps dans l’industrie sur les machines de production, comme en témoignent les articles [7, 8, 9, 10], son utilisation dans le secteur automobile pose encore des verrous non adressés à ce jour.\nEn effet, [11] évoque les difficultés à implémenter la maintenance prédictive dans l’industrie automobile du fait des limites des capteurs et des efforts nécessaires pour l’analyse des données recueillies. Le principal obstacle cité par les auteurs est l’impossibilité de concevoir des algorithmes de diagnostic capables de détecter différents types de fautes qui seraient applicables à une large palette de configurations de véhicules dont l’utilisation est singulière à chacun. Une modélisation unique des véhicules devient alors impossible, d’autant plus que la complexité d’un système dynamique tel qu’un véhicule complique cette étude.\n\nPar ailleurs, de nombreux types de défaillances peuvent survenir dans un véhicule et les capteurs existants ne permettent d’en détecter qu’un seul type chacun [5].\nLes capteurs acoustiques, par exemple, permettent de détecter des défauts mécaniques, des problèmes électriques ou des fuites grâce aux ultrasons. Les capteurs thermiques, quant à eux, mesurent les variations de température pour déceler des anomalies. Enfin, les capteurs vibratoires servent à repérer les problèmes mécaniques tels qu’un mauvais serrage de vis ou le mauvais alignement d’un composant.\n\nUn autre verrou considérable, encore non adressé, réside dans les relations qui existent entre de multiples grandeurs mesurables dans un véhicule. [11] fournit une première méthode de machine learning par apprentissage non supervisé visant à révéler des relations intéressantes entre des paramètres afin d’interpréter les données acquises par les capteurs. [12] souligne également l’intérêt du machine learning pour initialiser les modèles et les rendre auto-apprenants, mais en faisant appel à un apprentissage supervisé. Les données manipulées dans cet article proviennent d’une part de bases de données agrégées et d’autre part de données acquises en temps réel grâce à des capteurs disposés à bord du véhicule.\nSi ces deux articles [11, 12] fournissent les premières pistes de recherche explorées dans leurs laboratoires sur des systèmes prototypes, les autres acteurs du domaine gardent secrets leurs avancements. Avec les avènements du Big Data et les évolutions des capteurs capables de détecter des niveaux de vibrations très subtils et étendus sur de larges spectres de gammes, de nombreuses entreprises se lancent dans des projets de R&D visant à concevoir des méthodes de maintenance prédictive. Nous pouvons citer la SNCF [13] et Valéo [14] qui abordent ces problématiques en 2017.\n\n\n2.2.2\tVéhicules connectés\n.2.2.2.1\tProtocole CAN\nLe protocole CAN (Control Area Network) est un protocole de communication série qui supporte des systèmes temps réel avec un haut niveau de fiabilité [15]. Ses domaines d’application s’étendent des réseaux moyens débits aux réseaux de multiplexages faibles coûts.\nLe CAN se présente comme un bus de communication série développé à la fin des années 1980 par l’entreprise allemande Bosch. L’objectif était alors de fournir à l’industrie automobile un bus peu coûteux pour l’électronique embarquée [16]. En effet, la sécurité et le confort des utilisateurs dans les véhicules ont entrainé une augmentation des organes électroniques. Le câblage point à point est devenu très complexe, encombrant et lourd.\nLe multiplexage répond ainsi à ces limites en permettant la communication à tour de rôle sur le même câble par différents organes, grâce à une structure esclave et multi-maîtres.\nEn pratique, il y a trois bus CAN différents dans un véhicule, à des débits différents :\n-\tUn bus rapide pour gérer la sécurité (freinage, ABS, détection chocs, airbags...) ;\n-\tUn bus à vitesse moyenne pour gérer le moteur (commandes et capteurs) ;\n-\tUn bus lent pour gérer tous les accessoires (lampes, moteurs d’asservissements, boutons...).\n\nDu fait du grand nombre d’information transitant par ces canaux, les constructeurs sont très méfiants vis-à-vis des équipements destinés à connecter les véhicules [17], et chacun dispose de ses propres moyens de connexion et méthodes de codage spécifiques.\n\n.2.2.2.2\tGestion des données issues des véhicules\nAujourd’hui, le principal verrou à lever pour le développement de services en temps réel et connectés aux véhicules est la gestion des données, comme l’évoque l’article [18]. En effet, les quantités de données issues des véhicules sont particulièrement importantes et les approches de gestion de données traditionnelles peinent à relever les deux défis qui se posent :\n-\tLe stockage des données :\nLes solutions traditionnelles consistent à sauvegarder ces données sur de nombreux serveurs de stockage en réseau. Cependant, compte tenu des limitations de coûts et de performances afférentes aux systèmes basés sur serveur, cette solution ne répond plus aux besoins actuels ;\n-\tLe traitement des données :\nLes architectures traditionnelles ne sont pas adaptées pour les opérations de traitement de données à grande échelle qui sont requises pour tester les algorithmes par simulation. En effet, les pratiques actuelles posent deux défis fondamentaux. Le premier défi concerne les énormes volumes de données qui nécessitent d'être déplacés, ce qui requiert un temps considérable et une large bande passante. Le second défi concerne les postes de travail individuels, qui ne fournissent pas la puissance de calcul massive requise pour renvoyer les résultats de simulation dans un délai convenable [19].\n\nDes systèmes de gestion des données issues des véhicules se développent (Vehicle Data Management System, VDMS) pourtant ceux-ci ne répondent pas à nos problématiques.\nNos recherches de l’état de l’art nous ont permis d’identifier la solution décrite dans [20] qui consiste en un stockage des données issues d’un véhicule spatial. Ce système stocke les données issues de toutes les variables et interagit avec d’autres sous-systèmes afin de récupérer les données et les analyser. Dans notre cas, nous ne souhaitons relever que les grandeurs significatives pour les calculs de maintenance prédictive.\nL’article [21] rédigé par Toyota évoque le manque aujourd’hui de technologies pour le véhicule connecté. Toyota met ainsi ses efforts de R&D au service du développement de systèmes au service de la Smart Mobility, notamment en développant des solutions portant sur :\n-\tDes Systèmes de Gestion des Données Véhicule (VDMS) ;\n-\tDes Systèmes de Gestion des Données de Circulation (TDMS) ;\n-\tDes Systèmes de Gestion des Données Énergétiques (EDMS).\n\n[22] présente une initiative de VDMS visant à fournir des données de véhicules en circulation aux constructeurs afin d’améliorer les performances des diagnostics distants. Le système est basé sur un relevé des états des ECU (Electronic Control Unit) avant de les transmettre en utilisant une communication sans-fil avec un dispositif embarqué. Cette solution, bien que présentant les opportunités que présente un tel système, reste très cloisonnée aux constructeurs spécifiques.\n2.2.3\tÉtudes des dégradations des équipements de freinage\nDans sa publication, Tan Do [23] analyse l’ensemble du système de freinage en ligne droite pour un véhicule léger dans différentes conditions de circulation. Les plaquettes de freinage, en tant que sous-système ne sont pas considérées, mais cet article nous permet d’alimenter notre modèle pour l’estimation des influences des différents paramètres lors du freinage.\n\nLes travaux présentés par Ayadi [24] dans sa publication portent sur l’étude des systèmes industriels et notamment sur leur dynamique de dégradation en vue de décider des opérations de maintenance préventive les plus adéquates à entreprendre afin de limiter les défaillances de ces systèmes. Cette étude aborde du point de vue des métaheuristiques le développement d’une démarche de résolution générique d’un problème d’optimisation de la maintenance préventive des systèmes dynamiques multi-composants. L’étude aborde les processus de dégradation des systèmes multi-composants multi-états avant de les modéliser et d’intégrer cette approche de maintenance dans le modèle et la définition d’une politique de maintenance.\nCette étude, bien que très poussée sur les mécanismes de dégradation des dispositifs dynamiques ne nous renseigne pas sur la modélisation de l’usure des plaquettes de frein que nous souhaitons étudier.\nCette étude s’inscrit dans la lignée de Crowder & Lawless [25] qui s’intéresse de manière générale aux mécanismes d’usure afin de développer des politiques de maintenance prédictive. Cependant, les systèmes de freinage et le véhicule de manière générale n’est pas abordé. Ce document approche d’un point de vue financier les enjeux de la maintenance préventive. La maintenance est également abordée dans [26] et [27], mais du fait de la spécificité de l’organe automobile sur lequel nous nous concentrons, ces articles n’apportent pas de réponse aux verrous que nous devons lever. Une autre étude [28], qui s’intéresse à la modélisation de l’usure des organes en contacts avec revêtements ou non, a contribué à notre connaissance de l’état de l’art sur les modélisations des phénomènes d’usure sous chargement.\n\nDans sa thèse [29], Belhocine présente une étude du comportement thermomécanique des disques de frein automobiles pour la prédiction de leur tenue en fatigue. Une analyse des phénomènes thermiques opérant dans un disque de frein est d’abord réalisée. Celle-ci débouche sur une étude mécanique du contact sec entre le disque et les plaquettes de frein en fonction de différents paramètres se rapportant aux disques. Cette étude est en effet tournée vers les disques de freins et l’objectif premier du travail était de modéliser numériquement et réaliser des calculs par éléments finis en prenant en compte des algorithmes de gestion du contact avec frottement.\nCe travail s’inscrit plus généralement dans un objectif de dimensionnement en conception des systèmes de freinage, en prenant en compte les aspects thermiques imposant de nombreuses contraintes lors du choix des matériaux des systèmes de freinage. Cette étude ne nous servira pas de base puisque les problématiques relatives à la maintenance ne sont pas abordés, c’est plutôt le dimensionnement qui est pris en compte. De plus, les plaquettes de frein ne sont considérées que comme des éléments en contact avec les disques de freins, objets de l’étude.\n\nLa publication [30] présente une méthode de caractérisation morphologique de l’usure des freins, et s’intéresse particulièrement à l’usure des plaquettes de freins. Cependant, cette mesure de l’usure repose sur une méthode topographique, et nécessite que nous prenions une photo haute définition des plaquettes pour estimer leur dégradation. Cet article ne présente donc pas de solution viable à notre problématique qui consiste à concevoir un moyen permettant d’estimer l’usure des plaquettes sans devoir démonter d’équipements.\n\nL’article [31], qui se présente sous forme de Guide d’aide à l’estimation et à la validation de la fiabilité automobile s’intéresse à l’étude de la fiabilité d’une plaquette de frein. Le document présente une analyse des risques liés aux fonctions que doit rendre cet organe. Les mécanismes physiques de la défaillance de cet organe, les facteurs d’endommagements sont ensuite abordés. Cependant, ce document ne présente qu’un seul facteur d’endommagement : la pression exercée sur les plaquettes de freins. Il n’étudie pas l’influence de la distance de freinage, et nos travaux antérieurs nous ont permis de constater que la pression, sur un grand nombre de freinages ne présente pas d’impact significatif sur la durée de vie de cet équipement.\n\nDe nombreuses publications portent sur les phénomènes de crissement [32], [33], [34], [35]. Cependant ces études n’étudient pas les plaquettes de freins, ni leur dégradation. D’autres études se focalisent sur un autre organe contribuant au système de freinage : les disques de freins. La publication [36] en particulier, aborde d’un point de vue thermomécanique la modélisation de ces éléments. L’article [37] adopte un point de vue Matériaux pour étudier le système de freinage et la pollution associée au freinage. Cet article propose une étude de la toxicité des matériaux utilisés pour les systèmes de freinage, cependant aucune méthode quantitative n’appuie les estimations fournies sur les rejets toxiques et la consommation des plaquettes de frein en particulier. [38] s’intéresse également d’un point de vue tribochimique à l’usure due au frottement, et envisage le cas des additifs au molybdène pour réduire le frottement limite.\nEnfin, nous avons également parcouru des études portant sur l’usure des systèmes dans l’industrie ferroviaire prenant en compte des aspects de maintenance [39], [40]. Ces études présentaient pour nous des exemples qui sont venus enrichir les études théoriques de modélisation des contacts et de l’usure consultées, notamment dans l’ouvrage de Boher [41].\n2.2.4\tSolutions concurrentes\nDe nombreuses initiatives pour le véhicule connecté fleurissent, et il se développe ainsi des systèmes télématiques destinés à proposer de nouveaux services sur les véhicules. L’estimation de l’usure des équipements automobiles ne fait pas encore l’objet de développement de solution, mais nous pouvons citer les systèmes suivants.\nLe boitier Xee [42] propose des fonctionnalités de localisation GPS d’un véhicule, d’alerte aux proches si un problème survient ou si une effraction est détectée dans le véhicule.\nLe boitier Drust [43] propose quant à lui quatre solutions s’adressant à différentes industries (Après-vente automobile, Assurances, Mobilité urbaine, R&D des constructeurs). Ces solutions consistent en une plateforme d’intelligence, un accès aux données et enfin un drivelab permettant la mise à disposition d’un grand nombre de véhicules pour expérimenter à grande échelle des fonctionnalités nouvelles.\n\nCes deux boitiers ne proposent néanmoins pas de solutions pour la maintenance prédictive sur les organes de freinage.\nNous avons choisi de poursuivre les développements réalisés dans le cadre de notre projet avec la société Dunasys sur le boitier D-Car [44]. Ce système consiste en une solution d’enregistrement, transmission et pilotage de données pour réseau CAN utilisable sur tous les modes de transport (véhicule, camion, train, avion…). Ce boitier permet d’acquérir et générer des données CAN pour réseaux embarqués et communique à distance par GPRS les données à un serveur."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Les techniques de recherche et de filtrage booléennes actuelles sont insuffisantes pour répondre à la complexité de l'appariement candidat-emploi [1].\n• Les chercheurs envisagent l'utilisation de systèmes de recommandation pour mieux comprendre les exigences d'un poste et faire des suggestions pertinentes, une technique déjà largement acceptée dans divers domaines.\n\n• Les systèmes de recommandation, souvent appelés « Job Recommender Systems » (JRS) dans le contexte de la recherche d'emploi, sont classés en quatre catégories principales : filtrage collaboratif, filtrage basé sur le contenu, approches basées sur les connaissances, et systèmes hybrides [2].\n• Les systèmes JRS appliquent ces techniques pour améliorer l'appairage entre candidats et emplois :\n  - Recommandation basée sur le contenu (CBR) : recommande des éléments similaires à ceux déjà préférés par l'utilisateur [5].\n  - Filtrage collaboratif (CFR) : recommande des éléments en fonction des préférences d'utilisateurs similaires [6].\n  - Recommandation basée sur les connaissances (KBR) : utilise des règles et modèles basés sur des connaissances fonctionnelles pour faire des recommandations [7].\n  - Systèmes hybrides : combinent plusieurs techniques pour améliorer les performances et surmonter les limitations individuelles des méthodes.\n\n• CASPER est un exemple de système de recommandation d'emploi utilisant le filtrage collaboratif automatisé (ACF), où la similarité entre utilisateurs est basée sur les emplois qu'ils ont consultés ou postulés [6].\n• Les défis du filtrage collaboratif incluent la rareté des données, rendant difficile la détection de similitudes significatives entre utilisateurs, et la qualité variable des résultats de recherche, ce qui affecte la qualité des recommandations.\n• Farber et al. ont proposé un système hybride combinant le filtrage basé sur le contenu et le filtrage collaboratif pour surmonter le problème de rareté des données [8].\n• Malinowski et al. distinguent deux types de systèmes de recommandation d'emploi : « CV-recommender » pour les recruteurs et « job recommender » pour les demandeurs d'emploi [1].\n\n• Les grandes entreprises informatiques, confrontées à un débordement d'informations lors du traitement des candidatures, ont développé des systèmes pour faciliter le filtrage des CV.\n• IBM a créé PROSPECT, un système qui utilise un modèle de champ aléatoire conditionnel (CRF) pour segmenter et étiqueter les CV, améliorant ainsi l'extraction d'informations pertinentes pour la présélection des candidats [5].\n• HP a développé un système similaire, en mettant l'accent sur l'extraction d'informations et l'utilisation de données sémantiques pour améliorer la classification des CV, avec un apprentissage adaptatif pour découvrir de nouveaux termes [9].\n• Les systèmes de HP et IBM aident les entreprises à sélectionner des candidats, mais ne sont pas conçus pour aider les demandeurs d'emploi à trouver des postes.\n\n• Lu et al. ont utilisé l'analyse sémantique latente (LSA) pour calculer les similitudes entre les emplois et les candidats, mais n'ont pris en compte que deux facteurs : « intérêt » et « éducation » [12].\n• Xing et al. ont appliqué des modèles de pertinence structurés (SRM) pour faire correspondre les CV et les offres d'emploi [13].\n• Drigas et al. ont présenté un système expert utilisant des règles Neuro-Fuzzy pour évaluer la correspondance entre les profils d'utilisateurs et les offres d'emploi, nécessitant des données d'entraînement pour fonctionner [14].\n• Daramola et al. ont également proposé un système expert basé sur la logique floue pour le recrutement en ligne, utilisant une métrique de distance floue pour classer les candidats en fonction de leur éligibilité [15].\n• Yao et al. ont proposé un système hybride combinant les relations basées sur le contenu et les interactions pour améliorer la précision des recommandations [12].\n\n• Problèmes identifiés dans les systèmes de recommandation d'emploi existants :\n  - La plupart ne traitent que des données structurées, alors que CV et descriptions de poste sont souvent en formats non structurés.\n  - Les systèmes d'extraction d'informations sont principalement conçus pour les recruteurs, et non pour aider les demandeurs d'emploi.\n  - Les champs d'information dans les CV et descriptions de poste sont souvent trop grossiers, nécessitant une granularité plus fine pour améliorer la qualité des recommandations."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nD’après la littérature, les techniques de recherche et de filtrage booléennes actuelles ne peuvent satisfaire la complexité de l'exigence d'appariement candidat-emploi [1]. Les chercheurs espèrent que le système pourra comprendre les exigences du poste, déterminer quelles exigences sont obligatoires et lesquelles sont facultatives, mais préférables. Ils ont donc adopté la technique des systèmes de recommandation pour résoudre le problème du débordement d'informations. Les systèmes de recommandation sont largement acceptés dans divers domaines pour suggérer des produits, des services et des informations aux clients latents.\n\n1.2.1\tSystèmes de recommandation\nLa recherche d'emploi, qui a fait l'objet de certains sites Web commerciaux de recherche d'emploi et d'articles de recherche, n'est pas un nouveau sujet de recherche d'informations. Habituellement, les chercheurs les appelaient « Job Recommender Systems » (JRS), car la plupart d'entre eux utilisaient des technologies issues de systèmes de recommandation. Wei et al. ont classé les systèmes de recommandation en quatre catégories [2]: filtrage collaboratif, filtrage base sur le contenu, approches basées sur les connaissances et hybrides. \n\nCertaines de ces techniques avaient été appliquées au JRS, Zheng et al. [3] et Al-Otaibi et al. [4] ont résumé les catégories de plates-formes de recrutement en ligne existantes et énuméré les avantages et les inconvénients des approches techniques dans différents JRS. Les catégories comprennent :\n-\tRecommandation basée sur le contenu (CBR). Le principe d'une recommandation basée sur le contenu est de suggérer des éléments qui ont des informations de contenu similaires aux utilisateurs correspondants, comme Prospect [5].\n-\tRecommandation sur le filtrage collaboratif (CFR). La recommandation de filtrage collaboratif trouve des utilisateurs similaires qui ont le même goût que l'utilisateur cible et recommande des éléments en fonction de ce que les utilisateurs similaires apprécient, comme CASPER [6].\n-\tRecommandation basée sur les connaissances (KBR). Dans la recommandation basée sur les connaissances, des règles et des modèles obtenus à partir de la connaissance fonctionnelle de la façon dont un élément spécifique répond aux exigences d'un utilisateur particulier, sont utilisés pour recommander des éléments, comme Proactive [7].\n-\tLes systèmes de recommandation hybrides combinent deux techniques de recommandation ou plus pour obtenir de meilleures performances et surmonter les inconvénients de chaque technique individuelle. Habituellement, le filtrage collaboratif est combiné à une autre technique pour tenter d'éviter le problème de montée en puissance.\n\n1.2.2\tSystèmes de recommandation d'emploi\nRafter et al. ont commencé à utiliser le filtrage collaboratif automatise (ACF) dans leur système de recommandation d'emploi, « CASPER » [6]. Dans le système, les profils utilisateur sont obtenus à partir des journaux du serveur, qui incluent : les données de révision, les données de temps de lecture et les données d'activité. Tous ces facteurs sont considérés comme une mesure de la pertinence parmi les utilisateurs. Le système recommande des travaux en deux étapes : tout d'abord, le système trouve un ensemble d'utilisateurs liés à l'utilisateur cible ; deuxièmement, les emplois qui ont plu aux utilisateurs concernés seront recommandés à l'utilisateur cible. Le système utilise une stratégie de filtrage collaborative basée sur des clusters. La similitude entre les utilisateurs est basée sur le nombre d'emplois qu'ils ont tous deux examinés ou appliqués.\n\nCASPER permet également aux utilisateurs de rechercher des emplois par une requête qui est une combinaison de certains champs : comme l'emplacement, le salaire, la compétence, etc. Le système utilise une telle requête pour trouver des travaux, et les travaux renvoyés sont classés avec l'algorithme de filtrage collaboratif. Dans leur article, les auteurs ne donnent pas de description détaillée sur la façon de détecter les domaines connexes dont ils ont besoin et comment transférer la description de poste semi-structurée vers les données structurées.\n\nLes pénuries de filtrage collaboratif : Premièrement, étant donné que le nombre de résultats de recherche est énorme et que les résultats sont triés de manière aléatoire, la probabilité que deux utilisateurs similaires examinent les mêmes emplois est faible, ce qui pose le problème de la rareté du filtrage collaboratif. Les auteurs ont également remarqué le problème de parcimonie causé par peu de profils d'utilisateurs, ils essaient donc d'utiliser une solution basée sur des clusters pour résoudre ce problème. Deuxièmement, comme les offres d'emploi recommandées proviennent des résultats de recherche d'autres utilisateurs, la qualité des résultats de la recherche actuelle étant faible, la qualité de la recommandation ne peut pas être élevée.\n\nFarber et al. [8] ont présenté un système de recommandation base sur une approche hybride. Le système intègre deux méthodes, le filtrage base sur le contenu et le filtrage collaboratif, qui tente de surmonter le problème de la rareté des données de notation en exploitant un modèle combiné. Dans le système, la source de données est des CV synthétiques. Le modèle d'aspect latent est illustre dans la Figure 1 ci-dessous.\n\n \n\nFigure 1 : Modèle d’aspect latent\n\n\nDans Malinowski et al. [1], ils classent les systemes de recommandation d'emploi en deux catégories, CV-recommender, qui recommande les CV au recruteur, et le job recommender, qui recommande des emplois aux demandeurs d'emploi. Le système collecte les données de profil des utilisateurs en leur demandant de saisir leurs profils dans l'interface basée sur le formulaire Web champ par champ. Les données d'entrée collectées sont :\n-\tDonnées démographiques (par exemple : date de naissance, coordonnées) ;\n-\tDonnées sur l'éducation (par exemple : cours scolaires, notes, université, type de diplôme, examens universitaires intermédiaires et finaux, études de troisième cycle) ;\n-\tExpérience professionnelle (par exemple :  nom de l'entreprise, type d'emploi, groupe industriel, domaine professionnel) ;\n-\tCompétences linguistiques (par exemple : langue, niveau de connaissance) ;\n-\tCompétences informatiques (par exemple, type de compétences, niveau de connaissances) ;\n-\tPrix, bourses, publications et autres.\n\nLe système a également demandé aux utilisateurs de télécharger leurs CV, mais les CV étaient uniquement pour faciliter le jugement humain. Dans l’étude de Malinowski, le modèle d’aspect latent est un modèle statistique qui doit être formé avant d’être appliqué à la recommandation. Le système utilise les résultats de recherche des utilisateurs comme données de formation pour entrainer le modèle a des fins de recommandation, de sorte que le système a besoin d'une formation de longue durée pour chaque utilisateur.\n\n1.2.3\tExtraction d'informations dans le systeme de recommandation de travaux\nLes grandes entreprises informatiques ont rencontré le même problème de débordement d'informations lorsqu'elles recevaient plusieurs CV pour une offre d'emploi. Le recruteur devait filtrer toutes les candidatures manuellement, mais cette tâche était également fastidieuse et longue. Pour cette raison, ces entreprises ont essayé de créer des systèmes pour aider à filtrer les CV.\n\nAmit et al. a IBM ont présenté un système, « PROSPECT » [5], pour aider à présélectionner les candidats pour les emplois. Le système utilise un chercheur de CV pour extraire les informations des CV, qui utilisent un modèle de champ aléatoire conditionnel (CRF) pour segmenter et étiqueter les CV. Le modèle CRF utilisait trois types de fonctionnalités, à savoir : Lexique, Visuel et Entité nommée. L'article a comparé certains algorithmes pour classer les candidats, ces méthodes incluent : Okapi BM25, KL, Lucene Scoring et Lucene Scoring + SkillBoost.\n\nHP a également construit un système pour résoudre le problème similaire, qui est présenté dans l'article de Gonzalez et al. [9]. Le système accorde également beaucoup d'attention à l'extraction d'informations.\n\nLes dictionnaires utilisés pour baliser les entités doivent être mis à jour régulièrement car de nouveaux termes apparaissent toujours. Un module d'apprentissage adaptatif permet donc d'atteindre deux objectifs : utiliser des données sémantiques pour améliorer l'extraction d'informations et découvrir de nouveaux termes.\nUne ontologie orientée domaine est utilisée pour représenter les connaissances, les règles d'inférence sont définies en fonction de la base de connaissances de l'ontologie. Lorsqu'un terme détecté est trouvé, le système effectuera une recherche dans une base de connaissances externe, telle que DBpedia. Les curriculum vitae sont également classés dans différentes catégories comme « Technologie Web » et « Aucune technologie Web » par le classificateur naïf Bayes. L'entreprise peut affecter les employés appropries aux postes requis avec le système.\n\nL'objectif principal des systèmes construits par IBM et HP est d'aider les entreprises a sélectionner de bons candidats, mais ne peuvent pas aider les demandeurs d'emploi à trouver des emplois appropriés.\n\nYu et al. [10] ont utilisé un cadre IE en cascade pour obtenir les informations détaillées des curriculum vitae. Dans un premier temps, le modèle de modélisation de Markov cache (HMM) est utilisé pour segmenter le CV en blocs consécutifs. Sur la base du résultat, un modèle SVM est utilisé pour obtenir les informations détaillées dans le certain bloc, les informations comprennent : nom, adresse, éducation, etc.\n\nCelik Duygua et Elci Atilla ont proposé un Rsum Parser (ORP) [11] base sur une ontologie, qui utilise l'ontologie pour assister le processus d'extraction d'informations. Le système traite un résumé dans les étapes suivantes : conversion du fichier de résumé en texte brut, séparation du texte en quelques segments, utilisation de la base de connaissances de l'ontologie pour trouver les concepts dans les phrases, normalisation de tous les termes et classification des phrases pour obtenir les termes.\n\nToutefois, les informations personnelles que le système a récupérées comme le nom et les adresses ne sont pas les informations qui intéressent les recruteurs. Les recruteurs veulent des informations relatives à l'offre d'emploi et peuvent les aider à juger de la compétences des candidats.\n\n1.2.4\tAlgorithmes de correspondance dans les systèmes de recommandation d'emploi\nLu et al. [12] ont utilisé l'analyse sémantique latente (LSA) pour calculer les similitudes entre les emplois et les candidats, mais ils n'ont sélectionné que deux facteurs « intérêt » et « éducation » pour comparer les candidats. \n\nXing et al. [13] ont utilisé des modèles de pertinence structures (SRM) pour faire correspondre les curriculum vitae et les emplois.\n\nDrigas et al. [14] ont presenté un système expert pour faire correspondre les emplois et les demandeurs d'emploi, et pour recommander les chômeurs aux postes. Le système expert a utilisé des règles Neuro-Fuzzy pour évaluer la correspondance entre les profils d'utilisateurs et les offres d'emploi. Le système utilise une matrice de relations pour représenter la relation floue entre ces spécialités. Le système a besoin des données d'entrainement pour entrainer ce réseau Neuro-Fuzzy. Les données de curriculum vitae et les données sur les offres d'emploi ont été entrées manuellement dans le système.\n\nDaramola et al. [15] ont également proposé un outil de système expert base sur la logique floue (FES) pour le recrutement de personnel en ligne. Dans l'article, les auteurs ont supposé que les informations étaient déjà collectées. Le système utilise une métrique de distance floue pour classer les profils des candidats dans l'ordre de leur éligibilité à l'emploi.\n\nYao et al. [12] ont également présenté un système de recommandation hybride qui intégrait une relation basée sur le contenu et basée sur l'interaction. Dans la partie basée sur le contenu, les relations entre emploi-emploi, emploi-demandeur d'emploi et demandeur d'emploi-demandeur d'emploi peuvent être identifiées par leur similitude de profils. Deux approches sont utilisées pour calculer les similitudes. Pour les données structurées, comme l’âge et le sexe, les valeurs de somme de poids seront renvoyées ; pour les données non structurées, comme la similitude entre l'emploi et le profil utilisateur, l'analyse sémantique latente est appliquée dans le système.\n\nEn résumé, il existe certains problèmes dans les systèmes de recommandation de travaux précédents :\n-\tLa plupart des systèmes ne peuvent traiter que les données structurées des curriculum vitae et des descriptions de poste, mais en réalité les deux sont dans des formats non structurés, tels que des fichiers texte ou des pages Web HTML.\n-\tLes systèmes dotés de modules d'extraction d'informations sont conçus pour que les recruteurs sélectionnent les candidats, et non pour que les demandeurs d'emploi sélectionnent des emplois.\n-\tDans les systèmes, les champs d'information correspondant aux curriculum vitae et aux descriptions de poste sont grossiers. Pour améliorer la qualité de la recommandation, nous devons améliorer la granularité des champs d'information."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Le besoin de représentation flexible d’entités complexes est courant dans divers domaines, notamment dans le médical [1].\n• Les modèles de données relationnels, bien que courants, présentent des limites importantes pour s'adapter à ces besoins.\n\n• La première approche consiste à figer le modèle dans les bases de données relationnelles (MySQL, Oracle, SQL Server). Modifier ce modèle pour ajouter des attributs (comme une marque) est une opération lourde et déconseillée en production, car cela nécessite la mise à jour de chaque ligne de la table concernée, entraînant une indisponibilité potentiellement longue [2].\n• Certaines bases de données offrent un mode « on line » pour réduire le temps de blocage, mais il n'est disponible que pour certaines opérations, ce qui complique la gestion homogène des données [3].\n• De plus, ces bases de données ont des limites sur le nombre de colonnes ou la longueur maximale d’une ligne, ce qui peut poser problème pour des entités avec de nombreux attributs [4].\n• Cette rigidité complique également l’import de données hétérogènes, nécessitant une restructuration du schéma à chaque changement d’organisation des attributs des entités.\n• Aucune étude scientifique ou concurrente n’a été identifiée pour traiter ces aspects de manière efficace.\n\n• Une seconde approche consiste à re-normaliser le modèle en créant un méta-modèle au-dessus du schéma de la base de données, souvent en utilisant le modèle EAV (Entity-Attribute-Value) pour apporter de la flexibilité, particulièrement dans le domaine médical [1, 6].\n• Le modèle EAV contourne les problèmes liés à la manipulation du schéma en traitant les attributs comme des données, mais il présente des limites telles que la multiplication des tables, l'illisibilité du modèle et l’augmentation exponentielle du nombre de requêtes [7].\n• Le modèle EAV, bien que flexible, est complexe à maintenir et doit être utilisé avec prudence.\n\n• Une troisième approche consiste à utiliser des systèmes NoSQL, qui définissent le schéma par entité, comme MongoDB ou CouchDB [8].\n• Ces systèmes offrent une flexibilité théorique pour la gestion des variations de modèles, mais présentent des limites telles que leur instabilité relative, la difficulté d'adoption par les DSI des grands groupes, et l'incompatibilité avec les caractéristiques ACID en raison de leur nature distribuée.\n• Le CAP Theorem démontre que les bases de données distribuées ne peuvent garantir simultanément Consistency, Availability et Partition tolerance, souvent au détriment de la Consistency [9].\n\n• Enfin, la recherche a identifié plusieurs outils de stockage de documents, mais aucun ne répond parfaitement aux besoins de stockage unique pour des catalogues avec un grand nombre d’entrées.\n• Nuxeo [10] est une plateforme ECM utilisant PostgreSQL, MySQL, Oracle, MongoDB, et Elastic Search pour stocker des documents avec une gestion flexible des métadonnées.\n• EZ Platform [11] et Spree Commerce [12] utilisent des bases de données relationnelles pour le stockage, mais leurs solutions sont limitées à des structures simples.\n• Magento 2 [14], Odoo commerce [15], et Alfresco Enterprise [16] sont également cités, mais ils utilisent principalement des modèles EAV ou des systèmes de stockage avec des limitations similaires."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nLe besoin exprimé par les marchands aujourd’hui est également présent dans beaucoup d’autres domaines d’activité. La représentation flexible d’une entité complexe a fait l’objet de nombreuses études scientifiques et universitaires, en particulier dans le monde médical (cf. [1]). Plusieurs pratiques ont émergées suite à ces recherches pour permettre aux modèles de données relationnels de s’adapter à ces besoins. Toutes ces pratiques possèdent aujourd’hui leurs propres limites. Nous citons dans la suite les limites des modèles existants. \n1.2.1\tFiger le modèle \nLes bases de données relationnelles (MySQL, Oracle, SQL Server, etc.) imposent de figer à un instant T un modèle (par exemple, un vêtement est composé d’un nom, d’un prix et d’une catégorie). \n \nFigure 1 1. Exemple de structure de données\nL’ajout d’une marque nécessite l’altération du modèle de données initial. C’est une opération lourde et qui est déconseillée en production dès que l’on touche les données centrales de l’application. En effet, modifier le modèle de données signifie rendre indisponible les tables concernées par cette modification. La durée de cette opération pouvant être élevée (plusieurs minutes voire dizaine de minutes) lorsque le volume de données est important. Par exemple, dans le cas de SQL Server, l’ajout d’une colonne à une table peut nécessiter la mise à jour de chaque ligne de la table en question [2]. \n\nAfin de réduire ces problématiques, certaines bases de données disposent d’un mode « on line », réduisant ainsi le temps de lock. Malheureusement, ce mode n’est valable que sur certaines opérations [3], ce qui rend très difficile un mode de fonctionnement homogène vis à vis de l’utilisateur. \n\nPar ailleurs, certaines bases de données présentent des limitations sur le nombre de colonnes ou sur la longueur maximale d’une ligne. Par exemple, sous MySQL, y compris dans les versions les plus récentes, une ligne de table ne peut excéder 65 Ko [4]. Dans le cas d’une gestion de produits, il n’est pas impossible d’imaginer plusieurs centaines d’attributs de type chaîne de caractères, ce qui peut amener rapidement à atteindre ce type de limite.\n\nCette pratique complexifie par ailleurs fortement les processus d’import de données hétérogènes puisque dans ce cas le schéma de données devra être restructuré à chaque changement d’organisation des attributs des entités. Pour pouvoir gérer tous nos clients, et la masse de données émanant de ces derniers, de la même façon nous avons travaillé sur cet aspect. Nous nous étions fixé comme objectif d’être capables de figer sur trois niveaux ou plus. Nous n’avons pas identifié des travaux dans la littérature scientifique, ni dans la concurrence traitant de ces aspects. \n1.2.2\tRe-normaliser le modèle en EAV\nUne autre approche consiste à re-normaliser le modèle en créant un méta-modèle au-dessus du schéma de la base de données. Ce méta-modèle utilise les fonctions de manipulation de données (Data Manipulation Language telles que DELETE, INSERT et UPDATE) pour manipuler la structure des données en lieu et place des fonctions de manipulation de schéma (Data Definition Language telles que CREATE, DROP, TRUNCATE et ALTER). Par exemple, pour créer un nouvel attribut d’une entité, il suffira de rajouter une ligne dans une table plutôt qu’une colonne. \n\nAinsi, tous les problèmes liés à la manipulation du schéma au sein d’une base de données sont contournés en les remplaçant par de la manipulation de données. Le méta-modèle de ce type le plus utilisé est le modèle EAV (Entity Attribute Value). Il a été conçu à l’origine pour répondre au besoin de souplesse dans le domaine médical, ou la description des maladies peut être très variable en fonction de la diversité des symptômes et surtout évoluer aux grés des découvertes médicales. \n\nPlusieurs études universitaires ont été menées pour enrichir et améliorer ce modèle. Elle a notamment été assez largement étudiée et décrite par Nadkarni et al. [1], Baker et al. [6], etc. En revanche, ces approches présentent des  limites intrinsèques : \n\n-\tMultiplication du nombre de tables dans le SGBD\n-\tIllisibilité du modèle\n-\tAugmentation exponentielle du nombre de requêtes en base\n\nEn outre, dans son travail d’expertise, Robert Raszczynski [7] explique que le modèle EAV, s’il permet d’apporter une grande flexibilité à un système, doit toutefois être utilisé avec parcimonie de par sa grande complexité et sa difficulté à maintenir. \n1.2.3\tSystème à schéma dynamique\nLa dernière approche (encore peu répandue) pour résoudre le problème de variations dans un modèle de données, c’est de renoncer au modèle traditionnel tel qu’on le connaît aujourd’hui dans les SGBD de type Oracle, SQLServer, MySQL, etc. \n\nDes systèmes de base de données dits NoSQL (pour Not Only SQL) orientés documents proposent une définition du schéma par entité. Parmi ces systèmes, on peut citer MongoDB ou CouchDB. \n\nSelon Cormack [8], l’usage que pourrait trouver le NoSQL dans le domaine de la gestion de données de références, permet en théorie de palier au problème qui nous intéresse, néanmoins elles possèdent un ensemble de limites : \n\n-\tLes bases NoSQL (Not Only SQL) sont encore récentes et pas totalement stables. \n-\tAdoption difficile par les DSI des grands groupes (habitués à des solutions telles qu’Oracle ou SQLServer). \n-\tCaractéristiques ACID incompatibles de par leurs aspects distribués. \n\nLe CAP Theorem présenté par Eric Brewer en 2000 et détaillé dans une étude du MIT par Seth Gilbert et Nancy Lynch [9] indique en effet que parmi les 3 caractéristiques Consistency, Availability et Partition tolerance, seules 2 de ces caractéristiques peuvent être réellement respectées par une implémentation de base de données distribuée. Généralement, les caractéristiques Availability et Partition tolerance sont choisies car elles fournissent ces caractéristique de données distribués, au détriment de Consistency qui est un prérequis à ACID. \n1.2.4\tSingle Storage\nNous avons ici réalisé une recherche bibliographique pour identifier les principaux outils présentant des systèmes de stockage des documents. Les principaux outils que nous avons identifiés sont les suivants. \n\nNuxeo [10] C’est une plateforme ECM (Enterprise Content Management) libre codée en Java. Elle permet de stocker des documents, de définir des flux de travail pour l’approbation, etc. Les technologies utilisées sont des bases de données SQL (PostGres, MySQL, Oracle, MSSQL, …), Mongo DB et Elastic Search. Les documents sont soit stockés dans une base de données SQL ou dans une base de données Mongo DB. L’utilisateur peut donc choisir entre PostgreSQL, MySQL, Oracle, MSSQL ou Mongo DB pour stocker ses documents.  \n\nEn PostgreSQL, la table principale du document contient l’ID, le nom, la taille et un résumé du contenu du document. Toutes les autres informations (méta-data, relations, etc.) sont contenues dans des tables séparées selon le type du document (une tables pour les vidéos, une autre pour les images, etc.). \n\nDans Mongo DB, les documents sont stockés en utilisant leur représentation JSON. Les ACL (Access Control List) sont inclus à l’intérieur du document. Ils réduisent les problèmes de cohérence liés aux mises à jour multi-documents. Le principe de fonctionnement de Nuxeo est décrit à la  située ci-dessous. \n \nFigure 1 2 : Fonctionnement de Nuxeo [10]\nLes index du moteur d’indexation sont complétés par défaut chaque fois qu’un document est importé.\n\nEZ Platform [11] est une plateforme CMS libre développée en PHP / Symphony. Les technologies utilisées sont MySQL, Maria DB et Solr. Les documents sont stockés grâce à de simples tables SQL. Les documents sont retrouvés grâce à  Sorl par défaut.\n\nSpree Commerce [12] est une plateforme d’e-commerce libre développée en Ruby on Rails. Les technologies utilisées sont MySQL, Postgres et SQLite. Les produits sont stockés grâce à un EAV classique. Les produits, leurs attributs, les familles et les attributs des familles sont stockés sur quatre tables différentes. Les produits sont retrouvés par recherche sur les tables par nom ou description. \n\nROR Ecommerce [13] est également d’une plateforme d’e-commerce libre développée en Ruby on Rails. Les technologies utilisées sont MySQL et SQLite. Les produits sont stockés de la même manière qu’avec la plateforme Spree. Les produits sont retrouvés par Solr.\n\nMagento 2 [14] est une plateforme libre d’e-commerce codée en PHP / Zend. Les technologies utilisées sont MySQL, Solr et Elastic Search. Les produits sont stockés via un modèle EAV. Les produits, leurs attributs, leur description et leurs catégories sont stockés dans quatre tables différentes. Les produits sont retrouvés par recherche sur les tables par nom ou description. Il est également possible d’effectuer des recherches par Elastic Search ou Solr.\n\nOdoo commerce [15] est une plateforme d’e-commerce libre développée en Python. Les technologies utilisées sont PostgreSQL et MySQL. Les produits sont stockés selon le modèle EAV. Les produits, leur description, leurs attributs et leurs catégories sont stockés dans quatre tables différentes. Les produits sont retrouvés grâce à une recherche par nom sur la table des produits. Par défaut, Elastic Search n’est pas intégré à la plateforme Odoo. \n\nAlfresco Enterprise [16] est une  plateforme libre de gestion de contenu codée en Java. Elle permet de stocker des documents, de définir des flux d’approbation, etc. Contrairement à Nuxeo, il semble qu’il soit impossible d’ajouter facilement des propriétés personnalisées aux documents. Cela signifie que ce concurrent ne nous apporte pas d’informations sur de nouvelles pratiques. Les technologies utilisées sont PostgreSQL et Solr par défaut. Le contenu est stocké dans diverses bases de données : PostgreSQL, Oracle,  SQL Server, MySQL, DB2. La requête du contenu est effectuée par Solr.\n\nAucun des outils de la littérature ne propose une solution qui réponde à nos besoins, notamment un système de stockage unique permettant de gérer des catalogues contenant un nombre très grand d’entrées. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Les applications modernes isolent la logique métier, optimisent la réutilisation et la réitération, et automatisent les tâches de maintenance d'infrastructure, permettant aux développeurs de se concentrer sur l'écriture du code.\n• Le composant UI retourne un statut de manière asynchrone, améliorant l'expérience utilisateur en fournissant des retours immédiats sans attendre la validation complète du formulaire.\n• Dans le contexte du Rules Engine, le front-end doit gérer des règles complexes, validant rapidement les champs obligatoires pour fournir un résultat à l'utilisateur. L'API de React Hook Form a été choisie pour valider ces règles, représentant un compromis entre les bibliothèques de formulaires lourdes et les formulaires simples mais encombrants.\n• Dès 2014, la structure du catalogue produit du PIM a été conçue pour supporter l'enrichissement de valeurs de type « mesures », cruciales pour la qualité de l'information produit. Par exemple, un téléviseur peut inclure des mesures telles que la largeur et la hauteur en centimètres, la diagonale en pouces, et le taux de rafraîchissement en hertz.\n• Dans le PIM, ces mesures sont modélisées via des familles de mesures, incluant : des unités, une unité standard, et des opérations de conversion. Un type d'attribut « metric » permet d'enrichir les données produit avec une valeur numéraire et une unité compatible avec la famille de mesure.\n• Le PIM contient par défaut 19 familles de mesures, mais les utilisateurs peuvent ajouter des familles de mesures personnalisées via des extensions logicielles au format YML.\n\n• Le PIM nécessite une modélisation performante pour les opérations sur bases de données telles que la recherche, la mise à jour et la quantification des produits.\n• Les bases de données relationnelles (MySQL, Oracle, SQL Server) imposent de figer le modèle à un instant T, rendant les modifications lourdes et déconseillées en production. Par exemple, sous SQL Server, l'ajout d'une colonne peut nécessiter la mise à jour de chaque ligne, ce qui peut être long et provoquer des indisponibilités. MySQL impose également des limites comme la taille maximale d'une ligne, ce qui complique la gestion d'entités avec de nombreux attributs [1, 2, 3].\n\n• Une autre approche est la re-normalisation du modèle en EAV (Entity-Attribute-Value), où la structure des données est manipulée via des opérations DML plutôt que DDL. Cette flexibilité est essentielle dans des domaines comme le médical, mais présente des limites telles que la multiplication des tables, l'illisibilité du modèle, l'augmentation des requêtes et la dégradation des performances pour de grands volumes de données [4, 5, 6].\n\n• Les bases de données relationnelles ont évolué avec l'ajout de fonctionnalités pour gérer des données sans schéma, comme les champs JSON en MySQL 5.7 et PostgreSQL. Les nouveaux types d'index GiST et GIN permettent de filtrer et d'indexer les valeurs JSON, mais chacun a ses avantages et inconvénients. Les index GiST sont mieux adaptés pour des bases dynamiques de moins de 100 000 entrées, tandis que les index GIN sont plus performants pour des bases statiques de grande taille [7].\n\n• Les systèmes NoSQL (comme MongoDB et CouchDB) permettent une définition flexible du schéma par entité. MongoDB gère des données en JSON binaire, offrant une grande flexibilité et des performances supérieures à MySQL pour l'insertion et la recherche. MongoDB est également scalable horizontalement via l'auto-sharding, mais sa performance diminue au-delà de 64 indexes, ce qui limite son efficacité [8, 9, 10, 11].\n\n• ElasticSearch, un moteur de recherche orienté document, est scalable, performant et capable de gérer de grands volumes de données sans schéma prédéfini. Il permet d'indexer des données en parallèle de leur insertion, offrant de bonnes performances pour les recherches complexes sur de gros volumes de données, là où MongoDB est limité [12, 13, 14].\n\n• Trois solutions ont été développées : MySQL avec EAV, MongoDB, et ElasticSearch, chacune ayant des avantages et des défauts selon les besoins. Toutefois, les limitations spécifiques de chaque solution, notamment pour les produits complexes avec de nombreux attributs ou les grandes volumétries de données, ont conduit à la nécessité de développer une solution unique plus performante. Cette nouvelle solution reposera sur le principe Command-Query-Separation, permettant d'améliorer les performances tout en évitant les erreurs de comportement [15, 16, 17]."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nLes applications modernes permettent d'isoler la logique métier, d'optimiser la réutilisation et la réitération, ainsi que de supprimer autant que possible les dépassements. Elles sont créées à l'aide de services qui nous permettent de nous concentrer sur l'écriture du code tout en automatisant les tâches de maintenance de l'infrastructure.\n\nLe composant UI retourne un statut de manière asynchrone, il n’est donc pas nécessaire d’attendre que l’utilisateur valide tout le formulaire pour lui retourner l’information. Cela fournit ainsi une meilleure expérience à l’utilisateur.\nDans notre contexte (celui du Rules Engine), les conditions ou actions peuvent avoir des champs obligatoires, comme par exemple, un champ texte, une liste déroulante, un champ nombre, etc… Le front-end doit donc avoir une forte intelligence concernant les règles et doit pouvoir réagir rapidement en fournissant un résultat à l’utilisateur. \nSuite à plusieurs investigations, nous avons identifié l’API de React Hook Form pour la validation des règles dans le PIM.\nEffectivement, React Hook Form représente pour nous le juste milieu entre les implémentations utilisant des bibliothèques de formulaires lourdes, d'un côté, et des formulaires React simples et encombrants, de l'autre.\nPar ailleurs, dès 2014, la structure d’un catalogue produit a été conçue pour supporter l’enrichissement de valeur de produit de types “mesures”. Les mesures sont essentielles du point de vue de la qualité de l’information produit. Pour nous représenter au mieux ce type d’information, prenons un exemple concret. Sur un poste de télévision nous pouvons retrouver des informations de types mesures concernant :\n-\tLa largeur et la hauteur qui sont des longueurs exprimées en centimètres\n-\tLa diagonale qui est une longueur exprimée en pouces\n-\tLe taux de rafraîchissement de l’image exprimée en hertz\nA chaque produit correspond des informations de types mesures très différentes qui peuvent être portées sur un poids, un angle, une aire, un volume de son, etc. \n\nDans le PIM, cette modélisation se traduit par plusieurs éléments présents dans la structure du catalogue et ainsi que dans l’enrichissement des produits.\n\nTout d’abord, des familles de mesures qui contiennent l’ensemble des informations la concernant :\n-\tDes unités\n-\tUne unité standard\n-\tDes opérations de conversion d’une unité vers l’unité standard\n\nEnsuite, un type d’attribut appelé “metric” est disponible dans le PIM afin de pouvoir enrichir de la donnée produit lié à ces familles de mesures. Les valeurs produits liées à cet attribut sont donc composées d’une valeur numéraire et d’une unité. Cette unité est compatible avec la famille de mesure auquel l’attribut est lié.\n\nHistoriquement, le PIM contient nativement 19 familles de mesures qui permettent de modéliser la plupart des cas d’usage de l’enrichissement produit.\n\nPour les utilisateurs ayant besoin d’enrichir de la donnée produit lié à des familles de mesures n’étant pas disponible dans le PIM, il est possible de développer une extension logicielle au PIM décrivant des familles de mesures additionnelles et personnalisées.\n\nCette extension se matérialise au travers d’un fichier au format YML ayant le format suivant :\n \n\n\nDans cet exemple, nous déclarons une nouvelle famille de mesure “Capacitance” contenant trois unités “Farad”, “Kilofarad”, “Megafarad” et dont l’unité standard est le “Farad”. Pour chaque unité nous pouvons voir les opérations à effectuer pour passer de cette unité à l’unité standard. Si nous voulons passer du “Kilofarad” au “farad” il suffit de multiplier par 1000 la valeur.\n\nNous devons ainsi concevoir la modélisation de produits qui permet une performance très grande pour les opérations usuelles d’une base de données (recherche de produit, mise à jour des données, quantification, etc.).\n.1.2.1.1\tModélisation en bases de données\nLes bases de données relationnelles (MySQL, Oracle, SQL Server, etc.) imposent de figer à un instant T un modèle (par exemple, un vêtement est composé d’un nom, d’un prix et d’une catégorie). \n \n\nFigure 2 : Exemple de structure de données\n\nL’ajout d’une marque nécessite l’altération du modèle de données initial. C’est une opération lourde et qui est déconseillée en production dès que l’on touche les données centrales de l’application. En effet, modifier le modèle de données signifie rendre indisponible les tables concernées par cette modification. La durée de cette opération pouvant être élevée (plusieurs minutes voire dizaine de minutes) lorsque le volume de données est important. Par exemple, dans le cas de SQL Server, l’ajout d’une colonne à une table peut nécessiter la mise à jour de chaque ligne de la table en question [1]. \n\nAfin de réduire ces problématiques, certaines bases de données disposent d’un mode « on line », réduisant ainsi le temps de lock. Malheureusement, ce mode n’est valable que sur certaines opérations [2], ce qui rend très difficile un mode de fonctionnement homogène vis à vis de l’utilisateur. \n\nPar ailleurs, certaines bases de données présentent des limitations sur le nombre de colonnes ou sur la longueur maximale d’une ligne. Par exemple, sous MySQL, y compris dans les versions les plus récentes, une ligne de table ne peut excéder 65 Ko [3]. Dans le cas d’une gestion de produits, il n’est pas impossible d’imaginer plusieurs centaines d’attributs de type chaîne de caractères, ce qui peut amener rapidement à atteindre ce type de limite.\n\nCette pratique complexifie par ailleurs fortement les processus d’import de données hétérogènes puisque dans ce cas le schéma de données peut devoir être restructuré à chaque changement d’organisation des attributs des entités. Pour pouvoir gérer tous nos clients, et la masse de données émanant de ces derniers, de la même façon nous avons travaillé sur cet aspect. Nous nous étions fixés comme objectif d’être capables de figer sur trois niveaux ou plus. Nous n’avons pas identifié des travaux dans la littérature scientifique, ni dans la concurrence, traitant ces aspects. \n.1.2.1.2\tRe-normaliser le modèle en EAV (Entity – Attribute -Value)\nUne autre approche consiste à re-normaliser le modèle en créant un méta-modèle au-dessus du schéma de la base de données. Ce méta-modèle utilise les fonctions de manipulation de données (Data Manipulation Language telles que DELETE, INSERT et UPDATE) pour manipuler la structure des données en lieu et place des fonctions de manipulation de schéma (Data Definition Language telles que CREATE, DROP, TRUNCATE et ALTER). Par exemple, pour créer un nouvel attribut d’une entité, il suffira de rajouter une ligne dans une table plutôt qu’une colonne. \nAinsi, tous les problèmes liés à la manipulation du schéma au sein d’une base de données sont contournés en les remplaçant par de la manipulation de données. Le méta-modèle de ce type le plus utilisé est le modèle EAV (Entity Attribute Value). Il a été conçu à l’origine pour répondre au besoin de souplesse dans le domaine médical, ou la description des maladies peut être très variable en fonction de la diversité des symptômes et surtout évoluer aux grés des découvertes médicales. \n\nPlusieurs études universitaires ont été menées pour enrichir et améliorer ce modèle. Elle a notamment été assez largement étudiée et décrite par Nadkarni et al. [4], Baker et al. [5], etc. En revanche, ces approches présentent des limites intrinsèques : \n\n-\tMultiplication du nombre de tables dans le SGBD\n-\tIllisibilité du modèle\n-\tAugmentation exponentielle du nombre de requêtes en base\n-\tDégradation rapide des performances de lecture et écriture lors de la manipulation d’un volume significatif de données\n\nEn outre, dans son travail d’expertise, Robert Raszczynski [6] explique que le modèle EAV, s’il permet d’apporter une grande flexibilité à un système, doit toutefois être utilisé avec parcimonie de par sa grande complexité et sa difficulté à maintenir. \n.1.2.1.3\tNouvelles recherches et technologies sur des bases relationnelles \nLes bases relationnelles se sont trouvées améliorés notamment avec MySQL 5.7 et Postgresql, et ont désormais de nouvelles fonctionnalités pour les données sans schéma et les bases orientées document. L’apparition du champ json (et jsonb) permet de stocker des données de façon plus flexible avec une structure d’enregistrement qui est différente d’un produit à un autre. \n\nIl est également désormais possible de filtrer et indexer des valeurs de ce champ avec de nouveaux types d’index [7] : les GiST pour Generalized Search-Tree et les GIN pour Generalized Inverted Index. \tAvec un index GiST, chaque document est représenté par une signature de taille déterminée et unique, ce qui entraîne une probabilité non-nulle de collusion (et donc des résultats de recherche faux). Cela nécessite donc une vérification supplémentaire qui peut être coûteuse. La performance peut donc être dégradée en fonction du nombre de mots uniques à indexer. \n\nLes index GIN ne peuvent pas donner de résultats de recherches faux. En revanche, la performance dépend de façon logarithmique du nombre de mots uniques. Ainsi, globalement et avec un nombre de mots raisonnable, les index GIN sont plus rapides pour les recherches mais sont plus lents à construire et mettre à jour et prennent plus de mémoire. Les index GiST sont donc plus adaptés à une base de données dynamique si le nombre d’entrée est inférieur à 100 000, tandis que les index GIN peuvent gérer de plus grandes bases si elles sont statiques. \n.1.2.1.4\tSystème à schéma dynamique\nLa dernière approche (encore peu répandue) pour résoudre le problème de variations dans un modèle de données, c’est de renoncer au modèle traditionnel tel qu’on le connaît aujourd’hui dans les SGBD de type Oracle, SQLServer, MySQL, etc. \n\nDes systèmes de base de données dits NoSQL (pour Not Only SQL) orientés documents proposent une définition du schéma par entité. Parmi ces systèmes, on peut citer MongoDB ou CouchDB. \n\nSelon Cormack [8], l’usage que pourrait trouver le NoSQL dans le domaine de la gestion de données de références, permet en théorie de pallier aux problèmes qui nous intéressent, néanmoins elles possèdent un ensemble de limites : \n-\tLes bases NoSQL (Not Only SQL) sont encore récentes et pas totalement stables. \n-\tAdoption difficile par les DSI des grands groupes (habitués à des solutions telles qu’Oracle ou SQLServer). \n-\tCaractéristiques ACID incompatibles de par leurs aspects distribués. \n\nMongoDB est un système de gestion de base de données orienté documents. Il permet de manipuler des données au format JSON binaire. Cela permet la gestion de données plus riches ainsi que d’obtenir une base de données plus flexible [9]. Il n’y a donc pas de schémas prédéfinis pour les types et tailles de clés et valeurs et les clés peuvent être ajoutées à la volée sans reconfigurer la base. \n\nMongoDB s’avère beaucoup plus rapide que MySQL pour l’insertion et la recherche de données [10]. De plus, MongoDB scale très bien avec le nombre de données et permet de stocker des données structurées. MongoDB est en fait horizontalement scalable via l’auto-sharding qui permet de partitionner les données sur plusieurs serveurs et permet d’équilibrer dynamiquement la charge de façon similaire aux bases de données parallèles [11]. \n\nPlus généralement, MongoDB est la technologie NoSQL la plus polyvalente et l’une des plus performantes pour l’insertion, la suppression et la recherche de données [11].\n\nCependant une limitation inhérente de MongoDB est la limitation du nombre d’indexes de recherche à 64 indexes sur une collection. Une fois tous ces index utilisés, les performances de la recherche sur un champ non indexé chutent drastiquement et ne sont plus compatibles avec nos attentes.\n.1.2.1.5\tElasticSearch\nElasticSearch est un moteur de recherche orienté document, scalable horizontalement et capable de recherche quasiment en temps réel [12, 13]. Ce moteur a été conçu pour pouvoir être distribué sur plusieurs nœuds et ainsi distribuer la charge équitablement. \n\nIl n’y a pas de schéma prédéterminé et il est donc possible de changer la structure des données facilement. De plus, l’auto-sharding et l’indexation des shards permettent de faire des recherches sur plusieurs serveurs puis de combiner les résultats. \n\nEnfin, le mapping de chaque index est crucial puisqu’il détermine les types de documents et comment ils sont analysés et indexés [12]. ElasticSearch est ainsi capable de gérer de grandes quantités de données sans schéma. \n\nMongoDB est limité pour les opérations de recherche : s’il y a trop d’index, la performance est très réduite. Ainsi le moteur ElasticSearch s’avère beaucoup plus performant que MongoDB pour la recherche et les lectures complexes et intensives de données [14]. En particulier : \n\n-\tIl permet d’indexer les données en parallèle de leur insertion en base. Ceci ne ralentissant pas le processus d’insertion très important dans notre métier.\n-\tL’utilisation d’ElasticSearch est faite pour gérer de grands volumes de données tout en apportant de bonnes performances lors de l’interrogation de la base (pour effectuer des recherches sur du texte par exemple). Ces niveaux ne sont pas atteignables avec MySQL.\n\nNous avons mis au point trois solutions différentes basées sur les technologies MySQL avec EAV, MongoDB, et ElasticSearch. Ces trois solutions ont leurs avantages et leurs défauts dépendant du besoin du client. L’objectif est de mettre au point une solution globale unique regroupant ces trois méthodes de façon transparente pour le client. \n\nEn particulier, la solution la plus évoluée que nous avons pu développer ne permettait pas de traiter efficacement un grand nombre de propriétés par produit, par exemple pour des produits complexes ayant des milliers d’attributs différents. Cette solution se trouvait également limitée sur des volumétries de plusieurs dizaines de millions de produits et encore plus à cause de l’application de permissions avancées sur les données. Par ailleurs, le système était également limité par le nombre d’utilisateurs simultanés qu’il pouvait traiter. \nEnfin, il était tout simplement trop coûteux de continuer à maintenir trois solutions différentes et il était impératif de réussir à développer une nouvelle solution unique dépassant tous ces défauts. \n\nNotre nouvelle solution se basera par ailleurs sur un paradigme de programmation introduit pour la première fois par Bertrand Meyer [15] puis repris par Greg Young [16] et Martin Fowler [17] : le principe Command-Query-Separation. Ce principe impose qu’une méthode est soit une commande qui effectue une action, soit une requête qui retourne la valeur d’une donnée. En d’autres termes, « poser une question ne doit pas en changer la réponse ». Plus précisément, cela implique que des modèles complètement différents, ce qui permet par exemple d’améliorer les performances d’un système sans en altérer le comportement, ou encore d’éviter certains types de bugs."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Les applications modernes isolent la logique métier, optimisent la réutilisation et la réitération, et automatisent les tâches de maintenance d'infrastructure, permettant aux développeurs de se concentrer sur l'écriture du code.\n• Le composant UI retourne un statut de manière asynchrone, améliorant l'expérience utilisateur en fournissant des retours immédiats sans attendre la validation complète du formulaire.\n• Dans le contexte du Rules Engine, le front-end doit gérer des règles complexes, validant rapidement les champs obligatoires pour fournir un résultat à l'utilisateur. L'API de React Hook Form a été choisie pour valider ces règles, représentant un compromis entre les bibliothèques de formulaires lourdes et les formulaires simples mais encombrants.\n• Dès 2014, la structure du catalogue produit du PIM a été conçue pour supporter l'enrichissement de valeurs de type « mesures », cruciales pour la qualité de l'information produit. Par exemple, un téléviseur peut inclure des mesures telles que la largeur et la hauteur en centimètres, la diagonale en pouces, et le taux de rafraîchissement en hertz.\n• Dans le PIM, ces mesures sont modélisées via des familles de mesures, incluant : des unités, une unité standard, et des opérations de conversion. Un type d'attribut « metric » permet d'enrichir les données produit avec une valeur numéraire et une unité compatible avec la famille de mesure.\n• Le PIM contient par défaut 19 familles de mesures, mais les utilisateurs peuvent ajouter des familles de mesures personnalisées via des extensions logicielles au format YML.\n\n• Le PIM nécessite une modélisation performante pour les opérations sur bases de données telles que la recherche, la mise à jour et la quantification des produits.\n• Les bases de données relationnelles (MySQL, Oracle, SQL Server) imposent de figer le modèle à un instant T, rendant les modifications lourdes et déconseillées en production. Par exemple, sous SQL Server, l'ajout d'une colonne peut nécessiter la mise à jour de chaque ligne, ce qui peut être long et provoquer des indisponibilités. MySQL impose également des limites comme la taille maximale d'une ligne, ce qui complique la gestion d'entités avec de nombreux attributs [1, 2, 3].\n\n• Une autre approche est la re-normalisation du modèle en EAV (Entity-Attribute-Value), où la structure des données est manipulée via des opérations DML plutôt que DDL. Cette flexibilité est essentielle dans des domaines comme le médical, mais présente des limites telles que la multiplication des tables, l'illisibilité du modèle, l'augmentation des requêtes et la dégradation des performances pour de grands volumes de données [4, 5, 6].\n\n• Les bases de données relationnelles ont évolué avec l'ajout de fonctionnalités pour gérer des données sans schéma, comme les champs JSON en MySQL 5.7 et PostgreSQL. Les nouveaux types d'index GiST et GIN permettent de filtrer et d'indexer les valeurs JSON, mais chacun a ses avantages et inconvénients. Les index GiST sont mieux adaptés pour des bases dynamiques de moins de 100 000 entrées, tandis que les index GIN sont plus performants pour des bases statiques de grande taille [7].\n\n• Les systèmes NoSQL (comme MongoDB et CouchDB) permettent une définition flexible du schéma par entité. MongoDB gère des données en JSON binaire, offrant une grande flexibilité et des performances supérieures à MySQL pour l'insertion et la recherche. MongoDB est également scalable horizontalement via l'auto-sharding, mais sa performance diminue au-delà de 64 indexes, ce qui limite son efficacité [8, 9, 10, 11].\n\n• ElasticSearch, un moteur de recherche orienté document, est scalable, performant et capable de gérer de grands volumes de données sans schéma prédéfini. Il permet d'indexer des données en parallèle de leur insertion, offrant de bonnes performances pour les recherches complexes sur de gros volumes de données, là où MongoDB est limité [12, 13, 14].\n\n• Trois solutions ont été développées : MySQL avec EAV, MongoDB, et ElasticSearch, chacune ayant des avantages et des défauts selon les besoins. Toutefois, les limitations spécifiques de chaque solution, notamment pour les produits complexes avec de nombreux attributs ou les grandes volumétries de données, ont conduit à la nécessité de développer une solution unique plus performante. Cette nouvelle solution reposera sur le principe Command-Query-Separation, permettant d'améliorer les performances tout en évitant les erreurs de comportement [15, 16, 17]."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n\nDe la même façon que la technologie HTML5 a permis des fonctionnalités dédiées à la navigation mobile il y a quelques années, les concepteurs de navigateurs web s’attèlent au développement de solutions compatibles avec la réalité virtuelle (VR). L’avantage de cette initiative est double : d’une part, elle permet de recourir à des technologies telles que HTML5, WebGL ou JavaScript pour créer les applications visées, les rendant ainsi plus rapides à développer mais aussi davantage compatibles avec les autres plateformes ; et d’autre part, elle permet de s’appuyer sur l’ensemble de l’infrastructure web au sein des applications développées en connectant des éléments de cloud ou en intégrant des données web par exemple. \n\nMozilla est aujourd’hui leader de l’expérimentation de la VR sur son navigateur et Google suit de très près derrière en développant des fonctionnalités similaires sur Chrome. Au-delà de la conception de l’infrastructure permettant de supporter des applications VR, l’équipe de recherche Mozilla explore la navigation à travers les informations (moteur de recherche) en concevant des interfaces et des visuels propres à la VR [1].\n\nEn vue du développement de solutions VR intéressantes du point de vue des possibilités d’utilisation, nous devons les équiper de technologies audios et vidéo performantes. Nous nous sommes ainsi intéressés aux technologies compatibles avec des navigateurs internet pour servir notre solution à développer.\n\nLes techniques de streaming « classiques » telles que le HTML adaptative streaming se caractérisent par des temps de latence très prolongés [2], pouvant être de l’ordre de quelques secondes dans le meilleur des cas. Cette latence les rend incompatibles avec les applications collaboratives par vidéo qui requièrent un niveau d’interactivité très élevé.\n\nPour supporter les applications collaboratives (de type réunion en ligne), des solutions de Real-Time-Communication (RTC, Communication en Temps Réel) sont généralement utilisées pour permettre des cas d’usage d’interactivité et de faible latence.\n\nLa technologie de WebRTC est un projet open-source initié par Google en 2011 pour développer des applications de vidéoconférence en temps réel supportées par un navigateur web [3]. Si cette technologie répond bien aux contraintes d’interactivité et de faible latence, elle souffre d’un important inconvénient. En effet, cette technologie a été conçue selon une architecture peer-to-peer (p2p) dans laquelle un faible nombre d’utilisateurs échangent directement [4,5].\nDans cette architecture, chaque émetteur doit encoder un flux séparé et indépendant pour chaque récepteur participant à la session, comme montré sur la Figure 1. Cette architecture p2p rend ainsi très coûteuse en termes d’encodage les sessions avec un grand nombre de participants et s’avère ainsi peu scalable.\n\nDe plus, les différentes études visant à optimiser les performances de la technologie WebRTC ne prennent en considération ni les contraintes de la VR, ni ne se fixe l’objectif d’accueillir jusqu’à 10 000 personnes dans les sessions.\n\nEn particulier, aucune étude publiée n’aborde notre problématique de connexion d’un serveur en WebRTC à une session VR. Ainsi, nos verrous et incertitudes restent non adressés et nous devons suivre une démarche expérimentale pour évaluer la faisabilité de la solution que nous envisageons.\n\n\n \n\nFigure 1 : Présentation de l’architecture p2p [4].\n\n\nAu-delà de l’architecture p2p, d’autres architectures sur lesquelles nous pourrions éventuellement nous appuyer ont été proposées dans la littérature. C’est notamment le cas de l’architecture client/serveur, utilisée particulièrement pour la mise en place de certains jeux dits massivement multijoueur (MMOG) [4]. Cette architecture a pour but une gestion centralisée des différentes tâches ou processus via un serveur unique. Cependant, elle nécessite un coût élevé en termes de latence et d’utilisation de la bande passante. \n\nL’architecture multi-serveur, qui vise à surmonter les obstacles précédents, propose à cet effet l’établissement de plusieurs serveurs alloués chacun à la gestion d’une partie des processus associés au système [4]. Cette architecture peut également se révéler assez gourmande en ressources si plusieurs clients ou utilisateurs exploitent au même moment des tâches affectées au même serveur. \n\nFinalement l’architecture orientée service (SOA), qui pourrait également être envisageable, aspire à la mise en place de services compatibles entre eux et s’occupant de la gestion de sous-processus associés aux différentes tâches du système [6]. Cependant, cette architecture, comme les autres d’ailleurs, n’a jamais fait l’objet d’études en vue de supporter un très grand nombre d’avatars. \n\nLa gestion de la surcharge des serveurs est également une question primordiale pour la mise en place de plateformes dédiées à de telles applications. En effet, il faut réussir à faire migrer le client d’un serveur à un autre lorsque ce dernier est surchargé. Des algorithmes d’équilibrage dynamique de la charge de travail entre les serveurs (dans une architecture utilisant évidemment plusieurs serveurs) ou « Finding Less-loaded Server algorithm », exploitant à la fois des approches locale (répartition de la charge entre serveurs voisins) et globale (prise en compte de tous les serveurs dans la répartition de la charge de travail) ont été proposés à cet effet [7]. Dans cette approche hybride, le serveur surchargé à l’initiative du processus de répartition de la charge induit à la fois la sélection des serveurs sous-chargés et des autres serveurs surchargés dans l’architecture, de sorte que les différents serveurs surchargés contribuent pendant le même processus au partage de la charge de travail avec les différents serveurs adjacents sous-chargés qui leur sont voisins. \n\nUne architecture hybride p2p (utilisée notamment dans le domaine des MMOGs) visant à répondre à cette problématique, a également vu le jour. Dans cette dernière, l’architecture cloud mise en place propose la gestion de la charge de travail sur deux niveaux, à savoir la gestion de la charge sur les différents serveurs (correspondant ici à des machines virtuelles) réalisée par l’établissement de seuils de chargement sur plusieurs couches, et la gestion de la charge entre les serveurs [8]. Comme dans les cas précédents, la viabilité de ces solutions reste encore à évaluer dans le cadre d’un nombre illimité de connexions. \n\nToujours dans le cadre d’une approche à plusieurs serveurs, des problématiques de communication entre différentes régions de l’environnement virtuel gérées par des serveurs distincts se posent elles aussi. Les solutions proposées dans la littérature visent à l’utilisation d’une architecture « publish/subscribe », où les serveurs souscrivent ou s’abonnent à leurs voisins, et reçoivent ainsi les informations en provenance de leurs voisinages [9].  Dans cette configuration, les clients gérés par ces serveurs sont donc tenus au courant des événements se déroulant à proximité. Cependant, une telle architecture n’a été testée au mieux qu’avec trois serveurs s’occupant de la gestion de 400 clients.  Pour un nombre illimité d’utilisateurs, comme dans notre cas de figure, des ressources importantes en termes de calcul peuvent être engagées et donc impliquer des soucis en ce qui concerne la latence et l’utilisation de la bande passante, comme nous n’avons cessé de l’évoquer.\n\nSur le marché, nous avons identifié les produits proposés par NextVR et Videostitch qui offrent des solutions pour développer des expériences virtuelles utilisant des vidéos 360°. Des réseaux sociaux en VR sont également apparus : nous pouvons notamment citer AltspaceVR et vTime qui offrent des espaces virtuels où les utilisateurs peuvent se retrouver à travers leurs avatars pour échanger et interagir. Toutefois ces solutions sont insuffisantes pour offrir une expérience véritablement immersive aux utilisateurs en raison de leur manque de réalisme.\n\nNous avons également identifié dans l’état de l’art une publication décrivant la conception d’un framework web-based modulaire visant à étendre les capacités de visioconférence en ajoutant des fonctionnalités de VR [10]. Le schéma en Figure 2 illustre l’architecture de la solution développée par l’équipe de recherche.\n\n\n \n\nFigure 2 : Architecture de la solution TogetherVR [10].\n\n\nSi cette solution est, comme la nôtre, web-based et dans un environnement VR, elle n’est accessible que via un HMD (Head Mounted Device – casque). De plus, elle ne vise pas du tout à accueillir un grand nombre d’utilisateurs. En effet, elle vise à offrir à deux utilisateurs un espace virtuel immersif que l’équipe de recherche se propose d’améliorer via l’intégration de nouvelles technologies telle que la possibilité d’utiliser l’application sans HMD, l’introduction de réalité mixte, etc."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Le calcul des revenus des créateurs de musique est crucial pour quantifier précisément les gains générés par l'utilisation et la diffusion de leurs œuvres. Ce processus implique la collecte, l'agrégation et l'analyse de données provenant de diverses sources telles que les plateformes de streaming, les radios, les télévisions et les performances en direct. Les indicateurs utilisés incluent le nombre de diffusions, les taux de redevances, les ventes d'albums et les droits d'auteur, permettant aux créateurs d'optimiser leur stratégie de monétisation et de garantir une rémunération équitable.\n\n• Les méthodes de calcul des revenus ont évolué avec les changements technologiques et les nouveaux modèles de distribution. Dans le passé, les ventes d'albums et de singles, ainsi que les diffusions radio et télévisuelles, étaient des indicateurs clés pour calculer les redevances. Une étude de Quatro et al. [1] a analysé comment les chansons des compositeurs étaient diffusées à la radio et vendues sous forme physique, tandis que Dolata et al. [2] ont exploré l'impact de l'Internet sur ces modèles traditionnels, notamment avec l'avènement de la distribution numérique.\n\n• Les diffusions radio et télévisuelles restent des sources de revenus cruciales pour les créateurs de musique, jouant un rôle essentiel dans la promotion des œuvres et la génération de redevances. Les sociétés de gestion collective, telles que la SACEM en France [3], la BMI [4] et l'ASCAP [5] aux États-Unis, et la PRS for Music au Royaume-Uni [6], sont centrales dans la collecte des redevances, en agissant comme intermédiaires entre les diffuseurs et les créateurs. Elles utilisent des systèmes de surveillance sophistiqués pour suivre et quantifier les diffusions, permettant de calculer les redevances en fonction de la fréquence de diffusion et de la taille de l'audience.\n\n• Une étude par L. Ivan [7] a examiné les aspects économiques de la propriété intellectuelle dans l'industrie musicale, en se concentrant sur l'impact de la réglementation du droit d'auteur sur les revenus des artistes, y compris les revenus issus des ventes de musique enregistrée, des diffusions et des performances en direct.\n\n• Avec l'essor de la musique numérique et des plateformes de streaming, de nouvelles méthodes de suivi des diffusions en ligne ont été développées. Marc Ivaldi et al. [8] ont étudié l'impact de ces plateformes sur les ventes de musique enregistrée et les dynamiques concurrentielles, révélant une relation complexe entre les ventes physiques et numériques, ainsi que la consommation en streaming. L'étude souligne la coexistence de la substitution et de la complémentarité entre les canaux de consommation.\n\n• Luis Aguiar et al. [9] ont exploré l'impact des listes de lecture de Spotify sur la promotion des chansons et la découverte de nouveaux artistes. Ils ont montré que l'inclusion dans des listes populaires, telles que Today’s Top Hits, augmentait considérablement les écoutes et la probabilité de succès d'une chanson, particulièrement pour les nouveaux artistes.\n\n• De nouvelles technologies, telles que la blockchain et les contrats intelligents, ont été développées pour améliorer la transparence, la traçabilité et la distribution des revenus dans l'industrie musicale. Halgamuge et al. [10] ont discuté de la mise en œuvre des contrats intelligents pour assurer une distribution équitable des revenus, tout en soulignant les défis de scalabilité et d'intégration avec les systèmes existants.\n\n• Les technologies de marquage et d’identification, comme le watermarking audio, sont cruciales pour la gestion des droits et la répartition des revenus. Junjie He et al. [11] ont proposé une méthode robuste de watermarking utilisant la technique de spread spectrum, bien qu'ils aient noté des défis en termes d’imperceptibilité et de résistance aux attaques complexes.\n\n• Malgré les avancées dans l'analyse des listes de lecture et la promotion musicale, il n'existe pas encore de solution universelle pour identifier en temps réel la musique diffusée sur les plateformes de streaming, ce qui complique le calcul précis des revenus des créateurs. Une solution performante est donc nécessaire pour surmonter ces défis et fournir une étude complète des œuvres diffusées ainsi qu'un calcul détaillé des revenus des créateurs."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nLe calcul des revenus des créateurs de musique est une composante essentielle de l'industrie musicale, visant à quantifier de manière précise les gains générés par l'utilisation et la diffusion de leurs œuvres. Ce processus complexe implique la collecte, l'agrégation et l'analyse de données provenant de diverses sources telles que les plateformes de streaming, les radios, les télévisions et les performances en direct. En utilisant des indicateurs spécifiques comme le nombre de diffusions, les taux de redevances, les ventes d'albums et les droits d'auteur, les créateurs et les gestionnaires de droits peuvent estimer les revenus générés par chaque utilisation de leurs œuvres. Cette analyse approfondie permet aux créateurs de mieux comprendre leur impact financier et d'optimiser leur stratégie de monétisation, tout en garantissant une rémunération équitable pour leur travail créatif.\n\nLes méthodes de calcul des revenus des créateurs de musique ont évolué au fil du temps pour s’adapter aux changements technologiques et aux nouveaux modèles de distribution.  Dans le passé, les formats physiques tels que les vinyles, les CD et les cassettes dominaient le marché, les ventes d'albums et de singles étaient un indicateur clé de la popularité et du succès d'une œuvre musicale. De nombreux artistes et maisons de disques se fondaient sur ces données pour calculer les redevances dues aux créateurs. De plus, les diffusions à la radio et à la télévision étaient également cruciales pour déterminer les droits d'auteur, car elles fournissaient des informations sur la portée et la fréquence d'exposition d'une chanson au public. Dans ce contexte, une étude a été publiée par Quatro et al., [1] ou ils ont examiné comment les chansons de ces compositeurs ont été diffusées à la radio et vendues en format physique, fournissant ainsi des informations sur la façon dont les créateurs ont été rémunérés pour leur travail. Une autre étude a également traité le même sujet, publié en 2011 par Dolata at al., [2] ou ils ont exploré comment l'avènement de l'Internet a bouleversé ces modèles traditionnels en permettant de nouvelles formes de distribution et de consommation de musique, impactant ainsi les revenus des créateurs. \nAinsi, les diffusions à la radio et à la télévision ont toujours été des sources de revenus cruciales pour les créateurs de musique, jouant un rôle essentiel dans la promotion de leurs œuvres et dans la génération de revenus. Ces médias traditionnels offrent une large portée et une exposition significative aux artistes, ce qui peut se traduire par des revenus substantiels grâce aux redevances. \nAinsi, les sociétés de gestion collective jouent un rôle central dans la collecte des redevances pour les artistes à partir de ces diffusions. Ces sociétés, telles que la SACEM en France [3], la BMI [4] et l'ASCAP [5] aux États-Unis, et la PRS for Music au Royaume-Uni [6], agissent en tant qu'intermédiaires entre les diffuseurs (stations de radio, chaînes de télévision) et les créateurs de musique. Elles négocient des accords de licence avec les diffuseurs pour l'utilisation des œuvres musicales et collectent les redevances pour le compte des artistes et des compositeurs.\nLes redevances versées aux artistes à partir des diffusions radio et télévisuelles sont généralement basées sur des métriques telles que le nombre de diffusions et la taille de l'audience. Les sociétés de gestion collective utilisent souvent des systèmes de surveillance sophistiqués pour suivre et quantifier la diffusion des œuvres musicales sur les ondes radio et les chaînes de télévision. Ces systèmes permettent de calculer les redevances dues à chaque artiste en fonction de la fréquence de diffusion de leurs chansons et du nombre d'auditeurs atteints. \nDans ce contexte, une étude a été faite par L. Ivan [7] ou il a exploré les aspects économiques de la propriété intellectuelle dans l'industrie musicale. Il a analysé en profondeur les différents aspects de la réglementation du droit d'auteur dans le domaine musical, en se concentrant sur la manière dont cela influe sur les revenus des artistes, les médias et les performances en direct. Il a examiné également les modèles économiques liés à la musique, y compris les revenus provenant de la vente de musique enregistrée, de la diffusion radio et télévisuelle, ainsi que des performances en direct. \nPar ailleurs, avec l’avènement de la musique numérique et des plateformes de streaming, de nouvelles méthodes ont été développées pour suivre les diffusions en ligne. Des algorithmes sophistiqués sont utilisés pour comptabiliser les écoutes sur des services de streaming tels que Spotify, Apple Music et Deezer. Par exemple, Marc Ivaldi et al., [8] ont examiné l'impact de ces plateformes sur les ventes de musique enregistrée et les dynamiques concurrentielles de l'industrie. En analysant les ventes et les prix de plus de 4 millions de produits musicaux sur le marché français entre 2014 et 2017, les chercheurs constatent une relation complexe entre les ventes physiques et numériques ainsi que la consommation en streaming. Au niveau de la piste, le streaming semble remplacer les ventes numériques, tandis qu'au niveau de l'artiste, les ventes numériques remplacent les ventes physiques, mais le streaming a un effet promotionnel sur ces dernières, en particulier pour les genres populaires comme la pop et la variété. Cette étude met en lumière la coexistence de la substitution entre les canaux de consommation et la complémentarité à un niveau agrégé plus élevé, offrant ainsi une perspective nuancée sur l'impact des plateformes de streaming sur l'industrie musicale.\n\nEt dans le même contexte, Luis Aguiar et al. ont démontré dans leur article [9] l'importance de la numérisation et son impact sur l'augmentation de la production de nouvelles musiques. En effet, ils ont exploré l'effet des listes de lecture de Spotify à la fois sur la promotion des chansons et la découverte de musique par de nouveaux artistes, en utilisant quatre approches. Premièrement, ils ont examiné les volumes d'écoute des chansons avant et après leur ajout ou leur suppression des listes de lecture mondiales majeures. Deuxièmement, ils ont comparé les volumes d'écoute des chansons uniquement présentes sur les listes de lecture algorithmiques du top 50 avec celles qui n'y sont pas. Troisièmement, ils ont exploité les différences entre les pays concernant l'inclusion dans les listes New Music Friday, en utilisant des effets fixes de chanson pour expliquer les variations d'écoute. Quatrièmement, ils ont développé une approche à variables instrumentales pour expliquer les différences de classement entre les pays dans les listes New Music Friday en fonction du biais national. Être ajouté à Today’s Top Hits, une liste suivie par 18,5 millions de personnes pendant la période d'échantillonnage, augmente les écoutes de près de 20 millions et est évalué entre 116 000 et 163 000 dollars. L'inclusion dans les listes New Music Friday augmente considérablement la probabilité de succès d'une chanson, notamment pour les nouveaux artistes.\nD’autre part, d’autres technologies ont récemment été développées pour améliorer la transparence, la traçabilité et la distribution des revenus dans l'industrie musicale, en particulier à travers l'utilisation de contrats intelligents. Citant par exemple la technologie blockchain, qui permet de créer un registre transparent et immuable des transactions, ce qui aide ce à suivre et distribuer les revenus des artistes de manière plus équitable. Chaque transaction de diffusion ou de vente de musique peut être enregistrée sur une chaîne de blocs, assurant ainsi un suivi précis de l'utilisation des œuvres musicales et des paiements associés. Cela réduit les risques de piratage et protège les droits d'auteur en rendant chaque action traçable et vérifiable.  De plus, à travers les contrats intelligents qu’elle propose, qui sont notamment des programmes auto-exécutables où les conditions de l'accord entre acheteur et vendeur sont directement écrites dans des lignes de code.  Ces contrats peuvent automatiser la distribution des redevances pour les créateurs de musique. Par exemple, lorsqu'une chanson est diffusée ou achetée, les conditions prédéfinies du contrat intelligent déclenchent automatiquement le paiement des redevances aux artistes, producteurs et autres parties prenantes. Cela réduit les retards de paiement et minimise les erreurs administratives. Et dans ce contexte, une étude à été publiée par Halgamuge, et al., [10], ou ils ont discuté de la mise en œuvre des contrats intelligents sur la blockchain pour assurer une distribution équitable des récompenses dans l'industrie musicale. Il souligne le rôle de la transparence et de l'immutabilité pour maintenir des enregistrements précis des droits et des revenus musicaux. Cependant, il note également des limitations telles que les problèmes de scalabilité et la complexité de l'intégration avec les systèmes existants. \nDe plus, les technologies de marquage et d’identification jouent un rôle crucial dans la gestion des droits et la répartition des revenus dans l'industrie musicale. Citant par exemple la technologie de watermarking audio qui consiste à insérer des marques invisibles dans les fichiers musicaux afin de tracer leur utilisation et garantir une répartition équitable des revenus. Des études ont été publiées par Junjie He et al.,[11] ont discuté d’une méthode robuste de watermarking en utilisant la technique de spread spectrum. Les auteurs mettent en évidence la robustesse du watermarking face à diverses manipulations de signaux, bien que des défis subsistent en termes d’imperceptibilité et de résistance aux attaques complexes.\nCependant, malgré les progrès réalisés dans l'analyse de l'impact des listes de lecture sur la promotion musicale et la découverte d'artistes, il reste une lacune importante à combler. En effet, il n'existe pas encore de solution universelle permettant d'identifier en temps réel et de manière exhaustive la musique diffusée sur les différentes plateformes de streaming . Cette limite entrave la capacité à calculer précisément les revenus des créateurs de musique, car les données de diffusion sont souvent fragmentées et difficilement accessibles. Par conséquent, nous proposons une solution performante permettant de surmonter ces défis fournissant une étude complète des œuvres diffusés et un calcul détaillé des revenus des créateurs de ces œuvres."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• La maîtrise de la fréquence du réseau électrique est essentielle, car de petites fluctuations peuvent entraîner des coupures massives, comme lors de l'incident du 4 novembre 2006 en Europe qui a privé 15 millions de foyers d'électricité. Une baisse importante de la fréquence, autour de 48,5 Hz, peut entraîner l'arrêt des centrales nucléaires [1, 2].\n\n• En France, le Réseau de Transport d'Électricité (RTE) est responsable du maintien de l'équilibre entre la consommation et la production d'électricité. Cet équilibre est parfois géré en connectant des batteries au réseau, mais en l'absence d'une stratégie d'optimisation prédictive, la gestion des batteries reste coûteuse et inefficace. Actuellement, la gestion de la charge des batteries n'anticipe pas les fluctuations de fréquence, ce qui entraîne des coûts élevés [3].\n\n• Des techniques de Machine Learning, telles que la méthode WNN (k-weighted-nearest-neighbor), ont été utilisées pour prédire la fréquence électrique. Bien que cette méthode fournisse des prédictions relativement précises, elle échoue à capturer certaines propriétés aléatoires de la fréquence, limitant ainsi sa robustesse [4].\n\n• D'autres techniques, comme les réseaux neuronaux NAR (Nonlinear Auto-Regressive) et LSTM (Long Short Term Memory), ont montré des performances satisfaisantes pour la prédiction de la fréquence, avec une erreur quadratique moyenne (RMSE) d'environ 0,01. Cependant, ces modèles sont limités par leur utilisation de séries temporelles unidimensionnelles et un manque d'étude approfondie des hyperparamètres, ce qui pourrait améliorer leur précision [5].\n\n• Les technologies NILM (Non Intrusive Load Monitoring), couplées aux compteurs intelligents, permettent de désagréger la courbe de charge pour détecter des activités ou des équipements en cours d'utilisation. Ces technologies peuvent être utilisées pour sécuriser les logements ou aider à l'autonomie des seniors. Des techniques utilisant des réseaux neuronaux convolutifs (CNN) couplés à la récurrence graphique pondérée ont amélioré la classification des appareils électroménagers, mais le choix des hyperparamètres reste un défi important [6].\n\n• Des classifieurs basés sur des réseaux neuronaux à propagation avant multicouches (Feedforward Neural Network) ont été appliqués pour détecter des équipements et activités à partir de données de compteurs intelligents, atteignant une précision moyenne de 76 % sur sept types d'équipements électroménagers (sèche-cheveux, aspirateur, fer à repasser, micro-ondes, bouilloire, grille-pain, four). Cependant, ces modèles nécessitent un entraînement sur plusieurs échantillons, ce qui peut affecter leur robustesse et leur performance [7].\n\n• Dans le cadre du développement durable, la flexibilisation du réseau électrique et la décarbonation des bornes de recharge des véhicules électriques sont des enjeux essentiels. Des systèmes de gestion de l'énergie (EMS) couplés à des sources d'énergie photovoltaïque permettent une gestion dynamique de la recharge. Toutefois, des paramètres importants, comme la réduction du TCO2 et l'optimisation des coûts de recharge, ne sont pas encore intégrés. Des réglementations comme le décret tertiaire, le décret BACS et la réglementation RE2020 visent à réduire la consommation énergétique et l'empreinte carbone [8, 9, 10, 11].\n\n• La production d'hydrogène vert, utilisant des énergies renouvelables comme les panneaux solaires et les éoliennes, est une piste prometteuse pour la décarbonation. Les méthodes BWM (Best-Worst Method) et ARAS (Additive Ratio Assessment) sont utilisées pour sélectionner les meilleurs sites de production en fonction de critères comme les coûts de production, les conditions naturelles, et l'impact environnemental. Cependant, les coûts liés à la production ne sont pas encore suffisamment mis en avant pour attirer les investisseurs [12].\n\n• L'évaluation de l'état de santé des batteries, notamment les batteries lithium-ion, est cruciale en raison de leur importance pour les véhicules électriques et des défis environnementaux qu'elles posent. Des modèles hybrides combinant des approches physiques et de Machine Learning ont été proposés pour prédire leur durée de vie, mais ils nécessitent des jeux de données massifs et des bibliothèques performantes, ce qui représente un défi majeur. Les batteries sodium-ion, bien qu'elles présentent une alternative potentiellement plus écologique, offrent pour l'instant des performances bien inférieures [13, 14, 15].\n\n• Une méthode basée sur un modèle bayésien hiérarchique a été utilisée pour optimiser les protocoles de cyclage des batteries lithium-ion-phosphate/graphite, avec des résultats prometteurs et une erreur de prédiction de 6,5 %. Cependant, cette approche dépend fortement des données d'entraînement et des paramètres utilisés, ce qui pose des problèmes de reproductibilité et de fiabilité [16]."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nLa parfaite maitrise de la valeur de la fréquence du réseau électrique revêt d’une importance majeure, étant donné que de faibles fluctuations sur ce paramètre peuvent conduire à des coupures à grande échelle de l’approvisionnement électrique. Des conséquences désastreuses peuvent ainsi découler de l’abaissement de la valeur de la fréquence, avec l’arrêt des centrales nucléaires autour de 48.5 Hz par exemple [1].\n\nNous nous rappelons, à cet effet, des événements du 04 Novembre 2006, ayant engendré l’une des pannes électriques les plus importantes en Europe, avec environ 15 millions de foyers privés d’électricité [1,2]. Cette situation, ayant été provoquée par la mise hors tension d’une ligne électrique de 400 kV visant à permettre le passage d’un paquebot, déclencha la surcharge de plusieurs autres lignes électriques. Ce grave incident engendra ainsi des perturbations autour de la fréquence électrique européenne s’établissant à 50 Hz, cette dernière reflétant l’équilibre entre la consommation et la production électrique qui doit constamment être préservé. \n\nEn France, c’est particulièrement le réseau de transport d’électricité (RTE) qui s’occupe du maintien de cet équilibre [3]. Cet équilibre entre la consommation et la production peut être réalisé notamment par la connexion de batteries au réseau électrique, permettant ainsi à la batterie de se charger grâce au surplus au cours des fluctuations de fréquence supérieures à 50 Hz, ou de se décharger dans la configuration inverse.  Cependant, le point de consigne de la batterie utilisée n’est actuellement pas anticipé, et il n’y a donc aucune stratégie d’optimisation marché permettant de prédire l’équilibre sur le réseau électrique.  La gestion de la charge de la batterie se révèle ainsi très coûteuse, altérant les performances financières de cette dernière. Cette situation implique donc la nécessité de disposer de méthodes précises de prédiction de la fréquence électrique facilitant l’anticipation d’éventuelles perturbations qui peuvent intervenir sur le réseau.\n\nDes techniques de Machine Learning ont été appliquées en vue d’apporter une réponse à cette problématique. La méthode WNN (k-weighted-nearest-neigbor), permettant notamment de réaliser des prédictions à partir de motifs observés sur une période antérieure, a été employée à cet effet, sur des données fréquentielles publiques récoltées auprès de la Grande Bretagne, de l’Europe continentale et de l’Europe du Nord [4].  Grâce à cette technique, les prédictions réalisées quant à l’évolution de la fréquence sont relativement précises dans la plupart des cas, comme nous pouvons d’ailleurs l’observer sur la figure 1.a. Cependant, comme nous pouvons également le constater à la figure 1.b, la méthode WNN employée ne parvient pas à capturer certaines propriétés aléatoires intrinsèques à la nature de la fréquence, induisant dans la majorité des cas où ces effets aléatoires interviennent, des prédictions dans une direction opposée à celle des fréquences réelles observées. Nous comprenons donc par-là les limites de robustesse de cette méthode quant à la prédiction fréquentielle. \n\n\n\n\n\n\n\nFigure 1 : Prédictions des valeurs de fréquence réalisées à partir de la méthode WNN [4] :        a. Meilleures prédictions ; b. Mauvaises prédictions.\n\n\nD’autres techniques, exploitant des réseaux neuronaux tels que la NAR (Nonlinear Auto-Regressive) et les LSTM (Long Short Term memory) ont également été appliquées sur des données simulées et des données provenant de dispositifs dits PMU (Phasor Measurement Unit) permettant notamment d’évaluer la phase du courant électrique, toujours concernant la problématique relative à la prédiction de la fréquence électrique [5]. Les performances de prédiction de cette méthode, évaluées entre autres à partir de la RMSE (Root Mean Square Error), comme présenté à la figure 2, sont assez satisfaisantes, avec des valeurs de RMSE d’environ 0,01. Cependant, ces prédictions ne sont réalisées qu’en tenant compte de l’unidimensionnalité des séries temporelles de fréquence utilisées. De plus, l’étude d’hyperparamètres (paramètres ajustables permettant d’administrer les entrainements des algorithmes exploités), qui pourrait notablement permettre d’améliorer ces prédictions, n’a pas été l’objet d’investigations approfondies par les auteurs. \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2 : Valeurs de RMSE obtenues pour les modèles NAR et LSTM [5].\n\n\n\nLes questions relatives à la consommation électrique ont également toujours été des problématiques fondamentales. Aujourd’hui, les technologies NILM (Non intrusive Load Monitoring) couplées aux compteurs intelligents, proposent des solutions permettant de désagréger la courbe de charge, cette dernière correspondant à la consommation électrique sur une période donnée et représentée sous la forme d’un diagramme. Cette désagrégation peut être exploitée ultérieurement pour la détection d’activités ou encore d’équipements en cours d’utilisation. La détection d’activités peut par exemple, permettre la sécurisation des logements secondaires ou des logements mis en location temporaire, l’aide à l’autonomie des seniors en leur facilitant notamment un maintien à domicile via l’alerte des proches ou du personnel médical en cas d’accident, ou encore la détection de l’heure du retour de l’école des enfants. \n\nDes techniques dites de récurrence graphique pondérée (RG) couplées à des réseaux neuronaux convolutifs (CNN), ont été appliquées en vue d’améliorer l’identification des appareils électroménagers en ce qui concerne la détection d’équipements [6]. L’exploitation de ces méthodes a notamment permis d’obtenir une nette amélioration quant à la classification des différents appareils électroménagers par rapport aux méthodes traditionnelles d’identification impliquant l’utilisation des caractéristiques courant-tension de ces appareils, ces dernières présentant bien souvent des similarités entre plusieurs équipements. Cependant, la question relative au choix des hyperparamètres, qui peut fortement dépendre du type de données analysé, n’a pas été investigué dans cette étude. Cela pourrait donc avoir un fort impact quant à l’efficacité de cette technique de classification sur d’autres types ou structures de données. \n\nDes classifieurs NILM basés sur des méthodes de Machine Learning, exploitant un modèle avec des réseaux neuronaux à propagation avant multicouches (Feedforward Neural Network), dans laquelle l’information ne se propage que vers l’avant, ont également été appliqués en vue de la détection d’équipements et d’activités journalières, ceci à partir de données provenant de compteurs intelligents [7]. Ces outils ou algorithmes élaborés avec 500 nœuds et 3 couches cachées ont permis d’aboutir à des résultats satisfaisants avec une précision moyenne d’identification d’environ 76 % sur sept différents types d’équipements électroménagers (le sèche-cheveux, l’aspirateur, le fer à repasser, le micro-onde, la bouilloire, le grille-pain et le four).  Cependant, les auteurs soulèvent la nécessaire importance pour leurs modèles de réseaux neuronaux d’être entrainés avec plusieurs échantillons des différents appareils devant être identifiés. Des problèmes majeurs de performance et de robustesse peuvent donc subvenir quant à l’exploitation de ces algorithmes.\n\nS’inscrivant toujours dans une politique de développement durable, nous nous sommes intéressés à la flexibilisation du réseau électrique, mais aussi à la décarbonation des bornes de recharge des véhicules électriques. Il faut dire que des mesures sont de plus en plus adoptées au sujet de cette politique. Nous pouvons citer en l’occurrence le décret tertiaire [8] visant à réduire la consommation énergétique des entreprises occupant des bâtiments de plus de 1000 m2 dans le cadre d’activités tertiaire, le décret BACS [9] (Building Automation and Control System) visant à doter d’ici le 1er Janvier 2025 les infrastructures à usage tertiaire de dispositifs d’autonomisation pour la réduction de la consommation énergétique, et la règlementation environnementale RE2020 [10], visant à limiter l’effet carbone dans les bâtiments.\n\nLa demande sur le réseau devient beaucoup plus forte avec la mise en circulation massive des véhicules électriques, et implique de plus en plus la prise en compte de systèmes de production photovoltaïques (dits propres) pour répondre à ce besoin. Des systèmes de gestion de l’énergie (EMS) sont exploités et permettent une gestion beaucoup plus flexible des sites résidentiels de recharge en ce qui concerne ces véhicules électriques, ceci via l’intégration ou le couplage de ces systèmes d’énergie photovoltaïques au réseau électrique [11]. Grâce à cette stratégie, l’EMS assure de manière dynamique l’énergie à octroyer à la charge du véhicule électrique dépendamment de l’état de charge initial, de l’état de charge souhaité, du temps prévu pour le départ du véhicule, mais également de la capacité de la batterie permettant la charge. Cependant, comme nous l’aurons remarqué, des paramètres relativement importants, entre autres la réduction de l’intensité carbone via la recharge de la batterie au moment où le TCO2 (représentant la mesure du dioxyde de carbone) est faible, particulièrement dans le cadre de la transition énergétique, ou encore la réduction du coût économique via des recharges à des périodes de faible tarification par exemple, ne sont pas pris en compte ou évoqué dans cette étude. Cela nécessite donc de mener des travaux ou investigations complémentaires visant à intégrer ces paramètres dans toute solution proposée.  \n\nDes réflexions portant sur la production d’hydrogène vert pour la décarbonation, notamment via l’utilisation de panneaux solaires et d’éoliennes ont aussi fortement émergé ces dernières années. Il s’agit là d’une énergie propre (ici sans émission de CO2), offrant par la même occasion une flexibilité en termes de stockage d’énergie. Les travaux qui se sont penchés sur cette problématique ont permis d’établir plusieurs critères ou paramètres affectant la production d’hydrogène suivant différents sites, à savoir les énergies potentielles du vent et du soleil, les désastres naturels, la température de l’air, l’humidité relative, les coûts de production et bien d’autres paramètres encore [12]. Ainsi, grâce à la technique BWM (best-worst method) visant à attribuer un poids à ces différents paramètres, et à la méthode ARAS (Additive Ratio Assessment) visant à déterminer le meilleur choix via la prise en compte des paramètres définis et des poids accordés à chacun de ces paramètres, les meilleurs sites pour la production de l’hydrogène peuvent donc être obtenus. Cependant, même si les coûts liés à la production et l’exploitation sont pris en compte dans la méthode proposée par les auteurs, ils constituent des paramètres comme tant d’autres (qui ne sont pas ici les plus décisifs) dans la détermination de l’emplacement idéal. Si nous souhaitons attirer le maximum d’investisseurs à la cause de l’hydrogène vert, nous devons nous efforcer, comme nous l’aurons compris, à développer des analyses robustes proposant des coûts optimisés quant à l’exploitation et la production de cet hydrogène vert.\n\nNous nous sommes également intéressés à l’évaluation de l’état de santé des batteries au cours de nos différents travaux. Il faut dire que les batteries, notamment les batteries lithium-ion, sont énormément exploitées en ce qui concerne les véhicules électriques. Elles connaissent un grand succès en raison de leurs capacités de stockage, leur permettant d’emmagasiner environ quatre fois plus d’énergie que les autres batteries. De plus, elles présentent une charge rapide. Cependant, l’utilisation des batteries lithium-ion impliquent des problèmes environnementaux, avec à titre d’illustration la pollution des sols au cours de leurs productions [13]. D’autres alternatives, comme les batteries sodium-ion, cependant avec pour le moment des performances beaucoup plus faibles, sont également envisagées [14], en ce qui concerne cette dernière problématique environnementale. \n\nCes batteries (lithium-ion ou sodium-ion) présentent des performances qui s’atténuent plus ou moins vite dans le temps, en raison de l’irréversibilité des cycles de charge et de décharge auxquels elles sont soumises. Il apparait donc nécessaire de prédire leurs états de santé en vue d’une meilleure exploitation de ces systèmes, avec aussi des coûts optimisés. \n\nDes travaux ont été réalisés en vue de prédire la durée de vie de ces batteries. Des architectures hybrides combinant à la fois des modèles physiques (s’appuyant notamment sur l’électrochimie par exemple) et des modèles de Machine Learning ont été proposées à cet effet [15].  Cependant, le développement de telles architectures nécessite des jeux de données massives pour l’entrainement et la validation des modèles élaborés, ce qui n’est pas totalement le cas, avec la récente mise à disposition partielle des données de recherche relativement à ce domaine. De plus, il faudrait développer des bibliothèques performantes pour réaliser des modélisations adéquates en électrochimie, ce qui reste encore un défi même si des travaux sont actuellement en cours. \n \nUne autre méthodologie employant notamment un modèle Bayésien hiérarchique a été proposée et appliquée sur un jeu de données obtenu sur des cellules lithium-ion-phosphate/graphite, en vue de prédire les protocoles de cyclage permettant d’optimiser les performances de la batterie [16]. Cette nouvelle méthodologie a permis par exemple, d’obtenir de bons protocoles avec des durées de vie plus que satisfaisantes, ceci avec une erreur de prédiction d’environ 6,5 %. Cependant, cette approche tout comme la précédente, ne présente généralement des résultats acceptables que dépendamment des données exploitées quant à l’entrainement des modèles et des paramètres utilisés dans l’établissement du modèle. Des problèmes de reproductibilité et de fiabilité peuvent ainsi découler de l’exploitation de cette approche. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Nos recherches sur l'état de l'art se sont concentrées sur l'analyse de la consommation de gaz au sein des foyers, bien que les études existantes soient souvent centrées sur la consommation d'électricité.\n\n• Jeong et al. (2011) ont étudié les modèles de consommation d'énergie des ménages, notamment la substitution entre l'électricité et le gaz. Ils ont utilisé un modèle de valeurs extrêmes pour identifier les préférences des ménages en matière de chauffage, révélant une préférence pour le gaz. Cependant, leur étude se base sur des données estimées par les utilisateurs plutôt que sur des mesures directes, et n'aborde ni la prédiction ni la décomposition de la consommation par usage.\n\n• Völlink et Meertens (2010) ont exploré l'impact des compteurs de gaz à prépaiement comme moyen de rétroaction pour réduire la consommation de gaz des ménages. Les résultats montrent une réduction de 4,1 % de la consommation annuelle pour les ménages équipés de ces compteurs. Bien que leur étude soit davantage comportementale, elle souligne l'importance d'un pilotage actif de la consommation basé sur une information économique claire.\n\n• Boukarta et Berezowska-Azzag (2018) ont analysé l'augmentation de la consommation d'énergie malgré les politiques d'efficacité énergétique. Leur étude, menée dans 36 municipalités de Djelfa (Algérie), a révélé que la taille du ménage et la surface du logement sont des facteurs clés influençant la consommation de gaz et d'électricité. Ils ont utilisé des techniques SIG et des analyses statistiques pour modéliser et évaluer les impacts des caractéristiques des ménages sur la consommation énergétique.\n\n• Nordahl et al. (2019) ont appliqué des techniques de clustering pour modéliser le comportement de consommation d'électricité des ménages. Bien que cette approche ait permis d'identifier des profils de consommation, elle n'est pas adaptée à la modélisation de la consommation de gaz, en raison de la variabilité de la consommation des appareils à gaz selon leur niveau d'échauffement.\n\n• Papageorgiou et al. (2019) ont analysé les facteurs influençant la consommation d'électricité dans les foyers, incluant le comportement des ménages, les caractéristiques démographiques, et les spécifications techniques des équipements. Ils ont utilisé des questionnaires pour recueillir des données sur les équipements et les pratiques des ménages, mais cette approche ne permet pas une analyse détaillée de la consommation de gaz.\n\n• Pavlović et al. (2019) ont étudié la consommation de gaz naturel sur une période de dix ans dans des foyers en Serbie, en se concentrant sur l'impact des paramètres météorologiques. Leur étude inclut une enquête auprès des résidents, mais elle ne propose pas de solutions pour la modélisation prédictive ou la désagrégation de la consommation par usage.\n\n• Beckel et al. (2014) ont proposé une méthode d'analyse des données de compteurs intelligents pour estimer automatiquement les caractéristiques des ménages en fonction de leur consommation d'électricité. Bien que leur approche soit efficace pour l'électricité, elle ne prend pas en compte la complexité de la consommation de gaz dans un contexte de thermostat variable.\n\n• Indikawati et Zamroni (2019) ont utilisé l'algorithme d'Extreme Gradient Boosting pour prévoir la consommation d'électricité des ménages en utilisant des données de compteurs intelligents. Leur modèle fonctionne bien avec de petits ensembles de données, mais il se concentre uniquement sur l'électricité, sans aborder la consommation de gaz.\n\n• Singh et al. (2017) ont proposé un mécanisme non supervisé pour extraire des motifs de consommation d'énergie à partir de données de compteurs intelligents. Leur approche vise à identifier les appareils responsables de la consommation importante, mais elle est limitée à l'analyse de l'électricité et ne traite pas de la complexité des systèmes de chauffage au gaz.\n\n• Kipping et Trømborg (2016) ont modélisé et désagrégé la consommation d'électricité par heure en utilisant des modèles de régression multiple. Leur méthode permet d'estimer la consommation d'énergie pour le chauffage, mais elle ne prend pas en compte les phénomènes influant sur la température intérieure, tels que les pertes thermiques ou l'ensoleillement, et n'est pas applicable à la consommation de gaz."}, {"role": "assistant", "content": "1.3.1\tEtat de l’art\nNos recherches de l’état de l’art ont porté sur les travaux d’analyse de la consommation de gaz au sein de foyers.\n\nNous avons identifié une initiative de recherche visant la classification de foyers en fonction de leur consommation énergétique, en abordant celle-ci du point de vue de la consommation d’électricité plutôt que de gaz.\n\nLa consommation d'énergie a radicalement été modifiée en raison de l'épuisement des sources d'énergie, des fluctuations de prix, de la prise de conscience collective de l’effet de la pollution due à la surconsommation d’énergie et des politiques gouvernementales. Les sources d'énergie des ménages sont interdépendantes, et le prix de l'énergie et les caractéristiques des ménages, en particulier la composition du ménage et la taille du logement, influent sur l'utilisation. Pour fournir de l'énergie de manière cohérente et atteindre un équilibre entre la production et la consommation, les parties prenantes doivent comprendre le comportement de consommation de leurs clients en matière d'énergie.\nJeong & al. (2011) ont identifié les modèles d'utilisation de l'énergie de chauffage des ménages et les relations de substitution et/ou de complémentarité entre l'électricité et le gaz. Sur la base d'un modèle de valeurs extrêmes multiples discrètes-continues, la structure de consommation des ménages a pu être identifiée à partir des données sur l'utilisation du chauffage au gaz. Les résultats montrent une plus grande appétence pour les chaudières à gaz que pour les chauffages électriques. Les effets des caractéristiques socio-économiques et environnementales des consommateurs sur le choix des sources d'énergie de chauffage ont été analysés. De plus, pour une comparaison plus poussée, les personnes interrogées ont été réparties en catégories haute et basse pour le revenu, les degrés-jours de chauffage, la taille du logement et l'utilisation du gaz. Le gaz s'est avéré être le choix de chauffage le plus économique pour les ménages. Cette étude porte sur le comportement des ménages en matière de consommation d'électricité et de gaz pour le chauffage. Elle étudie également la relation entre deux sources d'énergie et fournit des données quantitatives qui pourraient être utilisées pour concevoir des politiques énergétiques efficaces.\nSi cette étude vise le même objectif que nous en termes d’analyse de la consommation de gaz, elle s’appuie toutefois sur des données estimées par les utilisateurs et non des valeurs remontées par les équipements directement. En outre, la notion de prédiction et de décomposition de la consommation par usage est absente.\n\nEn outre, les travaux de Völlink & Meertens (2010) examinent l'effet des compteurs de gaz à prépaiement (les ménages paient leur gaz au fur et à mesure qu'ils l'utilisent) comme instrument de rétroaction sur la consommation de gaz des ménages. La moitié des ménages du groupe expérimental ont reçu un carnet leur permettant de comparer leur consommation de gaz avec un objectif qu'ils s'étaient fixé. Les résultats ont révélé que les ménages équipés d'un compteur à prépaiement ont réduit leur consommation annuelle de gaz de 4,1 % par rapport au groupe témoin ; le carnet n'a pas entraîné d'économies supplémentaires. En outre, les changements dans la consommation objective de gaz étaient liés aux changements autodéclarés dans l'intention de réduire la consommation. Si cette étude est plutôt comportementale et éloignée de nos objectifs scientifiques et technologiques, elle permet de confirmer l’intérêt d’un pilotage actif par les clients de leur consommation grâce à une information précise sur la conversion de leur comportement de consommation en un coût. Cette prévision économique semble plus pertinente qu’une prédiction quantitative du volume de gaz utilisé par un ménage.\n\nBoukarta & Berezowska-Azzag (2018) évaluent le comportement de consommation d’énergie dans un contexte d’augementation de celle-ci malgré les politiques publiques de maîtrise de l'énergie à travers l'amélioration de l'efficacité énergétique des bâtiments. En effet, si la consommation d'énergie continue d'augmenter au lieu de diminuer, les auteurs supposent le comportement des occupants, difficile à estimer et à prévoir, pourrait expliquer cette incohérence. L’article explore l'impact des ménages et des caractéristiques de l'habitat sur la consommation résidentielle de gaz et d'électricité dans 36 municipalités du département de Djelfa (Algérie) qui est situé dans une zone climatique aride et semi-aride. Des techniques SIG et statistiques sont abordées dans l’étude pour traiter les données de consommation énergétique annuelle de gaz et d'électricité (2013) des communes du département de Djelfa. La méthode est organisée en quatre étapes : (a) une régression linéaire multiple est utilisée pour construire deux modèles estimatifs du gaz et de l'électricité. Les modèles ont une précision de plus de 93% pour les deux modèles de gaz et d'électricité ; (b) l'estimation de la consommation de gaz et d'électricité pour 2008 selon les modèles développés ; (c) l'organisation des données du recensement de 2008 en cinq dimensions : la distribution de la population, les caractéristiques des ménages, le type de logement et l'occupation, et enfin la possession d'appareils ménagers ; (d) une série d'analyses de sensibilité est effectuée sur la base de l'analyse en composantes principales (ACP) et la corrélation bivariée de Pearson et enfin une analyse de chemin est effectuée sur la base du modèle d'équation structurelle (SEM) pour évaluer l'importance de chaque variable. L'impact global de toutes ces variables indique que l'augmentation de la taille du ménage est le premier facteur de réduction de la consommation d'électricité et de gaz, suivi de la surface du logement, de la densité, de l'occupation des pièces et des ménages plus âgés, tandis que l'augmentation du niveau d'éducation et la possession d'appareils électroménagers font augmenter la consommation de gaz et d'électricité par habitant.\n\nNordahl & al. (2019) ont quant à eux analysé les consommations d’électricité par les ménages. Ils ont appliqué des techniques de clustering pour analyser et comprendre les données de consommation d’électricité des ménages. Les connaissances extraites de cette analyse sont utilisées pour créer un modèle de comportement « normal » de consommation d'électricité pour chaque ménage. Initialement, les données de consommation d'électricité du ménage sont divisées en un certain nombre de clusters présentant des profils de consommation d'électricité quotidiens similaires. Les centroïdes des groupes générés peuvent être considérés comme des signatures représentatives du comportement de consommation d'électricité d'un ménage. L'approche proposée est évaluée en réalisant un certain nombre d'expériences sur les données de consommation d'électricité de dix ménages sélectionnés. Les résultats obtenus montrent que l'approche proposée est adaptée à l'organisation et à la compréhension des données, et qu'elle peut être appliquée à la modélisation du comportement de consommation d'électricité au niveau des ménages.\nToutefois, cette modélisation unitaire de chaque logement n’est pas présentée dans les travaux. En outre, l’approche électrique de l’étude plutôt que le gaz réduit la difficulté de l’estimation de la consommation réelle lors du fonctionnement des équipements puisque les équipements électriques ont une consommation nominale au courant et au voltage prévus ; contrairement aux chauffages au gaz qui peuvent subir des variations de consommation selon le niveau d’échauffement demandé.\n\nDe la même façon, Papageorgiou & al. (2019) ont mené une analyse approfondie des facteurs influençant la consommation d'électricité au sein des foyers. Les facteurs retenus sont basés sur les caractéristiques suivantes :\n\tLe comportement des ménages en matière de consommation d'électricité. C'est-à-dire la manière dont l'électricité est utilisée par les propriétaires. Par exemple, éteindre les appareils après utilisation, et régler la température appropriée des appareils de climatisation.\n\tLes caractéristiques démographiques des propriétaires, telles que la taille de la famille et le revenu total.\n\tLes caractéristiques techniques des équipements et matériaux électriques et mécaniques. C'est-à-dire l'existence et le type d'isolation, le type d'appareils électriques et le type d'unités de chauffage et de climatisation.\n\tLes attitudes/comportements des habitants et plus généralement, leur perception des questions environnementales, des économies d'énergie et de l'utilisation des énergies propres.\nL'objectif final est de comprendre ce qui influence la consommation d'électricité en établissant des relations entre les différents facteurs énumérés ci-dessus. Plus précisément, l'objectif de l'enquête était de recueillir des données sur la consommation d'électricité et les caractéristiques des bâtiments, des équipements ménagers et des membres du ménage. Les données recueillies à l’aide de questionnaires soumis aux participants à l’étude se résument à une collecte d’information sur le type d’équipements électriques dont dispose le ménage et une information sur les pratiques et sur le coût annuel de l’électricité.\n\nUne étude similaire a été menée par Pavlović & al. (2019) quant à la consommation de gaz naturel dans un échantillon de ménages en Serbie. Leur étude a porté sur des unités résidentielles d'exposition différente (nord et sud) situées dans deux rues, ainsi que sur un immeuble résidentiel multifamilial d'orientation neutre. La consommation de gaz naturel a été étudiée sur une période de dix ans (2006-2015), avec un examen comparatif de la déviation de certains paramètres météorologiques par rapport aux valeurs normales (couverture nuageuse et température quotidienne moyenne sur une base mensuelle). Pour les besoins de cette étude, l'enquête a été menée auprès des résidents de ces bâtiments et certains des résultats de l'enquête sont présentés dans ce document. Les questions de l'enquête étaient liées à la valorisation des habitudes et des caractéristiques des résidents en matière de consommation d'énergie.\n\nEn para\n\nLe déploiement de compteurs électriques intelligents dans des millions de foyers à travers le monde afin de collecter des données de consommation d'électricité à très petite granularité permet d’analyser les données recueillies et en déduire des caractéristiques et/ou envisager des prédictions. Beckel & al. (2014) proposent une approche de l'analyse automatique de ces données pour permettre la mise en place de programmes d'efficacité énergétique personnalisés et évolutifs pour les ménages. En particulier, ils développent et évaluent un système qui utilise des techniques d'apprentissage automatique supervisé pour estimer automatiquement les \"caractéristiques\" spécifiques d'un ménage à partir de sa consommation d'électricité. Ces caractéristiques sont liées au statut socio-économique du ménage, à son logement ou à son parc d'appareils. Ils évaluent leur approche en analysant les données des compteurs intelligents recueillies auprès de 4232 ménages en Irlande à une granularité de 30 minutes sur une période de 18 mois. L’analyse révèle la possibilité de déduire des caractéristiques à partir des données des compteurs intelligents, leur méthode atteignant une précision de plus de 70 % sur l'ensemble des ménages pour de nombreuses caractéristiques et dépassant même 80 % pour certaines d'entre elles. Les résultats sont applicables à tous les systèmes de compteurs intelligents sans avoir à modifier l'infrastructure de mesure. Les connaissances déduites ouvrent la voie à des programmes d'efficacité énergétique ciblés et à d'autres services qui bénéficient d'une meilleure connaissance du client.\nBien que ces travaux présentent une démarche originale pour la déduction de caractéristiques de composition des foyers selon leur consommation d’électricité, ils ne permettent pas de traduire la complexité de l’analyse de consommation du gaz dans un contexte de thermostat variable au sein des logements.\n\nEn outre, Indikawati & Zamroni (2019) se sont proposé de développer un système de prévision de la consommation d'électricité des ménages en utilisant un algorithme bien connu d'Extreme Gradient Boosting. Des séries temporelles de données provenant d'un ensemble de compteurs intelligents ont été utilisées pour créer un modèle prédictif. Tout d'abord, l'importance des caractéristiques des séries temporelles de l'ensemble de données a été évaluée ; l'ensemble de données original a ensuite été rééchantillonné pour entrainer le modèle et calculer la fonction de perte d’apprentissage. L’étude expérimentale avec les données réelles de l'IoT Smart Home a pu montrer que le système de prévision fonctionne bien avec un petit ensemble de données en utilisant un sous-échantillonnage d'une heure sur l'ensemble de données.\nDans le cadre de cette étude, encore une fois, il s’agit d’une évaluation de la consommation d’électricité qui ici, ne dépend que de caractéristiques comportementales du ménage. L’état de fonctionnement ou non de l’équipement n’altère pas son profil de consommation d’électricité, défini par son point de fonctionnement (courant, tension).\n\nPar ailleurs, les habitudes de consommation d'énergie des ménages, peuvent être déterminées en comprenant les activités des utilisateurs ainsi que les appareils utilisés et leurs heures d'utilisation. Ces informations peuvent être extraites des big data des compteurs intelligents. Cependant, le principal défi consiste à extraire les interdépendances complexes entre plusieurs appareils fonctionnant simultanément, et à identifier les appareils responsables d'une consommation d'énergie importante. En outre, en raison de la génération continue de données de consommation d'énergie, les associations d'appareils peuvent changer au fil du temps. Par conséquent, elles doivent être capturées de manière régulière et continue. Dans leur article, Singh & al. (2017) proposent un mécanisme d'exploration de données non supervisé appliqué aux données de consommation d'énergie des compteurs intelligents par le biais de l'exploration de motifs fréquents afin de surmonter ces difficultés.\n\nEnfin, les travaux de Kipping & Trømborg (2016) visent la modélisation et la désagrégation de la consommation électrique par heure. Les auteurs recourent à des modèles de régression multiple pour la consommation horaire d'électricité dans les habitations norvégiennes, basés sur des données de panel constituées de données horaires de compteurs intelligents, de données météorologiques et de données de réponse d'une enquête sur les ménages.\nDeux modèles basés sur les valeurs moyennes quotidiennes et horaires de la température extérieure, respectivement, sont comparés et discutés. Les résultats indiquent que la température extérieure moyenne quotidienne - représentée par les températures de consigne de chauffage - peut servir de variable d'entrée liée à la météo pour modéliser la consommation horaire globale d'électricité. Les modèles de régression sont ensuite utilisés pour décomposer la consommation horaire d'électricité en deux composantes, représentant la consommation modélisée pour le chauffage des locaux et les autres appareils électriques, respectivement. Ainsi, sans avoir à mesurer les appareils de chauffage électrique, une estimation de la consommation d'énergie de chauffage est disponible et peut être utilisée pour évaluer différentes options de gestion de la demande, par exemple la substitution de combustible ou le contrôle de la charge. En outre, les modèles peuvent être utilisés pour prévoir la consommation régionale globale d'électricité dans le secteur des ménages norvégiens avec une haute résolution temporelle, car les changements dans les conditions climatiques régionales, la structure des logements et les facteurs démographiques peuvent être pris en compte.\nBien que cette étude fournisse une méthodologie pour l’analyse et la désagrégation par usage de la consommation électrique, elle n’est pas réplicable dans notre projet qui porte sur l’analyse de la consommation de gaz. En outre, la modélisation simpliste de la consommation due au chauffage en termes de traduction de l’allumage et de la température de consigne, sans tenir compte des phénomènes influant sur la température intérieure du logement (pertes thermiques, ensoleillement, etc.).\n"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• L'état de l'art a été centré sur les méthodes de segmentation d'images en vision par ordinateur, avec un focus sur la détection de parkings via des images satellites, une problématique unique sans équivalent dans la recherche académique ou industrielle.\n\n• L'intelligence artificielle (IA) se concentre sur la reproduction de l'intelligence humaine à travers des algorithmes spécialisés, notamment en traitement du langage naturel et en vision par ordinateur.\n\n• La vision par ordinateur, sous-domaine de l'IA, cherche à analyser et interpréter les images pour extraire des informations utiles, via des tâches telles que la détection d'objets et la segmentation d'images.\n\n• Depuis la création d'AlexNet (Krizhevsky & al., 2012), les algorithmes de deep learning, comme ResNet, InceptionNet, et EfficientDet, dominent la vision par ordinateur grâce à leurs architectures de réseaux de neurones à convolution, optimisées pour l'efficacité.\n\n• Ces réseaux utilisent un « backbone » pour extraire des caractéristiques visuelles et des couches spécialisées pour accomplir des tâches spécifiques, facilitant le transfer learning, où le backbone peut être réutilisé pour diverses tâches après un premier entraînement (Weiss & al., 2016).\n\n• Bien que les algorithmes supervisés soient performants, ils peuvent poser des défis comme des difficultés de convergence ou des besoins en données d'entraînement élevés, parfois avec des performances finales insuffisantes pour une utilisation en production.\n\n• Pour la détection de parkings, des solutions reconnues en segmentation ont été explorées, notamment Mask R-CNN, qui étend Faster R-CNN avec une branche pour la prédiction de masques d'objets, et Deeplab (Chen & al., 2017), connu pour sa convolution atrous et son ASPP pour une segmentation multi-échelles.\n\n• Mask R-CNN est parmi les TOP 3 solutions de segmentation sur paperswithcode.com, simple à entraîner, généralise facilement à d'autres tâches, et fonctionne à 5 images par seconde.\n\n• Deeplab se classe dans le TOP 20 et excelle en segmentation sémantique avec la convolution atrous, l’ASPP pour capturer les objets à plusieurs échelles, et l’utilisation d’un CRF pour améliorer la localisation des objets.\n\n• Les travaux de Sağlam & Baykan (2017) sur la détection de toits via des images satellites ont été explorés. Leur méthode de classification d'images est basée sur des données d'entraînement supervisées, mais elle ne prend pas en compte la nécessité d'enrichir les données, un aspect crucial pour notre projet."}, {"role": "assistant", "content": "2.3.1\tEtat de l’art\nNotre état de l’art a porté sur les méthodes de segmentation des images en vision par ordinateur. En effet, notre problématique de détection de parking étant unique, nous n’avons identifié aucune initiative similaire, ni au niveau de la recherche académique ni émanant des communiqués de presses d’entreprises privées.\nL’intelligence artificielle (IA) vise à développer un ensemble de théories et de techniques capables de reproduire artificiellement l’intelligence humaine. Aujourd’hui, il n’existe pas de systèmes uniques capables d’intelligence généraliste mais uniquement des algorithmes capables de réaliser des tâches spécifiques avec une efficacité proche voire supérieure à celle des humains. Parmi ces tâches, celles qui ont aujourd’hui le plus d’impact sur nos sociétés s'intéressent au traitement du langage naturel et à la vision par ordinateur.\n\nLa computer vision est un sous-domaine de l’intelligence artificielle qui cherche à reproduire la vision humaine par le biais d’algorithmes capables d’analyser des images (photos, flux vidéo, etc.), et d’en extraire des informations utiles à leur compréhension. Ces algorithmes sont ainsi capables de détecter des objets dans des images (tâche de détection d’objets), y compris en créant des masques permettant de les détecter au pixel près (tâche de segmentation).\n\nDepuis bientôt 10 ans, depuis la création du réseau de neurones à convolution AlexNet (Krizhevsky & al. 2012), les algorithmes de deep learning sont de très loin les algorithmes les plus utilisés et les plus performants en termes d’efficacité à réaliser une grande variété de tâches et de temps d’inférence. Aujourd’hui, AlexNet n’est plus utilisé mais de nombreux réseaux de neurones reprenant son architecture ont vu le jour : ResNet (He & al., 2016), InceptionNet (Szegedy & al., 2015), EfficientDet (Tan & al., 2020), etc.\n\nCes réseaux utilisent tous une architecture de réseaux de neurones à convolution auxquels ils ajoutent un ensemble d’astuces permettant petit à petit d’améliorer leur efficacité. Ces algorithmes sont ainsi de plus en plus rapides en temps d’inférence, nécessitent moins de données pour leur entraînement, ou bien peuvent être si léger qu’ils peuvent être utilisés en temps réel sur les appareils mobiles MobileNetV2 (Sandler & al., 2018).\n\nCes réseaux utilisent tous une architecture proche de celles de ConvNets (dont fait partie AlexNet). Ils se composent d’une première série de couches dont l’ensemble forme un “backbone”, puis d’un ensemble de couches adaptées à la tâche ciblée. Le rôle du backbone est d’extraire des features, c'est-à-dire des caractéristiques visuelles qui permettent de décrire le contenu de l’image dans un format exploitable par les têtes du modèle. Les dernières couches sont, pour leur part, chargées de réaliser la tâche cible. La force de cette architecture est qu’elle permet de réaliser du Transfer learning. Le backbone peut être entraîné une première fois sur une tâche avec un volume de données conséquent, puis peut être réutilisé tel quel sur un grand nombre de tâches (Weiss & al., 2016).\n\nPour réaliser ces tâches, les algorithmes utilisés sont le plus souvent des algorithmes supervisés nécessitant d’être entraînés. Le processus d’entraînement implique des incertitudes, il peut ainsi arriver que, sur des tâches analogues à celles présentées dans la littérature, il soit très difficile voire impossible de faire converger les modèles entraînés. Il peut également arriver que l’entraînement nécessite un volume de données beaucoup plus important que celui des données disponibles. Enfin, les performances du modèle final peuvent se révéler insuffisantes pour être utilisables par un service en production.\n\nDans le cadre de notre projet, nous avons besoin de détecter des parkings sur la base d’images satellites. Pour réaliser cette tâche, nous avons recherché des algorithmes capables de réaliser une tâche de détection d’objets et de classification d’images.\nBien que des modèles pré-entraînés soient disponibles pour détecter une grande variété d’objets, il n’existe pas de modèles pré-entraînés capables de détecter des espaces assimilés à des parkings.\n\nNous nous sommes orientés vers des solutions de l’état de l’art reconnues pour leurs performances sur des tâches de segmentation.\n\nLa méthode, appelée Mask R-CNN, étend Faster R-CNN (Ren & al., 2015) en ajoutant une branche pour la prédiction d'un masque d'objet en parallèle avec la branche existante pour la reconnaissance de la boîte de délimitation. Le cadre conceptuel simple, flexible et général pour la segmentation d'instances d'objets s’appuie sur une approche permettant de détecter efficacement les objets dans une image tout en générant simultanément un masque de segmentation de haute qualité pour chaque instance. Mask R-CNN est simple à entraîner et n'ajoute qu'une faible surcharge à Faster R-CNN, fonctionnant à 5 images par seconde. De plus, Mask R-CNN est facile à généraliser à d'autres tâches.\nNous avons en outre constaté que cette méthode est classée dans le TOP 3 des solutions le plus implémentées pour des tâches de segmentation sur le site paperswithcode.com, traduisant les pratiques courantes en termes de développements IA.\n\nEn parallèle de cette méthode, nous nous sommes également particulièrement intéressés à Deeplab (Chen & al., 2017), classée dans le TOP20 du même site.\nDans ce travail, la tâche de la segmentation sémantique d'images avec l'apprentissage profond est abordée par les auteurs qui apportent trois contributions principales. Premièrement, ils mettent en évidence la convolution avec des filtres suréchantillonnés, ou « convolution atrous », comme un outil puissant dans les tâches de prédiction dense. La convolution atrous permet de contrôler explicitement la résolution à laquelle les réponses des caractéristiques sont calculées dans les réseaux de neurones convolutifs profonds. Elle permet également d'élargir efficacement le champ de vision des filtres pour incorporer un contexte plus large sans augmenter le nombre de paramètres ou la quantité de calcul. Deuxièmement, ils proposent la mise en commun de pyramides spatiales atrous (ASPP) pour segmenter de manière robuste les objets à plusieurs échelles. L’ASPP sonde une couche de caractéristiques convolutionnelles entrantes avec des filtres à plusieurs taux d'échantillonnage et champs de vision effectifs, capturant ainsi les objets ainsi que le contexte de l'image à plusieurs échelles. Troisièmement, ils améliorent la localisation des limites des objets en combinant des méthodes issues des DCNN et des modèles graphiques probabilistes. La combinaison communément déployée du max-pooling et du downsampling dans les DCNNs permet d'atteindre l’invariance mais a un impact sur la précision de la localisation. Ce problème est surmonté en combinant les réponses de la dernière couche du DCNN avec un champ aléatoire conditionnel (CRF) entièrement connecté, ce qui permet d’améliorer les performances de localisation tant sur le plan qualitatif que quantitatif. Le système « DeepLab » proposé établit un nouvel état de l’art lors de la tâche de segmentation sémantique d’images.\n\nEnfin, nous avons identifié les travaux de Sağlam & Baykan (2017) visant la détection de toits à partir d’images satellites. Les auteurs investiguent deux méthodes différentes de classification d'images numériques (l'analyse discriminante linéaire et les valeurs de distance normalisées) qui ont été comparées l'une à l'autre, en utilisant différents espaces de couleur (RVB, L*a*b* et HSV). L’étude est réalisée sur des images satellites qui ont été prises par des capteurs numériques aéroportés afin de détecter les objets sur le toit. Les caractéristiques communes des méthodes appliquées sont qu'elles sont supervisées parce qu'elles utilisent des données d'entraînement précédemment communiquées et qu'elles fonctionnent rapidement parce qu'elles opèrent linéairement en utilisant une valeur seuil. Pour cette raison, certaines des images de l'ensemble de données sont utilisées pour l'entraînement afin de détecter certains coefficients et la valeur seuil. Le jeu de données utilisé pour l’entrainement et le test sont les images acquises à partir de la base de données ISPRS WG III/4 2D Semantic Labeling. Dans la base de données, les images de référence de la classification sont également disponibles.\nBien que cette étude vise un cas d’utilisation proche du nôtre, la maitrise du contenu des images satellite ne confronte pas les auteurs à la problématique d’enrichissement des données que nous avons rencontré.\n"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• L'état de l'art se concentre sur la gestion efficace de l'énergie électrique et des infrastructures, avec un accent sur l'optimisation de la puissance dispatchée pour maximiser l'efficacité énergétique et la durabilité.\n\n• Les Energy Management Systems (EMS) sont cruciaux pour surveiller et gérer les réseaux électriques en temps réel, intégrant des outils de gestion de la charge et de la production. Kundur et Paserba (2004) ont discuté de l'adaptation des EMS à la sécurité, tandis que Srinivasan et Elia (2012) ont examiné les méthodes de contrôle et d'optimisation des smart grids [1,2].\n\n• Les algorithmes d'optimisation avancés permettent de minimiser des critères définis (perte économique, production de CO2, etc.) en s'appuyant sur des prédictions de la charge demandée [2]. Mohsenian-Rad et Leon-Garcia (2010) ont étudié le contrôle optimal de la charge dans les environnements de tarification en temps réel, et Chen et al. (2017) ont examiné la gestion de la demande avec l'intégration des énergies renouvelables [3,4].\n\n• La surveillance de l'état de santé des infrastructures électriques est essentielle pour la maintenance préventive et la prévention des pannes. Shahidehpour et Fu (2015) ont abordé la gestion des actifs d'une microgrid, et Gao et Simon (2012) ont discuté des défis de la mesure en ligne des décharges partielles, particulièrement pour les batteries où la mesure de l'état de santé (SoH) est complexe [5,6].\n\n• L'IA et la science des données sont de plus en plus utilisées pour améliorer la gestion de l'énergie, notamment à travers la prédiction de la demande et la détection d'anomalies. Wang et Hug (2016) ont montré que la consommation électrique peut être prédite par des algorithmes de régression ou des réseaux de neurones, tandis que Hu et Gao (2017) ont étudié l'apprentissage par renforcement pour l'optimisation en temps réel [7,8].\n\n• Les prédictions de comportement électrique nécessitent la compréhension des comportements thermiques des infrastructures, rendant les jumeaux numériques et les simulations basées sur des méthodes différentielles ou par éléments finis cruciales. Ces simulations, bien que précises, sont limitées par la puissance de calcul actuelle et la complexité spatiale et temporelle des modèles [11,12].\n\n• Les Smart Grids modernisent les réseaux électriques avec des technologies de communication avancées. Farhangi (2010) a présenté le développement des Smart Grids, mettant en évidence leur pertinence pour les zones difficiles d'accès à l'électricité, tandis que Chowdhury et Rahman (2013) ont discuté des défis des Smart Grids à grande échelle [9,10].\n\n• La gestion de la demande dans les foyers repose sur les Home Energy Management Systems (HEM), mais leur efficacité dépend fortement de divers facteurs temporels et démographiques, rendant les résultats difficiles à optimiser immédiatement. Nos travaux visent à améliorer la consommation personnelle à travers une analyse optimisée des données pour mieux prédire la charge utilisée par les particuliers sur le réseau [10,11]."}, {"role": "assistant", "content": "2.3.1\tEtat de l’art\nNotre état de l’art a porté sur les méthodes de segmentation des images en vision par ordinateur. En effet, notre problématique de détection de parking étant unique, nous n’avons identifié aucune initiative similaire, ni au niveau de la recherche académique ni émanant des communiqués de presses d’entreprises privées.\nL’intelligence artificielle (IA) vise à développer un ensemble de théories et de techniques capables de reproduire artificiellement l’intelligence humaine. Aujourd’hui, il n’existe pas de systèmes uniques capables d’intelligence généraliste mais uniquement des algorithmes capables de réaliser des tâches spécifiques avec une efficacité proche voire supérieure à celle des humains. Parmi ces tâches, celles qui ont aujourd’hui le plus d’impact sur nos sociétés s'intéressent au traitement du langage naturel et à la vision par ordinateur.\n\nLa computer vision est un sous-domaine de l’intelligence artificielle qui cherche à reproduire la vision humaine par le biais d’algorithmes capables d’analyser des images (photos, flux vidéo, etc.), et d’en extraire des informations utiles à leur compréhension. Ces algorithmes sont ainsi capables de détecter des objets dans des images (tâche de détection d’objets), y compris en créant des masques permettant de les détecter au pixel près (tâche de segmentation).\n\nDepuis bientôt 10 ans, depuis la création du réseau de neurones à convolution AlexNet (Krizhevsky & al. 2012), les algorithmes de deep learning sont de très loin les algorithmes les plus utilisés et les plus performants en termes d’efficacité à réaliser une grande variété de tâches et de temps d’inférence. Aujourd’hui, AlexNet n’est plus utilisé mais de nombreux réseaux de neurones reprenant son architecture ont vu le jour : ResNet (He & al., 2016), InceptionNet (Szegedy & al., 2015), EfficientDet (Tan & al., 2020), etc.\n\nCes réseaux utilisent tous une architecture de réseaux de neurones à convolution auxquels ils ajoutent un ensemble d’astuces permettant petit à petit d’améliorer leur efficacité. Ces algorithmes sont ainsi de plus en plus rapides en temps d’inférence, nécessitent moins de données pour leur entraînement, ou bien peuvent être si léger qu’ils peuvent être utilisés en temps réel sur les appareils mobiles MobileNetV2 (Sandler & al., 2018).\n\nCes réseaux utilisent tous une architecture proche de celles de ConvNets (dont fait partie AlexNet). Ils se composent d’une première série de couches dont l’ensemble forme un “backbone”, puis d’un ensemble de couches adaptées à la tâche ciblée. Le rôle du backbone est d’extraire des features, c'est-à-dire des caractéristiques visuelles qui permettent de décrire le contenu de l’image dans un format exploitable par les têtes du modèle. Les dernières couches sont, pour leur part, chargées de réaliser la tâche cible. La force de cette architecture est qu’elle permet de réaliser du Transfer learning. Le backbone peut être entraîné une première fois sur une tâche avec un volume de données conséquent, puis peut être réutilisé tel quel sur un grand nombre de tâches (Weiss & al., 2016).\n\nPour réaliser ces tâches, les algorithmes utilisés sont le plus souvent des algorithmes supervisés nécessitant d’être entraînés. Le processus d’entraînement implique des incertitudes, il peut ainsi arriver que, sur des tâches analogues à celles présentées dans la littérature, il soit très difficile voire impossible de faire converger les modèles entraînés. Il peut également arriver que l’entraînement nécessite un volume de données beaucoup plus important que celui des données disponibles. Enfin, les performances du modèle final peuvent se révéler insuffisantes pour être utilisables par un service en production.\n\nDans le cadre de notre projet, nous avons besoin de détecter des parkings sur la base d’images satellites. Pour réaliser cette tâche, nous avons recherché des algorithmes capables de réaliser une tâche de détection d’objets et de classification d’images.\nBien que des modèles pré-entraînés soient disponibles pour détecter une grande variété d’objets, il n’existe pas de modèles pré-entraînés capables de détecter des espaces assimilés à des parkings.\n\nNous nous sommes orientés vers des solutions de l’état de l’art reconnues pour leurs performances sur des tâches de segmentation.\n\nLa méthode, appelée Mask R-CNN, étend Faster R-CNN (Ren & al., 2015) en ajoutant une branche pour la prédiction d'un masque d'objet en parallèle avec la branche existante pour la reconnaissance de la boîte de délimitation. Le cadre conceptuel simple, flexible et général pour la segmentation d'instances d'objets s’appuie sur une approche permettant de détecter efficacement les objets dans une image tout en générant simultanément un masque de segmentation de haute qualité pour chaque instance. Mask R-CNN est simple à entraîner et n'ajoute qu'une faible surcharge à Faster R-CNN, fonctionnant à 5 images par seconde. De plus, Mask R-CNN est facile à généraliser à d'autres tâches.\nNous avons en outre constaté que cette méthode est classée dans le TOP 3 des solutions le plus implémentées pour des tâches de segmentation sur le site paperswithcode.com, traduisant les pratiques courantes en termes de développements IA.\n\nEn parallèle de cette méthode, nous nous sommes également particulièrement intéressés à Deeplab (Chen & al., 2017), classée dans le TOP20 du même site.\nDans ce travail, la tâche de la segmentation sémantique d'images avec l'apprentissage profond est abordée par les auteurs qui apportent trois contributions principales. Premièrement, ils mettent en évidence la convolution avec des filtres suréchantillonnés, ou « convolution atrous », comme un outil puissant dans les tâches de prédiction dense. La convolution atrous permet de contrôler explicitement la résolution à laquelle les réponses des caractéristiques sont calculées dans les réseaux de neurones convolutifs profonds. Elle permet également d'élargir efficacement le champ de vision des filtres pour incorporer un contexte plus large sans augmenter le nombre de paramètres ou la quantité de calcul. Deuxièmement, ils proposent la mise en commun de pyramides spatiales atrous (ASPP) pour segmenter de manière robuste les objets à plusieurs échelles. L’ASPP sonde une couche de caractéristiques convolutionnelles entrantes avec des filtres à plusieurs taux d'échantillonnage et champs de vision effectifs, capturant ainsi les objets ainsi que le contexte de l'image à plusieurs échelles. Troisièmement, ils améliorent la localisation des limites des objets en combinant des méthodes issues des DCNN et des modèles graphiques probabilistes. La combinaison communément déployée du max-pooling et du downsampling dans les DCNNs permet d'atteindre l’invariance mais a un impact sur la précision de la localisation. Ce problème est surmonté en combinant les réponses de la dernière couche du DCNN avec un champ aléatoire conditionnel (CRF) entièrement connecté, ce qui permet d’améliorer les performances de localisation tant sur le plan qualitatif que quantitatif. Le système « DeepLab » proposé établit un nouvel état de l’art lors de la tâche de segmentation sémantique d’images.\n\nEnfin, nous avons identifié les travaux de Sağlam & Baykan (2017) visant la détection de toits à partir d’images satellites. Les auteurs investiguent deux méthodes différentes de classification d'images numériques (l'analyse discriminante linéaire et les valeurs de distance normalisées) qui ont été comparées l'une à l'autre, en utilisant différents espaces de couleur (RVB, L*a*b* et HSV). L’étude est réalisée sur des images satellites qui ont été prises par des capteurs numériques aéroportés afin de détecter les objets sur le toit. Les caractéristiques communes des méthodes appliquées sont qu'elles sont supervisées parce qu'elles utilisent des données d'entraînement précédemment communiquées et qu'elles fonctionnent rapidement parce qu'elles opèrent linéairement en utilisant une valeur seuil. Pour cette raison, certaines des images de l'ensemble de données sont utilisées pour l'entraînement afin de détecter certains coefficients et la valeur seuil. Le jeu de données utilisé pour l’entrainement et le test sont les images acquises à partir de la base de données ISPRS WG III/4 2D Semantic Labeling. Dans la base de données, les images de référence de la classification sont également disponibles.\nBien que cette étude vise un cas d’utilisation proche du nôtre, la maitrise du contenu des images satellite ne confronte pas les auteurs à la problématique d’enrichissement des données que nous avons rencontré.\n"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Le Complexe Majeur d'Histocompatibilité (CMH), ou système HLA chez l'homme, est un groupe d'antigènes crucial dans le rejet de greffe. Les protéines HLA sont codées par des gènes situés sur le bras court du chromosome 6.\n\n• Le CMH se distingue par son polygénisme et son polymorphisme, ce qui crée une grande variabilité entre individus et fait du CMH un facteur clé dans le succès des greffes.\n\n• Les gènes HLA sont organisés en trois régions sur le chromosome 6 : Classe I, Classe II, et Classe III. Les régions I et II sont essentielles pour la présentation antigénique, contrairement à la région III.\n\n• Le génotypage HLA est essentiel pour identifier les gènes ou antigènes HLA hérités, particulièrement important dans les greffes d'organes et de moelle osseuse.\n\n• L'augmentation du nombre d'allèles typés a entraîné une hausse des ambiguïtés de typage, dépassant les capacités des technologies traditionnelles, rendant le séquençage nouvelle génération (NGS) indispensable.\n\n• Le NGS gère les ambiguïtés par un séquençage allélique, permettant de traiter de nombreux échantillons à un coût réduit tout en atteignant une résolution élevée (4, 6 ou 8 digits) et en ciblant des loci entiers.\n\n• La création d'une librairie de séquençage est une étape cruciale, avec deux méthodes principales : la méthode globale (séquençage du génome entier) et la méthode ciblée (séquençage de régions spécifiques).\n\n• Pour la méthode ciblée, l'enrichissement de la librairie peut se faire par capture d'hybrides (filtration des fragments d'ADN par hybridation), par circularisation de fragments d'ADN (enrichissement via une sonde universelle) ou par PCR (amplification des fragments d'intérêt).\n\n• Les critères fondamentaux pour l'analyse des variants incluent la profondeur de séquençage, l'homogénéité de couverture, la reproductibilité, la quantité d'ADN requis, le nombre d'échantillons à traiter et le coût global [4].\n\n• L'étude de Mertes et al. souligne que les performances varient significativement selon la méthode d'enrichissement employée, influençant la profondeur et l'homogénéité de la couverture [4].\n\n• Bionobis a opté pour une approche PCR multiplexée, adaptée à l'analyse d'un petit nombre de régions cibles de petite taille sur un grand nombre d'échantillons.\n\n• Le séquençage consiste à cloner les fragments d'ADN pour amplifier le signal et détecter les nucléotides incorporés. Les données obtenues sont enregistrées dans un fichier Fastq.\n\n• Les 'reads' (séquences des fragments) sont alignés soit de novo (reconstruction sans modèle) soit avec référence (alignement sur un modèle existant, par exemple le génome humain hg19). Le de novo est plus coûteux en termes de calcul, tandis que l'alignement avec référence est plus simple grâce à l'algorithme de Burrows Wheeler [4].\n\n• La capacité d'un séquenceur est définie par la longueur des reads produits, le nombre de reads, le nombre de nucléotides lus, le temps de séquençage, et la qualité du séquençage.\n\n• La technologie PCR Long Range, bien documentée, ne répond pas aux besoins en raison de sa complexité de mise en œuvre et des étapes multiples qui augmentent le risque d'erreurs.\n\n• Le brevet EP2599877 décrit une méthode de séquençage par PCR avec une étape de cisaillement de l'ADN, incompatible avec les objectifs de Bionobis [5].\n\n• Le brevet WO2008093098 propose une méthode d'indexation des échantillons pendant le séquençage utilisant la PCR Long Range, que Bionobis souhaite éviter [6].\n\n• Le brevet WO2005068656 présente une méthode de séquençage NGS Illumina, mais Bionobis conçoit ses propres index pour garantir son indépendance technologique [7].\n\n• Le brevet WO2007044091 décrit une technologie de séparation microfluidique utilisée comme support dans le futur kit de Bionobis [8].\n\n• Deux articles scientifiques, Lange et al. et Shiina et al., proposent des méthodologies théoriques pour le typage HLA, mais elles ne répondent pas pleinement aux exigences strictes de Bionobis en termes de performance et de rapidité [9][10]."}, {"role": "assistant", "content": "1.3\tÉtat de l’art\n1.3.1\tPositionnement scientifique de nos travaux\nLe Complexe Majeur d’Histocompatibilité (CMH) correspond à un groupe d’antigènes initialement identifiés comme jouant un rôle primordial dans les processus de rejet de greffe et pour lesquels la compatibilité entre donneur et receveur permet de prolonger la survie de la greffe. Chez l’homme, le CMH s’appelle le système HLA (Human Leucocyte Antigen). Les protéines du système HLA sont codées par des gènes regroupés sur le bras court du chromosome 6 (cf. Figure 1).\n\n \nFigure 1 : Organisation du système HLA (Chromosome 6)\nCe système immunogène, situé sur le bras court du chromosome 6 chez l’homme, est caractérisé par son polygénisme et son polymorphisme qui sont à l’origine d’une grande variabilité interindividuelle et en fait le déterminant principal dans la réussite des greffes.\nL’ensemble des gènes HLA est subdivisé en trois régions du chromosome 6 qui contiennent chacune de nombreux gènes avec ou sans fonction immunologique. On distingue ainsi la région CMH de Classe I, de Classe II, et de Classe III. La région III ne renfermant pas de gènes intervenant dans la présentation antigénique, nous nous intéressons tout particulièrement aux régions I et II.\nLe génotypage du système HLA revêt un caractère primordial dans la détermination de quel gène ou antigène HLA une personne a hérité. L’analyse est effectuée principalement dans le cas d’une greffe d’organe ou de moelle osseuse pour faire correspondre un donneur et un receveur.\nAssocié à une augmentation constante du nombre d’allèles typés, le nombre d’ambigüités lors du typage croît et met progressivement en difficulté les technologies conventionnelles qui atteignent leurs limites.\nL’exploitation du reste de l’information génomique permettrait potentiellement d’améliorer cette performance par un typage plus affiné. Le recours au séquençage nouvelle génération apparait donc inévitable.\nEn plus de gérer les ambiguïtés par un séquençage allélique, le NGS permet de traiter simultanément de grandes quantités d’échantillons réduisant ainsi le coût unitaire et d’accéder à un niveau de résolution supérieur (4, 6 ou 8 digits) tout en ayant la capacité de cibler des loci entier.\n\nLe séquençage nécessite de créer une librairie, c’est-à-dire l’ensemble des fragments d’ADN que nous souhaitons séquencer. Dans ce cas, deux méthodes existent selon l’objectif de séquencer l’ensemble du génome ou seulement des régions d’intérêts.\n\nLa première méthode, dite globale, consiste à sélectionner une séquence inconnue, la fragmenter et procéder au séquençage de ses fragments. Plusieurs méthodes existent pour fragmenter l’ADN, notamment en utilisant des ultra-sons (fragmentation par sonification) ou en faisant appel à des enzymes de restriction (fragmentation enzymatique).\n\nLa seconde méthode, dite ciblée consiste à séquencer uniquement la partie codante ou simplement séquencer une liste de gènes associés à une maladie.\nIl faut alors enrichir la librairie en sélectionnant uniquement les fragments d’ADN désirés, grâce à différentes techniques dont les principales sont décrites ci-après :\n\tEnrichissement par capture d’hybrides : La capture par hybridation des régions cibles est initiée à partir de la librairie de séquençage. Elle consiste à filtrer les fragments d’ADN en les hybridant soit en solution (billes magnétiques) soit sur support solide (microarrays) ;\n\tEnrichissement par circularisation de fragments d’ADN : En amont de la préparation de la librairie, les fragments d’ADNg (fragmentation enzymatique ou mécanique selon la méthode) sont enrichis via une sonde constituée d’une séquence universelle flanquée aux extrémités de séquences spécifiques de la région cible.\n\tEnrichissement par PCR : Les fragments désirés sont amplifiés par PCR. Nous pouvons amplifier une seule région avec un couple d’amorces (simplex), ou alors amplifier plusieurs régions d’intérêt avec plusieurs couples d’amorces (multiplex). Dans cette stratégie, toutes les séquences d’un même amplicon seront identiques. Une étape préliminaire de design des amorces est requise.\n\nLes considérations fondamentales dans l’analyse de variants résident principalement dans :\n\tLa profondeur de séquençage\n\tL’homogénéité de couverture des régions d’intérêt\n\tLa reproductibilité de la méthode\n\tLa quantité d’ADN « Input » requis\n\tLe nombre d’échantillons à traiter\n\tLe coût global\n\nEn particulier, l’étude menée par Mertes et al. souligne l’importance de ces notions et fait le postulat de performances de captures de régions cibles significativement différentes selon la méthode d’enrichissement employée.\nDans le cas de Bionobis, pour une utilisation en diagnostic, nous avons privilégié l’approche PCR multiplexée pour l’étude d’un nombre restreint de régions cibles de petite taille appliquée à un grand nombre d’échantillons.\n\nIl existe différentes méthodes de séquençage mais le principe général reste le même : chaque fragment est d’abord cloné plusieurs fois afin d’amplifier le signal. Puis le brin complémentaire de chaque fragment cloné est synthétisé. À chaque incorporation d’un nucléotide, un signal est détecté. À la fin du séquençage, chaque fragment a été séquencé en parallèle. L’ensemble des données est enregistré dans un fichier Fastq.\n\nÀ la fin du séquençage, la chimie fait place à la bio-informatique.\nLes séquences des fragments, maintenant appelées des « reads », sont sauvegardées dans un fichier Fastq contenant les séquences et leurs scores de qualité (score Phred).\nCe score évalue la confiance du séquençage.\nA ce stade, les données disponibles sont généralement uniquement des courtes séquences de 150 pb (paires de bases). A partir de ces données, nous souhaitons obtenir la séquence complète d’un gène ou d’un génome entier.\nPour cela, il faut reconstruire un puzzle en réalisant un alignement : soit un alignement de novo, soit un alignement avec référence.\n\nDans le cas de l’assemblage de novo, il s’agit de résoudre un puzzle sans son modèle.\nLes fragments d’ADN qui sont chevauchants permettent petit à petit de reconstruire un contig. L’assemblage des contigs entre eux permet d’obtenir un scaffold. Cette technique est très coûteuse en termes de calcul. Des algorithmes bio-informatiques comme les graphes de Bruijn, permettent de résoudre ce problème. Cette méthode est principalement employée pour reconstruire des génomes non connus. L’alignement de novo consiste ainsi à aligner les reads entre eux.\n\nL’alignement avec référence met quant à lui en œuvre la résolution d’un puzzle en s’aidant d’un modèle. Par exemple, une version du génome humain (hg19).\nChaque read est aligné sur cette référence. La complexité de calcul est plus simple qu’avec l’alignement de novo. L’algorithme de Burrows Wheeler permettant de rechercher de manière efficace une correspondance entre les reads et la référence est généralement utilisé. Après cet alignement, un fichier BAM associant à chaque read ses coordonnées génomiques (i.e. le chromosome et la position) est obtenu.\nLa profondeur caractérise le nombre moyen de reads qui se superpose tandis que le recouvrement désigne l’étalement des reads sur la zone d’intérêt. L’alignement avec référence consiste à aligner les reads sur une référence.\n\nLes capacités d'un séquenceur sont définies par :\n\tLa longueur des reads produits (L)\n\tLe nombre de reads produits (n)\n\tLe nombre de nucléotides lus : (L x n)\n\tLe temps de séquençage\n\tLa qualité du séquençage\n1.3.2\tEtat des connaissances existantes et limites\nLa première technologie qui est apparue sur le marché avant le démarrage de notre projet, repose sur le principe d’une amplification de longs fragments du gène HLA par PCR Long Range suivi d’une fragmentation et d’une ligation. Cette technologie est très abondamment décrite dans les articles cités dans la bibliographie. Elle ne répond cependant pas à notre besoin en raison de sa difficulté de mise en œuvre.\n\nEn effet, la PCR pour longs fragments (Long range PCR) permet l’amplification de produits PCR plus longs que ceux amplifiés par les Taq polymérases conventionnelles.\n\nIl est possible d’amplifier des fragments de plus de 20 kb avec un ADN génomique de bonne qualité. Les inconvénients sont les nombreuses étapes du mode opératoire, qui est une source d’erreur et pouvant donc entrainer la mise en place de contrôles supplémentaires et l’utilisation d’une main d’œuvre importante et consommatrice de temps.\nLe typage HLA par PCR Long Range consiste en une préparation de la librairie comprenant un workflow avec de nombreuses étapes.\nPlusieurs PCR ciblées par échantillon sont nécessaires, suivi d’un équilibre molaire après quantification. Une fragmentation est réalisée par coupure enzymatique, suivie d’une réparation et intégration des adaptateurs. Après des étapes de purification, une indexation par échantillon et/ou par locus est nécessaire.\nUn pool des échantillons est ensuite réalisé, et ce dernier est purifié, quantifié pour ensuite être séquencé sur une plateforme de séquençage.\n\nDe plus, la PCR Short Range n’est pas utilisée dans notre domaine d’application : l’histocompatibilité. Notre limite de connaissances repose sur l’absence d’expérience dans un contexte de la routine des laboratoires et la capacité de la Short Range PCR à fournir un résultat de typage HLA de qualité aux biologistes. Des évaluations seront nécessaires pour valider cette approche (SR).\n\nPar ailleurs, notre étude de l’état de l’art nous a amenés à considérer les brevets en lien avec nos sujets. Quatre brevets ont été identifiés comme adressant partiellement notre problématique. Une brève description de chacune des inventions et les limites de celles-ci par rapport à notre besoin sont proposées ci-après.\n\nLe brevet EP2599877 décrit une nouvelle méthode de séquençage par PCR et son utilisation dans le génotypage HLA.\nLa solution décrite dans ce brevet ne répond cependant pas à notre problématique car elle comporte une étape de cisaillement de l’ADN que nous souhaitons éviter.\n\nLe brevet WO2008093098 présente quant à lui des procédés pour indexer des échantillons pendant le séquençage de matrices polynucléotidiques, qui permettent l’attachement d’étiquettes spécifiques de la source de chaque échantillon d’acide nucléique, afin que, après un cycle de séquençage, la source et la séquence de chaque polynucléotide puissent être déterminées.\nAinsi, la présente invention concerne l’analyse de génomes complexes (par exemple, des génomes humains), ainsi que le multiplexage de génomes moins complexes, comme des génomes de bactéries, de virus, de mitochondries, et d’autres génomes similaires.\nCette invention décrit le principe de PCR Long Range décrivant toutes les étapes de préparation de la libraire Long Range que nous ne souhaitons pas.\n\nUn autre brevet, WO2005068656, de la même famille, présente une méthode de caractérisation d’acides nucléiques.\nL’invention concerne la caractérisation des acides nucléiques. Elle concerne, en particulier, une méthode de séquençage et de distinction des séquences d’acides nucléiques provenant de différentes sources sur une matrice, ladite méthode consistant à séquencer une caractéristique de marquage des acides nucléiques de la source des séquences d’acides nucléiques.\nCette invention décrit le principe du séquençage NGS Illumina pour l’utilisation des index et des Tags selon les séquences d’oligonucléotides fournies par la société Illumina. La conception de nos propres Index et Tags nous permet une totale indépendance vis-à-vis de la société Illumina.\n\nEnfin, un autre brevet, WO2007044091, présente une analyse utilisant des dispositifs de séparation microfluidiques.\nL’invention concerne des procédés, des réactifs et des dispositifs permettant de détecter et de caractériser des acides nucléiques, des cellules et d’autres échantillons biologiques. Un procédé de dosage permet de séparer un échantillon en sous-échantillons et d’analyser le contenu des sous-échantillons. L’invention concerne aussi des dispositifs microfluidiques destinés à mettre en œuvre le dosage ainsi qu’un procédé d’analyse utilisant des amorces et des sondes universelles à des fins d’amplification et de détection.\nCe brevet décrut la nano technologie conçue par Fluidigm que nous utilisons comme support réactionnel à notre futur kit.\n\n\nEnfin, deux articles scientifiques présentant une méthodologie ont été identifiés : Lange et al. Cost-efficient high-throughput HLA typing by MiSeq amplicon sequencing et Shiina et al. Super high resolution for single molecule‐sequence‐based typing of classical HLA loci at the 8‐digit level using next generation sequencers.\nCes deux articles ont été d’un intérêt tout particulier puisqu’ils présentent une méthodologie permettant, en théorie, et pour un faible nombre d’échantillons, d’adresser notre problématique.\nNos spécifications en termes de performances, précision et rapidité, sont cependant plus strictes que les conditions considérées dans ces deux publications. Ainsi, si elles proposent des directives, certaines de nos problématiques restent toujours non adressées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• La traduction des terminologies médicales est un sujet crucial, abordé par diverses approches de traduction automatique (TA), avec une transition notable des modèles statistiques vers les modèles neuronaux.\n\n• La traduction automatique statistique (Statistical Machine Translation - SMT) utilise des modèles statistiques basés sur l'analyse de corpus de textes bilingues pour déterminer les paramètres de traduction.\n\n• Eck et al. (2004) ont montré que l'utilisation d'une grande base de données médicales, le système de langage médical unifié (UMLS), combinée à des informations sémantiques, améliore les performances de la TA statistique dans les dialogues médecin-patient.\n\n• Claveau et Zweigenbaum (2005) ont proposé l'algorithme OS-TIA, basé sur l'apprentissage automatique supervisé, pour traduire des termes biomédicaux entre le français et l'anglais. Cet algorithme dérive des transducteurs à partir de paires de termes bilingues, permettant de traduire de nouveaux termes en anglais ou en français.\n\n• Nystrom et al. (2006) ont utilisé une collection parallèle de terminologies médicales (ICD-10, ICF, MeSH, NCSP, KSH97-P) pour créer un dictionnaire anglais-suédois semi-automatique, applicable à d'autres langues européennes.\n\n• Deleger et al. (2009) ont développé une méthode pour obtenir de nouvelles traductions de termes médicaux en utilisant l'alignement de mots dans des corpus parallèles anglais-français. Leurs résultats montrent qu'un pourcentage élevé de 61,5 % des traductions MeSH générées était jugé souhaitable pour le MeSH français.\n\n• La Neural Machine Translation (NMT) a émergé comme une approche de pointe, utilisant des réseaux neuronaux pour coder des séquences sources et générer des séquences traduites, surpassant les approches SMT basées sur les phrases (Bahdanau et al., 2014 ; Sutskever et al., 2014).\n\n• La NMT diffère des modèles SMT en intégrant des réseaux neuronaux récurrents (RNN) pour traiter la séquence d'entrée et produire une séquence traduite de longueur variable.\n\n• Arcan et Buitelaar (2017) ont démontré que l'adaptation au domaine améliore significativement la qualité de traduction des terminologies médicales spécifiques. Ils ont réentraîné des modèles NMT génériques avec un vocabulaire limité du domaine ciblé, montrant que les modèles de sous-mots (subwords) surpassent les modèles basés sur les mots pour ces tâches.\n\n• Khan et al. (2018) ont appliqué l'apprentissage par transfert pour améliorer la NMT dans le domaine médical, en réentraînant des modèles pré-entraînés hors domaine (News) sur le corpus Biomedical'18, surmontant ainsi les limitations de taille de vocabulaire et améliorant les performances de traduction.\n\n• La Figure 1 récapitule les techniques récentes pour la traduction de termes et textes médicaux, en détaillant les ressources utilisées, les approches spécifiques de TA, les langues étudiées, et les paramètres d'évaluation, classés par année."}, {"role": "assistant", "content": "3.2.\tÉtat de l’art\n\nLa traduction de terminologies médicales a été un sujet bien étudié, avec de nombreuses approches issues de la traduction automatique. Les modèles traditionnels de traduction automatique ont d'abord intégré des modèles statistiques, dont les paramètres sont fixés par l'analyse de corpus de textes bilingues.\n\n2.1.1. Traduction automatique statistique (Statistical Machine Translation - SMT)\n Eck et al. (2004) ont étudié l'utilité d'une grande base de données médicales (le système de langage médical unifié - UMLS) pour la traduction de dialogues entre médecins et patients à l'aide d'un système de traduction automatique statistique. Ils ont montré que l'extraction d'un grand dictionnaire et l'utilisation d'informations de type sémantique pour généraliser les données d'entraînement améliorent considérablement les performances de traduction.\nClaveau et Zweigenbaum (2005) ont présenté une méthode de traduction automatique d'une grande classe de termes du domaine biomédical d'une langue à une autre ; elle est évaluée sur des traductions entre le français et l'anglais. Leur technique repose sur un algorithme d'apprentissage automatique supervisé, appelé OS-TIA (Oncina, 1991), qui déduit des transducteurs à partir d'exemples de paires de termes bilingues. Ces transducteurs, lorsqu'on leur donne un nouveau terme en anglais (respectivement en français), doivent proposer le terme français (respectivement anglais) correspondant. Plus tard, Nystrom et al. (2006) font état d'une collection parallèle de rubriques provenant des systèmes de terminologie médicale ICD-10, ICF, MeSH, NCSP et KSH97- P et de son utilisation pour la création semi-automatique d'un dictionnaire anglais-suédois de terminologie médicale. Les méthodes présentées sont pertinentes pour de nombreuses autres paires de langues d'Europe occidentale. Deleger et al. (2009) ont présenté une méthodologie visant à faciliter ce processus en obtenant automatiquement de nouvelles traductions de termes médicaux basées sur l'alignement des mots dans des corpus de textes parallèles, et l'ont testée sur l'anglais et le français. Après avoir collecté un corpus parallèle anglais-français, les traductions françaises de termes anglais ont été détectées à partir de trois terminologies : MeSH, Snomed CT et MedlinePlus Health Topics. Un échantillon des traductions MeSH a été soumis à l'examen d'experts et un pourcentage relativement élevé de 61,5 % a été considéré comme des ajouts souhaitables au MeSH français. En conclusion, ils ont réussi à obtenir de nouvelles traductions de bonne qualité, ce qui souligne la pertinence de l'utilisation de l'alignement dans les corpus de textes pour aider à la traduction des terminologies. Leur méthode peut être appliquée à différentes langues européennes et fournit un cadre méthodologique qui peut être utilisé avec différents outils de traitement.\n\n2.1.2. Neural machine translation (NMT)\nCes dernières années, la NMT s'est imposée comme l'approche de pointe. La NMT utilise un grand réseau neuronal artificiel qui prend en entrée une phrase source (x1, . . . , xm) et génère sa traduction (y1, . . . , yn), où x et y sont respectivement les mots source et cible. Jusqu'à récemment, l'approche dominante de la traduction automatique codait la séquence d'entrée et générait ensuite une séquence traduite de longueur variable en utilisant des réseaux neuronaux récurrents (RNN) (Bahdanau et al., 2014 ; Sutskever et al., 2014). La NMT diffère entièrement des approches statistiques basées sur les phrases qui utilisent des sous-composantes conçues séparément (Wołk et Marasek, 2015).\n\n2.1.3. Adaptation au domaine\nEn traduction automatique, l'adaptation au domaine peut être appliquée lorsqu'une grande quantité de données hors-domaine coexiste avec une petite quantité de données dans le domaine. Arcan et Buitelaar (2017) ont présenté une comparaison de performances entre les méthodes SMT et NMT sur la traduction d'expressions hautement spécifiques au domaine, c'est-à-dire des terminologies, documentées dans l'ontologie ICD du domaine médical. Ils ont montré que l'adaptation au domaine avec uniquement des expressions terminologiques améliore significativement la qualité de la traduction, ce qui est spécifiquement évident si un réseau neuronal générique existant est réentraîné avec un vocabulaire limité du domaine ciblé. Enfin, ils ont observé l'avantage des modèles de sous-mots (subwords) par rapport aux modèles NMT basés sur les mots pour la traduction terminologique. Tous les travaux précédents se concentrent sur la formation avec des terminologies spécifiques. Bien que ces méthodes soient largement utilisées, leur vocabulaire peut être limité. De plus, leur taille n'est pas suffisante pour l'entraînement des méthodes NMT, ce qui entraîne de faibles performances de traduction.\n\nPour résoudre ces problèmes, Khan et al. (2018) ont formé des systèmes de NMT en appliquant l'apprentissage par transfert. L'apprentissage par transfert s'inscrit dans le cadre de l'adaptation au domaine. Dans l'apprentissage par transfert, les connaissances apprises d'un modèle existant pré-entraîné sont transférées à un nouveau modèle. Plus précisément, les auteurs ont utilisé un modèle existant hors domaine entraîné sur des données de News. Ensuite, ils entraînent leur système NMT sur le corpus Biomedical'18 du domaine (https://www.statmt.org/wmt18/ biomedical-translation-task.html). La Figure 1 résume les travaux connexes sur la traduction de termes et de textes médicaux, en indiquant les ressources, la famille d'approche de la traduction automatique, la méthode spécifique utilisée, les langues étudiées et les paramètres d'évaluation, classés par année.\n\n \nFigure 1 : Récapitulatif des techniques récentes pour la traduction de termes et textes médicaux"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• L'Intelligence Artificielle (IA) et la Data Science connaissent un essor considérable, offrant des opportunités pour augmenter la productivité, améliorer la puissance de calcul et analyser des données complexes. Cependant, elles posent également une menace aux entreprises traditionnelles non structurées pour les adopter [1].\n\n• Dans le domaine de l'expertise immobilière en France, le digital se manifeste principalement à travers des sites internet et des bases de données. L'émergence de concurrents digitaux en ligne a entraîné des transformations importantes dans le secteur, bien que l'IA ne soit pas encore largement implémentée, malgré son potentiel reconnu dans la spéculation immobilière [2][3].\n\n• En gestion d'actifs, la Data Science et l'IA sont de plus en plus utilisées, notamment pour la maintenance prédictive (Predictive Maintenance - PdM), qui réduit les coûts opérationnels et améliore la productivité et la sûreté grâce à des programmes basés sur le Machine Learning [4].\n\n• L'IA est également employée pour évaluer les stratégies d'investissement, en utilisant des outils comme la Valeur Actuelle Nette (VAN) des biens et composants, optimisant ainsi les investissements mobiliers et immobiliers [5].\n\n• Actuellement, il n'existe pas de programme d'évaluation automatisée du capital mobilier, bien que certaines assurances, comme la GMF, proposent des outils manuels pour cette tâche [6].\n\n• Les travaux du Cabinet Roux, bien qu'innovants, s'inscrivent dans la continuité des tendances du secteur et ne sont pas directement adressés par la littérature existante.\n\n• La classification automatisée, supervisée ou non, est bien documentée dans la littérature, avec des méthodes visant à identifier la classe d'appartenance d'objets en fonction de leur description [7].\n\n• La recherche récente accorde une attention particulière à la classification de données textuelles, un domaine en plein essor grâce aux données provenant de questionnaires et des réseaux sociaux, ce qui a conduit au développement du text mining pour la gestion de grandes quantités de textes [8].\n\n• Il n'existe pas de littérature sur l'évaluation automatisée de listes de biens industriels. La valeur de ces biens dépend de nombreux facteurs, comme l'année d'acquisition et le secteur d'utilisation. Bien que des méthodes de calcul existent, elles nécessitent actuellement une intervention humaine [9].\n\n• Cette recherche vise à combler le manque de littérature sur les algorithmes d'évaluation automatisée des biens industriels, tant mobiliers qu'immobiliers."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tLe digital et l’Intelligence Artificielle dans l’estimation des capitaux mobiliers et immobiliers\nL’Intelligence Artificielle et la Data Science sont des domaines qui bénéficient depuis quelque temps d’une attention sans précédent et séduisent de plus en plus de secteurs. Il s’agit là d’une véritable opportunité pour se décharger de certaines tâches, gagner en productivité, augmenter sa puissance de calcul ou analyser des données intéressantes. Néanmoins, elle représente aussi une menace pour les entreprises et secteurs traditionnels, qui ne sont pas structurés pour l’adopter et qui voient une nouvelle forme de concurrence arriver sur les marchés [1].\n\nDans le domaine de l’expertise immobilière en France, le digital prend la forme de site internet ou de bases de données. Néanmoins, avec l’arrivée de concurrents digitaux en ligne pour les ventes aux particuliers, le secteur subit des transformations importantes et s’oriente de plus en plus vers le digital [2]. L’intelligence artificielle n’est pas encore stablement implémentée dans le secteur mais l’opportunité est réelle et beaucoup étudiée dans le cadre de la spéculation immobilière [3].\nDans la gestion d’actif, domaine directement lié à l’évaluation d’actifs, la Data Science et l’Intelligence Artificielle sont de plus en plus utilisées et ont prouvé leur utilité. Dans la maintenance prédictive d’ensemble d’actifs, par exemple, le digital et la Data Science permettent la Predictive Maintenance (PdM – Maintenance prédictive). Cette fonction permet de réduire les coûts opérationnels, d’augmenter la productivité et la sûreté. Implémentées de l’Intelligence Artificielle (Machine Learning), les programmes de maintenance prédictive se sont considérablement améliorés et répandus [4].\n\nDe même, les stratégies d’investissements sont maintenant évaluées par l’intelligence artificielle, utilisant la Valeur Actuelle Nette (VAN) des biens et composants [5]. Les stratégies d’investissement mobiliers et immobiliers sont donc optimisées par le biais d’intelligence artificielle et de Data Science. \n\nBien qu’il n’existe pas actuellement de programme d’évaluation du capital mobilier, des assurances de particulier, comme la GMF, proposent un outil d’évaluation du capital mobilier dans lequel il faut rentrer manuellement les valeurs des biens dans chaque pièce [6].\n\nLes travaux du Cabinet Roux ne sont donc pas adressés par la littérature mais s’inscrivent dans la continuité de celle-ci et des tendances du secteur. \n\n1.2.2\tLa classification et évaluation automatisée de biens \nLa classification automatisée supervisée ou non est un domaine bien renseigné dans la littérature scientifique [7, 8], les méthodes de classement ayant pour objet d’identifier la classe d’appartenance d’objets définis par leur description [7].\n\nQuant à la classification d’une liste de mots en particulier, la recherche accorde ces dernières années, beaucoup d'importance au traitement des données textuelles [8]. Ces données, obtenues par des questionnaires ou encore des réseaux sociaux sont une des sources principales des données alternatives aujourd’hui. Ainsi le domaine de la fouille de texte (text mining) s'est développé pour répondre à volonté à la gestion par contenu des sources volumineuses de textes [8].\n\nNéanmoins, il n’existe pas de littérature sur une évaluation automatisée de liste de biens industriels. La valeur des biens dépend de nombreux facteurs, notamment leur année d’acquisition, mais aussi le secteur dans lequel ils sont utilisés. On peut trouver de la donnée à propos de méthodes de calculs d’experts pour estimer un bien industriel, qui est donc théorisé, mais qui nécessite actuellement l’intervention d’un humain pour chaque calcul et n’est donc pas automatisé [9].\n\nC’est dans cette partie de la littérature que cette recherche s’inclut, en tentant de s’adresser au manque d’algorithme évaluant automatiquement les biens d’un industriel (mobiliers et immobiliers)."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• La composante juridique de cette recherche ne vise pas à modéliser le raisonnement des juges, mais à identifier et systématiser les critères utilisés pour prendre des décisions dans des contentieux spécifiques (prestation compensatoire, concurrence déloyale, préjudice corporel, etc.).\n\n• L'état de l'art actuel ne fournit pas de cadre suffisant pour systématiser ces critères. Les connaissances disponibles proviennent de textes de loi, de la doctrine, de commentaires, ainsi que de discussions avec des professionnels (magistrats, avocats, universitaires).\n\n• Certains critères sont explicitement définis par la loi, comme dans le cas de la prestation compensatoire où la durée du mariage et la disparité de revenus sont considérées. D'autres critères, plus implicites, sont tirés de la jurisprudence et des pratiques spécifiques à chaque contentieux.\n\n• La recherche de ces critères commence par l'analyse de 100 à 200 décisions de jurisprudence pour établir une liste initiale, qui est ensuite affinée par des échanges avec des experts du domaine.\n\n• Contrairement à d'autres approches qui se contentent d'accumuler des décisions, cette recherche vise à analyser en profondeur les liens entre les critères pour modéliser un raisonnement juridique.\n\n• Aucune solution concurrente complète n'existe actuellement, bien que des études aient été réalisées sur des sujets spécifiques comme le préjudice corporel, les prestations compensatoires, la rupture brutale des relations commerciales, la franchise, et la concurrence déloyale [1-15].\n\n• Cette recherche cherche à aller au-delà des études existantes en modélisant le risque juridique et en permettant une quantification chiffrée des décisions possibles.\n\n• Une fois les critères identifiés, ils doivent être extraits des décisions de justice, ce qui nécessite l'utilisation de techniques avancées de traitement automatique du langage naturel (TALN).\n\n• Le TALN permet de décomposer des textes juridiques complexes, d'analyser la syntaxe, d'extraire des entités nommées et des relations, de classifier des textes, entre autres, mais il pose des défis spécifiques en raison du volume et de la complexité du langage juridique.\n\n• La langue juridique est caractérisée par un lexique riche, des structures syntaxiques complexes, des ambiguïtés sémantiques, et des spécificités propres à chaque domaine du droit, ce qui complique l'analyse automatique.\n\n• Bien que la littérature sur le TALN juridique soit riche, les systèmes actuels ne sont pas encore suffisamment performants pour les besoins spécifiques de cette recherche, notamment en raison de la limitation des corpus disponibles en français.\n\n• Les systèmes TALN les plus performants ont été entraînés sur des corpus en anglais, et la jurisprudence française est moins volumineuse, ce qui complique l'utilisation de techniques comme l'apprentissage profond.\n\n• Après la création d'une base de données à partir des critères et des décisions analysées, l'objectif est de créer des modèles prédictifs qui simulent les décisions judiciaires possibles, en tenant compte de la variabilité des jugements entre magistrats.\n\n• Ces modèles doivent prédire des distributions de probabilités plutôt que des valeurs précises, pour éviter un effet performatif où la machine pourrait influencer le jugement humain.\n\n• La recherche utilise des techniques classiques d'apprentissage automatique (forêts aléatoires, boosting, SVM, réseaux de neurones) et des modèles de comportement des juges, combinés avec des méthodes avancées pour obtenir des résultats robustes, comme le stacking et la prise en compte d'attaques adversariales [22-30].\n\n• Les algorithmes pour calculer des quantiles par régression, basés sur des méthodes de simplexe, sont utilisés pour affiner les prédictions [23-25].\n\n• La modélisation du comportement des juges s'appuie sur un modèle propriétaire, complété par des travaux existants en logique déontique et phénoménologie [26-30]."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tComposante juridique\nIl s’agit ici de caractériser, non pas le raisonnement du juge, ce qui est contraire à notre éthique car cela conduirait à construire des « juges machines », mais seulement l’ensemble des critères sur lesquels celui-ci se fonde pour prendre sa décision dans un type de contentieux donné : prestation compensatoire, concurrence déloyale, préjudice corporel, etc.\nL’état de l’art ne donne pas d’informations suffisantes sur ce sujet. Nous trouvons, pour chaque domaine, une grande quantité de textes, loi, doctrine ou commentaires, qui expose ou discute les éléments déterminants dans l’analyse d’une situation judiciaire. Ainsi, avant l’étude d’un contentieux, nous utilisons de nombreuses sources, écrites et orales (discussions avec des magistrats, avocats et universitaires) qui nous permettent d’identifier les critères pertinents. Mais la démarche de systématiser et de rendre objectif les critères légaux et extra-légaux (par exemple économique ou sociaux) est nouvelle.\n \nDans chaque domaine du droit que nous étudions, il existe des connaissances quant aux critères qui doivent être considérés par les juridictions au moment de prendre une décision. Certains de ces critères sont explicitement définis dans les textes de loi. Par exemple, pour une prestation compensatoire, le législateur a indiqué qu’il fallait tenir compte de la durée du mariage, de la disparité entre les revenus et patrimoines des époux, etc.\n \nIl existe cependant des critères de diverses natures, spécifiques à chaque contentieux, qui ne sont pas en général explicités : ceux-ci peuvent être liés à des usages, des considérations économiques, etc. Ils sont pour la plupart issus de la jurisprudence et procèdent d’une construction prétorienne.\n \nNous ne disposons donc pas d’une liste a priori des critères à considérer, et celle-ci doit être « découverte » par des recherches en amont et l’étude fine d’une grande masse de jurisprudence qui permet de repérer des occurrences répétées d’arguments parfois inattendus.\n\nDans tout contentieux que nous envisageons, nous commençons par analyser entre cent et deux cents décisions de jurisprudence afin d’initialiser notre liste de critères. Ceux-ci sont par la suite affinés, augmentés, réduits, adaptés puis soumis à la contradiction avec les avocats, magistrats, notaires, experts.\nNotre démarche ne consiste pas, comme d’autres, à empiler des décisions de jurisprudence mais à en analyser le plus grand nombre afin de faire apparaître les liens qui peuvent déboucher sur un raisonnement. Ces liens sont les critères qui vont nous permettre de le faire apparaître.\n\nIl n’y a pas, à notre connaissance, de solutions concurrentes sur ce sujet, notre approche étant totalement nouvelle, même si, ponctuellement, sur certains sujets, des études ont pu être faite, comme par exemple sur :\n-\tLe préjudice corporel [1] ;\n-\tLes prestations compensatoires [2-6] ;\n-\tLa rupture brutale des relations commerciales établies [7-9] ;\n-\tLa franchise [10-12] ;\n-\tLa concurrence déloyale [13-15].\n\nToutes ces études et beaucoup d’autres ne font qu’une photographie à un moment donné de l’état du droit dans un domaine particulier. \n\nNotre travail consiste à mettre tous ces critères « en mouvement » pour modéliser un risque et permettre une quantification chiffrée ce que ne permettent pas les différentes études car ce n’est pas leur rôle ni leur souhait.\n1.2.2\tComposante TALN\nUne fois les critères déterminés, il faut les extraire des décisions de justice. Nous avons donc une forte activité sur le traitement automatique de la langue naturelle (TALN), afin de développer des algorithmes à même de reconnaître les éléments recherchés dans les textes que nous étudions. La recherche dans ce domaine est extrêmement active, et nous nous appuyons sur diverses techniques récentes.\n \nLe TALN est un sous-domaine de l'intelligence artificielle qui propose des techniques permettant de décomposer des textes et des phrases, lemmatiser et étiqueter des mots, analyser des phrases, enrichir le texte avec des attributs sémantiques, reconnaître des entités nommées, extraire des relations entre ces entités, identifier des marqueurs de discours, effectuer des références, classifier des textes, etc. Ces techniques sont quotidiennement appliquées avec succès pour l’analyse de divers corpus, comme par exemple journalistiques, mais leur application au langage juridique pose des problèmes spécifiques liés tout d’abord au volume des textes, qui peuvent contenir plusieurs centaines de pages.\nUne deuxième classe de difficultés particulières tient au caractère technique des structures rencontrées en droit. Le premier point est que la langue juridique est elle-même complexe, avec un lexique riche et souvent très spécifique y compris dans sa structure interne (« sans préjudice d'une action en justice »). Chaque domaine du droit possède sa propre terminologie, sans cohérence totale entre les domaines. La syntaxe privilégie des phrases souvent longues et structurés, avec de nombreux emboîtements et de fréquentes ambiguïtés. Le niveau sémantique n’est pas en reste, avec des anaphores difficiles à résoudre, des fortes polysémies, synonymies et homonymies, des figures de style et tournures chantournées faisant par exemple un fréquent usage des doubles négations et de références internes difficiles à saisir même pour un humain.\n\nUn dernier niveau de difficulté a trait à la spécificité des exigences de performance. Pour espérer non pas fonder une décision sur une analyse automatique mais simplement aider à celle-ci, il ne suffit pas de pointer des ressemblances de surface, il faut pouvoir reconstituer des raisonnements, ce qui exige un niveau de compréhension très élevé à l’étape TALN.\n \nMalgré, ou peut-être à cause de ces difficultés, la littérature sur le traitement automatique de la langue juridique est déjà très riche, tant au plan français qu’international. Nous trouvons ainsi des travaux ayant trait exclusivement à l’analyse de décisions de justice [16-21]\n\nComme nous le voyons, le sujet de l’analyse automatique de décisions est foisonnant, et de nombreux progrès ont été faits ces dernières années. Il faut cependant noter que les performances ne sont pas encore au niveau nécessaire pour notre application, et surtout que les systèmes les plus performants à l’heure actuelle ont été entraînés sur des corpus essentiellement en anglais. Au-delà du fait que c’est un biais général en TALN, il y a une difficulté fondamentale ici, c’est que la jurisprudence existante en France est bien plus réduite que, par exemple aux USA, situation encore aggravée par le fait que, typiquement, seules les décisions d’appel et de cassation sont accessibles, alors que c’est beaucoup moins le cas pour les jugements de première instance, qui forment numériquement le gros du matériel. Ainsi, les approches nécessitant de grandes bases de données, comme l’apprentissage profond, nous sont a priori interdites.\n•\tComposante AA\nUne fois la base de données constituée à partir des critères définis en phase 1 et des décisions analysées en phase 2, il s’agit de créer des modèles permettant de reproduire l’ensemble des décisions qui pourraient être prises par une juridiction donnée sur un dossier donné. La difficulté particulière ici est qu’il ne s’agit pas de prédire « la » décision, parce qu’il est essentiel de tenir compte le fait que pour le même cas, deux magistrats ne produiront pas nécessairement la même issue. Il arrive même [22] qu’un seul et même magistrat juge deux fois de façon différente sur le même cas à deux moments de la journée par exemple. Il serait donc trompeur et dommageable à moyen terme pour le fonctionnement de la justice de présenter « la » solution à un dossier donné, car cela pourrait entraîner un effet performatif très fort, par lequel ce serait la machine qui jugerait à la place de l’humain.\n\nEn termes techniques, notre tâche est donc de prédire des distributions de probabilités et non des valeurs réelles par exemple pour les montants d’indemnisation.\n\nDans ce but, nous nous appuyons d’une part sur (i) des techniques classiques d’apprentissage automatique, comme les forêts aléatoires, le boosting, les SVM, les réseaux de neurones (quand le nombre de données est suffisant), que nous étendons selon nos besoins et que nous complétons pour obtenir des résultats robustes par des raffinements tels que le stacking et la prise en compte d’attaques adversarielles, et d’autre part sur (ii) des modèles plus ou moins avancés du comportement des juges.\n \nEn ce qui concerne le point (i), nous utilisons des algorithmes connus permettant de calculer des quantiles par régression, et qui s’appuient sur des approches de programmation linéaire utilisant des méthodes de simplexe [23-25].\n\nPour la modélisation (point (ii)), il existe de nombreux travaux fondés sur la logique (par exemple déontique), ainsi que des modèles plus phénoménologiques [26-30]. Dans notre pratique, nous nous fondons cependant sur un modèle propriétaire qui est brièvement décrit dans la partie travaux de ce document."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Le langage GO (ou GoLang) est un langage de programmation open-source développé par Google, reconnu pour sa robustesse, sa scalabilité, sa rapidité de compilation, et sa facilité de maintenance. Ces caractéristiques en font un langage idéal pour les applications utilisant des ressources cloud.\n\n• GO se distingue par ses mécanismes de concurrence, permettant aux fonctions de s'exécuter indépendamment, ce qui est particulièrement efficace sur des machines multicœurs et en réseau. Son système de type flexible et modulaire renforce la portabilité et la scalabilité des applications développées en GO.\n\n• Malgré ses nombreux avantages, GO présente des limites, notamment en production, avec des défis liés à la gestion des erreurs et à l'absence de gestion manuelle de la mémoire.\n\n• Google App Engine est une plateforme de développement et d’hébergement d’applications web, qui offre une gestion automatisée des serveurs, haute disponibilité, performance optimale et scalabilité grâce à l'infrastructure de Google Cloud.\n\n• L'usage de Google App Engine dispense les développeurs de la gestion de l’hébergement, leur permettant de se concentrer sur le développement d'applications, mais impose des contraintes, comme l'absence d'accès direct au serveur, l'utilisation de librairies spécifiques, et des limitations dans l'exécution des tâches en fond.\n\n• Le web scraping est une technique qui permet de collecter des données depuis des sources multiples sur le web. Ces données, une fois obtenues, doivent être regroupées, analysées et structurées pour être exploitées par divers outils.\n\n• Des travaux récents, comme ceux de Kais Dai et al. de l'Université de Vigo, ont exploré les techniques de scraping et de clustering pour caractériser des profils LinkedIn. Ils ont utilisé des bots pour explorer les couches profondes des sites web et appliqué des techniques de traitement du langage naturel pour traiter les données collectées.\n\n• Leurs recherches incluent également des méthodes de classification des sections collectées et de clustering des profils en fonction du domaine d'activité et du niveau d'éducation. Toutefois, ces travaux ne permettent pas de déduire précisément les compétences ni d'estimer le niveau de maîtrise des technologies à partir des descriptions de profils LinkedIn.\n\n• Pour répondre à cette lacune, il est nécessaire de développer un modèle de données qui inclut un champ permettant aux utilisateurs de qualifier eux-mêmes leur niveau de maîtrise des compétences et technologies listées."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tLe langage GO\nLe langage GO (ou GoLang) est un langage de programmation développé par Google et disponible en open-source. Les avantages du GO sont ses caractéristiques particulièrement compatibles avec une programmation volumineuse : robustesse, scalabilité et rapidité de compilation. De plus, les programmes développés en GO sont faciles à maintenir et performants, notamment dans le cas de l’utilisation de ressources cloud. La concurrence dans GO est la capacité des fonctions à s'exécuter indépendamment l'une de l'autre. Ses mécanismes de concurrence facilitent l'écriture de programmes qui tirent le meilleur parti des machines multicœurs et en réseau, tandis que son système de type novateur permet une construction de programme flexible et modulaire.\nDe plus, un autre avantage mis en avant dans la littérature est la forte maintenabilité des programmes développés en GO.\nLes caractéristiques de GO, que nous avons cité plus haut, et son degré de portabilité rendent ce langage très approprié pour développer des applications de Cloud. Cet attribut est d’autant plus important que nous souhaitons développer un outil scalable pour accompagner notre montée en charge, les serveurs Cloud devenant incontournables pour notre utilisation.\nSi les principaux avantages du Go résident dans sa performance et la facilité de la programmation concurrente avec des canaux et des goroutines, de nombreuses limites ont été relevées par les utilisateurs de ce langage.\nCes inconvénients se posent essentiellement en production, notamment la gestion des erreurs par le langage GO et l’absence de gestion manuelle de la mémoire.\n\n1.2.2\tOutils Google App Engine\nApp Engine est une plateforme de conception et d’hébergement permettant de générer des applications web pour un trafic élevé, grâce à l’utilisation de l’architecture technique de Google Cloud.\nL’avantage de cet outil est de dispenser l’équipe de développeurs de la gestion de l’hébergement grâce à l’infrastructure Google. Ainsi, les programmateurs peuvent se concentrer sur les développements de leurs applications sans se soucier du besoin de maintenir des infrastructures complexes.\nEn contrepartie de la gestion automatisée des serveurs, de la haute disponibilité, des performances optimales et de la forte scalabilité apportée par Google App Engine, cela demande plusieurs contraintes du côté des utilisateurs : aucun accès au serveur, pas d'exécution possible sur celui-ci, utilisation de librairies spécifiques à App Engine, utilisation d’un protocole spécifique pour les requêtes HTTP, utilisation des tâches en fond limitée.\n1.2.3\tWeb scraping\nLes réseaux sociaux en ligne sont devenus, ces dernières années, d’importantes sources d’informations. Des applications se sont alors développées comme le data scraping ou le data crawling qui consistent à récupérer des données depuis des sources multiples, principalement le web, dans des couches plus ou moins profondes de la page visitée. L’obtention des données est une première problématique de cette opération. Une fois collectées, ces données doivent être regroupées afin de subir diverses analyses, et enfin être structurées de manière à être exploitées par différents outils.\n\nDivers travaux s’intéressent à cette problématique récente, nous avons notamment consulté une publication de Kais Dai et al. de l’Université de Vigo, qui traite des techniques de scraping et clustering pour la caractérisation des profils LinkedIn. Les travaux des auteurs présentent différentes techniques de crawling d’informations qui consistent à introduire un bot dans les couches profondes des sites web dont on souhaite collecter des informations, de même que diverses méthodes de traitement du langage naturel pour cette application. Enfin, des méthodes de clustering de ces informations collectées sont passées en revue. Les auteurs proposent également une technique de classification des différentes sections collectées depuis les curriculums vitae. Enfin, l’étape ultime de ce papier était de regrouper (cluster) les profils LinkedIn selon le domaine d’activité, le niveau d’éducation…\n\nLes travaux présentés dans l’état de l’art en termes de traitement du langage naturel ne répondent que partiellement à notre problématique puisque notre objectif est de pouvoir déduire des compétences à partir d’une description dans un profil LinkedIn. Par ailleurs, nous souhaitons, pour chaque compétence, estimer au plus juste le niveau de maitrise d’une technologie. Les solutions de l’état de l’art ne permettent aujourd’hui pas de réaliser cette analyse et nous contraignent à inclure dans notre modèle de données un champ permettant au freelance, lui-même, de qualifier sa maitrise des différentes technologies et compétences listées."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Depuis les années 2000, la formation en entreprise s'est fortement numérisée, avec un accent sur les plateformes de e-learning, influencée par l'utilisation croissante des outils digitaux et la consommation rapide d'informations.\n\n• Les concepts de micro-learning (sessions courtes de 30 secondes à 10 minutes) et de mobile learning (apprentissage sur mobile) ont émergé pour répondre à cette demande d'apprentissage rapide et accessible.\n\n• Sharples, Taylor et Vavoula ont étudié l'impact des nouvelles technologies, notamment des smartphones, sur l'apprentissage en milieu scolaire, soulignant leur potentiel, mais sans explorer l'optimisation de l'apprentissage via serious games [1].\n\n• Cheon et al. ont examiné la perception des étudiants universitaires concernant l'apprentissage mobile, montrant une acceptation croissante de ce média, et suggérant que l'apprentissage mobile pourrait compléter ou remplacer les méthodes traditionnelles [2].\n\n• Kearney et al. ont analysé l'enseignement mobile du point de vue des enseignants, identifiant trois caractéristiques clés : authenticité, collaboration et personnalisation, et proposant un cadre pour étudier leur influence [3].\n\n• Yu-Chang et Ching ont utilisé la méthode de Delphes pour identifier les domaines de recherche prioritaires en apprentissage mobile, mettant en avant la nécessité de stratégies d'enseignement adaptées et le manque de recherches sur la rétention des apprenants [4].\n\n• Connolly et al. ont compilé la littérature sur l'impact des jeux vidéo, montrant que les serious games ont un effet positif sur l'apprentissage, la motivation et le développement de compétences chez les utilisateurs âgés de 14 ans et plus [5].\n\n• Breuer et Bente ont exploré la relation entre l'apprentissage et les serious games, insistant sur l'importance du caractère ludique pour l'efficacité des jeux sérieux, et classant ces jeux selon les mécaniques de jeu et les compétences qu'ils développent [6].\n\n• Arnab et al. ont proposé un modèle associant les mécaniques d'apprentissage avec celles des jeux, permettant de concevoir des serious games avec des objectifs pédagogiques spécifiques, bien que des aspects comme la modélisation des joueurs et la rétention soient encore à développer [7].\n\n• Wouters et al. ont démontré que les serious games sont plus efficaces que les méthodes traditionnelles pour l'apprentissage et la rétention, montrant que ces jeux améliorent significativement la motivation des apprenants [8].\n\n• L'ergonomie cognitive a joué un rôle crucial dans la conception de plateformes de formation à distance, en optimisant l'efficacité de l'apprentissage, bien que des défis subsistent, comme l'effort nécessaire des apprenants et les difficultés liées à l'utilisation des TIC [9][10].\n\n• Plusieurs éléments doivent être pris en compte dans la conception de dispositifs de formation à distance, notamment le degré de nouveauté des connaissances, l'architecture du contenu, la granularité des connaissances, les processus d'apprentissage, la progression pédagogique, l'évaluation des connaissances, la cohésion graphique, et la structuration de l'interface [11-19].\n\n• L'évaluation des dispositifs de formation doit combiner une évaluation par inspection et une évaluation empirique pour identifier les erreurs et améliorer la conception, tout en adaptant les contenus aux différents profils d'apprenants [16][17].\n\n• Les travaux de R&D en 2018 ont porté sur l'identification des critères les plus pertinents pour classifier les apprenants et définir la granularité optimale des contenus."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nDepuis le début des années 2000, la formation des collaborateurs en entreprise s’est fortement numérisée avec le développement progressif des plateformes de e-learning basées sur des plateformes web. \n\nPar ailleurs, l’utilisation toujours plus intensive des outils digitaux entraîne un profond changement dans le rapport à l’information et dans la manière de la consommer. Nous consommons toujours plus d’informations, avec une impatience accrue, en cherchant une solution toujours plus facile et plus rapide. Les formats les plus courts et les plus attrayants sont largement préférés.\n\nC’est dans ce contexte que sont nés les concepts de micro learning (durée courte) et de mobile learning (apprentissage sur mobile). L’approche micro-learning, repose sur la notion de micro contenus, dits bite-sized (en grain) permettant des sessions allant entre 30 secondes et 10 minutes.\n1.2.1\tNouvelles stratégies d’apprentissage sur mobile\nSharples, Taylor et Vavoula ont étudié [1] la place des nouvelles technologies, notamment des smartphones, dans les processus d’apprentissage en milieu scolaire et académique. Les auteurs mettent en avant certains avantages de cette forme d’apprentissage, et l’impact que les nouvelles technologies peuvent avoir sur les plus jeunes utilisateurs. Toutefois, les auteurs n’étudient pas la notion de serious game, et comment optimiser l’apprentissage et la rétention des apprenants dans le cadre de l’apprentissage mobile. \n\nCheon, Lee, Crooks and Song se sont intéressés [2] plus spécifiquement à la perception qu’ont les étudiants d’universités de l’apprentissage mobile dans le supérieur. L’utilisation de téléphones portables est évidemment très répandue dans ce milieu, pourtant, ils sont peu exploités pour l’apprentissage. Les auteurs décrivent un modèle basé sur la théorie du comportement planifié. Ce modèle explique comment les croyances des étudiants impactent leur intention d’adopter le téléphone portable comme média d’apprentissage. Les résultats montrent que les étudiants acceptent ce moyen d’apprentissage, et suggèrent des moyens d’augmenter cette acceptation. Cette étude suggère donc que l’apprentissage sur mobile est une piste très importante à explorer et capitaliser et pourrait compléter ou remplacer en partie d’autres moyens plus classiques d’apprentissage. \nÀ l’inverse, Kearney, Schuck, Burder et Aubusson se sont intéressés [3] à la perspective d’enseignants sur l’apprentissage mobile. Un cadre pédagogique a été formalisé et testé au travers de deux projets éducatifs. Le formalisme adopté permet d’analyser l’apprentissage mobile. Ainsi, trois caractéristiques de l’enseignement via téléphone portable ont émergé : l’authenticité, la collaboration, et la personnalisation. L’organisation de l’enseignement influe fortement la façon dont les utilisateurs perçoivent ces trois caractéristiques, et le cadre formel développé par les auteurs permettrait d’examiner ceci plus en détail. \n\nCes études suggèrent l’intérêt du e-learning et de l’apprentissage sur mobile par rapport aux techniques plus classiques d’enseignement, mais donnent peu d’information sur la façon précise dont cet apprentissage doit être réalisé. Yu-Chang et Ching ont ainsi utilisé la méthode de Delphes pour obtenir un consensus des experts sur les domaines nécessitant le plus de recherche sur l’apprentissage mobile [4]. Dix catégories ont été identifiées et classées par ordre de priorité, avec en tête les stratégies d’enseignement et d’apprentissage dans le cas spécifique de l’apprentissage mobile. En particulier, peu de recherches s’intéressent aux mécanismes de rétention des joueurs. Il y a donc un clair manque de travaux de recherche dans le domaine de l’apprentissage mobile. \n1.2.2\tApprentissage et serious games\nThomas Connolly, Elizabeth Boyve, Ewan MacArthur, Thomas Hainey et James Boyle ont rassemblé et étudié toute la littérature s’intéressant à l’impact des jeux vidéo chez les personnes âgées de 14 ans et plus [5], en particulier en termes d’apprentissage, d’amélioration de compétences diverses, et d’engagement. Les différents articles décrits montrent que jouer à des jeux vidéo entraîne une variété d’impacts aussi bien perceptuels que cognitifs, comportementaux, affectifs ainsi que motivationnels. En particulier, les études suggèrent que l’apprentissage et la motivation étaient les résultats les plus impactés. \n\nBreuer et Bente ont étudié en particulier la relation entre l’apprentissage et les serious games [6], c’est-à-dire les activités combinant une intention sérieuse, comme l’apprentissage, avec des ressorts ludiques typiques des jeux ou des jeux vidéo. Les jeux sérieux sont essentiellement utilisés, notamment dans le milieu académique, pour l’apprentissage. Ce modèle d’apprentissage diffère du e-learning, et se base sur l’aspect ludique et les mécaniques de jeux vidéo telles que la notion de progression, de récompense, etc. Ainsi, les auteurs insistent sur le fait que lorsqu’un serious game est évalué, son aspect divertissant est primordial. Ils ont ainsi classé les jeux en différentes catégories à l’aide de tags détaillant les mécaniques mises en œuvre et ce qu’elles permettaient d’apprendre. Cette étude permettra de mieux cibler les prochains développements en termes de serious games, en lien avec l’apprentissage. \n\nEn effet, retranscrire des buts et pratiques d’apprentissages directement dans les mécaniques d’un jeu est le principe fondateur d’un serious game. Arnab et ses co-auteurs proposent [7] un modèle associant directement les mécaniques d’apprentissage avec des mécaniques de jeux. Ce modèle permet de concevoir et d’analyser des serious games en ayant des objectifs pédagogiques en tête. Toutefois, ces travaux ne comportent pas de modélisation des joueurs et ne tient pas compte de leur progression, ni des problématiques de rétention des joueurs. \n\nEnfin, Wouters, van Nimwegen, van Oostendorp et van der Spek ont étudié l’impact des serious games sur les processus cognitifs des utilisateurs, en affectant notamment la motivation des apprenants [8]. Les auteurs montrent que les jeux sérieux sont significativement plus efficaces que les méthodes traditionnelles en termes d’apprentissage et de rétention des apprenants. En effet, une comparaison directe de l’enseignement à travers plusieurs cours avec l’utilisation de jeux sérieux en groupe a démontré l’efficacité supérieur des jeux sérieux. \n\nAinsi, si la littérature démontre l’efficacité de l’apprentissage sur mobile et a permis d’associer des buts d’apprentissage à des mécaniques de jeux spécifiques, certains problèmes restent ouverts. En particulier, les problèmes de la motivation et de la rétention des joueurs sont peu abordés et n’ont pas de solution évidente actuellement. \n1.2.3\tErgonomie cognitive [9]\nL’ergonomie cognitive nous a permis d’améliorer le processus de conception de notre plateforme, ainsi qu’à aboutir à un dispositif qui favorise et rend plus efficace l’activité mentale des apprenants.\n\nSi elle apporte parfois un gain en efficacité pédagogique, la formation à distance utilisant les TIC présente cependant des freins [10] :\n\n-\tPlus d’efforts et d’investissement nécessaires pour réussir ;\n-\tDégradation de la communication entre les apprenants et les formateurs, ainsi qu’entre les apprenants ;\n-\tRéticences et difficultés liées à l’utilisation des TIC.\n\n Il faut donc partir du postulat d’une faible acceptabilité du dispositif. En outre, la conception d’un dispositif de formation à distance est un problème complexe à résoudre parce qu’il faut anticiper et résoudre les problèmes des utilisateurs. \n\nNous avons identifié dans la littérature plusieurs éléments nous permettant de définir une approche ergonomique de la conception de dispositifs de formation à distance utilisant les TIC. Nous avons dû nous approprier ces différentes connaissances et méthodologies afin d’imaginer la notre et ainsi créer une approche optimale.\n\nD’abord, il est primordial d’évaluer le degré de nouveauté des connaissances enseignées. En effet, les caractéristiques de la situation d’apprentissage à concevoir sont différentes selon le degré de nouveauté des connaissances abordées. D’une part, une connaissance radicalement nouvelle est mieux acquise si la situation présente de la redondance. D’autre part, la situation ne devra pas présenter de redondance si les apprenants possèdent déjà la connaissance et qu’il s’agit d’une simple révision [11]. \n\nEnsuite, il faut déterminer l’architecture générale du contenu présenté aux apprenants. Les travaux empiriques classiques de [12] ont montré qu’un même contenu structuré différemment n’est pas compris ni appris de la même manière. Il est donc particulièrement crucial de décrire l’architecture. \n\nTroisièmement, il faut évaluer le grain des connaissances apportées par le cours. L’organisation des contenus implique par ailleurs une définition précise de la granularité souhaitée des connaissances. Si l’on va trop dans le détail, ou au contraire pas assez, on passe à côté de l’apprentissage visé. Cette granularité est définie en fonction de contraintes techniques, des objectifs de formation et du niveau de connaissances des apprenants [13]. \n\nEn outre, il faut définir les processus d’apprentissage envisagés durant le cours. Il faut déterminer quels processus d’apprentissage vont permettre à l’apprenant de construire ses connaissances. [14] : l’apprenant va-t-il reproduire dans un contexte B une connaissance habituellement mobilisée dans un contexte A ? L’apprenant va-t-il transformer un savoir théorique en savoir-faire ? L’apprenant va-t-il utiliser consécutivement deux savoir-faire utilisés habituellement de façon séparée ?\n\nIl est également nécessaire de définir quelles sont les tâches d’apprentissage. La catégorisation des tâches d’apprentissage met généralement en relation la tâche et la connaissance à acquérir [15]. Une tâche d’apprentissage peut consister à comprendre une explication, une illustration ou à s’entraîner à réaliser un geste technique. \n\nEnsuite, il faut organiser la progression des contenus de façon pédagogique. \nLes tâches d’apprentissage doivent être ordonnées de façon plus ou moins contraignante et linéaire afin que l’apprenant acquière l’ensemble des connaissances visées. Tout objectif de formation peut donc être décrit à un niveau général (la compétence professionnelle à créer, à améliorer) et à un niveau particulier (de quels savoirs, savoir-faire, méthodes est composée cette compétence). Le dispositif de formation à distance prend alors en compte ces deux niveaux et le scénario didactique relie les étapes qui correspondent aux différents objectifs particuliers. [16]. \n\nUn autre élément à prendre en compte concerne la modalité d’évaluation des connaissances des apprenants.  L’entrée et la sortie de la formation doivent proposer un dispositif d’évaluation des connaissances. Les résultats communiqués très rapidement à l’apprenant révèlent entre autres les aspects positifs de l’évaluation. L’évaluation doit être fonction du format de la connaissance évaluée. Une absence d’évaluation ou une évaluation décalée par rapport aux objectifs d’apprentissage dérégule l’apprentissage et affecte généralement la motivation des apprenants [17]. \n\nPar ailleurs, il faut développer une cohésion graphique au sein de l’interface. La cohésion est maintenue par la récurrence de codes visio-graphiques, elle permet d’assurer une meilleure perception de la prévisibilité des tâches d’interaction, et de la prévisibilité des contenus associés à une focalisation de l’attention adéquate [18]. Comment valoriser les contenus par des effets de structuration visuelle ? \n\nIl faut également structurer l’espace au sein de l’interface pour faciliter l’apprentissage. L’un des premiers critères ergonomiques est celui du groupement et de la distinction [19]. La localisation, le format, les couleurs sont utilisés comme des repères. Le regroupement d’items les apparente à une même fonction, un même thème, etc. Un écran bien structuré rendra la lecture plus efficace et permettra de mieux comprendre la relation de dépendance entre les éléments. La mise en forme matérielle du texte n’a pas que des incidences fonctionnelles, elle en a aussi sur les stratégies de lecture et la compréhension.\n\nDe plus, il faut réfléchir à la représentation des connaissances pour qu’elles soient acquises. Pour représenter des connaissances, le concepteur dispose de quelques formats classiques : textes, images, sons, animations, simulations. Ces formats testés sont en quelque sorte des préconisations pour une lecture optimale sans prendre en compte le contenu traité mais le degré d’attention possible pour une lecture à l’écran multimodale. \n\nLe dernier élément à prendre en compte est relatif à l’évaluation du dispositif : quand et comment l’évaluer ? L’équipe des concepteurs procède au cours du processus de conception à une évaluation plus ou moins formalisée du dispositif. En fin de processus, par exemple quand la première maquette est disponible, une évaluation plus formalisée est réalisée. Les résultats de ces évaluations sont injectés dans le processus de conception, modifiant celui-ci. \n\nL’évaluation par inspection est réalisée en appliquant des grilles d’évaluation. L’évaluation empirique, quant à elle, consiste à interpréter les performances des usagers, à qui l’on prescrit une tâche, et plus généralement à interpréter leurs comportements, attitudes, opinions. Ces deux types d’évaluation sont strictement distincts et complémentaires. L’évaluation par inspection permet de repérer rapidement des erreurs grossières et d’en expliquer la cause. L’évaluation empirique permet de voir l’ensemble des erreurs mineures et majeures, et de diagnostiquer ce qui ne va pas dans le dispositif de formation à distance, sans nécessairement en expliquer les raisons. \n\nEnfin, chaque apprenant étant différent, il est primordial de pouvoir les classifier afin de pouvoir leur proposer le contenu et la méthodologie optimum. Nos travaux de R&D de 2018 ont notamment porté sur l’identification et l’analyse des critères les plus pertinents et le niveau de granularité des contenus."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Les Learning Management Systems (LMS) sont des plateformes de formation à distance permettant de gérer les parcours, suivre les apprenants et diffuser les contenus d'apprentissage. Ils combinent des outils de gestion, communication, évaluation et suivi pour soutenir enseignants et étudiants [1].\n\n• Les LMS peuvent être utilisés pour l'apprentissage à distance, en classe (présentiel) ou en mode hybride. Différents termes sont utilisés pour décrire ces systèmes, tels que plateforme pédagogique, centre de formation virtuel, ou plateforme e-learning, chacun pouvant reposer sur différentes technologies comme la télé présence ou le multimédia interactif [2][3][4][5][6].\n\n• xAPI est une norme qui permet de tracer toutes les activités d'apprentissage sur un LMS, y compris les interactions sociales, la remise de travaux, et les évaluations. Contrairement au format Scorm, xAPI enregistre des expériences d'apprentissage variées, provenant de sources diverses comme le mobile learning et les serious games, et les stocke dans un Learning Record Store (LRS) [7].\n\n• Le Learning Record Store (LRS) est une base de données conçue pour stocker et gérer les traces d'apprentissage. Les LRS peuvent être orientés vers le stockage de données, l'analyse d'apprentissage (Learning Analytics), ou intégrés directement dans un LMS. Ils sont essentiels pour exploiter les données xAPI, et sont le seul type de solution pouvant être certifié conforme à xAPI par l'ADL [8].\n\n• Il existe trois types de LRS : ceux orientés stockage de données, ceux orientés Learning Analytics, et ceux intégrés dans les LMS. Les LRS intégrés sont utilisés par les LMS pour enregistrer les traces d'apprentissage tout en restant invisibles pour l'utilisateur, avec la possibilité d'échanger des données avec des applications tierces.\n\n• Nos travaux de R&D en 2019 ont porté sur l'intégration du LMS avec un LRS pour remonter les informations d'apprentissage, mais les solutions xAPI existantes, comme celle de CSOD, ont montré des limitations, notamment dans l'écriture des Statement Objects, ce qui nous a poussés à opter pour une intégration spécifique du LMS au lieu d'adopter le standard xAPI.\n\n• Bien que l'intérêt pour xAPI dans la communauté LMS soit croissant, les solutions disponibles manquent encore de maturité, en partie à cause de l'absence de définition claire du vocabulaire, des bonnes pratiques, et des modalités d'intégration entre LMS et LRS."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tLearning Management System (LMS)\nLes plateformes de formation à distance sont des outils logiciels dont le rôle est de permettre le pilotage des enseignements à distance [1]. Le vocabulaire informatique utilise de façon conventionnelle le terme Learning Management System (LMS). Les LMS sont des solutions logicielles conçues pour la gestion des parcours, le suivi des apprenants et la diffusion (en ligne) des contenus d’apprentissage.\n\nLes plateformes d'apprentissage en ligne intègrent différents outils de gestion, communication, évaluation, suivi, etc. dans le but d'apporter un soutien technologique aux enseignants et aux étudiants pour optimiser les différentes phases du processus d'enseignement-apprentissage. \n\nDe cette façon, les LMS ont le mérite d’être à la fois dispositif technologique et humain combinant nombre d’outils et fonctionnalités à des fins d'enseignement et d'apprentissage [2]  . Les plateformes permettent la gestion de l’apprentissage totalement à distance, en classe (présentiel) ou encore en mode hybride en combinant présence et à distance [3]. \n\nPlusieurs expressions et variantes sont utilisées pour désigner les environnements LMS à l’instar de : plateforme pédagogique, plateforme d'apprentissage en ligne, système de gestion de l'apprentissage, centre de formation virtuel, plateforme e-learning, plateforme d'e-formation, plateforme de téléformation, plateforme d'enseignement virtuel, plateforme de téléformation, plateforme de cyber enseignement, plateforme de formation à distance… [2][4][5]. Les différences résident très souvent dans le type de technologie utilisée. Par exemple, le télé-enseignement se base essentiellement sur les technologies de la diffusion et de la télé présence, la télévision broadcast, la radio, l’audio et la téléconférence. Le modèle e-learning lui, capitalise sur les atouts du multimédia interactif, de la communication médiatisée par ordinateur (CMO) et des cours basés sur un accès aux ressources d'internet [6].\n1.2.2\tLMS : norme xAPI\nxAPI permet potentiellement de tracer toutes les activités supportées par un LMS. Pas seulement la consultation des contenus, comme le faisait Scorm [7], mais bien l’ensemble des activités : interactions sociales, remise de travaux personnels, évaluation sous toutes ses formes, etc.\nNouvelle génération du format Scorm, en effet, xAPI est une spécification technique qui enregistre les expériences d’apprentissage les plus diverses. Elle permet de suivre les parcours de formation réalisés à partir de n’importe quelles sources.\nCe standard intègre, reconnaît, communique et enregistre les informations issues du mobile learning, des simulations, des serious games, du social learning, de l’offline learning, collaborative learning, etc. Toutes ces expériences sont enregistrées sur un LRS (Learning Record Store) qui est souvent intégré à un LMS. C’est un système qui s’adapte à tous les supports et tous les appareils intelligents et connectés.\nxAPI permet ainsi d’obtenir énormément de données concernant les modules de formation et leur exécution. Il est possible par exemple d’obtenir plusieurs scores pour le même module, d’avoir le détail des résultats ou encore savoir si l’apprenant a apprécié le module ou non.\n1.2.3\tLearning Record Store (LRS)\nLes LRS (Learning Record Store) s’inscrivent peu à peu dans le paysage des nouveaux outils du Digital Learning. C’est la conséquence directe de la mise en œuvre du standard xAPI. \n\nLa définition officielle fournie par ADL (Advanced Distributed Learning) [7], auteur du standard xAPI, est la suivante :\n« A server (i.e. system capable of receiving and processing web requests) that is responsible for receiving, storing, and providing access to Learning Records. »\n\nAutrement dit, le LRS est une base de données dont le rôle est de stocker des traces d’apprentissage. Mais ce n’est pas tout. Ces données, une fois stockées, sont accessibles par d’autres applications. L’enregistrement des données dans le LRS, ainsi que leur récupération par des applications tierces, se fait par un mécanisme standardisé. On parle de manière générale d’API.\n\nLe LRS est actuellement le seul type de solution qui peut être officiellement certifié « conforme xAPI » par ADL [8]. Nous distinguons 3 types de LRS :\n\n-\tLes LRS orientés « stockage de données »\nCes LRS se concentrent sur leur fonction primaire : stocker des données et assurer leur échange avec d’autres systèmes. Ils mettent en œuvre l’ensemble du standard xAPI, mais ne vont pas plus loin, à l’exception de quelques fonctions d’administration et d’exploration des données. Le LRS est vu comme un composant technique, que la plupart des acteurs de la formation ne voient pas.\n\n-\tLes LRS orientés « Learning Analytics »\nCes LRS mettent eux aussi en œuvre l’ensemble du standard xAPI, mais ajoutent de nombreuses fonctions d’exploration et de visualisation de données. Il est généralement possible de créer ses propres rapports et alertes, de sorte que le LRS devient un vrai outil de monitoring, à destination des responsables formation, formateurs et tuteurs. Une gestion des droits est bien sûr assurée.\n\n-\tLes LRS « intégrés »\nLes LMS sont peu évoqués par le standard xAPI, mais ils n’en restent pas moins bien présents dans le paysage du Digital Learning. Bien qu’il n’y ait pas de définition officielle, on peut considérer qu’un LMS est « conforme » à xAPI lorsqu’il intègre un LRS dans lequel il enregistre tout ou partie des traces des apprentissages qu’il supporte. Le LRS est dans ce cas invisible pour l’utilisateur. C’est une base de données au service du LMS, qui est aussi potentiellement ouverte sur l’extérieur, c’est à dire capable d’échanger des données avec des applications tierces.\n\nADL propose une implémentation d'un LRS sur lequel nous nous sommes basés pour nos développement en attendant la mise à disposition d'un LRS par un grand LMS du marché, CSOD. Malheureusement, une fois entre nos mains, le LRS de CSOD nous permet d'écrire des Statement Objects seulement pour le propriétaire du token. La solution xAPI proposée par CSOD ne nous permettait pas donc de faire le chantier d'intégration souhaité. À défaut de nouveau LMS proposant un LRS xAPI, nous avons décidé de changer de stratégie et d'opter pour une intégration spécifique du LMS et non plus d'adopter le standard xAPI.\n\nAinsi, l’intérêt pour xAPI est grandissant dans la communauté LMS. Pourtant, les solutions actuellement disponibles manquent encore de maturité. L’explication se trouve dans la nature même du standard qui ne définit pas l’ensemble des concepts nécessaires à sa mise en œuvre. Effectivement, xAPI définit le format général des traces – leur grammaire – mais ne définit pas le vocabulaire et les bonnes pratiques associées. De plus, il ne définit pas non plus précisément les modalités d’intégration entre un LMS et un LRS.\nFace à ces manques, nos travaux de R&D de 2019 ont porté sur l’intégration du LMS afin de remonter les informations d’apprentissage de nos apprenants directement dans le LMS utilisé par nos clients."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Le digital learning s'est imposé dans les entreprises depuis plus d'une décennie, offrant des contenus uniformisés et accessibles en tout temps pour les salariés. Cette approche vise à améliorer la qualité pédagogique et la disponibilité des formations [1-7].\n\n• Cependant, le digital learning modifie les habitudes de formation des salariés, notamment en termes de lieux et de temps, ce qui peut entraîner des taux d'abandon élevés. La gestion du changement est donc cruciale pour les projets de digital learning, notamment pour favoriser l'engagement des salariés [8-15].\n\n• Les recherches académiques soulignent l'importance du taux de suivi des participants dans l'efficacité du digital learning, au même titre que l'apprentissage et la satisfaction des participants [16]. Le digital learning englobe diverses modalités pédagogiques, telles que l'autoformation multimédia, les classes virtuelles, les forums de discussion et la gamification [17-18].\n\n• Pour répondre aux défis du déploiement du digital learning, il est nécessaire de créer des configurations spécifiques (technologiques, humaines, spatiales et temporelles) pour favoriser l'engagement des salariés dans leurs parcours d'apprentissage, en adaptant les parcours et en proposant des recommandations adaptées.\n\n• Les Learning Management Systems (LMS) sont des plateformes de digital learning conçues pour gérer les parcours, suivre les apprenants et diffuser les contenus d'apprentissage. Ils intègrent divers outils pour optimiser les phases d'enseignement-apprentissage [19-21].\n\n• Les LMS combinent des outils technologiques et humains pour l'enseignement et l'apprentissage, et peuvent être utilisés pour l'apprentissage à distance, en classe, ou en mode hybride. Différentes expressions sont utilisées pour désigner ces systèmes, selon les technologies employées [20-24].\n\n• Nos travaux de R&D en 2020 ont porté sur l'intégration des LMS pour remonter les informations d'apprentissage directement dans le LMS utilisé par nos clients, en réponse aux lacunes de l'état de l'art."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tLe digital learning\nLe digital learning s’est ancré dans le paysage de la formation en entreprise depuis plus d’une décennie. Cette tendance a été confirmée par l’enquête menée par l’observatoire CEGOS en 2013.\nCet ensemble de modalités multimédias de formation peut être considéré comme innovant, et impliquant des changements de méthodes de formation qui ne sont pas neutres [1]. D’un côté, l’utilisation du digital learning en entreprise vise à offrir des contenus uniformisés, disponibles en tout temps et à tout moment pour l’ensemble des salariés d’une entreprise [2][3][4][5][6][7]. L’enjeu est d’améliorer la qualité pédagogique des contenus délivrés aux salariés et favoriser la disponibilité de ces mêmes contenus.\nD’un autre côté, l’utilisation du digital learning peut modifier les habitudes de formation des salariés : modification des lieux et temps de formation [8]. Cette progression du digital learning dans les dispositifs de formation s’accompagne en effet de taux d’abandon qui peuvent rester importants [4][9][10][11][12]. La question de la conduite du changement s’avère dès lors capital pour ce type de projets [13] notamment pour les chefs de projets digital learning ou les responsables de formation. Les modes de déploiement des parcours de learning auprès des salariés dans les grandes entreprises deviennent ainsi un enjeu primordial pour tenter de faire face à cette situation, le but étant alors de favoriser l’engagement des salariés dans leurs parcours digital learning [14][15]. Au niveau académique, les auteurs de [16] dans leur taxonomie de l’efficacité du digital learning, soulignent l’importance du taux de suivi des participants au même titre que d’autres dimensions comme l’apprentissage ou la satisfaction des participants.\nSelon [17] et [18], le digital learning recouvre un continuum de modalités pédagogiques en ligne. Modules multimédias d’autoformation, classes virtuelles, forums de discussion ou encore jeux (gamification) en sont quelques illustrations.\nPlusieurs chercheurs se sont employés à décrire les différentes dimensions du déploiement du digital learning dans les entreprises. La majorité se sont intéressés à une ou deux dimensions particulière(s). \nNous visons donc à créer des configurations technologiques, humaines, spatiales et temporelles spécifiques susceptibles de favoriser l’engagement dans des parcours de digital learning par les salariés. Les dimensions technologique, pédagogique et organisationnelle du déploiement assurent ainsi le socle et un panel large des conditions favorables à l’engagement de tous les salariés.\nNous répondrons ainsi à la diversité (et à la grande majorité) des profils des salariés que ce soit sur les facteurs de suivi ou sur les apprentissages tout en adaptant les parcours, en proposant un accompagnement de soutien (principalement informatif) et/ou incitatif à travers des recommandations adaptées.\n1.2.2\tLearning Management System (LMS)\nLes plateformes de digital learning sont des outils logiciels dont le rôle est de permettre le pilotage des enseignements à distance [19]. Le vocabulaire informatique utilise de façon conventionnelle le terme Learning Management System (LMS). Les LMS sont des solutions logicielles conçues pour la gestion des parcours, le suivi des apprenants et la diffusion (en ligne) des contenus d’apprentissage.\n\nLes plateformes d'apprentissage en ligne intègrent différents outils de gestion, communication, évaluation, suivi, etc. dans le but d'apporter un soutien technologique aux enseignants et aux étudiants pour optimiser les différentes phases du processus d'enseignement-apprentissage. \n\nDe cette façon, les LMS ont le mérite d’être à la fois dispositif technologique et humain combinant nombre d’outils et fonctionnalités à des fins d'enseignement et d'apprentissage [20]  . Les plateformes permettent la gestion de l’apprentissage totalement à distance, en classe (présentiel) ou encore en mode hybride en combinant présence et à distance [21]. \n\nPlusieurs expressions et variantes sont utilisées pour désigner les environnements LMS à l’instar de : plateforme pédagogique, plateforme d'apprentissage en ligne, système de gestion de l'apprentissage, centre de formation virtuel, plateforme e-learning, plateforme d'e-formation, plateforme de téléformation, plateforme d'enseignement virtuel, plateforme de téléformation, plateforme de cyber enseignement, plateforme de formation à distance… [20][22][23]. Les différences résident très souvent dans le type de technologie utilisée. Par exemple, le télé-enseignement se base essentiellement sur les technologies de la diffusion et de la télé présence, la télévision broadcast, la radio, l’audio et la téléconférence. Le modèle e-learning lui, capitalise sur les atouts du multimédia interactif, de la communication médiatisée par ordinateur (CMO) et des cours basés sur un accès aux ressources d'internet [24].\n\nFace aux lacunes de l’état de l’art, nos travaux de R&D de 2020 ont porté sur l’intégration du LMS afin de remonter les informations d’apprentissage de nos apprenants directement dans le LMS utilisé par nos clients."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Les MOOC (Massive Open Online Courses) sont apparus en 2008 au Canada et ont rapidement été adoptés par les universités et grandes écoles pour attirer un public large et international, devenant un outil clé de rayonnement mondial [1].\n\n• Les MOOC sont des cours ouverts accessibles via Internet à un nombre illimité de personnes, proposés par des établissements d’éducation ou des plateformes dédiées comme Coursera, edX, Udacity, et France Université Numérique (FUN) [1].\n\n• Bien que les MOOC rencontrent un grand succès, avec plus de 3000 cours disponibles mondialement, leur efficacité reste à prouver. Par exemple, seulement 9 % des inscrits au MOOC MIT/Harvard ont suivi plus de la moitié des cours, et 5 % ont validé la formation, ce qui est en partie dû à la simple transformation de cours magistraux en formats numériques sans réelle adaptation pédagogique [2-3].\n\n• L’enseignement à distance a une longue histoire, mais les NTIC (Nouvelles Technologies de l’Information et de la Communication) ont révolutionné ce domaine en offrant de nouvelles opportunités pour l’apprentissage en ligne [2-3].\n\n• Le défi actuel est de passer d’un modèle figé à un modèle dynamique et interactif pour les MOOC, en intégrant des stratégies pédagogiques innovantes et en tirant parti de la collaboration et de la compétition pour améliorer l’engagement des apprenants [4-7].\n\n• Nos concurrents incluent Coursera, Khan Academy, edX, FUN, Teach On Mars, et Grovo, chacun utilisant des stratégies pédagogiques spécifiques telles que le Microlearning, la pédagogie inversée, et la Gamification pour engager les apprenants [8-10].\n\n• Notre plateforme se distingue par l’intégration de trois stratégies pédagogiques (Microlearning, pédagogie inversée, Gamification) et par une approche scientifique rigoureuse, y compris l’ergonomie cognitive, pour créer une solide ergonomie pédagogique et maximiser l’efficacité de l’apprentissage [11-13].\n\n• L’ergonomie cognitive aide à optimiser la conception des plateformes de formation en ligne, en facilitant l’activité mentale des apprenants, mais la formation à distance présente encore des défis tels que l’effort requis, la dégradation de la communication, et les réticences à l’utilisation des TIC [20].\n\n• Plusieurs éléments sont essentiels dans la conception de dispositifs de formation à distance, notamment le degré de nouveauté des connaissances, l’architecture du contenu, la granularité des connaissances, les processus d’apprentissage, la présentation des objectifs, les tâches d’apprentissage, la progression des contenus, l’évaluation des connaissances, la cohésion graphique, et la structuration de l’espace au sein de l’interface [9-18].\n\n• Jessica Dehler, spécialiste de l’éducation, a identifié la curiosité, la régularité et la persévérance comme des facteurs clés de réussite dans les MOOC. Ces facteurs sont associés à une meilleure performance et rétention des connaissances, et peuvent être promus à travers un design pédagogique qui stimule la curiosité, soutient la régularité, et encourage la persévérance [19-24].\n\n• En conclusion, la persévérance est fortement corrélée aux résultats obtenus dans les MOOC, et la conception des cours doit prendre en compte cet aspect pour entretenir la motivation des apprenants et maximiser leur succès [25]."}, {"role": "assistant", "content": "1.1\tÉtat de l’art\n1.1.1\tMOOC et stratégies pédagogiques du e-learning\nCréé en 2008 au Canada, le MOOC est rapidement devenu un des grands axes de développement des universités et des grandes écoles. Grâce à la force d'Internet et des réseaux sociaux, cela leur permet d’attirer un public large et international permettant de faire rayonner leur marque à l'étranger. De fait, les établissements les plus prestigieux dans le monde disposent dorénavant d’un MOOC. \n\nEmmanuel Davidenkoff [1] indique que les MOOC (Massive Open Online Course) sont des cours ouverts accessibles via Internet par un nombre illimité de personnes. Ils sont diffusés sur les sites des établissements d’éducation ou sur des plateformes qui accueillent des centaines de cours et s’apparentent à de véritables universités virtuelles (Coursera, edX, Udacity aux Etats-Unis, France Université Numérique en France, etc.).\n\nLes MOOC rencontrent aujourd’hui un succès fort : \n\n-\tPlus de 3000 MOOC sont aujourd’hui proposés dans le monde ; \n-\t25% en provenance de pays européens ;\n-\t53% des sujets traitent des sciences dures & technologie, 17% des sciences sociales, et seulement 15% adressent le business. \n\nCependant, par-delà un effet de mode ou un attrait de curiosité, l’efficacité de ces MOOC universitaires n’a pas encore fait ses preuves : par exemple seulement 9 % des inscrits au MOOC MIT/Harvard ont suivi plus de la moitié des cours et 5% ont validé la formation.\n\nCette déperdition est en grande partie liée au fait que peu d’intervenants conçoivent ces nouveaux outils de formation comme étant des projets en tant que tels et limitent leurs actions à la seule transformation de cours magistraux préexistants.\n\nL’enseignement à distance possède une longue histoire et a commencé à l’Université de Londres, au XIXe siècle, avec des cours par correspondance.\n\nLe développement des NTIC (Nouvelles Technologies de l’Information et de la Communication) a cependant bouleversé le paysage et offert de nouvelles opportunités. (cf. [2] & [3]). \n\nLa prise en compte de ces nouvelles technologies dans le domaine de l’enseignement est lente. Les systèmes éducatifs en Europe et outre-Atlantique donnent un poids important à l’évaluation et à la sanction du diplôme de fin d’études. Afin de pouvoir conserver, au moins pour un temps, ces mécanismes d’examen, il était nécessaire de passer par une première étape, de transformation de cours magistraux en supports électroniques mis à la disposition du plus grand nombre. Ce qu’ont fait, dans le cadre d’un partenariat, l’Ecole Polytechnique puis d’autres grandes écoles avec la plateforme Coursera.\n\nLe chalenge qui se présente aujourd’hui est d’un autre type : comment passer d’un mode figé à un mode dynamique et interactif ?\n\nNotre plateforme n’est pas la seule à se positionner dans le domaine des MOOC. En effet, nous comptons de nombreux concurrents directs : Coursera, Khan Academy, EdX, FUN (France Université Numérique), Teach On Mars, Grovo, Udacity, etc.\n\n-\tGrovo est une plateforme de Microlearning qui propose des modules de cours très courts permettant aux salariés d’apprendre de nouveaux concepts en très peu de temps : les modules ont pour objectif d’enseigner des comportements de manière motivante. Des liens et des répétitions sont régulièrement faits entre les modules de cours afin de favoriser la mémorisation. \n-\tTeach On Mars est une plateforme de Mobile Learning. Elle offre des contenus éducatifs personnalisés destinés à engager l’utilisateur apprenant : quiz, jeux, cours, notifications, etc. Le portail Mobile Learning de Teach on Mars a été pensé pour les smartphones et les tablettes mais il fonctionne aussi sur le web. Les outils « auteurs » permettent de créer simplement des formations intégrant des cours et des jeux pédagogiques. Une fois créée, une formation peut être aisément téléchargée par tous les apprenants directement sur leurs mobiles. Le formateur a ensuite accès aux statistiques d'apprentissage. \n-\tCoursera, lancée par deux enseignants de Stanford, regroupe désormais 83 universités. Cette plateforme propose de suivre des programmes universitaires à distance. Il est même possible de suivre des formations diplômantes de grandes universités. \n-\tEdX, monté par Harvard et le MIT, regroupe 35 établissements partenaires et revendique 2,5 millions d'utilisateurs. \n-\tFUN, France Université Numérique, qui regroupe les MOOC d'universités, de grandes écoles et d'autres acteurs de l'enseignement supérieur. Quelques 25 cours ont réuni 195.000 inscrits en 2013 et cette année 30 cours supplémentaires seront lancés. \n\nLa Figure 1 1 ci-dessous présente les différentes plateformes d’e-learning que nous avons recensées. Elles sont réparties selon leur cible (B2C ou B2B) et selon la stratégie pédagogique qu’elles utilisent. Nous pouvons observer que notre plateforme se trouve dans un océan bleu. \n \nFigure 1 1 : Matrice de positionnement concurrentiel\nCependant nos concurrents n’ont pas une approche aussi scientifique et rigoureuse de l’e-learning que nous. En effet, nous regroupons sur notre plateforme trois stratégies pédagogiques différentes, tandis que nos concurrents ont tendance à n’en utiliser qu’une. En outre, nous avons travaillé de concert avec des experts de l’ergonomie cognitive afin de proposer une plateforme proposant une solide ergonomie pédagogique. \n\nLes stratégies pédagogiques que nous utilisons sont le Microlearning (les cours sont mis sous forme de séquences interactives d’une durée allant de 30 secondes à 3 minutes portant sur un thème précis), la pédagogie inversée (les apprenants préparent leur leçon tout seuls puis effectuent des exercices et peuvent contacter un enseignant en cas de besoin), et la Gamification (utilisation d’éléments de jeu pour motiver les apprenants sous certaines conditions [4]). Nous utilisons les trois approches car elles sont toutes plus ou moins pertinentes selon les situations. À titre de comparaison, la plateforme Grovo n’utilise que l’approche Microlearning et se prive ainsi des autres approches qui peuvent s’avérer plus efficaces dans certains cas. \n\nCes nouvelles formes de pédagogie imitent le marché des jeux vidéo puisque ce dernier a réussi à adresser les problématiques de démotivation et de désengagement des joueurs. \n\nNous avons notamment imité le marché des jeux vidéo dans le cadre du développement de notre mode Battle, qui relève à la fois de la Gamification et de la mise en compétition des apprenants. \n\nLe mode Battle est une fonctionnalité particulière car la plupart des MOOC n’introduisent pas de notion de compétitivité entre les apprenants. Cette notion de compétition permet d’introduire une dimension sociale dans l’apprentissage. [5] \n\nEn outre, des recherches sur l’apprentissage ont montré que l’ajout d’une dimension sociale et collaborative permet d’obtenir de meilleurs résultats que lorsque chaque apprenant reste isolé.\n\nTandis que la collaboration augmente la vitesse d’apprentissage et la persistance des connaissances, la compétition augmente l’implication et la motivation des apprenants. [6]\n\nAinsi, le mélange de la collaboration et de la compétition en situation d’apprentissage sont complémentaires et doivent être considérés comme tels par les personnes concevant des cours. Il y a cependant toujours des controverses sur ce sujet, notamment dans le domaine de l’apprentissage par le jeu. [7] \n1.1.2\tErgonomie cognitive [20]\nL’ergonomie cognitive nous a permis d’améliorer le processus de conception de notre plateforme en aidant les différents participants à ce processus à partager une même représentation du but et du processus de conception. L’ergonomie cognitive permet également d’aboutir à un dispositif qui facilite et rend plus efficace l’activité mentale des apprenants. \n\nSi elle apporte parfois un gain en efficacité pédagogique, la formation à distance utilisant les TIC présente cependant des freins [8] :\n\n-\tPlus d’efforts et d’investissement nécessaires pour réussir ;\n-\tDégradation de la communication entre les apprenants et les formateurs, ainsi qu’entre les apprenants ;\n-\tRéticences et difficultés liées à l’utilisation des TIC.\n\n Il faudrait donc partir du postulat d’une faible acceptabilité du dispositif. En outre, la conception d’un dispositif de formation à distance est un problème complexe à résoudre parce qu’il faut anticiper et résoudre les problèmes des utilisateurs. \n\nNous avons identifié dans la littérature plusieurs éléments nous permettant de définir une approche ergonomique de la conception de dispositifs de formation à distance utilisant les TIC. \n\n-\tQuel est le degré de nouveauté des connaissances enseignées ? \nLes caractéristiques de la situation d’apprentissage à concevoir sont différentes selon le degré de nouveauté des connaissances abordées. D’une part, une connaissance radicalement nouvelle est mieux acquise si la situation présente de la redondance. D’autre part, la situation ne devra pas présenter de redondance si les apprenants possèdent déjà la connaissance et qu’il s’agit d’une simple révision [9]. \n\n-\tQuelle est l’architecture générale du contenu présenté ? \nLes travaux empiriques classiques de [10] ont montré qu’un même contenu structuré différemment n’est pas compris ni appris de la même manière. Il est donc particulièrement crucial de décrire l’architecture. \n\n-\tQuel est le grain des connaissances ? \nL’organisation des contenus implique par ailleurs une définition précise de la granularité souhaitée des connaissances. Si l’on va trop dans le détail, ou au contraire pas assez, on passe à côté de l’apprentissage visé. Cette granularité est définie en fonction de contraintes techniques, des objectifs de formation et du niveau de connaissances des apprenants [11]. \n\n-\tQuels sont les processus d’apprentissage envisagés ? \nIl faut déterminer quels processus d’apprentissage vont permettre à l’apprenant de construire ses connaissances. [12] : l’apprenant va-t-il reproduire dans un contexte B une connaissance habituellement mobilisée dans un contexte A ? L’apprenant va-t-il transformer un savoir théorique en savoir-faire ? L’apprenant va-t-il utiliser consécutivement deux savoir-faire utilisés habituellement de façon séparée ?\n\n-\tComment présenter les objectifs de la formation ? \nLa présentation des objectifs est-elle centrée sur les usagers afin de présenter les compétences qui vont être acquises et les problèmes que ces nouvelles compétences vont résoudre ? \nLa présentation des objectifs est-elle centrée sur l’institution, comme une entreprise qui met en place un plan de formation continue, afin de présenter le projet ou les problèmes de l’institution qui vont être traités par la formation ? \n\n-\tQuelles sont les tâches d’apprentissage ? \nLa catégorisation des tâches d’apprentissage met généralement en relation la tâche et la connaissance à acquérir [14]. Une tâche d’apprentissage peut consister à comprendre une explication, une illustration ou à s’entraîner à réaliser un geste technique. \n\n-\tQuelles sont les progressions parmi les contenus ?  \nLes tâches d’apprentissage doivent être ordonnées de façon plus ou moins contraignante et linéaire afin que l’apprenant acquière l’ensemble des connaissances visées. Tout objectif de formation peut donc être décrit à un niveau général (la compétence professionnelle  à créer, à améliorer) et à un niveau particulier (de quels savoirs, savoir-faire, méthodes est composée cette compétence). Le dispositif de formation à distance prend alors en compte ces deux niveaux et le scénario didactique relie les étapes qui correspondent aux différents objectifs particuliers. [15]. \n\n-\tComment évaluer les connaissances des apprenants ?  \nL’entrée et la sortie de la formation doivent proposer un dispositif d’évaluation des connaissances. Les résultats communiqués très rapidement à l’apprenant révèlent entre autres les aspects positifs de l’évaluation. L’évaluation doit être fonction du format de la connaissance évaluée. Une absence d’évaluation ou une évaluation décalée par rapport aux objectifs d’apprentissage dérégule l’apprentissage et affecte généralement la motivation des apprenants [16]. \n\n-\tComment développer une cohésion graphique au sein de l’interface ? \nLa cohésion est maintenue par la récurrence de codes visuo-graphiques, elle permet d’assurer une meilleure perception de la prévisibilité des tâches d’interaction, et de la prévisibilité des contenus associés à une focalisation de l’attention adéquate [17]. Comment valoriser les contenus par des effets de structuration visuelle ? \n\n-\tComment structurer l’espace au sein de l’interface ?\nL’un des premiers critères ergonomiques est celui du groupement et de la distinction [18]. La localisation, le format, les couleurs sont utilisés comme des repères. Le regroupement d’items les apparente à une même fonction, un même thème, etc. Un écran bien structuré rendra la lecture plus efficace et permettra de mieux comprendre la relation de dépendance entre les éléments. La mise en forme matérielle du texte n’a pas que des incidences fonctionnelles, elle en a aussi sur les stratégies de lecture et la compréhension [18].\n\n-\tComment représenter les connaissances pour qu’elles soient acquises ? \nPour représenter des connaissances, le concepteur dispose de quelques formats classiques : textes, images, sons, animations, simulations. Ces formats testés sont en quelque sorte des préconisations pour une lecture optimale sans prendre en compte le contenu traité mais le degré d’attention possible pour une lecture à l’écran multimodale. \n\n-\tQuand et comment évaluer le dispositif ? \nL’équipe des concepteurs procède au cours du processus de conception à une évaluation plus ou moins formalisée du dispositif. En fin de processus, par exemple quand la première maquette est disponible, une évaluation plus formalisée est réalisée. Les résultats de ces évaluations sont injectés dans le processus de conception, modifiant celui-ci. \n\nL’évaluation par inspection est réalisée en appliquant des grilles d’évaluation. \n\nL’évaluation empirique, quant à elle, consiste à interpréter les performances des usagers, à qui l’on prescrit une tâche, et plus généralement à interpréter leurs comportements, attitudes, opinions. Ces deux types d’évaluation sont strictement distincts et complémentaires. L’évaluation par inspection permet de repérer rapidement des erreurs grossières et d’en expliquer la cause. L’évaluation empirique permet de voir l’ensemble des erreurs mineures et majeures, et de diagnostiquer ce qui ne va pas dans le dispositif de formation à distance, sans nécessairement en expliquer les raisons. \n1.1.3\tTravaux de Jessica Dehler \nNotre spécialiste de l’éducation, Jessice Dehler, a réalisé des recherches sur les facteurs influençant la réussite et permettant de mieux retenir les apprenants : curiosité , régularité , persévérance . \n\nNous avons d’abord cherché à comprendre quel est l’impact de la curiosité dans l’apprentissage par MOOC. \n\nNous avons définit la curiosité comme un comportement exploratoire qui mène à apprendre et à comprendre de nouvelles choses, ainsi qu’à chercher de nouvelles stimulations intellectuelles. \n\nPourquoi la curiosité est-elle si importante dans le cadre de l’e-learning en entreprise ? \nResearch finds that curiosity is associated with better performance, including in regard to learning. This is attributed to the very evolutionary function of curiosity: motivation for the acquisition of new knowledge. In line with this, individuals with high curiosity measures retained information better in a one to two-weeks delayed test (Kang et al, 2009).\n\nHow can we promote curiosity in the instructional design of MOOCs?\nPerceptual curiosity is stimulated with UX design that uses bright and vivid stimuli, and a good dose of regular novelty.\n\nEpistemic curiosity is supported by self-directed learning paths, i.e. learning environments that provide the learners with a good portion of choice as to how and in which order they want to progress. This way, learners seek information that is directly useful for them.\n\nAccording to the information-gap theory of curiosity (Loewenstein, 1994), we can foster curiosity by identifying knowledge gaps and by initially providing only small amounts of information on a topic as a teaser.\n\nLearners are most curious on a topic or a question if they have some knowledge of it but lack confidence. This should be considered in the choice of training contents. Also, questions in the beginning of a course can help assess learners’ confidence level, and thus induce curiosity. Instructional Design should rely on curiosity-inducing tasks such as inquiry, search, experimentation, etc. \n\nIl y a des moyens de récompenser la curiosité. L’analyse des données des apprenants permet aux concepteurs de cours de mettre en place les meilleures récompenses. \n\nNous avons également cherché à comprendre quelle est l’importance de la régularité de l’apprenant pour réussir un cours, et comment cette régularité peut être mesurée. Nous voulions également savoir si les habitudes de régularité des apprenants peuvent aider les recruteurs à mieux cibler les candidats. Enfin, nous voulions comprendre comment un cours peut encourager la régularité. \n\nThe activity level of employed learners in MOOCs is typically equally distributed from Monday to Thursday with increasing engagement in the morning and a peak in the evening hours, while on Friday activity levels are very low. Peak time shifts to the afternoon on weekends. (Shirvani Boroujeni, Kidzinski & Dillenbourg, 2016)\n\nMOOC learners report that a time management problem is the number 1 reason to drop out of a course (Nawrot &Doucet, 2014). Apart from self-reported data, behavioral data on regularity have also been shown to predict completion rates in MOOCs: Learners with higher regularity values have higher chances of finishing a course. \n\nTime management during MOOCs depends on external factors, in particular professional constraints, and internal factors, that is self-regulation (Kizilcec & Halawa, 2015). Interestingly. learners who are full-time employed professionals are learning with significantly higher regularity compared to full-time student learners. Employees seem to compensate supposedly higher time constraints by better time management skills and these skills pay off in their lifelong learning journey given that employees also show higher engagement and better achievement in MOOCs. Thus, measuring the regularity of a MOOC learner has great potential in informing HR about a learner’s time-related self-regulation competence. \n\nIn order to increase regularity, online courses can act on communication and delivery modes in particular.\n\nPush notifications and course-related communication should be delivered at the right moment for each learner. Therefore, temporal information needs to be included in the user model and in predictive analyses.\n\nThe peak in evening hours indicates that learning is accomplished during commute times. Thus, courses need to be delivered in a mobile-friendly mode that supports short sessions and that allows learners to interrupt and continue the course easily.\n\nOne could even think of support tools for example for planning, prioritizing or monitoring, these would be integrated in a course with the goal of providing learners with time management guidance. This approach is especially useful if the development of metacognitive and self-regulation skill is a training goal by itself.\n\nEnfin, nous avons souhaité comprendre pourquoi des personnes également intelligentes n’obtenaient pas les mêmes résultats à l’école, puis au cours de leur carrière. \n\nEn étudiant cette question, Angela Duckworth, professeur à l’Université de Pennsylvanie, a identifié la persévérance comme étant un élément clé pour prédire les résultats d’une personne. (Duckworth et al. 2007). Ainsi, les personnes qui produisent un effort soutenu sur une longue période obtiennent de meilleurs résultats que les personnes moins déterminées. \n\nThis research encourages learning professionals to consider not only engagement which is often measured as pure activity levels, but rather activity that brings the learner forward towards completion. \n\nScales were developed to measure perseverance for kids and adults. Short tests are available which can be taken in a couple of minutes (e.g. Grit scale). As you can manage what you can measure, the availability of assessment further stimulated the creation of initiatives, especially in the US, to improve, act on and teach grit (cf. New York Times, 2016). Schools and teachers, for instance, have started talking about perseverance with students, make them reflect about their own level of grit, and help them create intentional habits to support self-control. \n\nIn the context of professional training, the focus is not so much on training perseverance, but rather on designing learning environments supporting the application of perseverance. The design of the Coorpacademy platform, for instance, supports progressing in small steps, a chapter is composed of four questions and one element of short-format content, and stimulates learning from errors by means of a gamification life system.\n\nCompanies and more specifically HR and L&D professionals can benefit from information about perseverance of their learner population. Research (Eskreis-Winkler et al., 2014) has shown that persons one standard deviation higher in perseverance measures are 32% more like to complete a selection course, 40% more likely to stay in their job, and 21% more likely to graduate from high school on time.\n\nSo there is a clear value for companies to consider perseverance. In case you need to decide whom to recruit, whom to promote, whom to propose ambitious projects, or whom to develop through training, consider looking for the perseverant. (cf. Edsurge, 2017).\n\nEn conclusion, nous pouvons affirmer que la persévérance est clairement corrélée aux résultats obtenus. En outre, la façon dont le cours est construit est très importante pour entretenir la motivation de l’apprenant. Il est donc certain que nous devons prendre en considération cet aspect lors de la création de nos cours."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• L'Infrastructure as Code (IaC) permet aux développeurs de gérer et d'automatiser l'infrastructure informatique via du code, éliminant ainsi les processus manuels répétitifs et offrant des performances techniques optimisées, notamment en termes de stabilité.\n\n• L'IaC présente des défis, notamment le choix entre des outils de management de configurations ou d'orchestration, le modèle d'infrastructure (mutable ou permanente), et le style de codage (procédural ou déclaratif). Le type d'architecture visée (client/serveur ou client uniquement) est également un facteur déterminant.\n\n• Les outils actuels d'IaC ne sont pas encore totalement stables, comme l'ont relevé Guerriero et al., et leur intégration dans des développements expérimentaux nécessite des compromis. La technologie étant émergente, de nombreux sujets de recherche visent à fiabiliser ces outils, comme l'ont formalisé Rahman et al.\n\n• Les solutions proposées par Amazon Web Services (AWS) sont particulièrement étudiées, avec des recherches telles que celles de Pelle et al. sur les performances des applications Cloud Natives et de Posey et al. sur les défis technologiques liés aux calculs complexes dans le Cloud.\n\n• Les algorithmes de matching que nous développons utilisent des méthodes basées sur le contenu, où une fonction de similarité est définie pour comparer des objets en se basant sur leur contenu plutôt que sur les opinions des utilisateurs.\n\n• Cette méthode de recommandation suggère des objets similaires à ceux appréciés par l'utilisateur en analysant les attributs des objets précédemment notés. Elle est avantageuse pour les utilisateurs ayant des goûts uniques et n'est pas biaisée par les objets populaires, offrant une solution au problème de « cold start ».\n\n• Cependant, cette approche présente des limitations : la qualité des recommandations dépend de la capacité à qualifier les objets, et certains contenus (comme les films ou la musique) sont difficiles à convertir en attributs pertinents. Il existe également un risque de surqualification, réduisant ainsi la diversité des recommandations.\n\n• Pour améliorer cette méthode, nous introduisons des initiatives basées sur une pondération des critères, afin de mieux adapter les recommandations aux préférences des utilisateurs."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tInfrastructure as Code\nL’Infrastructure as Code, ou IaC, décrit une configuration informatique permettant aux développeurs de gérer et d’approvisionner automatiquement l’infrastructure informatique par le biais du code sans passer par des processus manuels.\nNous nous sommes orientés vers cette technologie dans le but d’automatiser nos processus de conception de l’infrastructure. Ce choix technologique s’inscrit dans notre ambition de supprimer les étapes répétitives et de routine ; mais aussi d’offrir à nos utilisateurs les meilleures performances techniques possibles (notamment en termes de stabilité).\n\nToutefois, cette technologie pose de nombreux défis et son utilisation au sein de développements expérimentaux nécessite de réaliser des compromis en lien avec les objectifs à atteindre :\n\tChoix entre un outil de management des configurations ou d’orchestration ;\n\tChoix de modèle entre une infrastructure mutable ou permanente ;\n\tChoix entre un style procédural ou déclaratif pour le code régissant l’infrastructure.\nEnfin, le dernier choix repose sur le type d’architecture visée : client/serveur ou client uniquement.\n\nUne fois l’outil choisi, de nouvelles réflexions sont nécessaires pour assurer le fonctionnement de celui-ci avec les développements. En effet, des dysfonctionnements ont été relevés dans l’enquête menée par Guerriero et al. auprès d’utilisateurs des outils du marché : cette enquête n’a permis d’identifier aucune solution stable ou prête à être directement intégrée.\nDe plus, le développement du code gérant l’infrastructure est lui-même à l’origine de défis, notamment lorsque plusieurs versions cohabitent. Enfin, l’IaC reste une technologie émergente, et de nombreux sujets de recherche visent à fiabiliser les outils aujourd’hui disponibles, une liste des limites des outils existants et des perspectives de recherche a été formalisée par Rahman et al.\n\nNous nous sommes particulièrement intéressés aux solutions proposées par Amazon Web Services (AWS). Nous avons pu identifier les travaux de Pelle et al. portant sur les performances des applications Cloud Natives, notamment en termes de latences ainsi que les travaux de Posey et al. recensant les défis technologiques posés par les calculs complexes dans le Cloud ainsi que leurs limites de performances.\n1.2.2\tMatching\nLes algorithmes de matching que nous développons s’appuient sur des méthodes basées sur le contenu.\nPour cette méthode, l’objectif est de définir une fonction de similarité entre les objets. Cette fonction se base sur l’analyse du contenu. Les recommandations se basent sur les informations qui sont contenues dans les objets plutôt que sur les opinions (implicites et explicites) des autres utilisateurs. Cette méthode doit être utilisée conjointement à un algorithme d’apprentissage automatique (machine learning) afin d’induire un modèle de préférences pour les utilisateurs en se basant sur une description des contenus.\n\nPour ce type de méthodes de recommandation, le système tente de suggérer à un utilisateur des objets similaires à ceux qu’il a apprécié auparavant. Un système uniquement basé sur le contenu recommande à un utilisateur des objets seulement en se basant sur le profil qui a pu être bâti en analysant le contenu des objets qu’il a notés dans le passé.\nLe contenu d’un objet apparait comme un ensemble d’attributs dépendant de la nature de l’objet (pour un livre, nous pouvons choisir le genre, l’année, l’auteur…). Ces attributs peuvent également être du contenu textuel (un synopsis de film, livre, un titre, une description d’objet à vendre).\nDe nombreuses techniques servent ensuite à comparer deux objets dont les attributs sont renseignés. Certaines de celles-ci font appel à des algorithmes de Traitement du Langage Naturel afin d’extraire les caractéristiques d’un contenu textuel. Dans le cas de fichiers audio ou d’images, les attributs peuvent être extraits du signal lui-même (sans avoir besoin d’une description intermédiaire de l’objet).\nLes avantages de l’approche basée sur le contenu pour générer des recommandations sont multiples : cette méthode est capable de suggérer des objets qui conviennent à des utilisateurs avec des goûts uniques (donc qu’on ne pourrait rapprocher d’aucun autre utilisateur), cette méthode n’est par ailleurs pas biaisée par rapport aux objets « populaires » : les objets récemment ajoutés ou les moins populaires seront tout aussi visibles que les plus consultés. \nDe plus, cette méthode fournit une bonne solution de repli au problème de « cold start » fréquemment rencontré au démarrage. Ce problème se produit lorsqu’un système de recommandation n’a pas assez de données historiques sur lesquelles se baser et formuler des relations entre objets et/ou utilisateurs. Cette méthode a également l’avantage de pouvoir justifier pourquoi tel ou tel objet a été recommandé, notamment en listant les attributs qui ont mis en avant celui-ci par rapport aux attributs des objets consultés par l’utilisateur.\nCette méthode présente cependant aussi des inconvénients. Le principal est la limitation dans les performances de cette technique : nos recommandations sont au plus, aussi bonnes que notre capacité à qualifier les objets. Cette méthode nécessite par ailleurs que le contenu sur lequel nous souhaitons nous baser soit capable d’être converti en mots clés et en attributs pertinents. Certains types d’objets le sont cependant difficilement (films, musique…). Par ailleurs, le risque principal que présente cette méthode est de surqualifier un utilisateur avec des attributs et ainsi se retrouver avec une palette très réduite d’objet à lui suggérer.\nPour ce type de système de recommandation (basé sur le contenu), la méthodologie invite à construire un espace des attributs afin d’y qualifier d’une part les objets, et d’autre part les utilisateurs, qui donnent ainsi un ensemble de vecteurs. Une fonction de similarité (par exemple un calcul de cosinus) peut ensuite être appliquée à ces vecteurs.\nPour remédier aux limites de cette méthode, nous introduisons également des initiatives basée sur une pondération des critères."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Les méthodes de cryptographie existantes, telles que SSL (Secure Socket Layer) et AES (Advanced Encryption Standard), sont largement utilisées mais vulnérables aux attaques. Bien qu'elles soient bien maintenues et corrigées, des failles continuent d'être découvertes, comme celles identifiées par des chercheurs pour AES et SSL.\n\n• L'AES, développé par les cryptographes belges M. Daemen et M. Rijmen, est l'un des algorithmes de chiffrement les plus utilisés. Il repose sur des substitutions, permutations et transformations linéaires répétées, sécurisant les données en les cryptant bloc par bloc. Cependant, une faille a été découverte, permettant de récupérer la clé secrète plus rapidement que prévu [1].\n\n• SSL, conçu par Netscape, assure la confidentialité, l'authentification et l'intégrité des données lors des communications en ligne. Cependant, en 2011, une faille exploitée par Duong et Rizzo, baptisée BEAST, a montré que le protocole pouvait être compromis, notamment pour déchiffrer les cookies cryptés d'un site web ciblé [2].\n\n• RSA est un système de cryptage asymétrique utilisant une paire de clés, publique et privée, pour sécuriser les communications. Sa sécurité repose sur la difficulté de factoriser de grands nombres entiers. Cependant, une méthode de cryptanalyse acoustique a été développée en 2013, permettant de casser une clé RSA de 4096 bits en écoutant les bruits émis par un ordinateur lors du décryptage [3].\n\n• Les systèmes de détection de vulnérabilités, ou Vulnerability Management Systems (VMS), sont efficaces pour détecter et corriger les vulnérabilités classiques telles que le débordement de tampon et l'injection de code. Ils utilisent des bases de données de vulnérabilités connues pour tester les systèmes.\n\n• Cependant, ces VMS ne sont pas adaptés à la détection de vulnérabilités cryptographiques, car chaque configuration d'interface peut être unique, rendant inefficace l'application d'une liste de vulnérabilités connues. La détection de failles cryptographiques nécessite une analyse approfondie et l'intervention d'experts, ce qui peut être coûteux et prendre du temps.\n\n• Les travaux réalisés à l'INRIA ont confirmé qu'il n'existe pas encore de technologie capable de simuler automatiquement et efficacement les actions d'un attaquant cherchant à pénétrer un système cryptographique, soulignant ainsi un besoin de recherche et de développement dans ce domaine."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tMéthodes de Cryptographie existantes :\nLa limite des technologies existantes est, bien sûr que si elles sont connues, le fait qu’elles fassent l’objet de nombreuses tentatives de piratages. A l’inverse, elles sont mieux maintenues et corrigées. Ainsi, le SSL (Secure Socket Layer) et même l’AES (Advanced Encryption Standard) sont vulnérables.\n\nL’AES est l'un des algorithmes de cryptage aujourd'hui le plus fréquemment utilisé et le plus sécuritaire. Son histoire de succès a débuté en 1997, lorsque le NIST (National Institute of Standards and Technology) a annoncé la recherche d’un successeur au standard de cryptage vieillissant DES. Un algorithme, appelé \"Rijndael\", développé par les cryptographes belges M.Daemen et M.Rijmen, a excellé dans la sécurité aussi bien dans la performance et la flexibilité. Il est arrivé en tête de plusieurs compétions, et a été annoncé officiellement comme le nouveau standard de cryptage AES en 2001. Les algorithmes sont basés sur plusieurs substitutions, permutations et transformations linéaires, chacune réalisée sur des blocs de données de 16 octets - de là venant le terme chiffrement par bloc. Ces opérations sont répétées plusieurs fois, appelées \"rondes\". Durant chaque ronde, une clé de ronde unique est calculée de la clé de cryptage, et incorporée dans les calculs. \n\nLes chercheurs de Microsoft, de l’université belge de Leuven et de l’ENS de Paris ont découvert une faille dans l’algorithme de chiffrement AES. \n\nLe chiffrement AES (Advanced Encryption Standard) est actuellement le moyen le plus utilisé et le plus sûr pour sécuriser les transactions en ligne, les communications sans fil ou encore le stockage des données sensibles.\n\nLes travaux des chercheurs Dmitry Khovratovich de Microsoft Research, de Andrey Bogdanov de l’Université de Leuven et de Christian Rechberge de l’Ecole Nationale Supérieure de Paris ont été faits dans le cadre d’un projet de \"cryptanalyse\". Les chercheurs ont publié une méthode d’attaque permettant de récupérer la clé secrète utilisée pour un chiffrement AES et ce de manière cinq fois plus rapide que dans les estimations des experts en raccourcissant l’algorithme AES de deux bits. L’attaque serait applicable à toutes les versions du chiffrement AES.\n\n“Biclique Cryptanalysis of the Full AES”, Andrey Bogdanov, Dmitry Khovratovich, and Christian Rechberger. K.U. Leuven, Belgium; Microsoft Research Redmond, USA; ENS Paris and Chaire France Telecom, France\nhttp://research.microsoft.com/en-us/projects/cryptanalysis/aesbc.pdf\n\nConçu par Netscape, SSL est un protocole se plaçant entre la couche application et la couche transport permettant d'assurer la confidentialité, l'authentification et l'intégrité des données lors des communications. Il peut être appliqué à des applications comme HTTP, POP, FTP,... Il est obtenue par l'utilisation d'algorithmes à chiffrement symétrique de blocs comme DES, FORTEZZA, IDEA, 3DES ou RC2, ou par des algorithmes à chiffrement symétrique de flots comme RC4 (la longueur des clés peut être limitée suivant les législations propres à l'exportation, et le padding correspondant s'applique aux algorithmes par blocs). En 2011, Les chercheurs thaïlandais Duong et Juliano Rizzo ont annoncé pouvoir exploiter une faille leur permettant de déchiffrer discrètement les cookies cryptés d'un site Web ciblé. Baptisé BEAST, cette méthode d’attaque nécessite notamment un JavaScript et un \"sniffer\", un analyseur réseau. Il exploite une faiblesse du chiffrement SSL, qui s'effectue bloc par bloc.\n\nRSA est l'un des systèmes de cryptage asymétrique ayant le plus de succès de nos jours. Découvert à l'origine en 1973 par l'agence de renseignements britannique GCHQ, il a reçu la classification \"top secrète\". À l'opposé des systèmes de cryptage symétriques et traditionnels, le RSA fonctionne avec deux clés différentes : Une clé \"publique\" et une clé \"privée\". Les deux clés fonctionnent de manière complémentaire, un message crypté avec l'une des clés peut seulement être décrypté par l'autre clé. Puisque la clé privée ne peut pas être calculée à partir de la clé publique, cette dernière est généralement rendue disponible publiquement. Ces propriétés permettent des cryptosystèmes asymétriques pouvant être utilisés dans une vaste gamme de fonctions, comme les signatures numériques. Dans le processus de signature d'un document, une empreinte digitale, cryptée avec RSA, est jointe au fichier, et permet au destinataire de contrôler aussi bien l’émetteur que l'intégrité du document. La sécurité de RSA en soi est principalement basée sur le problème mathématique de la factorisation des nombres entiers. Un message sur le point d'être crypté est traité comme un très grand nombre. Lors du cryptage du message, il est élevé à la puissance de la clé, et divisé par un produit fixe de deux nombres entiers. En répétant le processus avec l'autre clé, le texte entier peut être ainsi récupéré. La meilleure méthode actuelle connue pour briser le cryptage demande la factorisation du produit utilisé dans la division.\n\nUn trio de chercheurs, M. Genkin, M. Shamir et M. Tromer ont révélé en 2013 une méthode de cryptanalyse acoustique grâce à laquelle ils sont parvenus à casser une clé RSA 4.096 bits en se servant simplement d’un microphone pour écouter les bruits émis par un ordinateur lorsqu’il décrypte un message chiffré. L’attaque a été élaborée à partir du logiciel GnuPG 1.x (version open source et gratuite d’OpenPGP), qui est une des implémentations de la clé RSA.\nhttp://www.tau.ac.il/~tromer/papers/acoustic-20131218.pdf\n\n1.2.2\tLes méthodes de détections de vulnérabilités existantes\nIl existe à ce jour un grand nombre de solutions informatiques ayant pour vocation la détection de vulnérabilités sur les systèmes informatiques. Ces logiciels sont appelés « Vulnerablity Manager System » (VMS). Ces outils ont depuis longtemps fait leurs preuves quant à leur capacité à détecter et éventuellement corriger les vulnérabilités « classiques » (débordement de tampon, injection de code, mauvaise configuration des firewalls, etc.). \nLa méthode de détection utilisée par les VMS s’appuie sur des bases de données contenant l’ensemble des vulnérabilités déjà connues pour effectuer les tests appropriés sur le système étudié.\nSi cette approche s’avère efficace pour détecter les vulnérabilités « classiques », elle n’est en revanche pas du tout adaptée à la détection de vulnérabilités cryptographiques. En effet, dans le cas de la cryptographie, chaque client aura potentiellement une configuration d’interface différente. Par conséquent, l’application d’une liste de vulnérabilités connues est totalement vaine, et la détection de failles nécessite une analyse approfondie.\nLa seule solution pour détecter des failles cryptographiques consiste alors à faire appel à une équipe d’experts qui effectue une série de tests, avec tous les désagréments que cela suppose (temps, coût, confidentialité, …etc.).\nAu cours des travaux effectués au sein de l’INRIA sur ce sujet, nous avons pu constater qu’effectivement il n’existait pas de technologie permettant de simuler automatiquement et efficacement les actions d’un attaquant tentant de pénétrer dans un système cryptographique."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Les recherches bibliographiques effectuées dans le cadre de ce projet ont permis d'explorer les mécanismes des principaux modèles d’automatisation de la configuration des systèmes d’apprentissage. Plusieurs travaux ont été identifiés, notamment la plateforme MLBase de l'AMPLab de l'université de Berkeley, qui propose une orchestration complète des étapes de Machine Learning avec un moteur d'optimisation automatique. Cependant, le projet n'a pas atteint un stade d'utilisation réelle depuis 2016 [1].\n\n• La plateforme DeepDive, décrite par Zhang et al., est un système d'essai-erreur pour l'extraction de features, mais elle ne prend en compte que les données textuelles et se concentre sur les bases de connaissances [2].\n\n• DataRobot, une entreprise basée à Boston, a développé une plateforme qui automatise l'apprentissage de modèles avec un plan d'exécution optimisé. En 2014, elle comptait déjà une centaine de clients pour sa version beta [4].\n\n• Kandel et al. ont développé un prototype d'interface interactive pour la transformation de données, permettant de visualiser les résultats des transformations. Cependant, ce prototype ne s'applique pas aux jeux de données de très grand volume [5].\n\n• Kandel et al. ont également proposé un outil de profilage des données qui diagnostique automatiquement la qualité des données, classifiant les problèmes et proposant des visualisations adaptées [6].\n\n• Bernard et al. ont proposé une interface d'analyse de données pour les séries temporelles, permettant à l'utilisateur de pré-visualiser ses résultats avec des paradigmes numériques pertinents [7].\n\n• En ce qui concerne les systèmes distribués pour l'analyse de données, Kepler et al. ont décrit un système de workflow pour le calcul scientifique, mais qui n'exploite pas les paradigmes de calcul distribué de type map-reduce [11].\n\n• Starfish et al. ont exploré l'optimisation des workflows sur un cluster Hadoop, en utilisant les historiques de calcul pour optimiser les paramètres de jobs Hadoop [12].\n\n• Liu et al. ont fourni un état de l'art sur les systèmes de workflow scientifiques orientés données dans le cloud, mais aucun des systèmes décrits ne supporte les architectures hétérogènes mêlant calcul en grille, map-reduce et bases de données relationnelles [13-16].\n\n• Zaharia et al. ont défini des jeux de données incrémentaux applicables à plusieurs types de sources de données, mais cette abstraction ne s'applique que pour utiliser Spark comme système de calcul [17].\n\n• Les travaux sur la sécurité dans l’écosystème Hadoop ont montré la nécessité de solutions originales, comme l'utilisation du service Kerberos pour sécuriser un cluster Hadoop [18]. De nombreuses études explorent des systèmes de sécurité plus complexes, garantissant un encryptage au niveau de la source et des fichiers [19-20].\n\n• Les travaux sur les plateformes de mise en production en environnement Big Data sont difficiles à simplifier. Les organisations doivent souvent construire des piles technologiques hybrides pour contourner les limitations des outils existants, qui sont soit propriétaires et limités, soit ouverts et configurables mais peu adaptés aux utilisateurs finaux.\n\n• En ce qui concerne la transformation des systèmes de prédiction en temps réel en API, les formats standards comme le PMML présentent des limitations en termes d’efficacité pour représenter des modèles complexes et pour exprimer des transformations riches de données [21].\n\n• Les architectures lambda, utilisées pour les workflows en temps réel, gèrent les données batch et les données flux, mais les nouveaux frameworks de gestion de workflow apparus après 2015 ne sont pas encore adaptés à une utilisation par des non-experts et nécessitent une ré-implémentation complète [21]."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tRevue des connaissances existantes\nLes recherches bibliographiques que nous avons effectuées dans le cadre de ce projet de R&D nous ont permis de comprendre les mécanismes des principaux modèles d’automatisation de la configuration d’un système d’apprentissage. Nous présentons les principaux travaux par la suite.\n\nNous avons tout d’abord identifié la plateforme MLBase, développée par le « AMPLab » de l'université de Berkeley. [1] MLBase décrit un projet d'orchestration complète des étapes de Machine Learning. MLBase décrit l'idée d'un système de « configuration automatique » d'un système d'apprentissage automatique, avec un moteur d'optimisation qui détermine automatiquement les algorithmes à effectuer en fonction de leur pertinence et de leur temps de calcul. Cependant, l’article auquel les recherches ont abouti n'explique pas comment l'optimiseur prend effectivement les décisions. D'autre part le projet n'est pas arrivé à un stade permettant une utilisation réelle (pas de nouvelles publications, ou d'utilisation commerciales avérées depuis 2016). \nD’autre part, Zhang et al. [2] décrivent la plateforme DeepDive. Il s’agit d’un système d'essai-erreur pour l'extraction de feature. Cependant cette plateforme ne prend en compte que les données textuelles et se focalise sur le cas d'utilisation de base de connaissances. \nEnfin, l’entreprise DataRobot [4], basée à Boston, a été fondée en 2012 et comptais une centaine de clients en 2014 pour sa version en beta. DataRobot construit une plateforme qui automatise l'apprentissage de modèles, suivant un plan d'exécution optimisé. \n\nNous avons également analysé les travaux relatifs aux approches et aux outils de préparation et de transformation de données interactives de très grande masse. Les principaux travaux sont cités ci-dessous. \n\nDans leur article, Kandel et al. [5] décrivent un prototype d'interface interactive pour la transformation de données. Ce prototype a été développé conjointement par les équipes de Standford et Berkeley. Ce prototype décrit un langage formel pour décrire les transformations, un mécanisme permettant à l'utilisateur de visualiser les résultats des transformations effectuées. Cependant ce prototype ne décrit pas comment effectuer de telles opérations sur des jeux de données de très grands volumes. \n\nDans un autre travail de recherche, Kandel et al. [6] décrivent un outil de profilage des données fournissant un diagnostic automatique sur la qualité des données. Le système classifie plusieurs types de problèmes associés à la qualité des données (record manquant, erreur orthographique) puis propose pour chacun des problèmes diagnostiqués, une visualisation adaptée à l'utilisateur.\n\nDe plus, Bernard et al. [7] fournissent une proposition d'interface d'analyse de données adaptée au cas des séries temporelles. L’interface propose des paradigmes numériques pertinents (normalisation, exclusion de valeurs extrêmes) et permet à l'utilisateur de pré-visualiser ses résultats. \n\nPar ailleurs, les travaux de recherche sur l’optimisation et les architectures pour systèmes distribués appliqués à l’analyse de données sont nombreux et fournis, notamment depuis l'émergence de calcul map-reduce. \n\nPar exemple, Kepler et al. [11] décrivent un système de workflow pour le calcul scientifique, et décrit des spécifications pour les workflows de calcul scientifiques proches de ceux de notre objectif de recherche. Cependant, les résultats de ces travaux ne fonctionnent que sur une grille unifiée, et n’exploitent pas les paradigmes de calcul distribué de type map-reduce. \n\nDe plus, dans son article, Starfishet al. [12] décrit des concepts d'optimisation des workflows sur un cluster Hadoop. L’optimisation fonctionne en exploitant les historiques de calcul pour optimiser les paramètres de Jobs Hadoop.\n\nEn outre, Liu et al. [13] fournit un rapport de l'état de l'art sur les systèmes de workflow scientifiques orientés vers les données dans le cloud et les techniques correspondantes pour la gestion du multisite. L'étude montre que le système de Chiron et al. [14] est le seul à permettre un parallélisme hybride, mais sans supporter le partage de workflow. Les systèmes Swift et al. [15] et Pegasus et al. [16] permettent une exécution (multisites), qui peut se faire sur cloud. Cependant, aucun des systèmes décrits ne supporte les architectures hétérogènes, mêlant calcul en grille, map-reduce et base de données relationnelle. \n\nZaharia et al. [17] définit une notion de jeux de données incrémentaux, pouvant potentiellement s'appliquer à plusieurs types de sources de données (Cloud, SQL, Hadoop, etc.), cependant cette abstraction ne s'applique que pour utiliser Spark comme système de calcul. \n\nLes travaux de l’état de l’art que nous avons identifiés se basent sur la mise en place de manière manuelle des règles de correspondance entre systèmes de sécurité ou sur une gestion avec une granularité plus large (donc moins sécurisée) de l’accès aux données dans l’environnement Big Data. Un système Big Data moderne doit donc être capable de : \n\n-\tFournir une abstraction de sécurité compatible entre les différents systèmes de calcul et de stockage, \n-\tRenforcer la sécurité des systèmes natifs, \n-\tPermettre le travail collaboratif, \n-\tPermettre de sécuriser certaines données de manière plus fine (masque des colonnes d’un certain type), \n-\tPermettre d’ajouter des règles de calcul adaptées. \n\nLes travaux sur la sécurité dans l’écosystème Hadoop sont nombreux. Hadoop n’ayant pas été conçu au départ en prenant en compte les contraintes de sécurité [18], il est nécessaire de penser des solutions originales. L’une de celles-ci utilise un service Kerberos (voir la Figure 1 1 ci-dessous). \n \nFigure 1 1 : Les composants du système Kerberos\nUne combinaison de ce système de sécurité avec les différentes couches de Hadoop est proposée par les auteurs [18] à la Figure 1 2. \n \nFigure 1 2 : Une façon d’utiliser le service Kerberos pour sécuriser un cluster Hadoop \nDe nombreuses études envisagent des systèmes de sécurité plus complexes, qui permettent d’aller au-delà des limites d’un mécanisme de sécurité par compte d’utilisateur, par exemple en garantissant un encryptage au niveau de la source [19] et des fichiers [20].\n\nL’Annexe 3 illustre une présentation de nos perceptions des modèles Big Data modernes dans les systèmes hybrides. \n\nPar ailleurs, les travaux sur les plateformes de mise en production en environnement Big Data sont extrêmement difficiles à simplifier. Ils comprennent : \n\n-\tD’un côté les spécialistes de l’infrastructure (Ecosystème Hadoop Spark, MapReduce, Flink, Base OLAP, MPP, NoSQL, etc.), \n-\tDe l’autre les spécialistes des solutions métiers (Rapidminer, SAS, SPSS) qui proposent des versions serveurs de leur logiciel mettant en production des projets réalisés selon leurs outils, \n-\tPour joindre ces deux mondes, des solutions (souvent open-source) introduisent des concepts de bas niveaux pour des utilisateurs moins avancés (Sqoop, Flume, Airflow, Talend, Navigator).\n\nAucune des solutions précitées ne proposent des environnements de déploiement à la fois cohérents et exhaustifs. Les organisations sont donc obligées de construire des piles technologiques hybrides pour contourner les limitations importantes inhérentes à ces outils :\n\n-\tSoit le moteur d’exécution est souvent propriétaire ou les options et le choix des algorithmes sont limités, \n-\tSoit les moteurs d’exécution sont ouverts et hautement configurables mais peu adaptés aux utilisateurs finaux.\n\nEn outre, en ce qui concerne la transformation des systèmes de prédiction temps réel en API, nous avons constaté que les travaux de littérature utilisent des formats standards type « PMML ». Cependant, ceux-ci ont plusieurs limitations : \n\n-\tIls manquent d’efficacité pour représenter les modèles issus des algorithmes modèles (modèles ensemble, modèles gradient, réseaux neuronaux), \n-\tIls ne permettent pas d’exprimer les transformations riches de données (agrégations, moyennes flottantes, enrichissements) qui sont utilisées dans la plupart des modèles de production, \n-\tIls ne fournissent pas de système de validation ou de passage sécurisé au temps réel.\n\nEnfin, les travaux sur des workflows en temps réel consistent principalement en des travaux sur des architectures hybrides gérant les données batch d’un côté, et les données flux de l’autre. Nous avons également utilisé des architectures lambda  (cf. Figure 1 3). \n \nFigure 1 3 : Architecture Lambda\nDe nouveaux frameworks [21] de gestion de workflow sont apparus après 2015. Ces frameworks définissent des logiques de plan de calcul distribué compatible tant au batch qu’aux flux et pourront utiliser des moteurs d'exécution comme Flink ou Spark. A ce jour, ils ne sont pas manipulables par un utilisateur non expert qui devrait tout ré-implémenter à partir de zéro, ce qui ne correspond pas à nos objectifs. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Le projet MEData.Lab est le fruit d'une collaboration entre Dataiku, spécialisée dans l'analyse de grands flux de données, et Bioserenity, experte en capteurs de santé connectés pour des pathologies comme l'épilepsie et les troubles du sommeil.\n\n• Les alternatives existantes dans la littérature se concentrent principalement sur l'enregistrement des données issues des capteurs IoT, mais manquent de fonctionnalités avancées comme l'apprentissage automatique pour les signaux à haute fréquence et l'analyse en temps réel. Ces solutions se limitent souvent à des applications de localisation GPS, comptage d'événements, et mesures simples, sans offrir de capacités avancées pour les applications médicales [1].\n\n• Les plateformes de machine learning cloud existantes ne sont pas adaptées aux modèles sur signaux ni aux contraintes spécifiques de l'embarqué, telles que l'encapsulation des modèles pour une prise de décision en temps réel [2].\n\n• Les technologies de traitement du signal utilisées dans l'embarqué, comme MatLab ou Simulink, ne gèrent pas efficacement les larges jeux de données ni l'apprentissage automatique requis pour les applications de santé [3].\n\n• Le projet CloudWave offre une plateforme cloud pour l'épilepsie, mais se concentre principalement sur l'ordonnancement et la visualisation des données, sans traiter en profondeur les signaux hébergés, contrairement à l'approche multimodale de MEData.Lab [4].\n\n• Aucun projet commercial Big Data ne propose actuellement de solution complète pour le diagnostic de l'épilepsie, bien que des projets comme ceux d'Intel, IBM, et SAP aient exploré des segments liés à l'analyse de larges jeux de données [2].\n\n• Les recherches sur l'analyse des données de capteurs pour la détection des crises épileptiques montrent un potentiel, mais les approches existantes sont limitées par la petite taille des cohortes, la dépendance aux données EEG exclusives, et l'incapacité à généraliser les modèles à d'autres individus [5-6].\n\n• Les méthodes actuelles pour la détection des crises épileptiques via EEG impliquent la réduction de la dimension des données, suivie de la modélisation, mais ces modèles perdent en efficacité avec le temps et ne peuvent pas être généralisés au-delà d'un seul individu [7].\n\n• Les travaux de R&D antérieurs de Dataiku ont été appliqués dans divers secteurs comme la logistique, l'automobile, et le stationnement urbain, démontrant la capacité de la plateforme à exploiter les données des objets connectés pour des analyses prédictives [8].\n\n• Bioserenity a validé la fonctionnalité de ses capteurs et de ses vêtements connectés pour l'épilepsie, et mène des projets dans la polysomnographie, la cardiographie, et la normalisation des formats de données pour les solutions cloud [9-10].\n\n• Le laboratoire TSI de Telecom ParisTech travaille sur le projet MNE, fournissant des bibliothèques de traitement du signal spécialisées pour les données EEG, un élément clé pour l'approche de MEData.Lab [11].\n\n• En conclusion, MEData.Lab vise à surmonter les limitations des approches actuelles en offrant une plateforme analytique prédictive multimodale pour la détection et la prédiction des crises épileptiques, permettant un déploiement à grande échelle des services de Bioserenity [12].\n\n• Les réseaux de neurones et le Deep Learning ont révolutionné le traitement d'images, et l'apprentissage par transfert permet de réutiliser des modèles pré-entraînés pour de nouvelles tâches. Cependant, les services API existants pour l'annotation d'images présentent des inconvénients tels que les coûts et l'instabilité des résultats [13-14].\n\n• L'apprentissage par transfert est particulièrement utile dans les modèles de réseaux de neurones, permettant de transférer des connaissances acquises sur un jeu de données pour les appliquer à d'autres tâches similaires, offrant ainsi un avantage significatif dans le domaine de l'analyse des données de santé [15]."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tTravaux et études externes \nLe projet MEData.Lab est né suite à la rencontre entre notre société Dataiku, qui a développé une plateforme d’analyse de grands flux de données, ayant permis avec succès l’industrialisation de méthodes analytiques dans le cadre de plusieurs projets ambitieux dans différents secteurs, et la société Bioserenity, spécialisée dans les capteurs dans le domaine de la santé et les vêtements connectés notamment pour l’épilepsie et les troubles du sommeil.\n\nActuellement, nous identifions dans la littérature un ensemble d’alternatives de projets de plateformes et des initiatives dans le domaine de l’IoT. Il s’agit d’alternatives diversifiées et très limitées telles que : \n\n-\tLes plateformes existantes dédiées IoT se focalisent sur l’enregistrement amélioré des données issues des capteurs mobiles, et ne gèrent pas de manière spécifique les signaux à très forte fréquence évoluant au cours du temps. Ces plateformes sont malheureusement encore pauvres en fonctionnalités de type « apprentissage ». Elles se focalisent sur les problématiques de localisation GPS ou de comptage d’événement, de mesures simples et de mise en place de boucles mesure, avec en ligne de mire les applications à fort potentiel des objets connectés adaptés au sport, aux loisirs, à la santé personnelle ou à la domotique [1].\n-\tLes plateformes de « Machine Learning » Cloud (gestion de modèles de marquage automatique, etc.) ne sont pas adaptées aux modèles sur signaux et ne traitent pas les problématiques propres à l’embarqué (encapsulation des modèles générés pour codécision effectuée avec l’objet ou au plus près, temps réel) [2]. \n-\tLes technologies de traitement du signal utilisé dans le domaine de l’embarqué (MatLab, Simulink, Labview) n’incorporent pas la dimension larges jeux de données partiellement pertinentes et fusion avec les métadonnées ni sur l’apprentissage automatique nécessaire [3]. \n-\tLe projet CloudWave [4] (travail de recherche publique) offre une plateforme cloud pour l’épilepsie. Cependant, elle est principalement utilisée pour l'ordonnancement et la visualisation des données : bien que capable de gérer rapidement de gros volumes de données, elle n'est pas utilisée pour traiter en profondeur les signaux qu'elle héberge, contrairement à ce qui est proposé par MEData.Lab [4]. \n-\tLes projets sur le segment de l’analyse de larges jeux de données sont nombreux (Intel, IBM, SAP, Oracle, Pivotal, Explorys, etc.). À l’heure actuelle, aucune solution commerciale Big Data de diagnostic de l’épilepsie n’est proposée [2]. \n\nLe sujet de l’analyse des données issues de capteurs pour la détection et la prédiction des crises épileptiques, cardiaques ou d’autres événements liés à des pathologies spécifiques n’est pas récent. \n\nDans les années 1980, des techniques non-linéaires ont été utilisées pour prédire les crises, mais sur des cohortes de petite taille. Plus récemment, au cours des 10 dernières années, la disponibilité de larges jeux de données sur des patients épileptiques a permis des avancées nouvelles dans ce domaine. Un récent concours KAGGLE [5] (Dataiku en a déjà été Premier Lauréat en 2014) a montré qu’il était possible de détecter a posteriori les crises épileptiques sur la base de données collectées, avec un taux de pertinence de 97%, et prédire leur survenue avec un taux de 84%, montrant l’utilité et la faisabilité technique des méthodes algorithmiques développées. \n\nUne récente étude de la société Neurovista, utilisant un dispositif invasif d’EEG a montré qu’il était possible d’obtenir des taux satisfaisants de détection et de prédiction des crises d’épilepsie si la quantité de données était suffisamment importante. Leur approche se basait sur l’utilisation exclusive des données EEG d’un individu pour la prédiction de ses crises, alors que notre objectif à travers l’approche MEData.Lab est d’utiliser des informations multimodales de larges cohortes de patients pour accélérer le traitement des données et affiner les performances des algorithmes. L’intégration de données externes est également un enjeu pour pouvoir combiner les informations EEG de bases de données existantes avec celles constituées au fur et à mesure par Bioserenity. Un exemple de bases de données existantes est EPILEPSIA ainsi que IEEG issue des travaux de MayoClinic aux USA (ieeg.org) [6]. \n\nD’autres projets se sont positionnés sur la détection de crises épileptiques. Ces derniers se focalisent soit sur la détection des convulsions, ce qui restreint leur utilisation à un ensemble restreint d’épilepsie et prévient de la survenue des crises, soit sur l’analyse monomodale des tracés EEG, de façon invasive ou non. Bien que potentiellement aptes à détecter et à prédire les crises, ces approches n’offrent pas les avantages de l’approche multimodale proposée (enregistrements continus et multimodaux hors du milieu hospitalier, edge analytics et streaming des données). \n\nLa détection d’épilepsie sur des signaux électroencéphalogrammes (EEG) consiste en la prédiction des crises sur des fenêtres de temps de taille fixe. Les méthodes les plus performantes actuelles, sur la base d’un concours sur des données d’épilepsie de 2015, consistent dans un premier temps en la réduction de dimension des données EEG à grandes fréquences sur un espace plus petit via différentes techniques : \n\n-\tMoyenne sur l’énergie et statistiques descriptives sur des sous fenêtres.\n-\tDes représentations temps fréquences sur des sous fenêtres. \n-\tDes réductions par bandes de fréquence. \n-\tDimension fractale, entropie. \n\nSur cet espace de dimension réduite, la détection de crises d’épilepsie sur des fenêtres est alors ramenée à des questions de modélisation standard dont les performances (quelle que soit la nature algorithmique du modèle) sont excellentes sur les mêmes individus dans des périodes temporelles courtes. Cependant d’importantes limitations empêchent toutes possibilité́ d’industrialisation : \n\n-\tLes modèles perdent en efficacité́ avec le temps sur un même individu ;\n-\tLes modèles ne passent pas la barrière d’un individu.\n\nSi on combine ce fait à la rareté́ des données étiquetées et au prix de leur production (mobilisation d’experts pour chaque période à étiqueter), les technologies de détection des crises ou de classification automatique de leur typologie ne sont pas satisfaisantes au regard des besoins de l’industrie. \n1.2.2\tTravaux et études internes  \nDans nos travaux de R&D précédents nous avons proposé la plateforme Dataiku pour des acteurs exploitant les données de leurs objets connectés, notamment : \n\n1)\tpar des acteurs dans le domaine de la logistique pour la création d’indicateurs associés au « dernier kilomètre ». \n2)\tpar des acteurs d’objets connectés automobiles pour la création d’indicateurs d’usage commercialisables sur le marché de l’analyse du risque automobile. \n3)\tpar des acteurs dans le domaine du stationnement urbain (indicateurs prédictifs de disponibilité). \n\nD’autre part, la plateforme Dataiku est utilisé dans le cadre de projets de recherche permettant d’utiliser des méthodes issues du monde de la recherche pour des applications réelles dans l’univers industriel (exemple : partenariat avec la plateforme TERALAB CASD qui permet à des acteurs industriels ou à des chercheurs de croiser des données démographiques et sociales, économiques et fiscales sur de grands volumes de données).\n\nEn outre, la société Bioserenity mène actuellement avec succès : \n\n1)\tLa validation du fonctionnement des capteurs et de présérie de vêtements connectés dédié aux patients épileptiques possédant 35 capteurs différents. \n2)\tLes études et les démarches pour les autorisations de mise sur le marché des premières solutions dédiées à l’épilepsie (qui ont été réalisées entre 2015 et 2016). \n3)\tUn projet de solution de vêtement dédié autour de la polysomnographie avec l’APHP.\n4)\tUn projet de cardiographie avec un grand acteur du secteur. \n5)\tUn travail de normalisation avec les américains de Mayo Clinic et de Upenn pour des propositions de formats de données issus de capteurs Polygraphiques (Ces formats sont indispensables pour être intégrés dans des solutions Cloud/télécom incluant des séries temporelles. Ils traitent de la complexité des données de santé de type cryptages, etc.). \n6)\tUn travail de convergence sur les moyens de poser les diagnostics entre l’analyse visuelle « assistée » et l’élaboration de nouvelles interfaces d’aide à la pratique médicale. \n\nEnfin, le laboratoire TSI (Signal Image Processing) de Telecom ParisTech et de l’IMT (Institut Mines Telecom), travaille depuis 2010 sur le projet MNE, qui fournit des primitives et bibliothèques de traitement du signal spécialisées dans le traitement des données EEG.\n\nEn conclusion, nous constatons que la première offre de Bioserenity permet de mettre en œuvre des mesures et visualiser les courbes sur une échelle de temps depuis le web. Néanmoins, cette dernière a besoin de s’appuyer sur une plateforme Analytique Prédictive offrant des performances de premier plan, afin d’accélérer son accès au marché et permettre effectivement le déploiement de ses services à grande échelle. \n\nDans ce cadre, nous avons lancé notre programme de R&D en collaboration avec la société Bioserenity, pour la conception et la mise en œuvre de notre plateforme Dataiku. Cette dernière apporte la solution technique nécessaire à la structuration des données médicales et elle englobe un ensemble d’outils pour le développement d’analyses temporelles multi-échelles ainsi que des outils permettant le design et l’industrialisation de « biomarqueurs algorithmiques ». \n1.2.3\tDeep Learning\nLes réseaux de neurones sont des algorithmes d’apprentissage qui ont connus un fort engouement ces dernières années. L’introduction de l’algorithme de rétro-propagation a permis d’entraîner des réseaux de neurones en parallèle dans des temps raisonnables, et a notamment révolutionné les techniques de traitement d’images. En classification d’images, la base de données ImageNet, composées de plusieurs millions d’images groupées en 1000 catégories, fait figure de référence. Le taux d’erreur sur ce problème de classification baisse constamment : de 25%, il est passé à 17% grâce à l’utilisation d’une approche « Deep Learning » avec le réseau AlexNet [7] et est descendu aujourd’hui aux alentours de 5% [8], [9] et [10].\n\nIl existe des services qui proposent une API d’annotation d’images. C’est le cas de Clarifai, Google Vision API et Amazon Rekognition. Si les modèles peuvent se révéler très précis, ces services présentent deux inconvénients : ils sont payants et n’apportent aucune garantie sur la stabilité des résultats (après une mise à jour du service, un « meuble » peut être classé par la suite en « meuble de salon ») ce qui est gênant lorsqu’on souhaite exploiter analytiquement le résultat de ces annotations. \n\nL'apprentissage par transfert (Transfer Learning en anglais) est un champ de recherche de l'apprentissage automatique. Il vise à transférer des connaissances d'une ou plusieurs tâches sources vers une ou plusieurs tâches cibles. Il peut être vu comme la capacité d’un système à reconnaître et appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes.\n\nEn pratique, lors de l’utilisation de modèles à réseaux de neurones (Deep Learning), cela revient à réutiliser des modèles pré-entraînés (entraînés sur une première tâche sur un premier data set) pour une autre tâche, potentiellement sur un autre data set. \n\nLa Figure 1 1 ci-dessous décrit le processus d’apprentissage par transfert avec un réseau de neurones. \n\n \nFigure 1 1 : Mécanisme de l'apprentissage par transfert avec les réseaux de neurones"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• La plateforme MLBase, développée par le « AMPLab » de l'université de Berkeley, est un projet d'orchestration des étapes du Machine Learning avec un moteur d'optimisation pour choisir les algorithmes en fonction de leur pertinence et du temps de calcul, mais le projet n'a pas abouti à une utilisation réelle [1].\n\n• La plateforme DeepDive de Zhang et al. propose un système d'essai-erreur pour l'extraction de caractéristiques, focalisé sur les données textuelles et les bases de connaissances, mais reste limitée dans son application [2].\n\n• DataRobot, une startup de Boston fondée en 2012, automatise l'apprentissage de modèles en suivant un plan d'exécution optimisé, mais nécessite des compétences avancées en codage pour être utilisée efficacement [3].\n\n• Le concours AutoML organisé par Microsoft en 2015 a introduit une bibliothèque d'apprentissage automatique basée sur Scikit-Learn, utilisant l'optimisation bayésienne et la similarité des jeux de données, mais cette solution est principalement accessible aux experts [4].\n\n• TPOT, un outil basé sur la programmation évolutionniste, propose d'automatiser le choix des méthodes et des paramètres en Machine Learning, mais son prétraitement des données est limité et le projet reste en version beta [5].\n\n• Auto-sklearn, une version améliorée d'AutoML, utilise l'optimisation bayésienne et les performances passées sur des données similaires, mais les temps d'apprentissage sont encore longs et le projet n'est pas suffisamment stable pour une application commerciale [6].\n\n• La loi européenne GDPR de 2016 exige que les décisions prises par des algorithmes soient explicables, ce qui pousse à développer des méthodes pour expliquer les résultats de modèles de Machine Learning, même ceux considérés comme des « boîtes noires » [7].\n\n• Les méthodes d'explication des modèles de Machine Learning peuvent être classées en deux groupes : les méthodes dépendantes du modèle, comme les modèles linéaires et les arbres décisionnels, et les méthodes indépendantes du modèle, comme LIME (Localy Interpretable Model-agnostic Explanations) [8-9].\n\n• Les modèles linéaires sont les plus faciles à interpréter en calculant les coefficients de régression pour déterminer l'influence des variables sur la prédiction, tandis que les arbres décisionnels nécessitent de suivre les chemins dans l'arbre pour extraire les contributions de chaque variable [8].\n\n• LIME crée une approximation locale d'un modèle complexe en le transformant en un modèle plus simple à interpréter, mais cette méthode a des limitations pour les jeux de données textuels et tabulaires [9].\n\n• Les vecteurs d'explication, similaires à LIME, utilisent le gradient autour du point de sortie d'un modèle pour fournir une approximation linéaire de la sortie, en se basant sur une estimation de la densité du noyau [10]."}, {"role": "assistant", "content": "3.2\tÉtat de l’art\n3.2.1\tTravaux scientifiques \nKraska et al. [1] ont décrit la plateforme MLBase, développé par le « AMPLab » de l'université de Berkeley. MLBase est un projet d'orchestration complète des étapes du Machine Learning. \n\nMLBase décrit l'idée d'un système de « configuration automatique » d'un système d'apprentissage automatique, avec un moteur d'optimisation qui détermine automatiquement les algorithmes à effectuer en fonction de leur pertinence et de leur temps de calcul. Cependant, le document n'explique pas comment l'optimiseur prend effectivement ses décisions (cf. Figure 3 1). \n\nPar ailleurs, le projet n'est toujours pas arrivé à un stade permettant une utilisation réelle. Cela se résume par l’absence de nouvelles publications, ou d’utilisations commerciales avérées. \n\nDans les travaux de Zhang et al. [2] nous retrouvons une description de la plateforme DeepDive. Cette plateforme décrit un système d'essai-erreur pour l'extraction de caractéristiques. \n\nCependant la plateforme DeepDive fait l'hypothèse d'un seul modèle de donnée (Textuelle) et se focalise sur le cas d'utilisation de bases de connaissances. La Figure 3 2 illustre le workflow d’analyse d’erreurs de DeepDrive. \n\n \nFigure 3 1. Processus d’optimisation [1]\n \nFigure 3 2. Workflow d’analyse d’erreurs de DeepDive [1]\nLa startup DataRobot , basée à Boston, a été fondée en 2012 et comptait une centaine de clients en 2014 (version beta). DataRobot construit une plateforme qui automatise l'apprentissage de modèles, suivant un plan d'exécution optimisé. \n\nMicrosoft a organisé un concours d’apprentissage automatique en 2015 appelé AutoML [4]. Une plateforme de partage de jeux de données et de calcul a permis à différents compétiteurs de comparer leurs performances. \n\nÀ l’issue de ce concours, des chercheurs ont publié un papier ainsi qu’une bibliothèque d’apprentissage automatique au-dessus de la bibliothèque Scikit-Learn. Celle-ci tire parti de la similarité entre les jeux de données pour initialiser à froid le démarrage de l’automatisation sur un jeu de données en particulier, et utilise l’optimisation bayésienne pour le choix des algorithmes et de leur paramètres et construit un modèle ensembliste comme proposition de modélisation optimale. Cette solution est malheureusement uniquement accessible à des experts sachant coder et le projet n’a pas reçu la traction attendue par la communauté open source. \n\nRandal et al. [5] utilise le concept de programmation évolutionniste pour sélectionner automatiquement les meilleurs paramètres et les meilleures méthodes d’un pipeline d’apprentissage automatique. \n\nReconnaissant que les connaissances métier et les compétences d’un Data Scientist sont nécessaires pour bien paramétrer les outils d’apprentissage automatique, l’outil TPOT prétend trouver automatiquement ces éléments, remplaçant effectivement le besoin d’un expert du problème étudié. \n\nLa Figure 3 3 donne une représentation du processus typique d'apprentissage automatique supervisé.\n \nFigure 3 3. Une représentation du processus typique d'apprentissage automatique supervisé\nTPOT propose d’automatiser le choix des méthodes et des paramètres d’un processus standard d’apprentissage supervisé. \n\nL’originalité de TPOT repose sur l’utilisation d’un algorithme évolutionniste, dont le coût de calcul est très avantageux par rapport à une méthode d’exploration systématique. \n\nEn pratique, à chaque étape TPOT ne dispose que d’un petit nombre de méthodes possibles, sinon l’espace de recherche serait trop important et limiterait la convergence de l’algorithme évolutionniste. \n\nLes auteurs obtiennent des résultats encourageants, en particulier en ce qui concerne l’architecture du pipeline. En effet il leur est possible de favoriser les pipelines composés du plus petit nombre d’opérateurs possible, sans perdre en performance.\n\nLe projet manque malheureusement de maturité sur de nombreux points, en particulier le prétraitement des données qui est très limité. C’est un projet expérimental encore en beta et dont le développement semble avoir fortement ralenti. Il n’est pas prêt pour une utilisation dans un environnement exigeant. \n\nFeurer et al. [6] introduit une nouvelle version plus robuste du système AutoML précédemment développé. Ce nouveau système, nommé Auto-sklearn améliore les performances d’AutoML en conservant le concept d’optimisation bayésienne, mais en ajoutant la prise en compte des performances obtenues précédemment sur des données similaires, ainsi qu’en créant des ensembles à partir de modèles entraînés. La Figure 3 4 présente le système de l’approche Auto-ML améliorée. \n\n \nFigure 3 4. L’approche Auto-ML améliorée \nLes performances prédictives sont relativement bonnes pour les tâches de référence présentées, mais le temps d’apprentissage est rédhibitoire (plusieurs jours). D’autre part le développement de la librairie n’est pas encore très avancé, et les erreurs rapportées par les utilisateurs reflètent des instabilités incompatibles avec une application commerciale (suppressions involontaires). \n\nNous avons ensuite réalisé une étude des différentes méthodes pour expliquer les prédictions individuelles effectuées par un modèle de Machine Learning. \n\nEn effet, la loi européenne du 14 Avril 2016 (GDPR – Loi de protection des données personnelles) stipule que toute personne a le droit d’obtenir une explication sur les décisions la concernant qui sont prises par des algorithmes (demande de crédit refusée, identification comme fraudeur ou terroriste, etc.). \n\nCette règlementation européenne entrera en vigueur en Avril 2018. Il faut donc que les entreprises soient rapidement capables d’extraire automatiquement des explications de leurs algorithmes, même lorsque ceux-ci se présentent sous la forme d’une boîte noire, c’est-à-dire qu’on saisit des données en entrée et que l’algorithme renvoi un résultat, sans que l’on connaisse les calculs sous-jacents. Cette régulation bannit les décisions seulement prises par un processus automatisé. Cela signifie que le processus doit inclure au moins un être humain. C’est pour cela que l’explication automatique a devant elle un avenir fleurissant, puisqu’aucun humain ne peut raisonnablement trouver une explication à une décision prise par un algorithme complexe. \n\nLes modèles de Machine Learning et les algorithmes sont souvent présentés à l’utilisateur final sous la forme de boîtes noires. Ces modèles ne permettent pas de déterminer les relations entre les variables et la prédiction en sortie. Nous voulons non seulement connaître la prédiction, mais aussi pourquoi cette prédiction a été effectuée. Nous voulons également savoir sur quelle caractéristique l’algorithme a basé sa prédiction et comment changer l’output de l’algorithme. Cela est spécialement vrai quand un humain se base sur un algorithme pour prendre sa décision : le preneur de décision doit être certain que sa décision mène au meilleur résultat. \n\nDans ce contexte, nous nous sommes interrogés sur les méthodes existantes pour expliquer un résultat et automatiser cette explication. \n\nNous nous sommes tout d’abord intéressés à la mesure de l’influence de chaque variable (ou caractéristique), ainsi qu’à l’extraction des variables qui contribuent le plus à un modèle de prédiction spécifique. Pour ce faire, nous avons étudié plusieurs méthodes. Elles peuvent être classifiées en deux groupes : les méthodes dépendantes et les méthodes indépendantes du modèle.\n\n-\tLes méthodes dépendantes du modèle : \n\nCes premières solutions consistent à étudier le système pour en comprendre le fonctionnement et d’essayer d’en extraire chaque caractéristique et sa contribution. \n\no\tLes modèles linéaires\n\nPar définition, les modèles linéaires sont les plus simples à interpréter sans faire appel à un algorithme d’explication complexe : il suffit de calculer les coefficients pour déterminer les importances relatives des différentes variables. On peut également calculer la contribution directe au résultat : la sortie dépend directement de la somme des variables multipliée par le coefficient de régression linéaire, plus d’éventuels biais. Ainsi, au niveau de la prédiction, le fait de multiplier le coefficient de régression par la valeur de la variable permet de connaître avec exactitude l’influence de cette variable sur la prédiction. Pour des régressions linéaires, la sortie est en réalité égale à la somme de chaque contribution (plus les éventuels biais). Pour classifier, par exemple avec une régression logistique, la dépendance implique la présence d’un logarithme. Mais la partie linéaire peut encore être décomposée en plusieurs contributions provenant de chaque variable. Dans le cas d’une régression logistique, le coefficient donne directement une idée de l’évolution de la prédiction dans le cas où on perturberait une variable.\n\no\tLes algorithmes basés sur des arbres décisionnels\n\nPour un arbre décisionnel, extraire les contributions de chaque variable revient à parcourir le chemin dans l’arbre qui permet d’atteindre un point donné. Dans le cas d’une régression, ce chemin mène à la valeur associée au cas étudié. Dans le cas d’une classification binaire, le chemin mène à l’une des classes (0 ou 1).\n\nPour des méthodes ensemblistes comme les « Random Forests » (Forêts aléatoires) ou les « Gradient Boosted Trees », chaque contribution correspond à la moyenne des contributions de chaque variable de l’arbre décisionnel.\n\nCependant, même si cette méthode est intéressante pour identifier les variables importantes impliquées dans la prédiction, il nous manque le signe : nous ne pouvons pas savoir si une multiplication par -1 rendra la prédiction positive ou négative. La Figure 3 5 ci-dessous décrit le fonctionnement de ce modèle. \n\n \nFigure 3 5 : Fonctionnement du modèle pour un arbre décisionnel\n-\tLes méthodes indépendantes du modèle\n\nLes méthodes indépendantes d’explication du problème ne peuvent souvent être comparées entre elles. Deux modèles différents avec les mêmes données peuvent mener à deux explications distinctes, qui sont donc difficilement comparables. Ainsi, nous cherchons désormais à parvenir au même résultat quelle que soit la méthode.\n\no\tLIME\n\nLIME (Localy Interpretable Model-agnostic Explanations) est une approximation locale d’un modèle complexe. On le transforme en modèle plus simple à interpréter, par exemple un modèle linéaire. Nos recherches sont basées sur l’article « Why Should I Trust You ? » de M.T. Ribeiro [7] qui propose des méthodes pour expliquer les prédictions de n’importe quel classificateur. La méthode LIME commence par un échantillonnage des points entourant le point d’intérêt. Ensuite, on attribue un poids à chacun de ces points en fonction de leur distance par rapport au point d’intérêt. Enfin, on insère un modèle linéaire entre ces échantillons et leur valeur prédite. Ainsi, on obtient une approximation locale de la sortie du modèle autour de la ligne que nous voulons expliquer. \n\nLa largeur du noyau est très importante, car plus elle est petite, plus les explications sont locales. \n\nIl faut tout de même noter que la méthode LIME ne fonctionne pas encore sur des jeux de données de types texte et tabulaire. Dans ces cas-là, il faut convertir les données en chiffres.\n\nLa Figure 3 6 ci-dessous décrit le fonctionnement de la méthode LIME. La Figure 3 7 illustre un exemple sur une image.\n \nFigure 3 6 : Fonctionnement de la méthode LIME\n \nFigure 3 7 : Exemple d'application de la méthode LIME à une image\no\tUtilisation de vecteurs d’explication\n\nUne autre façon d’expliquer localement une prédiction consiste à calculer le gradient autours du point de sortie d’un modèle. Cette méthode est similaire à la méthode LIME parce que la dérivée est une approximation linéaire de la sortie d’un modèle. Cependant, au lieu d’échantillonner un jeu de données synthétique, on récupère la distribution de la sortie du modèle grâce à une estimation de la densité du noyau (un noyau gaussien, par exemple). Une fois qu’on a récupéré cette densité, il est aisé d’en calculer le gradient qui correspond à notre vecteur d’explication. La Figure 3 8 décrit le fonctionnement de ces vecteurs. \n \nFigure 3 8 : Fonctionnement des vecteurs d'explication"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• Le dossier médical partagé (DMP) en France, lancé en 2004, permet la numérisation et l’accès en ligne aux dossiers médicaux, mais son adoption reste limitée en raison d’un manque de formation des professionnels de santé et de la méconnaissance des patients [1].\n\n• La prise en charge à domicile (PAD) inclut diverses formes telles que l'Hospitalisation à Domicile (HAD), le Maintien à Domicile (MAD), et les Soins à Domicile (SAD). Des solutions pour améliorer la continuité et la coordination des soins à domicile, notamment par la dématérialisation des échanges et l'utilisation des workflows, ont été proposées, mais la complexité des processus PAD rend cette tâche difficile [2].\n\n• Zefouni [2] propose un processus BPMN personnalisé pour chaque patient de la PAD, visant à améliorer la gestion des soins à domicile, mais la nécessité de remplacer BPMN par un langage plus adapté aux acteurs médicaux est soulignée.\n\n• Redjem [3] présente une solution d’aide à la décision pour optimiser le temps des soignants en HAD, en tenant compte des contraintes de disponibilité des patients et des soins. La modélisation mathématique permet de planifier les tournées des soignants, mais la gestion des dépendances temporelles complique l’optimisation [3].\n\n• Les troubles du sommeil touchent un tiers de la population française, avec des conséquences négatives sur la santé. Les dispositifs médicaux comme la polysomnographie sont utilisés pour mesurer le sommeil, mais leur disponibilité est limitée, ce qui a conduit au développement de solutions comme les actimètres. Toutefois, ces solutions actuelles restent coûteuses et peu ergonomiques [4].\n\n• Le projet MORPHEUS vise à développer un nouvel actimètre pour améliorer la caractérisation objective du cycle veille/sommeil. Les actimètres existants, comme Actillume I, Motionlogger, Actiwatch, présentent des limites en termes de sensibilité aux mouvements, de fréquence de mesure, et d’ergonomie, justifiant le développement de solutions plus avancées [5-7].\n\n• Trois algorithmes de détection des périodes de sommeil existent : Zero-threshold computation, Sadeh, et Cole. Chacun a des limitations, notamment pour distinguer les phases d’éveil pendant le sommeil. L’algorithme de correction de Webster améliore ces résultats, mais des biais demeurent [8-10].\n\n• Le secteur de la robotique d’assistance pour les personnes âgées est en plein essor, avec des robots comme Roboticbed de Panasonic, Buddy de Blue Frog Robotics, qui visent à améliorer l’autonomie et la sécurité des personnes âgées dépendantes [11-14].\n\n• Une interface robotique d'assistance à la mobilité, proposée par P. Médéric [15], intègre des fonctionnalités pour aider aux transferts (assis-debout) et à la déambulation, en utilisant un superviseur en logique floue pour garantir la stabilité des utilisateurs.\n\n• Les robots d’assistance actuels se concentrent souvent sur l’aspect social et affectif, mais notre projet se distingue par une intégration plus complète avec un espace numérique de santé, visant à améliorer la coordination des soins pour les personnes dépendantes. Notre solution robotique/plateforme inclut un robot capable de se déplacer, communiquer sur plusieurs canaux, interpréter des émotions, et évoluer avec l'état de santé de la personne, offrant une approche plus holistique pour la gestion de la dépendance."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tDossier médical numérique et solutions de prise en charges à domicile\nNotre plateforme Domicalis doit proposer le premier vrai système de suivi de patient à distance, sans cloisonnement des informations médicales, puisqu’elles sont partagées entre l’hôpital et la médecine de ville.\n\nLancé en 2004 en France, le DMP (dossier médical partagé) a pour but de numériser et de rendre accessible sur Internet les dossiers médicaux des français. Les informations et documents médicaux sont versés au dossier (ordonnances, radiographies, résultats d’analyses, mesure de constante, allergies, etc.) et sont consultables par les médecins autorisés par le patient. Cela permet un gain de temps considérable puisque les praticiens n’ont plus à soumettre leurs nouveaux patients à des questionnaires [1].\n\nCependant, le DMP s’intègre difficilement dans le paysage de soins français. D’abord, parce que les patients ne sont pas toujours au courant de son existence. Ensuite parce qu’il y a un manque de formation des professionnels de santé à propos du DMP. Il est donc peu alimenté par les médecins [1]. \n\nLa prise en charge à domicile peut prendre diverses formes, i.e. Hospitalisation à Domicile (HAD), Maintien à Domicile (MAD), Soins à Domicile (SAD), etc. [2] et [3]. \n\nDans sa thèse, Zefouni [2], propose des solutions pour améliorer la continuité et la coordination des soins lors d’une prise en charge à domicile (PAD) de patients dépendants. Ce processus d’amélioration passe par la dématérialisation et l’automatisation des échanges entre les personnes intervenant dans le projet thérapeutique du patient et par la gestion de la diversification de masse des projets thérapeutiques et des profils usagers. La technologie des workflows semble très appropriée pour le traitement de ces différents aspects. \n\nCependant les caractéristiques des processus de la PAD rendent cette tâche particulièrement complexe. La Figure 1 1 montre que les processus de la PAD peuvent être modélisés par le langage de modélisation des processus comme le langage standard BPMN (Business Process Modeling Notation). Ce processus représenté dans ce langage standard est interprétable et exécutable par la plupart des systèmes de gestion de workflow. \n\n \nFigure 1 1 : Représentation des relations (Zefouni [2])\nL’objectif de la thèse était donc de fournir des moyens méthodologiques et techniques permettant la conception d’un processus BPMN personnalisé pour chaque patient de la PAD. \n\nAu-delà de ce travail, il est nécessaire de remplacer à l’avenir le BPMN par un langage adapté et utilisable pour les acteurs médicaux.\n\nEn outre, dans sa thèse, Redjem [3], propose une solution d’aide à la décision pour la gestion des activités des ressources humaines en HAD. Il s’agit d’optimiser le temps des personnels médicaux intervenant auprès des patients à leur domicile afin d’optimiser le ratio nombre de soins / soignant, notamment dans le cas où un soin requiert la présence de plusieurs soignants en même temps. \n\nLa Figure 1 2 montre le cas où trois soignants doivent mener une série de soin, sachant que quatre des soins nécessitent la présence de 2 ou 3 des soignants. Cette caractéristique a été identifiée dans ce travail par la dépendance temporelle de type synchronisation et/ou précédence. Il faut alors optimiser les acticités des différentes ressources humaines (soignants) sur un horizon opérationnel, i.e. la journée. \n\n \nFigure 1 2 : Itinéraire de soins pour 3 soignants avec patients partagés\nLa problématique est d’autant plus complexe, car il est nécessaire de tenir compte d’un ensemble de contraintes, tel que les disponibilités des patients, les contraintes règlementaires, et les contraintes liées au projet thérapeutique (les dépendances temporelles, les temps de soins, etc.). Dans ce travail Redjem [3] a proposé plusieurs solutions basées sur la modélisation mathématique pour l’établissement des tournées (planning journaliers) des patients. \n1.2.2\tOutils de suivi et d’analyse des troubles du sommeil\nUn tiers de la population française se plaint de troubles du sommeil. De plus, notre société qui fonctionne 24/24h et 7/7j exige de plus en plus de souplesse en termes d’horaires de travail et d’activités et d’horaires de sommeil. Pour des raisons professionnelles ou sociologiques, les personnes réduisent volontairement leur durée journalière de sommeil, allongent leur durée d’éveil et effectuent des activités au cours de la nuit. Les troubles du sommeil et la réduction volontaire de la quantité de sommeil induit une somnolence diurne qui va réduire les performances et augmenter le risque accidentel. Le manque de sommeil a également des effets pervers sur notre système cardiaque et vasculaire, tels que l'inflammation et l’augmentation de la tension artérielle. Des études de laboratoire et épidémiologiques donnent à penser que la privation de sommeil peut jouer un rôle dans l’augmentation de la prévalence du diabète et de l’obésité. La relation entre la privation de sommeil, la prise de poids et le risque de diabète peut faire intervenir des altérations dans le métabolisme du glucose, une augmentation de la faim et une diminution de la dépense énergétique.\n\nPour mesurer objectivement la qualité et la quantité du sommeil, les centres de soins et les unités de recherche spécialisées dans l’étude des troubles du sommeil disposent de dispositifs médicaux, comme la polysomnographie (enregistrement du sommeil effectué à l’hôpital). Cependant, ces outils ne sont disponibles que dans des centres spécialisés et présentent des délais d’attente important. Pour répondre plus rapidement à la demande, des petits actimètres (accéléromètres portés au poignet) associés à des logiciels d’analyse sont également utilisés en médecine du sommeil. Toute fois ceux-ci sont encore relativement coûteux et l’interface de programmation et d’analyse est assez peu ergonomique ce qui implique une utilisation en centre spécialisé. Avec l’explosion des objets connectés voués à la santé et au bien-être, on peut compter actuellement une dizaine de bracelets connectés permettant d’objectiver le sommeil. Tous ces objets connectés de « bas coûts » et adaptés au marché grand public (interface simple, conviviale et fonctionnant sur Smartphone/tablette) n’ont jamais été validés scientifiquement et les informations données par ces objets restent à être vérifiées.\n\nNotre objectif est de pouvoir étendre les possibilités de caractérisation objective du cycle veille/sommeil en développant un outil de santé mobile, validé scientifiquement et cliniquement. Développer un outil d’e-santé paraît ainsi adapté aux nouvelles orientations de la santé à domicile et s’intègre également dans le marché du bien-être en pleine expansion. \n\nDans le cadre du projet MORPHEUS, nous souhaitions donc développer un nouvel actimètre afin d’équiper de nombreux patients et étudier leur sommeil de manière plus précise. Les actimètres sont des appareils de mesure des accélérations. Nous pouvons ainsi détecter les mouvements de nos patients. Nous avons donc mené une étude portant sur les technologies de mesures existantes. \n\nL’actimètre Actillume I (Ambulatory Monitoring) [4] est sensible aux accélérations supérieures ou égales 0.003 g. Il offre la possibilité de détecter les mouvements de l’axe radius-ulna du poignet, ainsi que la rotation du poignet. \n\nLe pont de mesure de l'accéléromètre est équilibré pour produire entre 1 et 2V quand l'appareil est immobile. Cela sert de référence pour enlever l’effet des phénomènes non physiologiques (par exemple la gravitation). La valeur absolue de la différence entre la référence et la tension produite par chaque accélération est calculée, puis envoyée comme résultat de sous-mesure. Ces sous-mesures sont ensuite additionnées en périodes de 10 secondes (stockées sur 1 octet) qui serviront à calculer les mesures finales en utilisant le mode de mesures choisi. Nous avons identifié à ce niveau deux modes de mesures : \n\n-\tChaque minute, on fait la moyenne des accélérations durant cette minute. Cela permet de faire la différence entre activité moyenne et activité élevée (expérimentation de jour)\n-\tOn mesure l'activité par période de 10 secondes. Pour chaque minute on sélectionne la valeur de la période ayant la plus grande accélération. Ce mode est le meilleur pour faire la différence entre le sommeil et l’éveil (expérimentation de nuit).\n\nL’actimètre Motionlogger (Ambulatory Monitoring) [5] est sensible aux accélérations supérieures ou égales 0.003 g. Il détecte les mouvements de l’axe radius-ulna du poignet, ainsi que les axes longitudinal et transverse. \n\nOn mesure la tension qui est ensuite comparée à un seuil prédéterminé. Cet outil utilise trois modes de calcul : \n\n-\tThreshold crossing : On compte, durant la période de la mesure, le nombre de fois où le signal passe par le seuil spécifié. \n-\tTime-Above-Threshold : Dès qu'une activité commence, un compteur est initialisé, et à chaque dixième de seconde où l'activité est au-dessus du seuil on incrémente le compteur.\n-\tProportional-Integrating Mode : On mesure la surface de la courbe, au-dessus et en-dessous du seuil de 0 volt (60 000 max). Ce mode est le seul qui mesure une accélération de façon proportionnelle.\n\nL’actimètre Actiwatch et son accéléromètre Actical (Philips Respironics) [6] est sensible aux accélérations supérieures ou égales 0.005g. Il détecte les mouvements multidirectionnels. \n\nL’actimètreActiwatch Spectrum (Philips Electronics) [7] est sensible aux accélérations supérieures ou égales 0.025g. Il détecte les mouvements multidirectionnels. \n\nCes solutions ne nous ont pas satisfaits complètement. D’une part, elles ne rencontrent pas nos exigences (en termes de sensibilité aux mouvements, de fréquence de mesure, d’autonomie et de facilité pour récupérer les données enregistrées). Nous avons donc décidé de créer notre propre actimètre.\n\nIl existe actuellement 3 algorithmes de détection de période de sommeil en fonction de l’activité publiée par l’actimètre. Nous les détaillons ci-dessus. \n\nL’algorithme « Zero-threshold computation » [8] propose, pour chaque mesure, de compter le nombre de fois où l'activité passe au-dessus d’un seuil (zéro par défaut). Si le compte est supérieur à 0 alors c'est une période d'éveil, sinon c'est une période de sommeil. \n\nL’algorithme de Sadeh [8] permet quant à lui de calculer la probabilité que la personne soit endormie. Si la probabilité est supérieure ou égale à 0, on considère que la personne dort.\n\nL’algorithme de Cole [8] calcule, pour chaque mesure, un déterminant nommé D. Si D ≥ 1 alors la mesure est notée « éveil », sinon elle est notée « sommeil ». L’algorithme de Cole échelonné permet de calculer, pour chaque expérimentation sur un participant, la moyenne de toutes les mesures dont l'activité est non nulle. Ensuite, on divise chaque mesure par la moyenne. Ensuite on applique l'algorithme de Cole. \n\nCes algorithmes rencontrent des difficultés pour déterminer les phases d’éveil lors du sommeil. Les périodes notées « sommeil » au lieu « d’éveil » sont 3.5 fois plus nombreuses que celles notées « éveil » au lieu de « sommeil » (D’après Roger J. Cole 1992). Fort de ce constat, Webster a mis au point un algorithme de correction. Cet algorithme est à exécuter sur les résultats obtenus en sortie d’un des 4 algorithmes précédant. Les 5 règles de Webster ([9] et [10]) permettent de trancher sur le statut de la personne : après au moins 4 minutes notées « éveil », la prochaine minute notée « sommeil » est re-notée « éveil ». Après au moins 10 minutes notées « éveil », les 3 prochaines minutes notées « sommeil » sont re-notées « éveil ». Après au moins 15 minutes notées « éveil », les 4 prochaines minutes notées « sommeil » sont re-notées « éveil ». Une période de 6 minutes ou moins notées « sommeil » encadrées par deux périodes d’au moins 10 minutes notées « éveil » est re-notée « éveil ». Une période de moins de 10 minutes notée « sommeil » encadrée par deux périodes d’au moins 20 minutes notées « éveil » est re-notée « éveil » \n\nPour une expérimentation de jour, il est conseillé d’appliquer les règles de Webster. Pour une expérimentation de nuit, la corrélation est biaisée en faveur de l’éveil.\n\nCertains paramètres ont une influence sur la sensibilité des algorithmes utilisés : la sensibilité de l’actimètre et son placement sur le corps (poignet, tibia, etc.) en font partie. Le mode et la période de mesure ont également une influence sur la sensibilité de l’algorithme. Enfin, la population ciblée ainsi que la durée et le moment de la mesure peuvent avoir une incidence sur la sensibilité des algorithmes. \n\nNotre solution de surveillance du sommeil (MORPHEUS) est également en rupture technologique avec l’existant puisque l’actimètre développé par nos soins propose une meilleure sensibilité aux mouvements ainsi qu’une meilleure fréquence de mesure. L’autonomie et le mode de récupération des données constituent également des avancées. \n1.2.3\tRobotique d’assistance\nDe nombreux acteurs de la Silver Economy le disent, le secteur de la robotique d’assistance pour les personnes âgées dépendantes et souffrant de maladies chroniques est en plein essor [11] En effet, les robots peuvent permettre aux personnes dépendantes de retrouver une partie de leur autonomie, tout en les sécurisant. Cela facilite leur maintien à domicile le plus longtemps possible. \n\nPar exemple, Panasonic a développé un robot appelé « Roboticbed ». Il s’agit d’un lit médical robotisé convertible en fauteuil roulant. Cela permet aux personnes qui ont besoin d’aide pour se coucher et se lever de retrouver une certaine liberté : quand un intervenant vient lever et coucher la personne, cette dernière ne peut plus se permettre de grasse matinée ou de soirée télé improvisées. Le but de Roboticbed est donc de permettre aux personnes dépendantes de retrouver ces petits plaisirs de la vie. Panasonic a également développé un robot permettant de laver les cheveux d’une personne [12]. Il existe aussi un robot capable d’habiller une personne. [13]\n\nLa société Blue Frog Robotics a développé un robot nommé Buddy permettant d’assister les seniors isolés ou vivant seuls. [14] Équipé d’une caméra, il permet de communiquer avec sa famille. En cas de chute, la caméra permet également de détecter et de voir le senior le temps que les secours arrivent. Buddy rappelle les rendez-vous et les prises de médicaments à son senior. \n\nDans ses travaux, P. Médéric [15], présente une interface robotique pour l'assistance à la mobilité des personnes âgées. La conception de l'interface présentée repose sur l'observation expérimentale du geste clinique d'aide aux transferts (passage de la position assise à la position debout et inversement) des sujets âgés. L'interface d'assistance proposée intègre deux fonctionnalités : l'aide au transfert et l'aide à la déambulation. En caractérisant le geste clinique de rééducation aux transferts on paramètre l'assistance au transfert. Un générateur de trajectoire d'assistance est alors intégré sur l'interface. La compensation de la chute lors des transferts s'obtient par l'utilisation d'un superviseur en logique floue. Ce dernier nécessite l'observation des mesures des interactions utilisateur-interface au niveau des poignées de l'assistant et la mesure du centre de pression de l'utilisateur. Ces différentes observations sont couplées à des règles de bases identifiant la phase de transfert, l'état de stabilité et l'action à mener sur l'utilisateur. Des expérimentations permettent de valider le fonctionnement du superviseur en logique floue de l'interface.\n\nLes nombreux robots d’assistance qui commencent à apparaître se focalisent en général sur l’aspect « distractions », avec un début de relation affective avec la machine. Ils s’adressent à des populations allant de la famille aux seniors en passant par l’éducation des enfants mais sont très éloignés des besoins réels liés à la dépendance. De même, les systèmes d’information de santé ou médico-sociaux sont très peu collaboratifs et fonctionnent en général en silo (pour un établissement de santé donné) ou en réseau de soins (multi-acteurs mais seulement pour une pathologie donnée). \n\nNotre projet se focalise sur les apports unifiés et très prometteurs de la robotique associée à un espace numérique de santé pour les problèmes de la dépendance. Notre solution robotique/plateforme de coordination regroupe un nombre important d’éléments en rupture avec l’état de l’art : \n\n-\tLe robot est disponible en tout lieu de l’habitat ou de l’institution, il se déplace sur commande pour effectuer les « levées de doute » nécessaires en cas d’alertes (chute, malaise) ; \n-\tLe robot dispose de plusieurs canaux de communication (Internet, SIGFOX) et il gère lui-même son énergie ; \n-\tLe robot peut transmettre et interpréter des émotions, c’est un objet homogène et personnalisable, à l’image d’un animal domestique, on peut lui parler naturellement (papotage). Il peut aussi enquêter sur commande de manière non intrusive sur l’état général de la personne accompagnée (douleur, dépression, etc.) ; \n-\tSa configuration peut être adaptée mais aussi évoluer avec l’état de santé de la personne. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "• **Sources pour modéliser des profils utilisateurs** : Différentes sources d'information sont utilisées pour modéliser des profils d'utilisateurs, notamment l'historique de recherche [1], les enregistrements de clics [2], l'historique de navigation [3], les folksonomies [4], les annotations [5], les communautés [6] et les groupes [7]. \n\n• **Modèles vectoriels** : Ces modèles supposent que la ressource, la requête et le profil utilisateur sont représentés par des vecteurs de tags pondérés. Le poids d’un tag est souvent calculé à l’aide de la formule TF.IDF [4] [8] ou de ses variantes. Les similarités entre la requête et la ressource ainsi qu'entre le profil utilisateur et la ressource sont ensuite évaluées via ces vecteurs [4] [5] [8] [9]. \n\n• **Modèles probabilistes** : Dans ces modèles, une ressource et les tags associés sont considérés comme des événements aléatoires formalisés par un modèle probabiliste. Cette approche permet de trier les ressources selon leur probabilité à Posteriori, en se basant sur des requêtes et documents existants [10] [11] [12] [13] [14]. \n\n• **Modèles basés sur le clustering** : Ces approches regroupent les tags similaires dans des clusters qui reflètent les préférences thématiques des utilisateurs. Elles se concentrent sur la manière de calculer la similarité entre les tags, ce qui est crucial pour la précision des profils générés [15] [16] [17] [18] [19] [20] [21]. \n\n• **Modèles neuronaux et représentations distribuées** : Ces modèles visent à modéliser simultanément des documents, leur contenu, ainsi que les tags dans un espace vectoriel commun réduit. Cela inclut l'utilisation de représentations distribuées, efficaces dans le traitement de divers langages naturels, et récemment étendues à des entités relationnelles, des flux de documents et des réseaux sociaux [22] [23] [24] [25] [26] [27]. Ces représentations sont particulièrement utiles pour la personnalisation des recommandations. \n\n• **Étude des représentations distribuées** : Niebler et al. [28] ont évalué trois algorithmes de représentation de mots (Tag2Vec, GloVe, LINE) sur des ensembles de données (BibSonomy, Delicious, CiteULike) en utilisant des jugements de similarité humaine. Ils ont constaté que l'ajustement des paramètres (apprentissage, dimensions, échantillons négatifs) améliore la qualité des représentations des tags, en particulier pour GloVe. \n\n• **Comparaison des modèles de plongements lexicaux** : Le Tableau 1 compare la similarité des tags les plus proches du tag « Oscar » à l’aide de trois modèles (Tag2Vec, GloVe, Node2vec). Cette analyse révèle différentes perceptions sémantiques du tag, ce qui est crucial pour améliorer les recommandations dans des contextes spécifiques comme celui des films."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nCertains travaux utilisent de nombreuses sources d’information pour modéliser des profils d’utilisateurs : historique de recherche [1], enregistrement de clics [2], historique de navigation [3], folksonomies [4], annotations [5], communautés [6] ou groupes [7]. D’autres travaux ont mis en avant l’importance de la prise en compte de ces données en modélisant des profils utilisateurs à travers des représentations multiples : vectorielles, basées sur le clustering et probabilistes. Plus précisément, ces travaux exploitent les tags annotés par les utilisateurs pour la modélisation de profils à des fins de recommandation. Nous présentons dans ce qui suit une synthèse de ces différents modèles.\n\n1.2.1\tModèles vectoriels\nL’hypothèse de base des modèles vectorielles stipule que la ressource, la requête et le profil sont décrit par des vecteurs de tags pondérés. Le poids d’un tag dans les vecteurs associés au profil de l’utilisateur et du profil de la ressource est calculé par la formule TF.IDF [4] [8] ou des variantes de cette formule. De la sorte, l’appariement entre la requête (un ensemble de tags) et la ressource (décrites elle aussi par un ensemble de tags) ainsi que la correspondance entre profil utilisateur et ressource sont calculées via des similarités [4] [5] [8] [9].\n1.2.2\tModèles probabilistes\nCes modèles considèrent une ressource et le ou les tag(s) la décrivant, comme étant la formalisation d’une expérience aléatoire avec un modèle probabiliste donné. Cette considération permet d’estimer à partir d’une base de requêtes (tags) et de ressources (documents) les paramètres du modèle et permet aussi pour un nouveau tag de trier les ressources en fonction de leur probabilité à Posteriori [10] [11] [12] [13] [14]. \n1.2.3\tModèles basés sur le clustering\nCes approches consistent à rassembler les tags similaires dans des clusters qui reflètent les préférences de l’utilisateur pour différentes thématiques [15] [16] [17]. Les modèles de recherche d’information personnalisés exploitants ce type d’approches ont été abordés dans de nombreux travaux [15] [17] [18] [19] [20] [21]. La question centrale concernant ces approches réside dans la manière avec laquelle la similarité entre différents tags est calculée. En effet, cela permet de mettre en évidence des caractéristiques plus précises lors de la construction de profils [20] [21].\n1.2.4\tModèles neuronaux et représentations distribuées\nD'autres approches dans la littérature visent à modéliser simultanément des documents, leurs contenus (langage défini par le vocabulaire) ainsi que les tags avec lesquels ils ont été décrits dans un espace vectoriel commun réduit en termes de dimensions [22].\nDans [22], les auteurs combinent à ce vecteur (représentation distribuée des documents et des tags) un autre vecteur représentant les utilisateurs dans le même espace vectoriel. De ce fait, les auteurs parviennent à extraire des caractéristiques personnalisées servants à des tâches de recommandation. Ce type de représentations vectorielles continues (ou distribuées) entraînées à l’aide de modèles de langues neuronaux a prouvé son efficacité dans le traitement de divers langages naturels [23]. \nBien qu’il y ait eu récemment des travaux visant à étendre la portée des représentations distribuées au-delà des mots, i.e. les phrases et les documents [24], les entités relationnelles [25], les flux de documents [26], les réseaux sociaux [27] ; le mécanisme sous-jacent reste le même et dépend de la technique de base de la représentation des mots dans notre cas des tags. \nNiebler et al. [28] apportent d’ailleurs une exploration approfondie de l’applicabilité de trois algorithmes issus d'une méthode permettant la représentation de mots : les plongements lexicaux. Les auteurs [28] abordent aussi le paramétrage et évaluent chaque modèle généré. L’évaluation de ces modèles : Tag2Vec qui se base sur le modèle word2vec [24], GloVe [29] et LINE (rapportés dans [28]), sur trois collections de données : BibSonomy , Delicious , CiteULike , a pour objectif l’étude qualitative de la représentation distribuée des tags en se basant sur des collections de données issues de jugements de similarité attribués par l’homme. En outre, cette collection contient deux ensembles de paires de mots anglais ainsi que leurs jugements de similarité. Les résultats montrent qu’un réglage des différents paramètres dont : le paramètres d’apprentissage, le nombre de dimensions, ou le nombre d’échantillons négatifs, améliore considérablement les résultats de la distribution des tags, en particulier pour le modèle GloVE.\n\nAfin d’illustrer les liens sémantiques existants entre les différents tags lors de plongements lexicaux, nous les avons mis en évidence en utilisant différentes représentations issues de trois modèles (GloVE, Tag2Vec et Node2vec [30]) et nous les avons reportés dans le Tableau 1 ci-dessous.  \nSur la base des trois modèles sus-cités, nous avons établi la relation sémantique entre deux tags. Nous avons mesuré cette relation à l’aide d’un score de similarité sur des représentations vectorielles réelles de tags. \n Le Tableau 1 illustre donc un classement des tags les plus similaires au tag « Oscar », selon les modèles cités précédemment.  Les trois modèles ont été décrits précédemment dans l'état de l'art, il s'agit de se référer aux modèles de représentation distribués existants. Nous avons dissocié ces modèles des modèles classiques en apportant certains détails (similarité, classement) ainsi qu'une contribution en comparant les représentations distribuées issu de différents modèles dont Node2vec. \n\nLe tag « Oscar » est assez significatif compte tenue de la nature du Dataset utilisé pour les expérimentations. En effet, les tags de la collection se rapporte principalement au vocabulaire d'une communauté virtuelle qui recommande aux utilisateurs de regarder des films, en fonction de leurs préférences. De ce fait, le tag exprimant les préférences de l'utilisateur, l’intérêt donc de comparer ce dernier à travers différents modèles d'apprentissage de représentation de mots (Glove, Tag2vec, Node2vec) appliqués aux tags nous renseigne sur la perception que l'on peut se faire de ce tag, aussi, il apporte une source d'information sémantique qui peut se rapporter à la subjectivité, une illustration est proposée dans la Figure 1.\n\n\nTag\tModèles/scores\n\tTag2vec\t score_similarité\tGlove\t score_similarité\tNode2vec\t score_similarité\n\n\n\nOscar\tBest\t0.89\tPicture\t0.95\tDirecting\t0.72\n\tDirecting\t0.73\tActor\t0.94\tBest\t0.63\n\tPicture\t0.71\tBest\t0.94\tNominee\t0.61\n\tSupporting\t0.64\tActress\t0.91\tcinematography\t0.60\n\tActor\t0.63\tcinematography\t0.91\tPicture\t0.60\nTableau 1: Tableau présentant les modèles ainsi qu'un classement des tags les plus similaires par modèle pour le tag « Oscar »"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Une veille technologique et technique est réalisée en continu pour suivre les dernières connaissances, en se concentrant particulièrement sur les publications récentes et les brevets en lien avec les axes de travail du projet.\n- En raison du manque de maturité des technologies étudiées et des réglementations en cours, il est crucial de collecter activement les dernières informations disponibles.\n- Il n'existe pas de standard de communication entre les bornes de recharge, les véhicules et le réseau électrique, bien que plusieurs solutions aient été conceptualisées et développées.\n- Les solutions identifiées incluent des systèmes embarqués dans les véhicules [1], des interfaces utilisateur via des logiciels [2] ou des smartphones [3], mais ces solutions ne répondent que partiellement à la problématique du smart charging, car elles se concentrent sur des aspects spécifiques comme la durée de recharge disponible et l'état de charge final visé.\n- Les recherches se concentrent sur l'optimisation de la consommation et la répartition de la puissance pour permettre la recharge de plusieurs véhicules simultanément, en s'appuyant sur la durée de recharge disponible et l'état de charge final visé.\n- La problématique du smart charging est explorée principalement dans le cadre de la smart grid, avec des études portant sur les flux d'énergie entre les véhicules électriques branchés et les bornes de recharge, ainsi que sur l'optimisation des flux de charge et décharge [2], [4], [5].\n- La majorité des publications étudiées se concentrent sur la réduction des coûts et l'optimisation de la disponibilité des ressources [5], mais elles ne traitent pas suffisamment la problématique de la redéfinition du cadre de recharge lorsque le nombre de véhicules branchés varie dynamiquement.\n- Ma et al. [6] abordent partiellement cette problématique en étudiant les mécanismes de recharge pour une large population de véhicules électriques, mais sans prendre en compte les variations dynamiques des conditions de recharge.\n- Les principaux axes d'optimisation identifiés dans les études existantes incluent l'aplatissement de la courbe de demande de recharge [7] et la minimisation des coûts en favorisant la recharge pendant les heures creuses [8].\n- L'un des défis majeurs est de concevoir une borne universelle capable de s'interfacer avec tous types de véhicules, en prenant en compte les différents profils de charge (vitesse de recharge, puissance requise, etc.) comme mentionné par Moses et al. [9].\n- Wang et al. [10] détaillent les défis posés par la diversité des technologies de batteries, soulignant les implications en termes de puissance nécessaire et l'importance de l'état de charge du véhicule lors du branchement, qui affecte le régime de recharge requis (intensité et tension). Le dernier tiers de la batterie, en particulier, nécessite un régime différent d'ampérage et de voltage.\n- Le brevet [11] décrit une solution de recharge rapide qui ajuste la tension délivrée en fonction du régime de recharge requis par l'état de charge de la batterie.\n- Des initiatives utilisant des algorithmes de deep learning [12] sont explorées pour déterminer en temps réel les points de fonctionnement optimaux pour la recharge des véhicules électriques.\n- La norme ISO 15118 [13], en cours de développement depuis 2015, vise à standardiser les communications entre véhicules électriques et bornes de recharge, avec l'objectif de simplifier l'authentification et la gestion des conditions de recharge. Cependant, l'imminence d'une nouvelle version de cette norme rend les préconisations de la version actuelle difficilement exploitables, notamment en ce qui concerne le protocole OCPP [14]."}, {"role": "assistant", "content": "1.3.2\tLimites de l’état de l’art\nAu démarrage du projet, et tout au long de son déroulement, nous réalisons une veille technologique et technique sur les dernières connaissances disponibles en lien avec nos différents axes de travail. Nous consultons également les publications récemment parues ainsi que les brevets qui décrivent les techniques connues. Par ailleurs, en raison du manque de maturité des technologies que nous étudions et de leurs réglementations en cours, nous veillons activement à la collecte des dernières informations.\n\nFace à l’absence d’un standard de communication entre les bornes de recharge, les véhicules et le réseau électrique, de nombreuses solutions ont été conceptualisées et certaines développées.\nNos recherches de l’état de l’art au démarrage de notre projet nous ont permis d’identifier des solutions embarquées au sein du véhicule [1], des interfaces s’appuyant sur une saisie de données par l’utilisateur dans un logiciel [2] ou sur son smartphone [3]. Ces solutions adressent partiellement notre problématique de smart charging en raison de leur focalisation sur la détermination de la programmation optimale de recharge du véhicule en s’appuyant sur la durée de recharge disponible et l’état de charge final visé (par exemple +25% de recharge en 4 heures). Les défis que ces publications adressent portent ainsi davantage sur l’optimisation de la consommation et de la répartition de la puissance disponible afin de permettre la recharge de plusieurs véhicules.\nEn effet, la problématique du smart charging a été explorée par de nombreux chercheurs à travers le prisme de la smart grid, s’intéressant ainsi aux flux d’énergie entre les véhicules électriques branchés et l’installation de borne de recharge. Des programmations optimisées des flux de charge et décharge du véhicule branché sont notamment définies dans [2], [4], [5]. Des considérations de coût et de disponibilité sont ainsi menées dans la plupart des publications étudiées [5].\nBien que ces publications proposent une modélisation, ils n’adressent pas la problématique posée par le besoin de redéfinir le cadre de recharge lorsque le nombre de véhicules branchés varie dynamiquement. Cette problématique est partiellement identifiée par Ma et al. [6] qui étudient les mécanismes de recharge optimisés d’une large population de véhicules électriques, mais ils n’adressent pas la variation dynamiques des conditions déterminées statiquement. En effet, les principaux axes d’optimisation concernent l’aplatissement de la courbe de demande de recharge reçue par le réseau [7] ou encore la minimisation du coût en privilégiant un maximum de recharge durant les heures creuses [8].\n\nL’un des défis découlant de notre volonté de concevoir une borne universelle, capable de s’interfacer avec tous types de véhicules, est mentionné par Moses et al. [9]. En effet, les profils de charge (vitesse de recharge, puissance électrique requise, etc.) des différents véhicules doivent être pris en considération lors de la détermination du point de fonctionnement pour la recharge de l’ensemble des véhicules et des bornes du réseau considéré. Wang et al. [10] précisent davantage les défis attachés à la diversité des technologies de batteries de véhicules électriques de laquelle découlent des implications en termes de puissance nécessaire mais aussi l’état de charge du véhicule lors de son branchement qui affecte le régime de recharge (intensité et tension) requis (cf. Figure 1). Typiquement, le dernier tiers de la batterie ne suit pas le même régime d’ampérage et de voltage pour accumuler de la charge. Le brevet cité en référence [11] décrit justement une solution de recharge rapide des batteries par un ajustement de la tension délivrée.\n \nFigure 1 : Profil du niveau de charge (SOC State of Charge) de batterie atteint par rapport à la durée de recharge [10]\nDes initiatives recourant à des algorithmes d’intelligence artificielle (notamment du deep learning) [12] sont également étudiées pour déterminer à la volée des points de fonctionnement optimaux pour la recharge des véhicules électriques.\n\nDès 2015, une version de travail de la norme ISO 15118 [13] vise à uniformiser les communications de haut niveau entre les véhicules électriques et les installations de recharge électrique. L’objectif était de définir les protocoles de communication permettant à un utilisateur de simplement brancher son véhicule à une borne et voir son authentification réalisée automatiquement ainsi que la détermination des conditions de recharge octroyées par l’installation.\nToutefois, l’imminence d’un nouveau standard (version 2) rendait difficilement exploitables les préconisations de cette norme. En particulier, la norme ISO 15118 inclut le protocole OCPP [14] pour compléter la communication de la borne électrique avec le système d’information.\n"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- L'objectif principal du projet est d'optimiser la circulation des passagers tout en garantissant une sécurité absolue, en réponse à l'augmentation croissante du nombre de voyageurs.\n- Le projet vise à offrir un parcours en libre-service pour les voyageurs, de leur domicile jusqu'à la porte d'embarquement, incluant l'enregistrement, la dépose bagages, le passage des contrôles aux frontières et de sécurité, et l'embarquement de manière autonome.\n- La solution doit permettre un parcours biométrique rapide et sécurisé dans l'ensemble de l'aéroport, tout en répondant aux normes CE, notamment l'EN 62368-1 (sécurité électrique), la directive RED 2014/53/EU (radio), et l'EN 62471 (bandeaux de LEDs).\n- La borne à concevoir est destinée à l'identification biométrique des passagers et s'appuie sur divers modules de reconnaissance : photographique, empreintes digitales, scan des documents d'identité.\n- L'intégration des multiples composants biométriques dans une architecture globale basée sur des cartes IER (firmware) a nécessité la conception d'une infrastructure logicielle complexe pour contrôler les informations échangées entre les différents modules.\n- La borne intègre plusieurs technologies et modes de communication, posant des défis en termes de maîtrise des interférences, des contraintes d'encombrement liées au transport de la borne, et de l'intégration ergonomique des composants.\n- Des contraintes spécifiques ont été rencontrées, telles que l'installation d'un capteur derrière une plaque transparente à une distance précise pour éviter qu'elle ne soit détectée comme un objet, nécessitant une optimisation fine de la configuration.\n- Des problèmes de perturbations lumineuses ont été identifiés lors des lectures sur le lecteur passeport et le lecteur d'empreintes, nécessitant un éclairage direct unique et bien ajusté pour ces deux dispositifs.\n- La directive RED (2014/53/EU) s'applique à la borne en tant qu'émetteur radio, rendant la certification RED obligatoire. L'absence de certification RED pour un OEM radio installé pourrait rendre la certification de la solution complexe voire impossible.\n- Le processus de certification RED est lourd, impliquant une documentation extensive et un travail préparatoire conséquent, ce qui a augmenté la complexité et la durée du projet.\n- L'intégration des composants, tels que les lecteurs de passeport et les caméras, a posé des défis de conformité, particulièrement en raison des limitations des moyens internes de contrôle et d'essais CEM, rendant difficile la prédiction de la conformité.\n- Le projet se déroule dans un contexte économique et concurrentiel fort, avec peu ou pas de solutions \"sur l'étagère\" disponibles pour les aéroports, soulignant l'importance de l'innovation et de l'adaptabilité.\n- La littérature commerciale souligne l'importance croissante des bornes automatiques [1], mais la recherche académique sur leur influence est quasi inexistante, créant une lacune critique qui complique le développement de solutions optimales.\n- Les chercheurs appellent à un examen systématique de l'adoption des systèmes biométriques [2], et plusieurs articles offrent un aperçu de l'utilisation des systèmes biométriques généraux [3] [4].\n- La biométrie multimodale, qui combine plusieurs systèmes biométriques, est de plus en plus étudiée pour surmonter les limitations des systèmes monomodaux, telles que l'impossibilité d'acquérir des données ou la fraude intentionnelle, tout en améliorant les performances de reconnaissance [6].\n- Farell [7] identifie la borne automatique comme l'une des solutions de sécurité les plus viables pour relever le défi de la vérification d'identité face à un nombre croissant de voyageurs."}, {"role": "assistant", "content": "1.3\tObjet de l’opération R&D \nFace à l'augmentation croissante du nombre de voyageurs, l’objectif principal de ce projet consiste à proposer une solution pour optimiser la circulation des passagers tout en garantissant une sécurité absolue. Ainsi, nous répondons au besoin croissant d’un parcours en libre-service, qui accompagne les voyageurs de chez eux, jusqu'à la porte d'embarquement de leur vol.  \nPour cela, d’une part, notre solution doit permettre au voyageur de s'enregistrer via une borne libre-service à l'aéroport, accéder à des points de dépose bagages automatisés, passer le contrôle aux frontières ou les points de passage sécurité et embarquement en toute autonomie. D’autre part, elle doit permettre un parcours biométrique rapide et sécurisé dans l’ensemble de l’aéroport. Enfin, elle doit répondre à la norme CE et donc aux exigences et aux directives suivantes :\n-\tL’EN 62368-1 : L’exigence en sécurité électrique\n-\tLa directive RED 2014/53/EU : Pour la RADIO\n-\tL’EN 62471 : Pour les bandeaux de LEDs\n\nLa borne à concevoir étant destinée à permettre une identification biométrique des passagers, elle s'appuie sur différents modules assurant une fonction de reconnaissance : photographique, empreintes digitales, scan des documents d'identité. Face à la multitude des composants biométriques, nous avons dû concevoir une infrastructure logicielle permettant de contrôler les informations échangées entre toutes les briques et assurant leur intégration au sein d'une architecture globale s'appuyant sur des cartes IER (firmware).\n\nNotre borne se compose de nombreux modules biométriques dont l'intégration des uns par rapport aux autres doit être étudiée. La borne embarquant de nombreuses technologies et modes de communication, l'un de nos défis portait sur la maîtrise des interférences, la maîtrise de contraintes fortes d'encombrement liées au transport de la borne.\n\nDe plus, nous avons été confrontés à des contraintes d’ergonomie et de fonctionnement optimale. A titre d’exemple, la contrainte liée à l’installation du capteur derrière une plaque transparente afin qu’il puisse détecter la personne à 50 cm devant la borne. A quelle distance devons-nous installer la plaque transparente par rapport au capteur de tel sorte qu’il ne la détecte pas comme un objet ? \n\nEn outre, nous avons rencontré des problèmes de perturbations lumineuses lors des lectures que ce soit sur le lecteur passeport ou le lecteur d’empreinte, il fallait donc qu’on ait un seul éclairage direct qui vient sur ces deux lecteurs.\n\nPar ailleurs, dans le cadre CE c’est la directive RED (2014/53/EU) qui s’applique (et qui a remplacé la R&TTE). Ainsi, tout produit RADIO en tant qu’émetteur intentionnel doit être conforme ou certifié RED. Si un OEM radio installé dans la borne n’est pas lui-même certifié RED alors le processus de certification de notre solution risque d’être compliqué, voire même impossible dans certains cas. De plus la certification RED est un processus assez lourd, avec toute une base documentaire et administrative à réaliser. Ainsi, la demande de certification RED nous a demandé un certain temps de travail préparatoire en amont.\n\nEnfin, nous avons été confrontés à un ensemble de preuves de conformités d’intégration des composants (avec le lecteur de passeport, avec les caméras, …). Il nous a été très difficile de préjuger en avance de la conformité ou non des essais CEM d’autant plus que nous n’avons pas les moyens de contrôle et d’essais en interne.\n\nCe domaine étant directement lié à un enjeu économique et concurrentiel, il n’existe pas, ou peu, de littérature public abordant le sujet. Dans le cadre de Gemalto, leader mondial de la sécurité numérique, il n’existait pas de solution (fonctionnel et technique) « sur l’étagère » en libre-service complète pour les voyageurs. Une solution que les aéroports peuvent mettre en œuvre à leur propre rythme.\n\nEffectivement, la littérature commerciale actuelle souligne à l'unanimité la taille et la portée croissantes du déploiement des bornes automatiques [1]. Cependant, il n'y a aucun travail académique portant sur l’influence de son utilisation, représentant ainsi une lacune critique de recherche. \n\nToutefois, étant donné que les chercheurs ont constamment appelé à un examen systématique de l'adoption de systèmes basés sur la biométrie [2], un certain nombre d’articles académiques offrent un premier aperçu sur l’utilisation des systèmes généraux (par exemple, [3] [4]). En effet, depuis plusieurs années, des efforts importants sont fournis dans le domaine de la recherche en biométrie. Ce constat s’explique par la présence d‘un contexte mondial dans lequel les besoins en sécurité deviennent de plus en plus importants et où les enjeux économiques sont colossaux. Les applications biométriques sont nombreuses et permettent d’apporter un niveau de sécurité supérieur en ce qui concerne les accès (aéroports, bâtiments sécurisés, casinos, etc.) [5]. \n\nNotre étude/analyse de la littérature, nous a révélé que la biométrie multimodale, qui consiste à combiner plusieurs systèmes biométriques, est de plus en plus étudiée. En effet, elle permet de réduire certaines limitations des systèmes biométriques, comme l'impossibilité d'acquérir les données de certaines personnes ou la fraude intentionnelle, tout en améliorant les performances de reconnaissance. Ces avantages apportés par la multimodalité aux systèmes biométriques \"monomodaux\" sont obtenus en fusionnant plusieurs systèmes biométriques [6].\n\nPar ailleurs, Farell dans [7] représente la borne automatique comme l’une des solutions de sécurité les plus viables pour relever le défi de la vérification d'identité d'un nombre croissant de voyageurs. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Les solutions actuelles de virtualisation d’espaces, comme celles de Gabriela Coca [1], previsite.com [2], et Meero [3], se concentrent principalement sur la manipulation des espaces pour les visualiser, les meubler ou les visiter, mais elles reposent souvent sur des photographies 360° qui ne permettent pas un déplacement réel dans les pièces, limitant ainsi l’appréciation des volumes et introduisant des distorsions d’images.\n- Des exemples de visites virtuelles, comme celles de la Tour Eiffel ou du musée du Louvre, utilisent la technologie Google Street View Trusted, permettant une visualisation 3D plus convaincante mais avec des limitations sur la liberté de déplacement [4].\n- La virtualisation 3D est largement utilisée dans des secteurs comme l’aéronautique et la médecine pour des applications telles que les simulateurs de vol ou les simulations d’opérations complexes.\n- Dassault Aviation a développé des outils de réalité virtuelle pour visualiser des configurations intérieures et améliorer les processus d’achat dans l’industrie aérospatiale, avec des solutions comme le Falcon Configurator qui permet une immersion en temps réel dans l’avion configuré.\n- La 3D temps réel, utilisée pour le rendu immédiat des données tridimensionnelles, permet une grande interactivité et immersion, mais nécessite des prétraitements et une accélération matérielle pour un rendu fluide.\n- KinectFusion est une évolution de Kinect qui permet une reconstruction 3D en temps réel d'une scène intérieure, en utilisant uniquement les informations de profondeur [6].\n- Les chercheurs développent des systèmes d’acquisition de modèles 3D en temps réel, comme celui décrit dans [7], permettant une mise à jour continue du modèle pendant le scan et facilitant le remplissage des trous dans le modèle.\n- De nouvelles techniques de reconstruction 3D rapide, basées sur des algorithmes de projection et de déphasage numériques, ont permis d’atteindre des vitesses de traitement de 30 images par seconde avec 300 000 pixels par image [8].\n- Les solutions existantes de 3D temps réel ne sont pas directement applicables à la décoration d’intérieur en raison de leur complexité d'utilisation, de la qualité de rendu insatisfaisante, et de l'absence d'intégration à l'expérience client B2C.\n- CATIA (Dassault System) est un acteur clé dans le domaine de la 3D temps réel, mais ses solutions ne sont pas adaptées pour fournir des projets clefs en main ou des aménagements complets pour les architectes.\n- Le mapping 3D, souvent réalisé par des lasers, permet de scanner des environnements 3D avec un niveau de détail variable selon la méthode utilisée. Le « StreetMapper » est un exemple de système mobile permettant un mapping précis à l'échelle d'une ville [9].\n- Les techniques de mapping de textures, comme celles basées sur l’alignement d’images sous différents angles ou sur le mapping UV, sont utilisées pour réduire les distorsions dans les visualisations 3D et améliorer la fidélité des modèles [10], [11].\n- Une méthode de cartographie de textures basée sur des images de réflectance a été développée pour améliorer la précision des modèles 3D, en alignant les points de bord de réflexion 3D avec les points 2D, permettant des applications telles que la cartographie du Grand Bouddha de Kamakura [12].\n- Les méthodes traditionnelles de mapping 3D ne répondent pas aux besoins de notre projet car elles ne permettent pas de distinguer plusieurs objets dans un environnement complexe.\n- Le MIP Mapping est une technique utilisée pour économiser les ressources « machines », mais elle ne permet pas d'obtenir une qualité homogène sur l’ensemble de la scène, en particulier pour les objets éloignés de la caméra.\n- Le Deep Learning est utilisé pour la reconnaissance automatique d’objets sur une image, permettant par exemple de diagnostiquer des cancers du sein à partir d'images mammographiques, comme décrit dans [14].\n- L’utilisation du Deep Learning pour la reconnaissance d'objets complexes implique des techniques avancées comme les réseaux de neurones à convolution, appliqués dans des domaines comme la classification des masses malignes dans les images médicales.\n- La réalité augmentée, perçue comme l'avenir de l'expérience d'achat, est limitée par la complexité des interfaces homme-machine et le besoin d'une IA plus avancée pour personnaliser les recommandations et améliorer l'expérience utilisateur.\n- Les solutions actuelles, comme ARKIT et ARCORE, permettent de placer manuellement des objets dans un espace vide, mais notre projet vise à intégrer des algorithmes d'IA pour offrir des recommandations de décoration personnalisées et une visualisation adaptée à l'intérieur de l'utilisateur.\n- Le Machine Learning est largement utilisé dans le marketing pour proposer des produits adaptés aux clients. Dans notre projet, il permet de profiler les utilisateurs et de proposer des aménagements sur-mesure basés sur leurs goûts et leur projet.\n- Notre interface Hubstairs permet aux utilisateurs de personnaliser des inspirations de décoration en fonction de leur profil, avec une visualisation en réalité augmentée, comme démontré par le prototype présenté à IKEA."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nIl existe actuellement quelques solutions de virtualisation d’espaces : Visite virtuelle Gabriela Coca [1], previsite.com [2], Meero [3], etc. Elles sont souvent dédiées à la manipulation des espaces, pour les visualiser, les meubler ou les visiter. \n\nCependant, il s’agit le plus souvent de simples photographies 360° qui ne permettent pas de se déplacer dans les pièces et de réellement apprécier les volumes. Ce n’est donc pas de la 3D à proprement parler, ce qui entraîne couramment une distorsion des images, comme le montre la Figure 1 1 ci-dessous. \n\n \nFigure 1 1 : Capture d'écran d'une réalisation de Meero\nCes solutions de « visite virtuelle » ne répondent donc pas aux problématiques de recommandations basées sur une analyse posées par notre projet. \n\nNous pouvons également citer la Tour Eiffel, le musée du Louvre ou encore le Château de Versailles qui proposent des visites virtuelles [4] grâce à la technologie Google Street View Trusted. On ne peut pas s’y déplacer librement comme lors d’une vraie visite (pour la Tour Eiffel, on ne peut que se déplacer le long du garde-corps), mais la visualisation 3D est plus convaincante, avec moins de distorsion des images. \n\nLa virtualisation 3D est utilisée dans de nombreux autres domaines comme l’aéronautique (simulateur de vol ou virtualisation d’un avion en construction), ou la médecine (simulations d’opérations, notamment pour des opérations de reconstruction ou difficiles à effectuer). \n\nLes équipes de Dassault ont d’ailleurs développé de nombreuses applications de réalité augmentée et de réalité virtuelle. En effet, un avion d’affaires, un hélicoptère ou une fusée impliquent des processus d’achat très complexes. Afin de diminuer les coûts de prototypage et d’accélérer les itérations avec les clients, la réalité virtuelle apparaît comme une solution efficace. \n\nEn effet, les nouveaux outils d’aide à la spécification des aménagements intérieurs et l’imagerie de synthèse offrent de puissants moyens de visualisation 3D. Ces derniers sont destinés à faciliter les choix d’aménagement et d’options en situation de vente. \n\nNé des besoins de cette industrie, le Falcon Configurator développé par Dassault Aviation offre aux commerciaux la possibilité de matérialiser en temps réel, et au cours même du rendez-vous, toutes les demandes formulées par leur prospect. Grâce au réalisme offert par la 3D, le logiciel permet une immersion totale au travers d’une visite en temps réel de l’ensemble de l’avion configuré. La qualité des images virtuelles, proche de la réalité, permet au client de se projeter aisément dans son futur avion.\n1.2.1\t3D temps réel \nLa numérisation en 3D des objets réels est un domaine en pleine expansion, avec des applications dans le domaine du divertissement, du design et de l'archéologie. \n\nLa 3D temps réel, très utilisée en imagerie de synthèse, est une méthode de représentation de données tridimensionnelles pour laquelle chaque image composant l’animation est rendue dans l’instant qui précède son affichage. \n\nLa vitesse de rendu doit permettre une animation fluide. La 3D temps réel s’oppose à la 3D précalculée dans la mesure où le rendu des données 3D est immédiat. Pour être imperceptible, le temps de rendu en 3D temps réel doit être inférieur à la persistance rétinienne, ce qui explique le besoin de l’accélération matérielle et les nombreux prétraitements sur les scènes 3D. Le rendu des données 3D s’effectue directement sur le périphérique d'affichage, contrairement à la 3D précalculée où le rendu est une vidéo.\n\nLa 3D temps réel permet de ce fait un grand degré d'interactivité : possibilités d'immersion, de contrôle d’animation, d’interactivité avec la scène 3D… La 3D temps réel se trouve ainsi au cœur d’applications telles que la visite virtuelle, la simulation, la visualisation et la conception CAO/CFAO, le jeu vidéo, etc. La 3D temps réel constitue par exemple la base de la Kinect. \n\nUne évolution de la Kinect est la KinectFusion [6]. KinectFusion permet à un utilisateur de tenir et de déplacer sa Kinect pour créer en temps réel une reconstruction 3D d’une scène intérieure. La Kinect n’utilise que les informations de profondeur pour tracker finement les détails 3D. \n\nDans [7], les chercheurs proposent un nouveau système d’acquisition de modèle 3D qui permet à l’utilisateur de faire pivoter un objet à la main et de voir un modèle continuellement mis à jour au fur et à mesure que l'objet est scanné. Cette boucle de rétroaction étroite permet à l’utilisateur de trouver et de remplir des trous dans le modèle en temps réel, et de déterminer quand l’objet a été complètement couvert. Le système est basé sur un télémètre à lumière structurée, un algorithme ICP (Iterative Closest Points) en temps réel pour l’alignement, et des algorithmes de fusion et de rendu. Ce prototype permet de scanner les objets plus rapidement et avec plus de facilité que les pipelines classiques d’acquisition de modèles.\n\nAu cours des dernières années, des techniques de reconstruction 3D très rapides ont été développées [8]. Ces nouvelles techniques utilisent des algorithmes de projection et de déphasage numériques. Ces algorithmes se basent sur une programmation parallèle pour augmenter la vitesse de traitement. Des technologies matérielles avancées ont également été développées pour augmenter la vitesse des calculs de coordonnées, et donc le rendu 3D. Dans [8], les chercheurs ont réussi à obtenir simultanément l'acquisition, la reconstruction et l'affichage de formes absolues en 3D à une vitesse de 30 images par seconde, avec 300 000 pixels par image. \n\nDans [9], les chercheurs ont développé un modèle 3D de la structure d’une canopée de maïs pour des simulations de réflectance précises. L’objet de l’étude était un ensemble de plants de maïs complètement développés. \n\nPlusieurs expériences sont utilisées pour décrire la dimension, la forme, la position et l'orientation des feuilles et des tiges. Des modèles empiriques sont proposés pour estimer la position, la taille et la forme des feuilles et des tiges en fonction de trois variables principales : le nombre final de feuilles, la hauteur finale de la canopée et la surface foliaire cumulée par plante. \n\nLes distributions statistiques des paramètres de chaque sous-modèle de structure sont étudiées. Il permet de générer une représentation 3D réaliste de l'architecture des plantes et de la canopée. En outre, cette modélisation permet de visualiser les zones présentant le plus de réflectance.\n\nLa Figure 3 1 et la Figure 3 2 illustrent le processus de reconstruction 3D et le résultat de l’étude. \n \nFigure 1 2 : Processus de reconstruction 3D\n\n \nFigure 1 3 : Résultat de l'étude\nNous ne pouvons pas utiliser telles quelles les solutions de 3D temps réel (real time rendering) préexistantes au cours du développement de notre solution. En effet, ces méthodes existantes posent les problèmes majeurs suivants : \n\n-\tAbsence d’application à la décoration d’intérieur ;\n-\tUtilisation trop complexe ;\n-\tQualité du rendu non satisfaisante ;\n-\tLes machines (hardware) nécessaires à l’utilisation sont très lourdes car la quantité de calcul est très importante ;\n-\tAbsence d’intégration à l’expérience client B2C. \nDans le domaine de la 3D temps réel, l’acteur principal est CATIA (DASSAULT SYSTEM). Cette solution ne permet pas de fournir un projet clef en main au client, ni ne permet aux architectes de fournir un projet professionnel d’aménagement complet. \n1.2.2\tMapping 3D et Mapping de texture\nLa méthode de mapping 3D la plus répandue consiste à scanner un environnement 3D à l’aide d’un laser [9]. Il peut s’agir de lasers statiques situés au sol (pour des petites scènes), ou de lasers mobiles embarqués en avion pour des zones plus grandes (par exemple, une rue voire une ville entière). Cependant, ces méthodes de scan ne sont pas scalables puisqu’elles ne permettent pas d’avoir un niveau de détail très fin sur de grandes zones (par exemple, les détails architecturaux ne sont pas visibles avec les lasers embarqués). \n\nPour étendre l’usage des lasers à tous types d’environnement et de niveau de détail, des chercheurs ont mis en place un véhicule, le « StreetMapper » [9]. Ce véhicule est doté de quatre lasers, ce qui assure un mapping très fin des zones couvertes (on obtient ainsi un très bon niveau de détail). Pour assurer le mapping à grande échelle, le véhicule est doté d’un système de navigation GNSS. Cet outil permet d’obtenir des nuages de points très fins à l’échelle d’une ville. En comparant leurs résultats, avec les valeurs de référence des modélisations 3D existantes, les chercheurs ont obtenu un résultat très proche de la réalité. \n\nPour ce qui est des méthodes de mapping de textures, nous avons trouvé une riche littérature à ce propos. \n\nOn peut, par exemple, utiliser un algorithme qui recrée un modèle 3D d’un objet à partir de différentes prises de vue sous différents angles. Pour faire cela, on fait tourner un objet devant une caméra fixe étalonnée [10]. Cela permet à la fois de recréer un modèle en 3D de l’objet, et d’y appliquer les bonnes textures. \n\nL’objet est alors représenté par un réseau de triangles qui sont associés aux textures issues des prises de vue de la caméra. Cela permet de réduire les distorsions souvent présentes dans les images lors de visualisations 3D. Pour les triangles qui ne sont visibles sur aucune prise de vue, les chercheurs ont développé un filtre qui applique automatiquement la texture du plus proche triangle visible au triangle invisible. \n\nDes essais expérimentaux ont permis de prouver que cette méthode est applicable efficacement à des applications d’animation par ordinateur. \n\nDans [11], les chercheurs présentent un algorithme similaire qu’ils ont développé. Il permet de construire des visualisations compactes et réalistes d’objets colorés. Pour cela, ils appliquent un mapping des textures à un mapping de forme par triangulation.\n\nLes chercheurs ont commencé par construire un réseau haute-résolution de triangles à partir des données recueillies par un capteur laser, qui détecte les formes et les couleurs. À chaque triangle, est attribuée une valeur RGB. Le réseau de triangle est ensuite transformé en une triangulation compressée et en une carte de texture. Cette carte permet de connaître avec exactitude les informations couleur à chaque sommet des triangles, et donc de restitué une modélisation fidèle de l’objet, tant en termes de formes que de couleurs. \n\nDes expérimentations ont prouvé qu’un fort taux de compression peut être atteint sans faire de concessions sur la similarité visuelle entre le modèle original et le modèle compressé. \n\nDes chercheurs [12] ont choisi de travailler sur l’étude d’images de réflectance  et sur l’estimation du temps d’exposition grâce à un M-estimateur. Comme une image 2D est une vue partielle d’un objet prise d’un certain point de vue, il est nécessaire d’avoir plusieurs images afin d’obtenir un mapping intégral de l’objet. \n\nDans cet article, les chercheurs proposent donc une nouvelle technique d'enregistrement simultané de plusieurs images et d'un modèle géométrique basé sur la correspondance de bord 2D-3D et la contrainte épipolaire entre images.\n\nLes chercheurs ont proposé une nouvelle méthode d’étalonnage pour la cartographie de textures à l’aide d’images de réflectance, qui sont souvent considérée comme des produits secondaires des prises de vue pour la plupart des capteurs de distance. Dans leur méthode, les points de bord de réflexion 3D et les points de bord de couleur 2D sont d’abord respectivement extraits de l’image de réflectance et de l'image couleur. Ensuite, le temps d’exposition de chaque capteur est déterminé de sorte que les points de bord de réflectance 3D coïncident avec les points 3D calculés à partir des points de bord de couleur 2D grâce à l'utilisation du M-estimateur.\n\nEn outre, les chercheurs proposent une nouvelle technique d'enregistrement simultané de plusieurs images de texture et de modèle géométrique.\n\nEn utilisant la méthode proposée, des expériences de cartographie de texture 3D du Grand Bouddha de Kamakura ont été réalisées, et l'utilité de la méthode proposée a été vérifiée.\n\nUne dernière méthode de mapping de textures est le Mapping UV (ou plaquage de carte UV). Il s’agit d’une méthode de mapping de textures qui consiste à déplier un objet sur un plan désigné par les lettres U et V. On peut alors appliquer des textures à l’objet suivant les lignes de « pliage », ce qui permet de respecter la perspective grâce aux lignes de fuite. Ces textures donnent un aspect réaliste quand on redonne sa forme originale à l’objet. \n\nDans le cadre de nos travaux nous ne pouvons pas utiliser les méthodes traditionnelles de mapping 3D dans la mesure où elles ne retournent qu’un seul et unique objet au lieu de plusieurs objets distincts. Il existe toutefois des méthodes de mapping permettant de texturiser les objets séparément.\n\nLes deux premières méthodes sont le Bump Mapping et le normal mapping. Ils consistent à appliquer un relief sur un objet plat via l’application des cartes de textures. Le point négatif de ces deux méthodes est qu’elles ne permettent pas d’obtenir un niveau de réalisme élevé.\n\nUne autre méthode de mapping est le « parallax occlusion mapping ». Cette technique est utilisée en complément du Bump Mapping. Elle permet de réduire la complexité de la forme géométrique du modèle via la création d’un « pixel shader » grâce à l’information des cartes de textures, ainsi la méthode permet d’obtenir une impression de profondeur. Cette technique ne permet toujours pas d’obtenir un niveau de réalisme satisfaisant. \n\nUne dernière méthode de mapping est le MIP Mapping. Cette technique « temps réel » permet d’économiser les ressources « machines ». En effet, elle permet de donner une impression de réalisme sur les objets proche de la caméra. En revanche, les objets à distance sont en basse qualité. Cette technique utilise un algorithme qui calcule l’emplacement de la caméra et positionne les textures de meilleure qualité pour les objets proches de la caméra. Bien que cette technique soit intéressante, elle ne permet pas d’obtenir un ensemble de qualité homogène. \n\nÀ ce jour, nous sommes encore en phase de recherche pour trouver une solution qui nous permettra de profiter des avantages d’économies de ressources du MIP Mapping tout en obtenant un rendu de qualité sur les objets éloignés de la caméra. \n1.2.3\t Deep Learning pour la reconnaissance automatique d’objets sur une image\nLe Deep Learning consiste à entraîner un algorithme sur une base afin qu’il apprenne à identifier selon certaines caractéristiques (textures, contours, couleurs, …) des objets. Après avoir été entraîné, l’algorithme peut détecter seul des objets avec des plus en plus de précision. \n\nPar exemple, dans [14], l’auteur propose de diagnostiquer les cancers du sein en étant assisté par ordinateur. Cela devient possible en raison de la quantité sans cesse croissante d'images mammographiques issues des campagnes de dépistage. \n\nL'accent est mis sur les opacités mammaires en raison du risque élevé de cancer qui leur est associé. En effet, la variabilité des formes rencontrées et la difficulté à discerner les masses surtout quand ces dernières sont embarquées dans des densités importantes exigent une nouvelle stratégie plutôt adaptée aux cas les plus complexes à savoir les masses appartenant aux classes BI-RADS IV et V, c’est-à-dire respectivement les masses malignes spiculées et les distorsions architecturales. Dans ce travail, un système de diagnostic assisté par ordinateur entièrement automatique et conçu pour la segmentation et la classification des opacités dans les catégories bénigne/maligne ou graisseuse/dense, spécifiquement pour celles de type BI-RADS IV et V est abordé. Dans un premier temps, l’auteur a développé une approche de prétraitement des images fondée sur l'apprentissage d'un dictionnaire parcimonieux sur les bases d'images, combiné à une réduction de dimension afin de supprimer de façon efficace et rapide le bruit de numérisation des images mammographiques présentes dans les bases utilisées pour concevoir notre système de diagnostic en comparaison des approches déjà existantes. Une fois les images prétraitées, nous avons mis en place une procédure de segmentation non-supervisée des masses basées sur les champs de Markov et qui a l'avantage d'être à la fois plus rapide, plus efficace et plus robuste que les meilleures techniques de segmentation disponibles dans l'état de l'art. \n\nDe plus, la méthode proposée s'affranchit de la variabilité des masses et ce quelle que soit la densité de l'image. Dans l'idée de décrire convenablement les lésions malignes spiculées, nous avons conçu une méthode de segmentation des spicules qui présente la particularité de ne pas recourir à l'utilisation de descripteurs extraits manuellement dont les performances peuvent varier en fonction de leur qualité. L'approche proposée repose donc sur des hypothèses que nous avons formulées concernant l'aspect des spicules. Celles-ci nous ont conduits à développer un modèle Markovien combiné à une transformée de Radon locale pour extraire les structures curvilignes de l'image. Ensuite, nous servant d'un modèle a contrario, nous avons pu extraire les spicules de l'ensemble des structures détectées. Cette phase, vient clore la première partie de la conception de notre système, qui est en mesure d'extraire soit des masses spiculées, soit des distorsions architecturales. Afin de finaliser sa conception, nous avons procédé à la création d'un modèle d'aide à la décision qui, à l'inverse de ce qui s'est toujours fait dans l'état de l ‘art pour la discrimination des masses, procède à une extraction non-supervisée des descripteurs à l'aide d'une méthode issue du Deep Learning, à savoir les réseaux de neurones à convolution. \n\nLes descripteurs extraits, sont ensuite utilisés dans un classifieur SVM pour apprendre un modèle. Ce modèle servira par la suite à la reconnaissance du cancer du sein. Les résultats obtenus pour chacune des étapes du système de diagnostic sont très intéressants et viennent combler un vide important dans la classification des masses en général et dans la distinction des masses malignes entre elles en particulier en se fondant sur trois niveaux de décision que sont la forme, la densité et les spicules.\n\nNous proposons ci-dessous (Figure 3 3) un exemple de Deep Learning appliqué à la reconnaissance de modèle de voiture. \n \nFigure 1 4 : Reconnaissance de modèles de voiture par Deep Learning \n1.2.4\tTechnologies de réalité augmentée\nLa réalité augmentée est perçue, à juste titre, comme le futur de l’expérience d’achat pour l’intérieur de la maison. Cependant aujourd’hui, il existe deux points de blocage dans le déploiement de cette technologie :\n\n1)\tL’Interface Homme Machine (IHM) :\n\nAujourd’hui, l’Interface Homme-Machine, et par ricochet l’ergonomie des solutions existantes, est très loin de la simplicité nécessaire pour une expérience d’achat en ligne. En effet, les solutions déployées ne permettent pas d’accompagner l’utilisateur de manière simple, rapide et efficace vers l’achat. Nous travaillons donc sur une simplification sensible de l’expérience utilisateur afin de l’intégrer et de la rendre compatible avec l’expérience d’achat en ligne.\n\n2)\tL’Intelligence Artificielle :\n\nAujourd’hui les solutions existantes de réalité augmentée (principalement ARKIT et ARCORE, les autres solutions existantes ne correspondant pas à l’utilisation mobile de nos utilisateurs qui utilisent leur smartphone) permettent de placer manuellement dans une pièce vide un objet choisi par l’utilisateur. \n\nNotre solution utilise la technologie d’intelligence artificielle afin d’améliorer l’expérience et la proposition immersive :\na.\tCuration : Algorithmes de recommandation de produits et d’association de produits. Notre solution propose un design de la pièce, c’est-à-dire un ensemble (ou association) de mobiliers et agencement qui se basent sur des templates de propositions pré-réalisées par des architectes ;\nb.\tPersonnalisation : Technologie permettant d’adapter et de personnaliser à la demande les recommandations d’architectes ;\nc.\tVisualisation : Application de la « solution déco » personnalisée dans la pièce de l’utilisateur utilisant une technologie de réseaux neuronaux afin d’adapter la proposition à la pièce existante et au mobilier déjà présent dans la pièce de l’utilisateur.\n\nAujourd’hui, les acteurs mondiaux les plus en avance dans le domaine (IKEA avec l’application IKEA Place) pour apporter via la réalité augmentée des propositions de décoration d’intérieur automatiquement à leurs utilisateurs avec la réalité augmentée avec leurs produits.\n1.2.5\tTraitement d’images \nAujourd’hui les solutions existantes permettant de passer de la 2D à la 3D ne fonctionne que sur la base de photos de pièce simple et vide (avec focale et paramétrage de l’appareil de prise de vue connus). \n\nDès lors qu’il s’agit, soit de pièce complexe (forme), de pièces non vides ou d’appareil de prise de vue non connu, le post-traitement du fichier 3D obtenu (traitement des polygones) devient beaucoup plus complexe. \n1.2.6\tMachine Learning \nNous souhaitons être en mesure de proposer des planches en fonction des goûts exprimés par les utilisateurs (couleurs, matière, ambiance, …). \n\nLe Machine Learning est par exemple très répandu dans les domaines de la vente et du marketing afin de proposer aux clients des produits les plus adaptés possible à leur demande. \n\nLes méthodes d’apprentissage automatique sont des outils puissants pour l’exploration de données issues de grandes bases de données non structurées. Cela donne aux chercheurs l'opportunité d'acquérir de nouvelles connaissances sur le comportement des consommateurs et d'améliorer les performances des opérations marketing [13]. \n\nPour modéliser les réponses des consommateurs au marketing direct, Cui, Wong & Lui proposent des réseaux bayésiens appris par programmation évolutive. En utilisant un grand ensemble de données de marketing direct, ils ont testé le biais d'endogénéité dans les variables de récence, fréquence, valeur monétaire en utilisant l'approche de la fonction de contrôle. Ils ont également comparé les résultats des réseaux bayésiens avec ceux des réseaux neuronaux, de la classification et de l'arbre de régression, et de la régression de classe latente. \n\nEnfin, ils ont appliqué une validation croisée décuplée. Les résultats suggèrent que les réseaux bayésiens ont des avantages distincts par rapport aux autres méthodes en ce qui concerne la précision de la prédiction, la transparence des procédures, l'interprétabilité des résultats et la compréhension explicative. Ces résultats soutiennent fortement les réseaux bayésiens en tant qu’outils robustes pour la modélisation de la réponse des consommateurs et d’autres problèmes de marketing et pour faciliter la prise de décision en matière de gestion. \n\nAujourd’hui, les solutions de Machine Learning, d’intelligence artificielle et les technologies de virtualisation fonctionnent indépendamment. Dans le cadre des développements de notre plateforme Hubstairs, nous travaillons sur le couplage de ces technologies afin de proposer des aménagements sur-mesure au client dans son environnement. \n\nLa première méthode sur laquelle nous avons travaillé consiste à « profiler » l’utilisateur en fonction de ses goûts, de son itération et de son projet. Ce « profiling » est réalisé par l’analyse des likes de l’utilisateur sur les contenus de notre interface Hubstairs (styles, matières, inspirations) et de la co-occurrence des tags. Cela permet de « matcher » le profil de l’utilisateur aux profils décoration similaires déjà enregistrés sur la plateforme. \n\nL’aboutissement de ce profiling permet de proposer des produits matchant les goûts, le projet et le budget de l’utilisateur. Notre interface fournit également la possibilité de proposer automatiquement des associations de produits et des aménagements en temps réel dans l’intérieur virtuel de l’utilisateur. Ces aménagements pourront ensuite être affinés et modifiés par un architecte ou un designer. \n\nCes premiers travaux ont abouti au prototype présenté à IKEA. Ce prototype permet aux utilisateurs de personnaliser à l’infini des inspirations proposées en fonction de leur profil et de représenter les inspirations dans leur intérieur grâce à la réalité augmentée."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Un système de blockchain repose sur un algorithme de consensus garantissant l’accord sur l’état des données entre les nœuds distribués. Cet algorithme est crucial pour la sécurité et l'intégrité des systèmes distribués, notamment pour prévenir les attaques de double-dépense.\n- Les algorithmes de consensus les plus couramment utilisés sont la preuve de travail (PoW) et la preuve d’enjeu (PoS) (Saad & al., 2020).\n- Dans une blockchain sans permission, comme Bitcoin, les nœuds sont anonymes, ce qui peut entraîner des forks. L'algorithme de consensus vise à éviter ces forks en assurant que tous les nœuds s'accordent sur une seule version de la transaction.\n- Dans une blockchain permissionnée, les nœuds sont des entités connues et les transactions sont généralement mieux contrôlées.\n- La Proof-of-Work (PoW) requiert une puissance de calcul importante pour résoudre des problèmes mathématiques complexes, ce qui entraîne une consommation d'énergie massive et des temps de transaction lents.\n- La Proof-of-Stake (PoS) repose sur la possession de cryptomonnaie par les nœuds pour valider les transactions, évitant ainsi les coûts énergétiques du PoW. Les nœuds ayant misé plus de cryptomonnaie ont plus de chances d'être choisis pour valider un bloc.\n- La Delegated Proof-of-Stake (DPoS) combine les avantages du PoS et du PoW, en permettant aux membres de la communauté d'élire des délégués pour valider les blocs. Ce modèle hybride vise à éviter la centralisation et est perçu comme la troisième génération de technologie blockchain (Saad & al., 2020).\n- Le protocole Tendermint, proposé par Kwon (Kwon, 2014), offre un consensus sans minage, résilient jusqu'à 1/3 de participants byzantins. Il est basé sur une version modifiée du protocole DLS (Dwork, Lynch & Stockmeyer, 1988).\n- L'interopérabilité des blockchains est une des principales difficultés rencontrées par les crypto-monnaies. Cosmos propose une solution permettant aux blockchains de communiquer entre elles à travers le protocole IBC (Inter-Blockchain Communication), facilitant l'échange d'actifs et de données sur plusieurs réseaux.\n- L'IBC permet de résoudre le problème des blockchains en silos, en facilitant l’interopérabilité pour une création de valeur maximale. Les standards Interchain s’appuient sur des standards Internet établis pour promouvoir cette interopérabilité.\n- Les transactions on-chain se produisent directement sur la blockchain et nécessitent une validation par l’ensemble du réseau, ce qui peut entraîner des délais significatifs pour leur ajout au registre.\n- Les transactions off-chain sont exécutées en dehors de la blockchain, impliquant une tierce partie pour faciliter l'exécution de l'accord. Ces transactions sont presque instantanées, ont des frais moins élevés, et offrent plus de confidentialité, mais nuisent à la décentralisation et à l’évolutivité.\n- Les transactions on-chain ne sont pas scalables, tandis que les solutions de niveau 1 (layer 1) peuvent traiter un nombre limité de transactions par seconde, d'où l'intérêt pour les solutions off-chain pour des transactions de moindre valeur."}, {"role": "assistant", "content": "1.3.2\tEtat de l’art\n.1.3.2.1\tAlgorithme de consensus\nUn système de blockchain est, fondamentalement, un système distribué qui repose sur un algorithme de consensus garantissant un accord sur les états de certaines données entre les nœuds distribués. L’algorithme de consensus est le composant central qui détermine directement le comportement d’un tel système et les performances qu'il peut atteindre. Le consensus distribué a été un sujet de recherche largement étudié dans les systèmes distribués, cependant, avec l’avènement de la blockchain, il a reçu une attention renouvelée.\n\nL’algorithme de consensus est un élément crucial de tout réseau Blockchain, car il est responsable du maintien de la sécurité et de l'intégrité du système distribué dans le contexte des crypto-monnaies et permet notamment de se prémunir d’attaques du type double-dépense. En général, il existe différentes formes d’algorithmes de consensus. Les algorithmes le plus couramment utilisés sont la preuve de travail (PoW) et la preuve d’enjeu (PoS) (Saad & al., 2020).\n\nLe consensus est un concept central dans les systèmes distribués et ne se limite pas à la blockchain. Il s’applique aux scénarios dans lesquels plusieurs processus ou nœuds doivent maintenir un état commun d’un élément de données. Les blockchains avec et sans permission sont deux grands types de blockchains (Chaudhry & Yousaf, 2018).\nDans la blockchain sans permission, comme celle utilisée dans des environnements publics par le Bitcoin et d'autres crypto-monnaies, les nœuds sont anonymes. Un nouveau bloc de transaction falsifié peut être ajouté, ce qui entraîne un fork. Le fork se produit lorsqu'une transaction valide ne correspond pas à la transaction invalide. L'objectif principal de l'algorithme de consensus est de parvenir à un consensus de telle sorte que chaque nœud soit d'accord sur une valeur réelle.\nDans une blockchain permissionnée, mieux adaptée aux environnements privés administrés, les nœuds ne sont pas anonymes et sont considérés comme des entités connues.\n\nDans le protocole de Proof-of-Work, les différents nœuds du réseau sont appelés mineurs. Pour confirmer une transaction, les mineurs doivent résoudre un problème mathématique complexe sollicitant une puissance de calcul importante.\nLe bitcoin s'appuie sur le minage de preuve de travail pour sécuriser le consensus, ce qui est problématique : le minage nécessite une dépense massive d'énergie, la confirmation des transactions est lente et la sécurité est difficile à quantifier.\nLa Proof-of-Stake, ou preuve d’enjeu, repose sur une logique tout à fait différente que la preuve de travail et ne nécessite pas de puissance de calcul particulière. Concrètement, pour valider un bloc, les nœuds doivent ici prouver leur possession d’une certaine quantité de cryptomonnaie, et la mettre en gage sur le réseau. Plus cette quantité est importante, plus un nœud aura de chances d’être choisi pour mettre à jour le registre d’une blockchain. Le consensus de PoS considère en effet que ces personnes sont les plus susceptibles de vouloir lutter contre une attaque du réseau, qui pourrait entièrement les ruiner.\nEmergée plus récemment, la Delegated Proof-of-Stake (DPOS) tend à répondre aux faiblesses de la PoS et PoW en proposant un modèle hybride.\nLe consensus de DPoS fonctionne selon le même principe de base que la PoS. Toutefois, les personnes chargées de forger des blocs doivent être élues par les membres de la communauté. Le système d’élections permet de s’assurer que la blockchain ne soit pas contrôlée par une minorité de personnes, comme cela peut être le cas d’un mineur avec une grosse puissance de calcul, ou d’un forgeur PoS ayant une très grande quantité de jetons. La délégation de la preuve d'enjeu (DPoS) est perçue comme un nouvel algorithme de consensus connu comme la troisième génération de la technologie Blockchain (Saad & al., 2020).\nEn outre, dans la constatation de la différence de performances, il n'y a pas d'analyse pertinente dans la littérature pour distinguer le processus de Proof of Stake (PoS) et de Delegated Proof of Stake (DPoS) (Valdeolmillos & al., 2019).\n\n\nKwon (Kwon, 2014) propose la solution Tendermint en réponse au problème du consensus de la blockchain qui ne nécessite pas de minage en adaptant une solution existante au problème des généraux byzantins. Tendermint (Kwon, Buchman, 2019) est un nouveau protocole de consensus qui ne nécessite pas de minage de preuve de travail et qui possède un niveau élevé de protection contre les attaques de double dépense. La faible hypothèse émise sur la capacité des participants à garder le temps, et la supposition d’une synchronisation partielle du réseau. L’algorithme est basé sur une version modifiée du protocole DLS (Dwork, Lynch & Stockmeyer, 1988), et est résilient jusqu'à 1/3 de participants byzantins.\n.1.3.2.2\tInteropérabilité des blockchains\nL'une des principales difficultés rencontrées par la plupart des crypto-monnaies aujourd'hui est l'interopérabilité ou la capacité de communication inter-blockchain. Pour les utilisateurs, l’interopérabilité est la facilité de déplacer leurs actifs sur plusieurs réseaux de blockchain. Actuellement, le déplacement de crypto nécessite des frais de transaction assez élevés et plusieurs étapes compliquées. Cependant, plusieurs réseaux de crypto blockchain ont commencé à penser à l'interopérabilité intégrée au cœur de leurs réseaux et l'un d'entre eux est Cosmos.\nCosmos est une technologie qui permet à diverses blockchains de se connecter et de partager des données entre elles au sein d'un écosystème partagé.\nToutes les blockchains au sein de l'écosystème Cosmos peuvent communiquer entre elles en utilisant le protocole IBC. Cette communication peut se faire grâce à l’utilisation du même kit logiciel dans l’ensemble du réseau, à savoir le SDK Cosmos et l’algorithme de consensus Tendermint. (Cosmos, I. B. C. What is Cosmos (ATOM) https://pintu.co.id/en/academy/post/what-is-cosmos-atom#definition-of-cosmos-atom).\n\nLes applications et services Cosmos se connectent à l'aide du protocole IBC (Inter-Blockchain Communication). Cette innovation permet d'échanger librement des actifs et des données à travers des blockchains souveraines.\nNous pensons que les standards Interchain peuvent s’appuyer sur des standards Internet établis, pour être largement adoptés et promouvoir l’interopérabilité entre les blockchains, ainsi qu’avec les protocoles Internet existants. Ces développements devraient constituer un excellent complément au protocole de communication inter-blockchain (IBC), permettant la découverte, l'authentification, le contrôle et l'utilisation inter-réseaux des NFT et de leurs ressources de métadonnées.\n\nL'IBC résout le problème des blockchains existant en silos avec une interaction limitée. L'interopérabilité entre les blockchains est nécessaire pour une création de valeur maximale.\n\nChaque blockchain répond à un ou plusieurs cas d'utilisation spécifiques. Si ces cas d'utilisation ne peuvent pas être exploités sur plusieurs blockchains, leur utilité s'en trouve limitée. La percée de l'internet a permis aux informations de circuler facilement dans différentes parties du monde. De même, l'utilité des différentes blockchains doit être librement accessible sur plusieurs plateformes.\n.1.3.2.3\tOff-chain vs. On-chain\nLes transactions on-chain se produisent sur la blockchain. Ces transactions sont stockées sur le registre distribué et sont visibles par toute personne qui possède une copie du registre. Par conséquent, chaque fois qu'une transaction on-chain se produit, elle entraîne une mise à jour de l'ensemble du réseau blockchain.\nEn théorie, ces transactions devraient se produire en temps réel. Cependant, en pratique, elles peuvent prendre un temps considérable : il s’agit du délai nécessaire à la validation d’une transaction et son ajout au registre.\n\nLes transactions hors chaîne sont traitées en dehors de la blockchain. Elles impliquent une tierce partie qui joue le rôle de garant. Les parties à la transaction concluent un accord en dehors de la blockchain. Cependant, elles dépendent d'un tiers pour faciliter l'exécution de l'accord. La tierce partie fixe les termes et conditions de l'accord. La transaction est exécutée et enregistrée sur la blockchain lorsque toutes les conditions sont remplies.\nLe tiers est ici généralement une solution de niveau 2 (layer 2) conçue pour effectuer des transactions de manière indépendante et donc réduire la charge sur la chaîne principale.\nCe format de transactions off-chain nuit ainsi à la décentralisation que nous visons.\n\nLes transactions hors chaîne peuvent être exécutées presque instantanément, ont des frais de transaction moins élevés et offrent plus de confidentialité puisque les enregistrements des transactions sont conservés hors de la blockchain.\nDe plus, les transactions on-chain ne sont pas scalables. En effet, les solutions de niveau 1 ne peuvent traiter qu’un nombre limité de transactions par seconde. Par conséquent, les transactions hors chaîne peuvent être privilégiées pour les transactions de moindre valeur, qui se reflètent immédiatement.\n"}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Un système de blockchain repose sur un algorithme de consensus garantissant l’accord sur l’état des données entre les nœuds distribués. Cet algorithme est crucial pour la sécurité et l'intégrité des systèmes distribués, notamment pour prévenir les attaques de double-dépense.\n- Les algorithmes de consensus les plus couramment utilisés sont la preuve de travail (PoW) et la preuve d’enjeu (PoS) (Saad & al., 2020).\n- Dans une blockchain sans permission, comme Bitcoin, les nœuds sont anonymes, ce qui peut entraîner des forks. L'algorithme de consensus vise à éviter ces forks en assurant que tous les nœuds s'accordent sur une seule version de la transaction.\n- Dans une blockchain permissionnée, les nœuds sont des entités connues et les transactions sont généralement mieux contrôlées.\n- La Proof-of-Work (PoW) requiert une puissance de calcul importante pour résoudre des problèmes mathématiques complexes, ce qui entraîne une consommation d'énergie massive et des temps de transaction lents.\n- La Proof-of-Stake (PoS) repose sur la possession de cryptomonnaie par les nœuds pour valider les transactions, évitant ainsi les coûts énergétiques du PoW. Les nœuds ayant misé plus de cryptomonnaie ont plus de chances d'être choisis pour valider un bloc.\n- La Delegated Proof-of-Stake (DPoS) combine les avantages du PoS et du PoW, en permettant aux membres de la communauté d'élire des délégués pour valider les blocs. Ce modèle hybride vise à éviter la centralisation et est perçu comme la troisième génération de technologie blockchain (Saad & al., 2020).\n- Le protocole Tendermint, proposé par Kwon (Kwon, 2014), offre un consensus sans minage, résilient jusqu'à 1/3 de participants byzantins. Il est basé sur une version modifiée du protocole DLS (Dwork, Lynch & Stockmeyer, 1988).\n- L'interopérabilité des blockchains est une des principales difficultés rencontrées par les crypto-monnaies. Cosmos propose une solution permettant aux blockchains de communiquer entre elles à travers le protocole IBC (Inter-Blockchain Communication), facilitant l'échange d'actifs et de données sur plusieurs réseaux.\n- L'IBC permet de résoudre le problème des blockchains en silos, en facilitant l’interopérabilité pour une création de valeur maximale. Les standards Interchain s’appuient sur des standards Internet établis pour promouvoir cette interopérabilité.\n- Les transactions on-chain se produisent directement sur la blockchain et nécessitent une validation par l’ensemble du réseau, ce qui peut entraîner des délais significatifs pour leur ajout au registre.\n- Les transactions off-chain sont exécutées en dehors de la blockchain, impliquant une tierce partie pour faciliter l'exécution de l'accord. Ces transactions sont presque instantanées, ont des frais moins élevés, et offrent plus de confidentialité, mais nuisent à la décentralisation et à l’évolutivité.\n- Les transactions on-chain ne sont pas scalables, tandis que les solutions de niveau 1 (layer 1) peuvent traiter un nombre limité de transactions par seconde, d'où l'intérêt pour les solutions off-chain pour des transactions de moindre valeur."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\n1.2.1\tIntroduction au Cloud As A Service (Cloud SaaS) \nLe Cloud computing (ou informatique dans les nuages) est une métaphore désignant un réseau de ressources informatiques accessibles à distance par le biais des technologies Internet. C’est un modèle informatique selon lequel des ressources informatiques (logicielles et/ou matérielles) sont fournies sous la forme d’un service à la demande. Les XaaS (Anything as a Service) regroupent l’ensemble de ces services et sont régulièrement présentés en trois modèles de service suivant une approche top-down : Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS).\nDe multiples acteurs comme Google ou Amazon offrent des services de Cloud. Certains Clouds présentent des similarités en termes de services fournis, comme par exemple les Clouds SaaS proposant des services de bureautique, ou les Clouds IaaS proposant des services de calcul ou de stockage.\nLes principales caractéristiques du Cloud Computing sont les suivantes :\n1.\tHaut niveau d'abstraction\n2.\tSur demande\n3.\tLa réduction des coûts résultant des économies d'échelle\n4.\tFlexibilité et évolutivité\n5.\tPayer à l’usage\n6.\tDisposition presque instantanée\n\n1.2.2\tLe développement d’applications en Cloud Native \nUne Cloud Native Application (CNA) est une application spécialement développée pour évoluer dans un environnement Cloud. \nL’article [1] propose une définition de CNA et de ses caractéristiques. Il résume les exigences de telles applications par la résilience et l’élasticité. En effet, les CNA doivent anticiper les échecs et les fluctuations de qualité des ressources clouds mais aussi des services intermédiaires, dans le but d’avoir une application sans coupures de service. De plus, les CNA doivent pouvoir supporter des variations de volume de données et de demandes en adaptant leurs capacités en ajoutant ou soustrayant des ressources dans le but de produire une qualité de service (QoS) face à la variation de charge. Ces exigences sont basées sur les caractéristiques du cloud définies dans [2] comme étant ; un service mesuré, sur demande, doté d’une rapide élasticité, mettant en commun les ressources et avec un large accès au réseau. \nLa résilience étant le principal critère à atteindre, car indispensable au fonctionnement de l’application. L’élasticité quant à elle est consacrée au passage à l’échelle (scalabilité) ou pour des réductions de coûts. \n\nEn 2017, l’article [4] fait un état de l’art des différentes définitions d’une CNA. De plus, il étudie aussi les approches quant à l’architecture et l’infrastructure des CNA. Parmi celle-ci, nous retrouvons les méthodologies de développement orientés cloud comme le 12 Factor App. \nLe “12 Factor app” est un manifeste qui propose 12 bonnes pratiques concernant le développement d’applications web. Ce manifeste, écrit par Adam Wiggins [3] est né de ses observations et de son expérience dans le développement et le déploiement d’applications web. Ces facteurs sont adaptables à tous les langages et à toutes les plateformes car ils décrivent simplement les décisions de conception de haut niveau sans donner de détail sur l’implémentation. \n\nNous nous sommes appuyés sur cette méthodologie pour développer Timesquare V2. En effet, l’objectif était d’être Cloud Native et par ce fait naturellement scalable horizontalement. De plus, le 12 Factor App est une méthodologie reconnue et utilisée dans la plupart des développements aujourd’hui. \n\nParmi les principes, nous nous sommes particulièrement intéressés au 6iem principe : Les processus. En effet il faut découper et exécuter l’application comme plusieurs processus sans état (stateless) : le clustering. Les processus 12 facteurs sont sans état et ne partagent rien. Toute donnée qui doit être persistée doit être stockée dans un service externe stateful, typiquement une base de données. Ce modèle de conception “sans état et sans partage” est indispensable à la scalabilité horizontale.\nDe même, le 9iem principe dit de « concurrence » évoque l’utilisable de ces clusters en calculs parallèle pour la scalabilité, la réactivité, la rapidité mais aussi pour la tolérance aux pannes. \n\nAinsi, ces principes donnent des bonnes pratiques en termes d’architecture global de code, ainsi qu’en terme de processus (build/releases/run) pour développer une CNA, mais ils ne renseignent pas le développeur quant à la sécurité de ce type de solution. \n\n1.2.3\tCloud Computing et sécurité\nVéritable outil d’innovation pour les entreprises, le cloud n’en reste pas moins un vecteur potentiel de vulnérabilité pour les entreprises. En cela, la sécurité du cloud représente un enjeu majeur pour les directeurs sécurité. \n\nUn article publié à l’occasion de la seizième SKG (International Conference on Semantics, Knowledge and Grid (SKG)) [5] expose un sondage de grande ampleur effectué sur des acteurs du numérique s’intéressant à leurs préoccupations concernant les questions de sécurité et de confidentialité. Il en conclu un manque de littérature autour des questions de sécurité et d’intégrations de lois publiques de vie privée dans le Cloud Computing. \n\nDeux ans plus tard, l’article [6] effectue un état de l’art de la sécurité dans le Cloud Computing, insistant sur le fait que la sécurité est le plus grand défi à relever par les responsables informatiques qui souhaitent adopter des solutions et des services hébergés dans le Cloud. Il identifie les différents risques potentiels, notamment la perte de gouvernance, le manque d’isolement des environnements et des données, les risques de conformité, les risques de protection de donnes, les risques de suppression de données etc. \nAussi, il présente un panel de solution possible : \n-\tPlateforme de confiance basé sur le TPM, une puce sécurisée qui contient une clé privée d’identification et d’approbation, et certaines fonctions cryptographiques qui ne peuvent pas être modifiés ; \n-\tCryptage des données ; \n-\tLe HAIL (High-Availability and Integrity Layer), un système cryptographique distribué qui permet à un ensemble de serveurs de prouver à un client qu'un fichier stocké est intacte et récupérable.\nL’auteur conclu néanmoins que la sécurisation de ces services est un enjeu primordial et n’est pas une tâche aisée, dépendant de chaque architecture. Ainsi, elle doit mobiliser de nombreux travaux internes ou externalisés. \n\nDans [7], Stéphane Calé, Matthieu Chaveyriat, Stéphane Lenco, membres des Commissions Cybersécurité et Protection de l’Information du CDS présentent des bonnes pratiques pour assurer l’utilisation sécurisé d’un Cloud Externe. Selon eux, les mesures de sécurité d’un projet cloud doivent être déterminées de manière spécifique, partant d’une analyse systématique des risques, avant de procéder à leur mitigation à travers une procédure de maîtrise des risques. Afin de sécuriser au mieux un projet cloud, l’article suggère en dernier lieu de se baser sur les nombreux référentiels existants, y compris pour appréhender au mieux la phase de contractualisation.\nAussi, des publications spécialisées selon les secteurs exposent les risques et les précautions à prendre avec le Cloud Computing, comme [8] dans le domaine médical.          \n1.2.4\tSécurisation des authentifications\n.1.2.4.1\tLe Lightweight Directory Access Protocol (LDAP) [9]\n\nLe Lightweight Directory Access Protocol (LDAP) est initialement un annuaire utilisé par des applications pour gérer l’accès des utilisateurs aux ressources et services qu’elles prodiguent.  \nC’est aussi un protocole standard permettant de gérer des annuaires, c'est-à-dire d'accéder à des bases d'informations sur les utilisateurs d'un réseau par l'intermédiaire de protocoles TCP/IP (Transmission Control Protocol/Internet Protocol). Ces bases d'informations sont généralement relatives à des utilisateurs, mais elles sont parfois utilisées à d'autres fins comme pour gérer du matériel dans une entreprise.\nLe LDAP en tant qu’annuaire peut être utilisé en lieu et place de base de données pour conserver toutes sorte d’informations permettant aux utilisateurs de s’authentifier (mots de passe, certificats …). Ils sont d’ailleurs très souvent utilisés pour conserver les mots de passes des différents utilisateurs d’un réseau. \n\nLa connexion TCP qui permet l’échange de données n’est bien souvent pas chiffrée. Par conséquent, l’échange de connées peut être capturé et analysé ultérieurement et permettre des connexions frauduleuses. \n\nPour sécuriser l’annuaire on peut néanmoins mettre en place des solutions : \n•\tProtocole TLS (Transport Layer Security)  \n•\tMéthode d’authentification (en utilisant une couche SASL)\n•\tListes de contrôleur d’accès (ACL) pour contrôler qui a accès et a quelles données. \n\nLe LDAP peut-être lui-même utilisé pour sécuriser une solution, soit en mettant en place un LDAP pour conserver des informations relatives aux utilisateurs ayant le droit de se connecter à certaines machines d’un réseau ou à des ressources spécifiques, soit en l’utilisant en tant que base de données à un serveur d’authentification.\n\nLe LDAP présente les informations sous forme d'une arborescence d'informations hiérarchique appelée DIT (Directory Information Tree) illustré Figure 1 dans laquelle les informations, appelées entrées (ou encore DSE, Directory Service Entry), sont représentées sous forme de branches. Une branche située à la racine d'une ramification est appelée racine ou suffixe (en anglais root entry).\nChaque entrée de l'annuaire LDAP correspond à un objet abstrait ou réel (par exemple une personne, un objet matériel, des paramètres, ...).\nChaque entrée est constituée d'un ensemble de paires clés/valeurs appelées attributs.\n\n \nFigure 1 : Directory Information Tree de LDAP\n\n\t\n\n.1.2.4.2\tLe Security Assertion Markup Language (SAML) [10]\n\nLes transactions SAML utilisent le langage XML (Extensible Markup Language) pour les communications normalisées entre le fournisseur d'identité et les fournisseurs de services. SAML est le lien entre l'authentification de l'identité d'un utilisateur et l'autorisation d'utiliser un service.\nLe Consortium OASIS a approuvé SAML 2.0 en 2005. La norme a considérablement changé par rapport à la version 1.1, à tel point que les versions sont incompatibles. L'adoption de SAML permet aux services informatiques d'utiliser des solutions SaaS (Software as a Service) tout en maintenant un système de gestion des identités fédéré et sécurisé.\nSAML active le Single-Sign On (SSO), un terme qui signifie que les utilisateurs peuvent se connecter une seule fois et que ces mêmes identifiants peuvent être réutilisés pour se connecter à d'autres fournisseurs de services.\nSAML simplifie les processus fédérés d'authentification et d'autorisation pour les utilisateurs, les fournisseurs d'identité et les fournisseurs de services. SAML propose une solution permettant à votre fournisseur d'identité et à vos fournisseurs de services d'exister séparément les uns des autres, ce qui centralise la gestion des utilisateurs et donne accès aux solutions SaaS.\n\nSAML met en œuvre une méthode sécurisée de transmission des authentifications et autorisations utilisateur entre le fournisseur d'identité et les fournisseurs de services. Lorsqu'un utilisateur se connecte à une application SAML, le fournisseur de services demande l'autorisation du fournisseur d'identité approprié. Le fournisseur d'identité authentifie les informations d'identification de l'utilisateur, puis renvoie l'autorisation de l'utilisateur au fournisseur de services, et l'utilisateur peut maintenant utiliser l'application.\n\n.1.2.4.3\tOAUTH 2.0 \n\nFormalisé dans [11], OAUTH 2.0 est un protocole qui permet d’échanger des jetons d’accès afin d’obtenir des autorisations. Le token représente une chaîne de caractère unique permettant d’identifier le client et les différentes informations utiles durant le processus d’autorisation. L’objectif principal d’un jeton d’accès est de se substituer au traditionnel nom d’utilisateur et mot de passe et d’englober différentes autorisations compréhensibles par le serveur de ressource [12][13]. C’est un standard aujourd’hui utilisé par les leaders mondiaux du web (Google, Facebook, Amazon, Twitter, etc.) et qui a largement fait ses preuves.  \nOAUTH 2.0 se différencie en partie de son prédécesseur, OAUTH 1.0a (standardisé en 2010 [14]), par le fait que l’aspect sécuritaire lié à l’intégrité et la confidentialité des données transmises pendant la demande d’autorisation est délégué au protocole TLS.\n\n\n \nFigure 2 : Les rôles dans OAUTH 2.0\n\n\nNéanmoins, Il existe un ensemble de risques connus [15] qui pourrait rendre une implémentation de OAuth 2.0 vulnérable. Les règles de sécurité en jeu pour implémenter un serveur OAuth 2.0 fiable sont assez nombreuses et complexes. \n\nAinsi, de par sa définition et son implantation dans le réseau de nombreuses entreprises, notre solution doit permettre des connexions via de nombreux modes : LDAP client via une connexion TCP, SSO de l’intranet client via SAML, mais aussi la délégation de connexion _entre les différentes APIs notamment_ via OAUTH2 et les tokens. Ces multiples modes de connexions augmentent la complexité du système, ainsi que les contrôles de sécurité. Nous nous sommes donc intéressés aux Unified Data Access Layer dans le but de centraliser ces modes de connexions. \n\n.1.2.4.4\tLes Data Access Layer (DAL) [16][17]\n\nFace à la multiplication de sources et de formats de données, et dans le but d’avoir une vue unifiée des données sans que l’opérateur fasse un effort d’intégration permanant, se sont développés les DAL et plus particulièrement les UDAL (Unified Data Access Layer). L’objectif de ces solutions est donc d’unifier toutes ces données hétérogènes.  \nCes solutions sont encore que très peu documentées et principalement utilisées pour unifier des bases de données et non des couches d’accès. \n\n\n\nL’objectif de nos travaux de R&D de l’année 2019 est donc de repenser notre solution d’optimisation de planning RH pour entreprise en architecture Cloud Native, dans le but d’être scalable horizontalement, d’augmenter la disponibilité et la résilience. Cependant, du fait de son usage en entreprise, notre solution doit absolument être sécurisée et donc nous nous retrouvons à l’interfaçage de ces domaines de recherche : le développement d’une architecture multi-instances, hautement scalable, Cloud Native et hautement sécurisée. "}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Malgré la disponibilité de nombreuses solutions clés en main, les entreprises numériques prospères développent leurs propres solutions sur mesure, plutôt que de les acheter [1].\n- L'objectif du projet est de développer une solution sur mesure pour l'observabilité, la performance, la sécurité, et l'opérabilité de la plateforme Timesquare V2.\n- L'observabilité, dans le contexte de l'informatique et du cloud computing, est définie comme la capacité de mesurer l'état d'un système en se basant sur les données qu'il génère, telles que les logs, les métriques et les traces [2]. Ces données permettent de comprendre non seulement quand une erreur ou un incident se produit, mais aussi pourquoi [3], ce qui est crucial pour une résolution rapide des urgences.\n- Les outils d'observabilité actuels présentent des limitations, notamment en fournissant des données limitées et échantillonnées, ainsi qu'une masse d'alertes sur des points très spécifiques, souvent sans contexte ni informations directement exploitables. Cela limite la visibilité globale sur l'environnement IT [4].\n- Une enquête réalisée en 2021 auprès de grandes et moyennes entreprises d'Europe occidentale révèle que, malgré les investissements croissants dans les solutions d'observabilité, les entreprises rencontrent des freins liés à la complexité des technologies et des applications [5].\n- Bien que l'observabilité gagne en popularité, il n'existe aucune publication scientifique spécifique sur sa mise en œuvre pour un système informatique.\n- La sécurité informatique est définie par l'ITSEC comme la combinaison de trois propriétés : confidentialité, intégrité, et disponibilité [6]. Malgré les efforts pour améliorer la sécurité, les vulnérabilités dans les systèmes informatiques continuent d'augmenter, ainsi que les activités malveillantes qui exploitent ces vulnérabilités.\n- Dans sa thèse, FL Sang a identifié les vecteurs d'attaques et classifié les différentes attaques possibles sur un système informatique, illustrant la complexité de la sécurisation des systèmes (Figure 1) [7].\n- La diversité des canaux d'attaques rend la sécurisation d'un système informatique complexe, nécessitant plus qu'un simple algorithme ou protocole.\n- L'état de l'art révèle la difficulté de mettre en place une solution intégrée pour l'observabilité et la sécurité, combinée au suivi de l'opérabilité et de la performance, avec un manque d'études spécifiques sur cette approche holistique."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nMalgré la multitude des solutions clé en main et prêtes à l’emploi sur le marché d’aujourd’hui, les entreprises numériques prospères demeurent celles qui conçoivent leurs propres solutions et non pas celles qui les achètent [1].\nNotre objectif consiste à développer une solution sur mesure permettant l’observabilité, la performance, la sécurité et l’opérabilité de notre plateforme Timesquare V2. \nDans le domaine de l'informatique et le cloud computing, l'observabilité est la capacité de mesurer l'état actuel d'un système en fonction des données qu'il génère, telles que les logs, les métriques et les traces [2]. Ces données permettent non seulement de savoir quand une erreur ou un incident se produit mais, plus important encore, pourquoi [3]. Cette information est essentielle aux équipes pour réagir rapidement et résoudre les urgences des logiciels modernes.\nLa plupart des outils d’observabilité actuels ne fournissent que des données limitées et échantillonnées. Ils génèrent également une masse d’alertes portant sur des métriques ou des points très spécifiques, avec peu de contexte ou d’informations directement exploitables. Il est ainsi impossible dans ces conditions d’avoir une visibilité sur l’environnement IT dans son ensemble [4]. \nUne enquête sur l’état de l’observabilité auprès de 158 grandes et moyennes entreprises d’Europe occidentale (et 525 organisations à travers le monde) a été réalisée en 2021. Malgré l’amélioration des résultats en augmentant l’investissement dans les solutions d’observabilité, les entreprises interrogées évoquent quelques freins à l’utilisation de l’observabilité comme la complexité des technologies sur lesquelles elle repose [5]. D’autres émettent des réticences quant à la complexité de ses applications.\nIl est à noter que bien que l’observabilité gagne en popularité dans le monde des logiciels, nous n’avons trouvé aucun article scientifique sur la mise en place de l’observabilité pour un système informatique. \nEn ce qui concerne la sécurité, elle est définie par l’ITSEC (Information Technology Security Evaluation Criteria) comme la combinaison de trois propriétés : la confidentialité, l’intégrité et la disponibilité [6]. Malgré les efforts importants qui ont été déployés depuis un certain nombre d’années pour tenter de contenir les problèmes de sécurité, le nombre de vulnérabilités dans les systèmes informatiques demeure en augmentation et les activités malveillantes qui essaient et qui réussissent à les exploiter continuent toujours à se multiplier. \nDans sa thèse de doctorat, FL Sang a identifié les éléments sur lesquels portent les attaques de sécurité ainsi que les vecteurs d’attaque utilisés. Il a ensuite classifié les attaques possibles sur un système informatique (Figure 1) [7]. \n\n \nFigure 1 : Classification des attaques sur les systèmes informatiques\nLa multitude de canaux d’attaques rend la sécurisation d’un système informatique complexe et difficile à mettre en place puisqu’elle requière plus qu’un simple algorithme ou un protocole à suivre. \nL’étude de l’état de l’art a montré la complexité pour mettre en plus une solution d’observabilité et de sécurité d’un système informatique. De plus, aucune étude ne s’est intéressée au suivi de ces deux paramètres au même temps en plus du suivi de l’opérabilité et de la performance."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Les logiciels de planification RH et de gestion de projet (CRM) sont basés sur des fonctionnalités innovantes pour gérer des systèmes complexes, mais rencontrent des difficultés, notamment dans la construction de plannings légaux et la gestion des réseaux de données des SaaS en raison de la complexité croissante des infrastructures.\n- Les technologies actuelles permettant de générer automatiquement des plannings légaux sont gourmandes en ressources et présentent des limitations en termes de complexité temporelle, surtout lorsque le nombre d'utilisateurs est élevé.\n- Le recuit simulé et la programmation par contraintes (PPC) sont des méthodes prometteuses pour optimiser la construction de plannings légaux, mais elles posent des défis en termes de scalabilité et de performance à grande échelle [Siarry P. 2014] [Michael Comelli 2008].\n- Le recuit simulé, inspiré du refroidissement métallurgique, est efficace pour résoudre des problèmes complexes d'optimisation, mais il est sensible aux paramètres et à la scalabilité dans des contextes plus larges.\n- La PPC est plus expressive pour modéliser des contraintes complexes, mais sa complexité de modélisation et sa scalabilité restent des défis majeurs.\n- L'intégration hybride du recuit simulé et de la PPC apparaît prometteuse pour tirer parti des avantages des deux méthodes, bien que des questions demeurent sur l'optimisation de leur complémentarité et leur application pratique.\n- L'adaptabilité aux environnements dynamiques et l'optimisation automatique des paramètres, notamment pour le recuit simulé, sont des verrous technologiques critiques pour ces méthodes [Yew-Cheong Peter Chan 2002].\n- La cybersécurité des plateformes cloud est devenue une priorité avec la montée en puissance des services en ligne et la complexité croissante des architectures cloud.\n- L'intelligence artificielle (IA), les solutions SaaS, et les collecteurs de logs intelligents sont des approches prometteuses pour renforcer la sécurité des environnements cloud, notamment via la détection des menaces, la gestion centralisée des politiques de sécurité, et l'analyse approfondie des événements de sécurité [Iyer, S. S., & Rajagopal, S. 2020] [PanJun Sun, 2020] [Jeon, K. S., et al., 2015].\n- Les défis persistants incluent l'interprétabilité des modèles d'IA, la complexité de l'intégration des solutions SaaS dans les environnements existants, et la gestion massive des logs générés par les collecteurs de logs intelligents [Hariharan, Swetha, et al. 2021] [Tsai, WeiTek, et al. 2014] [Muthurajkumar, S., et al. 2015].\n- En conclusion, bien que l'IA, les solutions SaaS, et les collecteurs de logs intelligents apportent des avancées significatives, leur adoption généralisée est freinée par des problèmes d'interprétabilité, d'intégration, et de gestion de la scalabilité."}, {"role": "assistant", "content": "1.3.2\tEtat de l’art et limite des connaissances\nLes logiciels de planification RH et de gestion de projet (CRM) reposent sur des fonctionnalités innovantes permettant de gérer de la façon la plus efficiente possible des systèmes complexes. Cependant, des problèmes de constructions vis-à-vis de la planification horaire interviennent (construction de planning légaux), de même au niveau de la gestion/surveillance des réseaux de données dus à l’augmentation de la complexité émergente des infrastructures (pour les SaaS notamment). \n\nConcernant la planification légale, plusieurs technologies existantes permettent de construire des moteurs qui génèrent automatiquement des plannings légaux, mais elles sont généralement très gourmandes en ressources et leur complexité temporelle n’est pas satisfaisante (particulièrement quand le nombre d’utilisateurs est élevé). \n\nLa construction de plannings légaux, essentielle dans divers secteurs, requiert des approches sophistiquées pour optimiser l'utilisation des ressources tout en respectant des contraintes spécifiques. Parmi les méthodes émergentes, le recuit simulé et la programmation par contraintes se démarquent, chacun apportant des avantages distincts, tout en présentant des défis et des enjeux technologiques.\n\nLe recuit simulé, inspiré du refroidissement métallurgique, a démontré son efficacité dans la résolution de problèmes complexes d'optimisation combinatoire associés aux plannings légaux. Il tire profit de processus de recherche aléatoires et d'une diversification progressive, mais il demeure sensible aux paramètres, ce qui nécessite un réglage minutieux souvent dépendant du contexte spécifique du problème à résoudre. Par ailleurs, sa performance peut diminuer face à des instances de problèmes de plus grande envergure, soulevant des questions sur sa scalabilité et son application à grande échelle [Siarry P. 2014].\n\nD'un autre côté, la programmation par contraintes (PPC) offre une approche plus expressive pour modéliser les contraintes temporelles, de ressources et de préférences. Elle brille par sa capacité à traiter des contraintes dynamiques, permettant une adaptation fluide aux environnements complexes et changeants. Cependant, la complexité de modélisation demeure un défi, demandant une expertise pointue pour représenter fidèlement les contraintes spécifiques des plannings légaux. De plus, la scalabilité de la PPC reste une préoccupation majeure, en particulier dans le contexte de problèmes de grande envergure [Michael Comelli 2008].\n\nL'intégration hybride du recuit simulé et de la PPC se profile comme une avenue prometteuse, explorant comment combiner ces deux approches pour tirer le meilleur parti de leurs atouts respectifs. Cette approche hybride soulève toutefois des questions cruciales sur la manière d'optimiser la complémentarité de ces techniques dans des situations concrètes, notamment en termes de sélection des mécanismes de chaque méthode et de leur interaction.\n\nDu point de vue des verrous technologiques, l'adaptabilité aux environnements dynamiques reste une zone d'incertitude pour les deux approches. Comment garantir une réactivité rapide aux changements dans l'environnement de planification demeure un défi ouvert. De plus, l'optimisation automatique des paramètres, en particulier pour le recuit simulé, représente un enjeu critique. Le développement de méthodes automatisées pour ajuster ces paramètres en fonction du problème spécifique pourrait améliorer significativement l'efficacité de cette approche [Yew-Cheong Peter Chan 2002].\n\nLa cybersécurité des plateformes cloud est devenue une préoccupation majeure en raison de la prolifération des services en ligne et de la complexité croissante des architectures cloud. L'utilisation de l'intelligence artificielle (IA), des logiciels en tant que service (SaaS), et des collecteurs de logs représente une approche prometteuse pour renforcer la sécurité dans ces environnements. \n1.\tDétection des menaces avec l'IA : L'intégration de l'IA dans les systèmes de détection des menaces permet une identification plus rapide et précise des activités malveillantes. Les modèles d'apprentissage automatique, en particulier les réseaux de neurones profonds, ont montré des performances exceptionnelles dans la détection des anomalies et des comportements suspects [Iyer, S. S., & Rajagopal, S. (2020).]\n2.\tUtilisation de SaaS pour la sécurité cloud : Les solutions SaaS spécialisées dans la cybersécurité offrent une gestion centralisée et simplifiée des politiques de sécurité. Elles permettent également une mise à jour en temps réel des bases de données de menaces, renforçant ainsi la résilience des plateformes cloud [PanJun Sun, 2020]\n3.\tCollecteurs de logs intelligents : Les collecteurs de logs dotés de capacités intelligentes permettent une analyse approfondie des événements de sécurité. L'utilisation de techniques d'analyse sémantique et contextuelle améliore la compréhension des incidents, facilitant ainsi la réponse rapide aux attaques [Jeon, K. S., Park, S. J., Chun, S. H., & Kim, J. B. (2015, June).]\n\nDes limites subsistent pour ces différentes technologies, notamment :\n1.\tInterprétabilité des modèles d'IA : Bien que les modèles d'IA soient puissants, leur opacité peut représenter un obstacle à l'adoption généralisée. L'interprétabilité des modèles d'IA reste un défi majeur, car les décideurs ont besoin de comprendre comment les conclusions sont tirées pour une confiance accrue [Hariharan, Swetha, et al. 2021]\n2.\tIntégration des solutions SaaS : L'intégration de solutions SaaS dans des environnements cloud existants peut être complexe en raison de la diversité des architectures. Les inquiétudes liées à la compatibilité et à la cohérence des politiques de sécurité persistent [Tsai, WeiTek, XiaoYing Bai, and Yu Huang. 2014]\n3.\tGestion massive des logs : Bien que les collecteurs de logs intelligents améliorent la détection des menaces, la gestion massive des logs générés peut entraîner des défis en termes de stockage, de traitement et d'analyse. Les coûts associés et la scalabilité demeurent des problèmes à résoudre [Muthurajkumar, S., Ganapathy, S., Vijayalakshmi, M., & Kannan, A. (2015).]\n\nEn conclusion, l'utilisation de l'IA, des solutions SaaS et des collecteurs de logs représente une avancée significative dans la cybersécurité des plateformes cloud. Cependant, des défis tels que l'interprétabilité des modèles d'IA, l'intégration des solutions SaaS et la gestion des logs massifs nécessitent une attention particulière pour assurer une sécurité efficace et durable dans ces environnements complexes."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Les quatre principales méthodes de traitement des déchets sont le recyclage, l'incinération, le stockage (ISDND), et le traitement biologique (compostage et méthanisation).\n- Le recyclage réutilise les matériaux de produits en fin de vie, réduisant ainsi les déchets et préservant les ressources naturelles. Cependant, il peut être coûteux, surtout lorsque les matériaux sont mélangés, et pose des défis environnementaux, notamment lors du recyclage des équipements électroniques.\n- L'incinération détruit les déchets par combustion, permettant de récupérer de l'énergie sous forme de chaleur ou d'électricité. Bien que ce procédé réduise fortement le volume des déchets, il ne récupère pas 100 % du pouvoir calorifique des déchets et contribue à des émissions de gaz à effet de serre, bien qu'en moindre mesure que le stockage.\n- Le stockage des déchets en décharge est une méthode traditionnelle, mais la diminution des espaces disponibles pousse au développement de méthodes plus efficaces pour réduire les volumes à enfouir. Les décharges modernes utilisent des techniques de rétention des lixiviats et d'extraction des gaz pour minimiser les impacts environnementaux.\n- La méthanisation, ou digestion anaérobie, est un procédé biologique qui décompose la matière organique en l'absence d'oxygène, produisant du biogaz et du digestat. Ce procédé est utilisé pour traiter des déchets organiques dans des environnements contrôlés.\n- Le compostage transforme les matières organiques en compost à travers un processus biologique en présence d'oxygène, aboutissant à un produit stabilisé, hygiénique et riche en humus, utilisé comme terreau en agriculture.\n- Le traitement biologique et mécanique (TBM) combine un tri mécanique des déchets suivi d'un traitement biologique de la partie organique, souvent par digestion anaérobie ou compostage.\n- La pyrolyse et la gazéification sont des traitements thermiques qui décomposent les déchets à haute température avec peu d'oxygène, produisant des gaz de synthèse utilisés pour générer de l'énergie.\n- L'oxydation hydrothermale traite les déchets contenant une forte proportion d'eau en phase liquide sans évaporation préalable, et est utilisée pour les boues urbaines, industrielles et certains effluents liquides.\n- La vitrification chauffe les déchets à très haute température pour les transformer en un produit vitreux, stabilisant ainsi les déchets minéraux et réduisant leur impact environnemental.\n- Les processus de dégradation dans les ISDND (Installations de Stockage de Déchets Non Dangereux) comprennent des phases aérobies et anaérobies, avec la production de biogaz et de lixiviat. Les étapes de la dégradation anaérobie incluent l'hydrolyse, l'acidogenèse, l'acétogenèse, et la méthanogenèse, chacune ayant un rôle spécifique dans la transformation des déchets.\n- L'ajout de mâchefers (résidus solides de l'incinération) aux ISDND peut favoriser la méthanogenèse en augmentant le taux d'hydrogène, un phénomène peu étudié mais prometteur pour la production d'énergie.\n- Les méthodes actuelles de production d'hydrogène incluent le reformage des énergies fossiles, la décomposition de l'eau par électrolyse ou cycles thermochimiques, et la gazéification de la biomasse. Ces méthodes présentent des défis en termes de rentabilité, bilan écologique, et dépendance aux énergies fossiles.\n- Le projet de recherche vise à développer une méthode de production d'hydrogène à partir de mâchefers, offrant une alternative écoresponsable en valorisant ce résidu de déchet et en rendant possible l'implantation de sites de production à proximité des incinérateurs ou dans des zones où l'hydrogène est nécessaire."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nA.\tLes méthodes de traitement des déchets \nDe nos jours, les quatre principales filières d'élimination des déchets sont la valorisation matière (tri pour recyclage), l'incinération (avec ou sans valorisation énergétique, fabrication de combustibles solides), le traitement biologique (compostage et méthanisation) et le stockage (ISDND, ISDI ).\n\nA.1. Le recyclage\n\nLe recyclage est un procédé par lequel les matériaux qui composent un produit en fin de vie sont réutilisés en tout ou en partie. Dans les pays développés, les déchets de consommation les plus couramment recyclés sont les canettes en aluminium, le fer, les boîtes de conserve, les bombes aérosol, les bouteilles en plastique PEHD et PET, les bouteilles et pots en verre, le carton, les journaux, et les magazines. Les autres types de plastiques (PVC, PEBD, PP et PS) sont aussi recyclables mais peu collectés. Un objet composé d'un seul type de matériau voit son recyclage facilité.\n\nLe mélange de matériau augmente les coûts de recyclage (problèmes de séparation et d'extraction des composants) comme cela peut être le cas avec le recyclage des ordinateurs et des équipements électroniques. Le recyclage de ces équipements peut générer des problèmes environnementaux : les écrans et les composants électroniques contiennent du plomb et des métaux lourds, tels le sélénium et le cadmium.\n\nLe coût de collecte et de tri des matériaux augmente le prix de mise en œuvre ultérieure des matériaux recyclés et peut les rendre plus chers que les matériaux neufs. Cependant, leur  impact sur l’environnement est particulièrement réduit. En général, on dépense moins d'énergie, d'eau et de ressources autres pour recycler des matériaux que pour en produire de nouveaux.\n\nLe recyclage a deux conséquences écologiques majeures : la réduction du volume de déchets, et donc de la pollution qu'ils causeraient et la préservation des ressources naturelles.\n\nA.2. Les incinérateurs\n\nL’incinération est le processus de destruction d’un matériau en le brûlant. Incinérer signifie \"réduire en cendres\". Cette technique inclut la récupération du métal et de l’énergie des déchets solides municipaux comme le stockage adapté des résidus solides (mâchefers) et la réduction du volume des déchets.\n\nCe mode d'élimination de déchets a un taux de valorisation limité. L'incinération détruit les ressources contenues dans les déchets et ne permet pas de récupérer 100% du pouvoir calorifique du déchet : l'énergie récupérée, sous la forme de chaleur ou d'électricité, provient du refroidissement des fumées de combustion dans une chaudière, qui permet de récupérer de la chaleur, qui peut être utilisée directement ou à son tour entraîner une turbine pour produire de l'électricité.\n\nL'incinération est un traitement adapté à toutes sortes de déchets (dangereux et non dangereux, solides et liquides) contrairement aux autres modes de valorisation (traitement biologique par exemple) et qui réduit fortement le volume des déchets (90 % de réduction environ) de même que leur masse (70 % environ). L'incinération permet de tirer le meilleur parti du contenu énergétique des déchets en produisant de la chaleur susceptible d'alimenter un réseau de chaleur urbain et/ou d'être transformée en électricité.\n\nL'incinération contribue à minimiser les consommations de ressources énergétiques et certains impacts environnementaux tels que l'effet de serre du fait notamment de la valorisation énergétique. L'incinération émet moins de gaz à effet de serre que le stockage, (fuites de méthane, gaz à effet de serre, issu de la dégradation des déchets organiques).\n\nA.3. Les décharges ou installations de stockage de déchets\n\nStocker les déchets dans une décharge est la méthode la plus traditionnelle de stockage des déchets, et reste la pratique la plus courante dans la plupart des pays. Historiquement, les décharges étaient souvent établies dans des carrières, des mines ou des trous d'excavation désaffectés. Utiliser une décharge qui minimise les impacts sur l'environnement peut être une solution saine et à moindre coût pour stocker les déchets, néanmoins la diminution des espaces libres appropriés implique l'apparition de traitements plus efficaces et/ou la réduction des volumes de déchets à enfouir\n\nLes caractéristiques d'une décharge moderne sont des méthodes de rétention des lixiviats, tels que des couches d'argile ou des bâches plastiques. Les déchets entreposés doivent être compactés et recouverts pour éviter d'attirer les souris et les rats et éviter l'éparpillement. Beaucoup de décharges sont aussi équipées de systèmes d'extraction des gaz installés après le recouvrement pour extraire le gaz produit par la décomposition des déchets.\n\nOn peut voir sur la Figure 1 l’exemple d’une ISDND :\n \nFigure 1 : Coupe schématique d’une ISDND (modifiée d’après la thèse de Vigneron V).\n\nLes installations de stockage bioactives :\n\nLe stockage bioactif peut être défini comme une stratégie d'optimisation des processus d’évolution des déchets biodégradables dans une installation confinée en vue de transformer et de stabiliser la fraction facilement et modérément fermentescible des déchets. Il s’agit également de considérer les déchets comme une ressource énergétique capable de se substituer à des énergies fossiles. En effet, comme décrit précédemment, la dégradation naturelle des déchets produit du biogaz, qui est considéré comme une source d’énergie renouvelable (Directive 2001/77/CE du 27 septembre 2001). Ce concept de gestion des installations de stockage a également pour but d’améliorer l’image du stockage et de le faire apparaître comme un véritable procédé industriel permettant d’obtenir un état « éco-compatible » du déchet, c’est-à-dire en équilibre avec son environnement, en fin d’exploitation.\n\nA.4. La méthanisation\n\nLa méthanisation (encore appelée digestion anaérobie) est une technologie basée sur la dégradation par des micro-organismes de la matière organique, en conditions contrôlées (ce en quoi elle diffère des ISDND) et en l’absence d’oxygène (réaction en milieu anaérobie, contrairement au compostage qui est une réaction aérobie). Ce processus se déroule en enceinte fermée (appelée digesteur, fermenteur ou réacteur) généralement calorifugée afin d'y maintenir une température constante. On estime qu’entre 20 et 40% de l'énergie primaire produite par le biogaz est utilisée pour réchauffer et maintenir en température le digesteur (à température mésophile ou thermophile) (Ademe 2009). Cette dégradation aboutit à la production :\n\n•\tD’un produit humide riche en matière organique partiellement stabilisée appelé digestat. Il est généralement envisagé le retour au sol du digestat après éventuellement une phase de maturation par compostage.\n•\tDe biogaz, comparable à celui produit en ISDND.\nA.5. Le compostage\n\nLe compostage est un procédé biologique de conversion et de valorisation des matières organiques (sous-produits de la biomasse, déchets organiques d'origine biologique...) dans des conditions contrôlées, en présence de l'oxygène de l'air en un compost, produit stabilisé, hygiénique, semblable à un terreau et riche en composés humiques. Le compost est recyclé comme paillis ou terreau pour l’agriculture ou le jardinage.\n\nIl y a un très large éventail de méthodes de compostage et de fermentation qui varient en complexité, du simple tas de compost de végétaux à une cuve automatisée de fermentation de déchets domestiques divers.\n\nDeux phénomènes se succèdent dans un processus de compostage. Le premier, amenant les résidus à l'état de compost frais, est une dégradation aérobie intense : il s'agit essentiellement de la décomposition de la matière organique fraîche à haute température (50 à 70°C) sous l'action de bactéries ; le deuxième, par une dégradation moins soutenue, va transformer le compost frais en un compost mûr, riche en humus. Ce phénomène de maturation, qui se passe à température plus basse (35 à 45°C), conduit à la biosynthèse de composés humiques par des champignons.\n\nLe procédé s’étale sur trois phases toujours identiques mais pouvant être réalisées selon différentes méthodes. La phase de fermentation active, qui peut se dérouler à l'air libre (sous un auvent, en fosse, en tas), en enceinte close (digesteur, silo vertical). L'aération, qui peut se faire de façon passive (par la porosité, méthode chinoise, par brassage) ou de façon active (par soufflerie ou aspiration). La phase de maturation, qui se déroule habituellement à l'air libre en tas de grande dimension.\n\nA.6. Le traitement biologique et mécanique\n\nLe traitement biologique et mécanique (TBM) est une technique qui combine un tri mécanique et un traitement biologique de la partie organique des déchets municipaux. Le TBM est aussi parfois appelé TMB (traitement mécanique et biologique) cela dépend de l’ordre dans lequel s’effectuent les opérations.\nLa partie mécanique est souvent une étape de tri (suivi d'un broyage). Cela permet de retirer les éléments recyclables du flux de déchets (tels les métaux, plastiques et verre) ou de les traiter.\nLa partie biologique se réfère quant à elle à une digestion anaérobie de la fraction organique ou à un compostage.\n\n\tA.7. Pyrolyse et gazéification\n\nLa pyrolyse et la gazéification sont deux méthodes liées de traitements thermiques où les matériaux sont chauffés à très haute température avec peu d’oxygène (non pratiquées sur des déchets ménagers). Ces processus sont réalisés dans une cuve étanche sous haute pression. Transformant les matériaux en énergie ces méthodes sont plus efficaces que l’incinération directe, plus d’énergie pouvant être récupérée et utilisée.\n\nLa pyrolyse est la décomposition d'un composé organique par la chaleur (sans flamme, pour éviter l'oxydation et la combustion) pour obtenir d'autres produits (gaz et matière) qu'il ne contenait pas. La pyrolyse des déchets solides type agroalimentaire (noyaux d’abricots, noix de coco) transforme les matériaux en produits solides, liquides ou gazeux. L’huile pyrolytique et les gaz peuvent être brûlés pour produire de l’énergie ou être raffinés en d’autres produits. Les résidus solides (charbon) peuvent être transformés plus tard en produits tels que les charbons actifs. On peut atteindre 70% de perte de poids par rapport au matériau d'origine.\n\nLa gazéification à l'aide de torches à plasma est utilisée pour transformer directement les matières organiques en leurs composants élémentaires. Il se forme un gaz de synthèse appelé syngaz composé de monoxyde de carbone et d’hydrogène (la gazéification plasma ne brûle pas les déchets comme le font les incinérateurs). Ce gaz est ensuite brûlé pour produire de l’électricité et de la vapeur. La gazéification est utilisée dans les centrales produisant de  l’énergie à partir de la biomasse pour produire de l’énergie renouvelable et de la chaleur. Ce procédé transforme les déchets non organiques en solides vitrifiés amorphes.\n\nLes fours à plasma ne font pas de distinction entre les différents types de déchets (seuls les déchets nucléaires sont proscrits). La seule variable est la quantité d’énergie nécessaire pour détruire les différents déchets. C’est pourquoi aucun tri des déchets n’est nécessaire.\n\nA.8. Oxydation hydrothermale\n\nAppliquée au traitement des déchets ou d’effluents, l’OHT est destinée à traiter des produits contenant une forte proportion d’eau. La particularité de ce procédé est qu’il ne nécessite pas l’évaporation de l’eau avant oxydation de la matière organique. L’oxydation est réalisée en phase liquide (par différence avec l’incinération). Elle permet de traiter des charges organiques difficilement traitables par voies biologiques (teneur en organiques trop élevée, DCO \"dure\" c’est-à-dire difficilement oxydable par voie biologique, organiques toxiques, etc.), ainsi que des flux ayant une charge organique trop faible.\nLes déchets typiquement traités par l’OHT sont les boues urbaines et industrielles, des eaux de procédés industriels, des effluents liquides (aqueux), des déchets dangereux liquides…\n\nA.9. La vitrification\n\nLa vitrification des déchets consiste à chauffer les déchets à une température telle que la fraction non-combustible du déchet fonde (c’est-à-dire à une température supérieure au point de fusion). Ce liquide est ensuite refroidi de façon brutale, pour donner lieu à un produit s'apparentant à un verre (structure amorphe) dénommé \"vitrifiat\".\n\nLes procédés dont la fonction principale est de vitrifier le déchet traitent des déchets principalement minéraux, bien qu’ils soient à même de traiter également des déchets contenant une certaine quantité de matière combustible. La chaleur nécessaire pour obtenir la fusion peut être apportée par le déchet lui-même si son pouvoir calorifique est suffisant (ce qui est généralement le cas des ordures ménagères), ou par l’énergie des déchets complétée par une énergie externe (électricité, gaz naturel, etc.) si nécessaire.\n\n\nB.\tLes mécanismes de dégradation en ISDND et les effluents générés \n\nB.1. La phase aérobie\n\nLa phase de dégradation aérobie des déchets est en général très limitée lors d'une méthanisation hors sol (essentiellement lors de la collecte et du transport). Elle intervient par contre de manière significative dans le cadre d'un enfouissement en installation de stockage. A leur arrivée en ISDND, les déchets sont déposés et compactés dans des alvéoles, selon une succession de couches horizontales. Les déchets sont alors au contact de l'air et de son oxygène pendant la période qui s'étale du dépôt jusqu'à l'ajout d'une nouvelle couche de déchet ou de la fermeture de l'alvéole par une couverture imperméable.\n\nCette interaction déchets-oxygène va initier une phase de dégradation aérobie des déchets qui va durer pendant les premières semaines d'exploitation de l'alvéole. Cette phase est généralement caractérisée par une montée en température du volume de déchets qui peut s'élever jusqu'à 80°C, due aux réactions exothermiques de biodégradation des putrescibles. Cette phase s'accompagne d'une production de dioxyde de carbone et de vapeur d'eau. La température va ensuite diminuer et se stabiliser aux environs de 30 à 40°C. Il a été estimé que 5 à 15% de la matière biodégradable pouvait être consommée lors de cette dégradation aérobie.\n\nLa phase de décomposition aérobie, à travers cette augmentation de température, permet la croissance ultérieure des populations microbiennes anaérobies. Elle a une influence positive sur le démarrage de la biodégradation du massif de déchets et sur l'installation d'une méthanogenèse stable. En métabolisant une partie des déchets facilement biodégradables, elle permet également d'éviter une hydrolyse/acidogenèse trop rapide et intense qui peut entraîner un relargage important d'AGV (Acide Gras Volatil) et par conséquent bloquer les phases ultérieures de la dégradation anaérobie (Yuen 1999).\n\nB.2. Les phases anaérobies\n\nLes phases de dégradation anaérobie des déchets (cf. Figure 2) prennent place dès que le système ne présente plus d'oxygène et conduisent à la production de deux effluents distincts mais interdépendants que sont le lixiviat et le biogaz. Celui-ci est principalement composé de dioxyde de carbone et de méthane. Les processus de dégradation conduisant du déchet jusqu'à ces molécules mettent en jeu différentes populations de microorganismes (et donc des conditions optimales de croissance et d'activité optimales différentes). Ces populations travaillent en série (les produits d'une réaction seront les substrats d'une autre) et en parallèle : au sein d'un même massif, différents éléments de volume peuvent se trouver à des stades différents de dégradation anaérobie. On distingue plusieurs grandes étapes dans la dégradation anaérobie :\n-\tL'hydrolyse.\n-\tL'acidogenèse.\n-\tL'acétogenèse et l'homoacétogenèse.\n-\tL'oxydation syntrophique de l'acétate.\n-\tLa méthanogenèse.\nEt des réactions minoritaires :\n-\tL'oxydation anaérobie du méthane.\n-\tLa sulfato-réduction.\nPuis :\n-\tLa maturation et la stabilisation des déchets.\n \nFigure 2 : Représentation séquentielle de la dégradation anaérobie des déchets (d’après Christensen et Vigneron).\n\nB.2.1.L’hydrolyse\n\nLa phase d'hydrolyse correspond à la décomposition des polymères complexes des matériaux organiques sous forme des monomères les constituant. Ces réactions sont catalysées par des enzymes extracellulaires comme les cellulases, les lipases, convertissant les matières grasses en acides gras et glycérol, ou les protéinases, enzymes coupant les liaisons peptidiques des protéines et relâchant des acides aminés ou des oligopeptides dans le milieu. Les polysaccharides sont également transformés en sucres plus simples. De l'ammonium peut être libéré durant cette étape. \nCette phase permet aux microorganismes de type fermentatif d'incorporer des nutriments (Christensen 1989).\n\nOn considère que les fractions dites putrescibles des déchets (déchet alimentaires principalement) s'hydrolysent rapidement dans une ISDND ou un digesteur hors sol. Dans une ISDND, cette phase d'hydrolyse peut se dérouler en conditions aérobies ou microaérophiles, sous l'action de microorganismes respirant l'oxygène et de champignons. Les fractions lignocellulosiques (papier, cartons, bois,…) s'hydrolysent beaucoup plus lentement et la vitesse de réaction est inférieure aux vitesses de réaction des étapes suivantes. Ainsi, une fois que se sont installées les étapes aval de la dégradation, l'hydrolyse de ces fractions spécifiques devient l'étape cinétiquement limitante pour le processus global.\n\n\t\tB.2.2. L’acidogénèse\n\nLes monomères issus de l'hydrolyse sont disponibles pour les groupes bactériens acidogènes. La flore est constituée pour partie de bactéries anaérobies facultatives (genres Streptocoques, Bacillus, Micrococcus…) et pour une autre de bactéries anaérobies strictes (genre Clostridium). Les molécules vont être converties en de nouveaux composés organiques simples tels que des alcools et des acides gras volatils. \n\nParmi les AGV et les acides organiques, on trouve :\n-\tl'acide acétique (CH3COOH)\n-\tl'acide propionique (CH3CH2COOH)\n-\tl'acide lactique (CH3CHOHCOOH)\n-\tl'acide butyrique (CH3(CH2)2COOH)\n-\tl'acide iso-butyrique ((CH3)2CHCOOH)\n-\tl'acide valérique (CH3(CH2)3COOH)\n-\tl'acide iso-valérique ((CH3)2CHCH2COOH)\n-\tl'acide caproïque (CH3(CH2)4COOH)\n-\t…\n\nLes réactions de dégradation du glucose ci-dessous sont données à titre d'exemple : \n\nC6H12O6 + 2H2O -> CH3(CH2)2COOH + H2 + 2HCO3- + 2H+\nC6H12O6 + H2O -> CH3CH2COOH + CH3COOH + H2 + HCO3- + H+\nC6H12O6 + 4H2O -> 2CH3COOH + 4H2 + 2HCO3- + 2H+\nC6H12O6  -> 2CH3CHOHCOOH\n\nLes acides générés pendant l'acidogenèse vont provoquer une diminution du pH, avec des impacts aussi bien sur la physico-chimie du milieu (dissolution de matière organique) que sur sa biologie: les bactéries acidogènes préfèrent des environnements acides (pH entre 4,5 et 5,5). Leur action va donc être favorisée dans une certaine mesure par cette production d'AGV.\n\n\t\tB.2.3. L'acétogenèse, l'homoacétogenèse et l'oxydation syntrophique de l'acétate\n\nLa troisième étape de la dégradation anaérobie correspond à l'acétogenèse. Durant cette étape, ce sont les alcools et les AGV produits préalablement qui sont utilisés en tant que substrat pour former de l'acide acétique, de l'hydrogène et du dioxyde de carbone. Le profil des produits formés durant cette étape varie avec le type de bactéries dans le système, ainsi que les conditions environnementales comme la température et le pH.\n\nCH3CH2COOH + 2H20 -> CH3COOH + 3H2 + CO2\nCH3(CH2)2COOH + 2H2O -> 2CH3COOH + 2H2\n4CH3OH + 2CO2  -> 3CH3COOH + 4H2O\n4H2 + 2CO2  -> CH3COOH + 4H2O\n\nOn distingue trois types de bactéries acétogènes (Pelmont 1993) :\n-\tles homo-acétogènes, produisant essentiellement de l'acétate à partir de dioxyde de carbone et de dihydrogène, mais pouvant également utiliser le méthanol, la méthylamine ou le formate. Les microorganismes responsables de ces réactions sont strictement anaérobies et forment un groupe extrêmement hétérogène : on y trouve des microorganismes Gram-positifs, Gram-négatifs, des morphologies de type bacilles et coques, des températures de développement variées (psychrophiles, mésophiles, thermophiles). Cette étape est un processus consommant l'hydrogène des ISDND et MHS de façon significative.\n-\tles hétéro-acétogènes, convertissant les substrats organiques en caproate, butyrate et acétate\n-\tles mixotrophes, utilisant à la fois le dioxyde de carbone, le dihydrogène et les substrats organiques\n\nL'accumulation trop rapide d'acétate lors de cette étape peut inhiber les réactions de conversion du propionate par exemple ou les réactions suivantes de méthanogenèse. Dans le même ordre d'idée, la croissance des bactéries acétogènes nécessite une très basse pression partielle en H2 (inférieure à 10-4 atm) pour rendre la production d'acétate thermodynamiquement possible (Barlaz, Ham et al. 1990).\n\nL'oxydation syntrophique de l'acétate correspond à la réaction durant laquelle cette molécule est oxydée en H2 et CO2 (ces deux produits pouvant être convertis en méthane par la suite). L'oxydation syntrophique a été identifiée dans certains environnements naturels en condition psychrophile, mésophile et thermophile. \nDeux facteurs environnementaux influencent l'activité de l'oxydation syntrophique : la température et la concentration en acétate. La réaction est favorisée pour de faibles concentrations en acétate.\n\n\n\n\nB.2.4. La méthanogenèse\n\nLa méthanogenèse correspond à la phase de production biologique du méthane au sein d'un système de dégradation anaérobie. Elle se déroule en anaérobiose stricte, sous l'action de trois types de microorganismes méthanogènes, tirant de cette réaction l'énergie nécessaire à leurs activités métaboliques (Garcia, Patel et al. 2000) :\n\n-\tles hydrogénotrophes (Methanosarcina, Methanobacterium, Methanococcus), transformant l'hydrogène et le dioxyde de carbone en méthane selon la réaction :\n4H2 + CO2  -> CH4 + 2H2O\n\n-\tles acétoclastes (Methanosarcina, Methanosaeta), convertissant l'acide acétique en méthane et en dioxyde de carbone selon la réaction :\nCH3COOH  -> CH4 + CO2\n\n-\tles méthylotrophes (Methanosarcina), transformant le méthanol ou le méthylamine en méthane et dioxyde de carbone selon les réactions :\n4CH3OH  -> 3CH4 + CO2 + 2H2O\n4CH3NH2 + 2H2O  -> 3CH4 + CO2 + 4NH3\nCH3OH + 5H2 + CO2  -> 2CH4 + 3H2O\nMéthanol + Acétate -> Méthane\n\nLes trois types de méthanogenèse suivent différentes voies métaboliques. Différents substrats intermédiaires, enzymes et coenzymes interviennent. Le méthylcoenzyme M en particulier constitue un composé clef car il est commun aux trois processus de méthanogenèse : la production du méthane est catalysée par la méthylcoenzyme M réductase qui réduit le groupe méthyle du méthyl-coenzyme M en utilisant les électrons apportés par le coenzyme B, donneur d'électron commun chez tous les méthanogènes.\n\nLa littérature rapporte que 67% du méthane est issu de la conversion de l'acétate et la méthanogenèse hydrogénotrophe ne peut théoriquement dépasser 33% à cause de la balance entre production d'hydrogène et d'acétate des voies fermentatives et les nombres de moles respectives nécessaires pour la production de la même quantité de méthane.\n\nB.2.5. Les réactions minoritaires\n\n•\tLa sulfato-réduction :\n\nParallèlement à la méthanogenèse, des réactions de sulfato-réduction peuvent se produire sous l'action de bactéries sulfato-réductrices. Ces réactions nécessitent du sulfate pour prendre place. Les principaux microorganismes sulfato-réducteurs sont les  espèces Desulfovibrio et Desulfobacter.\n\nCH3COOH + SO42- + H+  ->H2S + 2HCO3-   (Drury 2006).\n•\tL’oxydation anaérobie du méthane :\n\nL'oxydation anaérobie du méthane (OAM) s'apparenterait à une réaction de méthanogenèse inversée (Hallam, Girguis et al. 2003), et met en jeu des bactéries sulfato-réductrices en association avec un partenaire archée ANME (Anaerobic Methanotroph). Le méthane est oxydé en présence de sulfate comme accepteur d'électrons Girguis, Orphan et al. 2003).\n\nCH4 + SO42-  -> HCO3-  + HS- + H2O\n\nLes mécanismes de l'OAM et la nature des microorganismes impliqués constituent un sujet de recherche et de débat important.\n\n•\tLa dénitrification :\n\nEn présence de nitrate dans le système, la méthanogenèse est remplacée par la réaction de dénitrification. Néanmoins, cette réaction ne peut être une réaction majoritaire dans une ISDND de par l'absence de nitrate dans le lixiviat.\n\n0,625 FeS + NO3- + H+  -> 0,625 Fe2+ + 0,625 SO42- + 0,5 H2O + 0,5 N2\n0,625 CH3COOH + NO3-  ->  HCO3- + 0,25 CO2 + 0,75 H2O + 0,5 N2\n\n•\tLa solventogénèse :\n\nLa solvantogenèse est une réaction de production de solvants organiques (éthanol, butanol), qui prend place après l'hydrolyse et l'acidogenèse dans des conditions riches en AGV (la concentration n'est pas clairement définie et varie entre 2 et 30mM) et en hydrogène. Le pH doit être bas : pH optimum pour la production de solvant autour de 4,5. Cette réaction permet d'allouer, en partant d'une mole de glucose, plus d'énergie aux microorganismes syntrophes (méthanogènes), qu'aux fermenteurs et donc d'améliorer leur croissance.\n\nB.3 Les affluents générés\n\n\t\tB.3.1 Le biogaz\n\nSa composition évolue avec la phase principale en place dans le massif de déchets. Les principaux gaz sont le dioxyde de carbone (produit durant l'acidogenèse et la méthanogenèse), le méthane (produit durant la méthanogenèse), l'hydrogène (produit durant l'acidogenèse), l'hydrogène sulfureux (produit s'il y a présence de sulfate à la fin de l'acidogenèse et au début de la méthanogenèse). Des traces de protoxyde d'azote N2O, quatrième gaz à effet de serre après la vapeur d'eau, le CO2 et le CH4, peuvent également être détectées, ainsi que de l'ammoniac NH3 produit à partir des protéines.\n\nLa prodution du biogaz à partir des déchets entrant peut être prédite via deux approches :\n-\tL’approche stoechiométrique : \nElle consiste à considérer la composition élémentaire des différents constituants des déchets, les produits de la réaction globale de dégradation, et à équilibrer les deux membres de l'équation-bilan ainsi formée (cf. Figure 3).\n\n \nFigure 3 : Equation bilan de la méthanisation de la matière organique.\n\nLorsque la composition élémentaire des déchets est connue, cette équation permet de déterminer à la fois la composition du biogaz et sa production théorique totale, elle donne donc un potentiel maximal.\n\n-\t L’approche via la biodégradabilité des déchets :\nEn réalité, la minéralisation est rarement complète, comme nous l'avons évoqué avec la maturation et stabilisation des déchets. Certains composants sont plus facilement biodégradables, comme les putrescibles. C'est pourquoi il est plus courant de s'intéresser à la biodégradabilité des déchets pour estimer au mieux le potentiel d'un déchet selon sa composition. Cette approche consiste donc à considérer différentes valeurs de biodégradabilité pour les différents composants des fractions des déchets et à utiliser un taux moyen pondéré de production de méthane.\n\n\t\tB.3.2. Le lixiviat\n\nLe lixiviat, au cours de sa percolation dans le massif de déchets, se charge en nombreux composés chimiques. Les concentrations de chacun varient en fonction des conditions physicochimiques du milieu et donc de la phase de dégradation des déchets : le profil de lixiviation des déchets ne sera pas le même à pH acide dans les zones du massif en phase d'hydrolyse ou d'acidogenèse qu'à pH neutre dans les zones en phase de méthanogenèse. Le fer est un bon exemple de ce phénomène (sa concentration dans le lixiviat est supérieure pendant la phase d'acidogenèse). Certains composés, tels les ions chlorure ou sodium, sont moins impactés par les conditions environnementales.\nLa composition des lixiviats varie entre les ISDND et à l'intérieur même d'une ISDND.\n\n\t\tB.3.3. Le digestat\n\nLorsque une ISDND est stabilisée, qu'elle ne produit plus ni biogaz ni lixiviat ou alors en quantités très faibles, elle contient tout de même des déchets qui n'ont pas pu être totalement dégradés dans les conditions de stockage. Ces fractions intactes ou partiellement dégradées  correspondent aux déchets ultimes d'une ISDND.\n\nC.\tProduction d’hydrogène à partir des mâchefers \n\nL’incinération produit un grand nombre de résidus solides (mâchefers) qui doivent être éliminés en décharge ou qui font l'objet d'un traitement si une valorisation en technique routière est envisagée. Les mâchefers représentent environ 25-30% du poids des déchets incinérés.\nUn certain nombre d’études ont permis de mettre en évidence le fait que l’ajout de mâchefers aux ISDND favorise le processus de méthanogénèse. Les résultats de cette coincubation de mâchefers avec des DND sur la dégradation des déchets sont peu documentés dans la littérature, et la raison de ce phénomène ont suscité l’émission de nombreuses hypothèses mais aucun travail de validation n’a encore été mené.\n\nLes travaux menés par Anthony BOULANGER lors de sa thèse de doctorat ont permis notamment de montrer que les mâchefers ont un effet sur les microorganismes méthanogènes du système et permettent leur croissance précoce, ce qui accélère la méthanogénèse.\n\nCela s’explique par l’augmentation du taux d’hydrogène dans le milieu, suite à la réaction de l’aluminium du mâchefer avec l’eau selon les réactions suivantes :\nAl + 3 H2O -> Al(OH)3 + 1.5 H2\nAl + 2 H2O -> AlOOH + 1.5 H2\n\nDes réactions mettant en jeu de l'hydroxyde de calcium Ca(OH)2 ont également été observées :\nAl + Ca(OH)2 + 6 H2O -> Ca(Al(OH)4)2) + 3 H2\n\nCe phénomène de production d’hydrogène par les mâchefers en milieu aqueux n’a été que très peu étudié par la communauté scientifique, et n’est pas du tout exploité à l’échelle industrielle. C’est sur ce phénomène que nous avons décidé de faire porter la majeure partie de nos travaux de R&D.\n\n\nD.\tLes autres méthodes de production d’hydrogène \n\nIl existe actuellement trois voies permettant la production d’hydrogène\n\nD.1. A partir des énergies fossiles \n\nCette approche s’appuie sur une technique de reformage : cette réaction chimique casse les molécules d’hydrocarbure sous l’action de la chaleur pour en libérer le dihydrogène.\nCes technologies sont matures et déjà utilisées à échelle industrielle pour la production d’hydrogène. Elles présentent toutefois un bilan énergétique peu intéressant car sont basées sur des ressources fossiles limitées et génératrice de gaz à effet de serre.\nDes problématiques d'ordre économiques sont à prévoir conjointement aux difficultés économiques d'approvisionnement en hydrocarbures.\nD.2. Par décomposition de l’eau\n\nDeux procédés envisageables sont actuellement à l’étude :\n-\tL’électrolyse (permet de décomposer chimiquement l’eau en dioxygène et dihydrogène sous l’action d’un courant électrique).\n-\tLa dissociation de la molécule d’eau par cycles thermochimiques (permet d’opérer la dissociation de la molécule à des températures de l’ordre de 800 °C à 1 000 °C). Cette seconde méthode a été envisagée pour récupérer l’énergie thermique des centrales nucléaires mais n’est a priori plus envisagée en France.\nCes technologies ne sont pas encore matures pour la production industrielle d’hydrogène. Elles présentent un bilan écologique très positif si la dissociation est faite à partir d’énergie non polluante. A l’heure actuelle le problème reste la rentabilité\n\nD.3. A partir de la biomasse\n\nPar gazéification de la biomasse (bois, déchets, etc …) on obtient un mélange (CO + H2) que l’on purifie ensuite. La quantité de CO2 émise au cours de la conversion de la biomasse en hydrogène est à peu près équivalente à celle provenant de la photosynthèse ; l’empreinte écologique est donc nulle.\n\nD.4 Autres voies de production\n\nDes recherches sont actuellement en cours sur d’autres méthodes de production d’hydrogène (nanotechnologies pour la photosynthèse,  …etc.).\nLes réactions chimiques de productions sont également intéressantes (aluminium et ses alliages) et c’est dans cette catégorie que s’inscrit ce projet de recherche. En effet, le potentiel hydrogène stocké sous forme d’aluminium est, à la différence de l’hydrogène, stable. De plus la combinaison de l’utilisation des mâchefers (résidu des centres d’incinérations) permet de valoriser un déchet et présente donc un intérêt\nComme évoqué précédemment, les méthodes de production d’hydrogène les plus efficaces à l’heure actuelle sont aussi les plus polluantes car reposant sur l’utilisation des énergies fossiles.\n\n\nDans le cadre de ce projet de recherche, nous souhaitons offrir une alternative écoresponsable en développant une nouvelle méthode de production d’hydrogène à partir d’un résidu de déchet tel que les mâchefers.\n\nIl est à noter que, la production de mâchefers étant « régionale », cela rend possible l’implantation d’unité de production à proximité des incinérateurs ou, au contraire, si aucun besoin en hydrogène n’existe localement, de délocaliser la production car les mâchefers sont stables et facilement transportables au contraire de l’hydrogène."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- Les radiographies standard sont essentielles pour diagnostiquer les fractures osseuses des extrémités, mais des études montrent un taux significatif de fractures manquées, notamment au niveau de la cheville, du pied, du coude et du poignet [8, 9, 10, 11].\n- Les réseaux neuronaux profonds, en particulier les réseaux neuronaux convolutifs, ont démontré des performances exceptionnelles dans l'analyse d'images, y compris en imagerie médicale, avec des résultats comparables à ceux des cliniciens pour la détection des fractures [12, 13, 14, 15, 16, 17].\n- Ces modèles sont capables de détecter les fractures avec une grande précision sur diverses parties du corps, mais certaines études sont limitées par des images incomplètes ou à basse résolution [1, 6], la focalisation sur des zones spécifiques du corps [3, 5], une approche monocentrique [5, 6], ou l'absence de comparaison avec les cliniciens [2, 4].\n- La détection d'objets est plus complexe que la simple classification, car elle implique de localiser précisément les objets dans une image, en tenant compte de leur taille, position et des occlusions.\n- Un détecteur d'objets efficace doit identifier correctement les objets sans produire de faux positifs (FP) ou manquer des objets réels (FN), ce qui est illustré par des exemples typiques [Figure 1 2].\n- Les réseaux neuronaux convolutifs peuvent prédire les coordonnées rectifiées d'une zone englobant un objet d'intérêt, mais la technique de « fenêtre glissante » est inefficace en raison du temps de calcul.\n- Des méthodes comme R-CNN [20] améliorent l'efficacité en générant des zones candidates pour la détection, mais restent lentes en raison de la génération des propositions.\n- Faster R-CNN [22] a introduit un mécanisme intégré pour proposer des zones candidates via un réseau de propositions (RPN), utilisant des « anchors » pour améliorer la prédiction des objets dans l'image. Les améliorations telles que FPN [23] et Mask R-CNN [24] ont encore optimisé les performances.\n- Des méthodes alternatives à une seule étape, comme YOLO [25], SSD [26] et RetinaNet [27], prédisent directement les objets en utilisant des grilles alignées sur des anchors, offrant une détection plus rapide, mais avec des performances souvent inférieures à celles des méthodes en deux étapes [Figure 1 5].\n- Les principaux jeux de données pour la détection d'objets incluent PASCAL VOC et COCO, ces derniers offrant une annotation détaillée pour des objets du quotidien avec des métadonnées complexes [29]. En imagerie médicale, les jeux de données tels que Chest-X Ray et les mammographies sont les plus proches des radiographies utilisées pour la détection de fractures, mais se concentrent généralement sur la classification plutôt que sur la détection d'objets.\n- La NAS (Neural Architecture Search) permet d'automatiser la création d'architectures de réseaux neuronaux, ce qui est crucial pour optimiser les modèles dans des tâches spécifiques, y compris l'analyse d'images médicales [37].\n- L'apprentissage par renforcement, souvent combiné avec des algorithmes évolutionnaires, est couramment utilisé dans la NAS pour optimiser les architectures, mais reste un processus complexe et chronophage [40, 41, 42, 43, 44, 45].\n- Dans le domaine médical, les architectures optimisées pour la reconnaissance d'objets classiques peuvent ne pas être adaptées aux spécificités des images médicales. L'application de la NAS pourrait améliorer significativement la performance des modèles pour des tâches comme la détection de fractures."}, {"role": "assistant", "content": "1.2\tÉtat de l’art\nLes radiographies standard sont le principal support diagnostique de l'évaluation radiologique de première ligne des lésions osseuses traumatiques des extrémités. De nombreuses études montrent un taux significatif de fractures osseuses manquées [8, 9, 10, 11], avec une prévalence des fractures de la cheville, de l'avant-pied, du coude et du poignet [8]. Fournir aux médecins des données préliminaires pertinentes sur la présence des fractures susmentionnées serait bénéfique pour la fiabilité du diagnostic.\n\nLes réseaux neuronaux profonds sont un domaine majeur dans le domaine de l'intelligence artificielle, ces dernières années, les réseaux neuronaux profonds et en particulier les réseaux neuronaux convolutifs profonds ont réussi à atteindre des performances exceptionnelles sur une grande variété de benchmarks basés sur les images. Ils ont été appliqués avec succès dans une grande variété de tâches stimulantes en mots réels, y compris des tâches d'analyse d'imagerie médicale [12, 13, 14, 15, 16, 17].\n\nLes réseaux neuronaux convolutifs profonds ont également été largement appliqués à la détection des fractures, ce qui a permis d'atteindre des niveaux de performance à égalité avec les médecins [1, 2, 3, 3, 4, 5, 5, 6]. Ces résultats démontrent la capacité de ces techniques d’intelligence artificielle à détecter avec précision les fractures sur diverses parties du corps. Quelques-unes des études cliniques évaluent l'algorithme de façon autonome [1, 2], en comparant les résultats aux résultats obtenus à ceux des cliniciens, tandis que d'autres mènent des expériences contrôlées significatives pour l’efficacité des médecins avec et sans l’assistance du modèle [3].\n\nÀ cet égard, de multiples preuves solides ont été avancées pour démontrer la pertinence et la fiabilité des modèles d'apprentissage en profondeur pour aider les cliniciens dans le diagnostic des fractures. Néanmoins, elles sont trop incomplètes pour être considérées comme une étude équivalente. En particulier, certaines d'entre elles incluent des images incomplètes ou à basse résolution [1, 6], ne considèrent que les parties étroites du corps [3, 5], sont monocentriques [5, 6] ou ne prennent pas du tout en compte les cliniciens [2, 4].\n1.2.1\tDétection d’objet\nLa détection d'objets est un problème plus compliqué que la classification car le problème est ici de réussir à localiser précisément les instances d'objets dans une image. Les difficultés supplémentaires de ce problème sont liées à la taille de l'objet, la position, et les occlusions. \nUn détecteur efficace va repérer l'ensemble des objets d'intérêt mais ne rien prédire là où il n'y a rien. On appelle détection une zone rectangulaire produite par l'algorithme à laquelle est associée un score de classification entre 0 et 1. Comme pour la recherche d'information, chaque détection est soit un vrai-positif (True Positive - TP), c'est une bonne détection, ou un faux- positif (False Positive - FP), c'est une mauvaise détection. Pour qu'une détection soit considérée comme correcte il faut que la classe soit correcte et que l'intersection avec la zone de la vérité terrain soit suffisante. De plus les objets de la vérité terrain qui ne sont pas détectés par le modèle sont des faux-négatifs (False Negative - FN). \n\nDes exemples des différents cas sont donnés en Figure 1 2.\n \n \nFigure 1 2 : En bleu, les vérités terrain, en vert les détections de l'algorithme. (1) est un faux négatif, il n'a pas été détecté. (2) est un faux positif, l'algorithme a détecté quelque chose alors que rien n'était indiqué. (3) est un vrai positif.\n\nAvec le bond en performance apporté par le modèle de Krizhevsky et al. en 2012 [18] sur le challenge ImageNet, différentes équipes ont essayé de transférer les résultats au problème de la détection d'objets. Il est tout d'abord important de se convaincre qu'un réseau de neurone peut être utilisé pour de la régression et prédire des coordonnées rectifiées ou offsets. Étant donné une image à peu près centrée sur un objet d'intérêt, le réseau doit être capable de trouver les coordonnées relatives de la zone rectangulaire entourant au plus proche l'objet et sa classe. \n\nCette idée est illustrée en Figure 1 3. Le réseau prédit la classe ainsi que 4 nombres pour ajuster la zone englobant l'objet. Dans ce cas-là, l'image d'origine est trop étroite, le réseau prédit une augmentation de la largeur ∆w positive pour améliorer la localisation.\n\nCette approche peut être utilisée dans toute une image en appliquant une fenêtre glissante ou sliding window que l'on va déplacer sur toute l'image. Comme notre réseau de neurones est capable de prédire la classe et les coordonnées rectifiées d'une image carrée ou rectangulaire, nous obtenons pour chaque position de la fenêtre un score de classification. Les détections sont obtenues en considérant le maximum local. Les objets peuvent être de différentes tailles et de différents rapports de taille, il faut aussi repasser avec une fenêtre plus grande et/ou plus étroite. Cette technique permet de transférer directement les résultats des CNNs de classification d'image à la détection. Cette technique n'est cependant pas réalisable en pratique du fait du temps de réalisation de l'inférence à chaque position.\n\nLe problème de la fenêtre glissante est qu'elle n'est pas sélective et beaucoup de positions sont évaluées pour rien. Une solution est de réaliser cette inférence uniquement sur des zones candidates que l'on appelle proposals. Des algorithmes comme Selective Search [19] permettent d'identifier les zones susceptibles de contenir des objets d'intérêt. On compte de l'ordre de 103 à 104 proposals de différentes échelles dans une image. Au total, il faut compter plusieurs dizaines de secondes pour réaliser l'inférence sur une image et les proposals associés. La méthode complète de R-CNN (pour Region CNN) est détaillée dans [20], et permet d'obtenir 60% mAP sur le dataset Pascal VOC.\nDans son article Fast R-CNN [21], Ross Girshick a l'idée de calculer une unique feature map et de la réutiliser pour les classifications suivantes en réalisant l'opération de pooling au niveau de la feature map plutôt que dans l'image. Le gain en vitesse est évident : l'image ne passe qu'une seule fois à travers l'ensemble des convolutions. L'inférence est 213 fois plus rapide qu'avec R-CNN. Le réseau reste cependant tributaire des zones proposées. Les performances de détection restent cependant similaires à celles de R-CNN avec 66.9% mAP.\n\nAprès Fast RCNN, l'étape la plus lente devient l'algorithme de génération des zones candidates. Un nouveau modèle, Faster RCNN, intègre la génération des zones de propositions dans le réseau de neurones [22]. Faster RCNN a introduit le principe d'anchors ou ancres pour désigner les objectifs du Region Proposal Network (RPN), la partie du réseau chargée de proposer des zones. Si ce n'est cette étape, la suite du modèle est similaire à Fast RCNN une fois les zones prédites.\nLe RPN va donc apprendre à prédire des zones candidates pour la détection d'objets. Les réseaux de neurones ne sont pas faits pour prédire un ensemble avec un nombre variable d'éléments. Pour surmonter ce problème, l'image est découpée en une grille et le réseau va prédire pour chaque élément de la grille si elle contient un objet d'intérêt ou non. Comme pour la fenêtre glissante, il faut aussi prendre en compte différentes tailles et différents rapports de longueur. Une ancre est représentée en Figure 1 4. Le réseau va prédire une grille de taille W H avec un nombre de canaux égal au nombre d'ancres à chaque position. Dans le cas de la Figure 1 4, cela fait 6 canaux. Chaque élément de cette grille va contenir le score de classification correspondant à une des ancres, ce score doit être de 1 si l'ancre associée recouvre suffisamment une vérité terrain dans l'image.\n\n \nFigure 1 4 : Représentation des ancres à différentes échelles dans une image. En gris, on retrouve la grille. En rouge, des ancres d'une certaine échelle et en bleu de l'échelle supérieure\nUne comparaison des différentes méthodes est montrée en Figure 1 5. Par la suite, des améliorations successives comme le Feature Pyramid Network (FPN) [23] ou Mask RCNN [24] ont respectivement amélioré les performances sur les détections de petits objets en utilisant une pyramide de features, et amélioré la précision de la segmentation (et détection) en raffinant l'opération de pooling avant classification réalisée jusqu'alors.\nLes solutions précédentes réalisent toute la détection en deux temps :\n-\tD'abord la génération de zones candidates ;\n-\tPuis la classification ne avec l'amélioration de la zone proposée.\nD'autres méthodes ont été proposées pour réaliser ces opérations en une seule étape. D'une façon similaire au RPN, You Only Live Once [25], Single-Shot Multibox Detector (SSD) [26] et RetinaNet [27] prédisent directement pour une grille alignée sur des anchors les prédictions en régression et en classification. Les derniers classements sur la détection d'objets montrent la supériorité des méthodes à deux étapes sur le score malgré un temps de calcul pour long [28].\n\n\n \nFigure 1 5 : Comparaison des différentes méthodes de détection d'objets - source : cours de Li Fei Fei et Andrej Karpathy\n\nDonnées en entrée :\nLes jeux de données (datasets) de détection d'objets se concentrent sur les images naturelles et les objets du quotidien principalement pour la simplicité d'accès à de grandes quantités d'images et l'annotation facile pour n'importe qui. Le premier jeu de données de détection d'objets a été PASCAL VOC en 2005 qui contenait alors 4 classes et 1500 images. De nouvelles versions de PASCAL VOC ont été mises à jour au fur et à mesure, augmentant progressivement la taille du jeu de données.\n\n\n\n\n\n\nDataset\tAnnée\tNb classes\tNb images\tNb annotations\tMétrique\nPASCAL VOC\t2005\t4\t1 580\t2 200\tAP50\nPASCAL VOC\t2007\t20\t2 600\t4 750\tAP50\nPASCAL VOC\t2012\t20\t11 500\t27 450\tAP50\nCOCO\t2014\t80\t80 000\t600 000\tAP50 :95\nCOCO\t2017\t80\t120 000\t850 000\tAP50 :95\nFigure 1 6 : Datasets avec de nouvelles images, les métriques (AP50 et AP50 :95) sont détaillées dans la partie suivante \nEn 2014, COCO (Common Objets in Context) [29] a été présenté, c'est un jeu de données plus complexe mais aussi beaucoup plus grand. Le tableau en Figure 1 6 présente les caractéristiques de ces datasets. COCO utilise aussi une métrique qui pénalise plus les détecteurs imprécis.\nD'autres datasets de détection d'objet plus spécifiques ont aussi été présentés comme :\n-\tLa détection de piétons [30, 31] ;\n-\tLa détection de visages : WiderFace [32], un dataset de détection de visages dans des conditions extrêmes : très petits, très nombreux.\n-\tCityscapes [33], un dataset qui rassemble des vidéos obtenues par une voiture dans une ville\nIl faut noter que certains datasets sont aujourd'hui principalement annotés au niveau des pixels avec un masque de segmentation, cela n'empêche pas d'en faire des datasets de détection d'objets en considérant la zone rectangulaire qui englobe complètement une instance. L'inverse est cependant plus difficile.\n\nImagerie médicale :\nDès les débuts de l'imagerie médicale numérique de nombreux algorithmes de traitement d'images ont été appliqués pour la reconstruction ou le débruitage. Les nombreuses modalités, rayons X, IRM, CT scan, échographie, EEG, ECG, ont toutes des besoins et des applications différentes.\nDes algorithmes ont ensuite été développés pour aider les docteurs dans leur analyse ou pour des mesures. Les Computer Aided Diagnostic (CAD) ont commencé à voir le jour il y a plus de 40 ans. Ils permettent de modéliser les formes des organes, détecter des anomalies dans les radiographies, ou estimer l'âge d'un patient. Dans le cas de la détection de cancer, ils ont reçu un accueil mitigé car leurs performances n'étaient pas à la hauteur de celles d'un expert.\nDe nombreux datasets et challenges sont publiés chaque année couvrant presque l'ensemble des modalités [34]. Un des freins reste le problème de l'annotation car dans beaucoup de cas, seul un expert est capable d'annoter correctement les données, ce qui est cher.\n\nPour la problématique de Gleamer, les jeux de données les plus proches dans leur modalité sont celui de Chest-X Rray et ceux de mammographie [35, 36]. Ils ne traitent cependant pas de la détection d'objets mais uniquement de la classification. Une grande partie des autres datasets portent sur la segmentation dans des IRM (3D).\n1.2.2\tNeural Architecture Search (NAS)\nAu cours des dernières années, un changement de paradigme a été observé, passant de la recherche « à la main » des meilleures caractéristiques et paramètres d’un modèle d’apprentissage, à la conception d’architectures de réseaux de neurones capables d’extraire ces caractéristiques automatiquement. Cependant, concevoir ces architectures est en soi une tâche très difficile, chronophage, et surtout très dépendante du domaine d’application. Une direction très prometteuse est donc de créer ces architectures de façon automatique. C’est ce qu’on appelle la « Neural Architecture Search », ou recherche d’architecture neuronale. \n\nComme remarqué par [37], la NAS a été essentiellement appliquée à des problèmes de vision ou de NLP, puisque ces tâches bénéficient le plus de l’apprentissage profond. Cependant, un autre domaine d’application intéressant est la recherche d’une meilleure politique, dans lequel un agent doit optimiser son comportement de façon à maximiser une fonction d’utilité inconnue. Pour résoudre ce type de problème, il est classique de représenter le comportement de l’agent par un réseau de neurones. Des expériences [38] montrent que, en pratique, les algorithmes de recherche de politique, et en particulier ceux utilisant l’apprentissage par renforcement, sont très sensibles aux architectures des réseaux. L’application de NAS à ce problème pourrait donc permettre de mieux comprendre les liens entre ces algorithmes et les architectures utilisées. \nL’automatisation de la création de ces architectures pourrait être un bon moyen de sortir de la crise de reproductibilité observée [39], car elle permettrait de comparer les différents algorithmes de recherche de politiques indépendamment de l’architecture neuronale. \n\nPlusieurs approches existent et ont été étudiées dans la littérature, utilisant principalement l’apprentissage par renforcement profond [40, 41] ou des algorithmes évolutionnaires [42, 43, 44].\n\nL’utilisation de l’apprentissage par renforcement pour résoudre un problème de NAS donné est devenue populaire après le succès des architectures obtenues par [40] et [41]. Un problème d’apprentissage par renforcement est caractérisé par un processus de décision markovien (MDP), qui décrit les dynamiques des interactions entre un agent et son environnement. La littérature sur l’apprentissage par renforcement se concentre essentiellement sur l’apprentissage dit « model-free » (par opposition à l’apprentissage dit « model-based »), fonctionnant sur de nombreux problèmes différents, et donc de nombreux MDP sous-jacents différents. Ces algorithmes peuvent alors être utilisés pour la recherche d’architecture neuronale, vue comme une instance d’un problème d’apprentissage par renforcement. \nCe problème peut être résolu par apprentissage par renforcement ou en utilisant des algorithmes évolutionnaires [45]. Il est également possible de développer un framework combinant ces deux familles d’algorithmes pour profiter des avantages des deux méthodes [46, 47, 37]. \nDans le domaine du médical, et notamment    l’aide au diagnostic, il est courant d’utiliser des architectures efficaces sur des problèmes classiques de reconnaissances d’objet et de les adapter légèrement au problème médical étudié (par exemple, la détection de fracture). Cependant, ces architectures sont initialement entraînées sur des ensembles d’images très différentes de celles qui nous intéressent par la suite, il est donc possible que ces architectures soient loin d’être optimales. L’application de techniques de types NAS pourrait donc être très largement bénéfique à ce domaine en permettant de rechercher de meilleures architectures."}]}
{"messages": [{"role": "system", "content": "Tu es un assistant spécialisé dans la rédaction 'État de l'art' pour des dossiers de Crédit Impôt Recherche (CIR). Ton objectif est de rédiger un état de l'art complet à partir des bullet points que tu reçois en input. Le texte doit être clair, précis et bien structuré pour s'inscrire dans une synthèse technique du projet de l'entreprise. Tu dois utiliser un langage technique, respecter la rigueur scientifique, et t'assurer que la structure soit logique et cohérente. Tu dois inclure des détails techniques pertinents, des comparaisons, des références aux innovations, et souligner les défis techniques et/ou les lacunes dans la littérature existante. Rédige le texte de manière fluide, en introduisant les concepts progressivement et en les expliquant de façon détaillée tout en restant concis et professionnel."}, {"role": "user", "content": "- L'étude « Association of Artificial Intelligence–Aided Chest Radiograph Interpretation With Reader Performance and Efficiency » de Jong Seok Ahn et al. [1] évalue l'impact de l'IA sur la performance et l'efficacité des radiologues dans l'interprétation des radiographies thoraciques.\n- Menée d'avril à novembre 2021, l'étude a impliqué des radiologues de différents niveaux d'expérience, testés avec et sans l'assistance de l'IA, à travers un ensemble de 497 radiographies thoraciques.\n- Les résultats montrent que l'IA augmente la sensibilité pour toutes les anomalies cibles (pneumothorax, nodules, etc.) sans compromettre la spécificité, tout en réduisant le temps de signalement de 10 %.\n- La radiographie thoracique, souvent utilisée pour son accessibilité et son faible coût, présente des défis d'interprétation dus à la subjectivité, ce qui justifie l'intégration d'IA pour améliorer l'exactitude des diagnostics.\n- L'étude a utilisé Lunit INSIGHT CXR version 3.1.2.0, une IA qui a démontré des améliorations notables dans la détection des anomalies thoraciques, en particulier pour les radiologues avec moins d'expérience.\n- Concernant ChestView, cette étude montre l'importance de l'IA pour améliorer la sensibilité et l'efficacité dans la détection des anomalies thoraciques, tout en considérant les défis liés aux annotations partielles.\n- Cependant, la généralisation des résultats est limitée par le fait que l'étude se concentre sur un ensemble spécifique d'anomalies thoraciques dans un contexte particulier, avec des données provenant de deux hôpitaux seulement.\n- L'article « Diagnostic accuracy of a commercially available deep-learning algorithm in supine chest radiographs following trauma » de Jacob Gipson et al. [2] évalue la précision diagnostique de l'algorithme Annalise CXR V1.2 pour la détection des lésions traumatiques sur les radiographies thoraciques en position supine.\n- L'algorithme Annalise CXR V1.2 a montré une performance supérieure à celle des radiologues pour identifier des pathologies spécifiques telles que le pneumothorax, avec une sensibilité accrue.\n- Cependant, les radiologues ont mieux performé pour détecter des fractures complexes (clavicule, scapula), soulignant la complémentarité possible entre l'IA et les experts humains.\n- Les résultats sont pertinents pour ChestView, particulièrement pour réduire les faux positifs et améliorer la précision diagnostique, notamment dans la détection des pneumothorax.\n- Néanmoins, l'étude se concentre uniquement sur des cas de traumatismes en position supine, ce qui limite l'applicabilité directe à ChestView, qui doit gérer un éventail plus large de pathologies et de positions de radiographie.\n- L'article « Deep Learning for Detection of Pulmonary Metastasis on Chest Radiographs » de Hwang et al. [3] explore l'utilisation d'un système CAD basé sur le deep learning pour améliorer la détection des métastases pulmonaires sur les radiographies thoraciques.\n- L'étude montre que l'intégration du CAD améliore significativement la détection des métastases, avec une sensibilité accrue sans réduction notable de la spécificité.\n- Cette étude est particulièrement pertinente pour ChestView en termes de réduction des faux positifs et d'amélioration de la sensibilité des radiologues.\n- Cependant, l'étude se concentre spécifiquement sur les métastases pulmonaires, limitant ainsi l'applicabilité directe à la diversité des pathologies thoraciques que ChestView vise à détecter.\n- De plus, l'approche rétrospective et l'accent mis sur une population spécifique (patients atteints de cancer) limitent la généralisation des résultats pour ChestView.\n- Enfin, l'étude ne traite pas des défis techniques liés à l'intégration de ces systèmes CAD dans les flux de travail cliniques existants, un aspect crucial pour assurer l'adoption et l'efficacité de ChestView."}, {"role": "assistant", "content": "1.3.2\tEtat de l’art\nL'article \"Association of Artificial Intelligence–Aided Chest Radiograph Interpretation With Reader Performance and Efficiency\" de Jong Seok Ahn et al. [1] aborde l'importance de l'interprétation efficace et précise des images radiologiques, en particulier des radiographies thoraciques. L'objectif de cette étude multicentrique était d'évaluer si un moteur d'intelligence artificielle (IA) basé sur l'apprentissage profond pouvait améliorer la performance et l'efficacité des radiologues dans l'interprétation des anomalies des radiographies thoraciques.\n\nL'étude, menée d'avril à novembre 2021, a impliqué des radiologues de différents niveaux d'expérience qui ont participé à deux sessions de tests de performance d'observateurs, avec et sans l'assistance de l'IA. Elle a utilisé des données provenant de deux hôpitaux universitaires à Boston, Massachusetts, et a inclus un total de 497 radiographies thoraciques frontales. Les résultats ont montré que l'IA augmentait la sensibilité pour toutes les découvertes cibles par rapport aux lecteurs, sans impacter négativement la spécificité. De plus, le temps de signalement avec l'IA était inférieur de 10% par rapport à celui sans IA.\n\n \nTableau 1 : Performance détaillée des lecteurs individuels avec et sans assistance de l’IA [1]\nLe Tableau 1 présente les mesures de sensibilité, spécificité et AUROC pour chaque radiologue participant à l'étude, à la fois avec et sans l'aide de l'IA. Les données soulignent comment l'assistance de l'IA affecte les performances de diagnostic de radiologues de différents niveaux d'expérience, offrant des insights importants pour l'optimisation des systèmes d'IA dans les contextes cliniques\n\nLa radiographie thoracique est l'une des modalités d'imagerie les plus courantes en raison de sa portabilité, de son faible coût et de son accessibilité. Cependant, l'interprétation des radiographies thoraciques est difficile et subjective, avec une faible à modérée concordance entre les lecteurs. Cela a conduit à un intérêt croissant pour l'utilisation des applications d'IA dans l'imagerie médicale pour améliorer l'interprétation des radiographies thoraciques.\n\nLa méthode de l'étude a été soigneusement conçue pour refléter l'environnement réel, en utilisant un outil de test de performance d'observateur personnalisé qui imite le système d'archivage et de communication des images (PACS) du monde réel. Cela a permis d'enregistrer les découvertes radiographiques et le temps de signalement, offrant ainsi une évaluation précise et réaliste de la performance et de l'efficacité des radiologues.\n\n \nFigure 1 : Modes d’affichages de l’IA dans l’interprétation des radiographies thoraciques [1]\nLa Figure 1 illustre les différentes manières dont les résultats de l'IA sont présentés aux radiologues, montrant comment l'IA s'intègre dans le flux de travail clinique. Les différentes représentations visuelles de l'IA, telles que les cartes thermiques (heatmaps) et les probabilités au niveau de l'image, sont cruciales pour assister les radiologues dans l'identification des anomalies thoraciques\n\nL'IA utilisée dans cette étude était Lunit INSIGHT CXR version 3.1.2.0, une IA commercialement disponible conçue pour traiter les images radiographiques thoraciques. Les résultats ont montré des améliorations significatives dans la détection des pneumothorax et des nodules, avec une réduction du temps de signalement.\n\n \nFigure 2 : Comparaison des performances de l’IA et des radiologues [1]\nLa Figure 2 résume l'efficacité de l'algorithme d'apprentissage profond pour identifier les différentes anomalies thoraciques et la compare aux performances des radiologues. Elle met en évidence les domaines où l'IA surpasse les radiologues humains et sert de référence pour évaluer les progrès possibles dans l'application de l'IA dans l'interprétation des radiographies thoraciques.\n\nConcernant nos travaux R&D sur ChestView, plusieurs aspects de cette étude sont pertinents. Premièrement, l'utilisation de l'IA pour améliorer la sensibilité dans la détection des anomalies thoraciques sans pénaliser la spécificité pourrait être cruciale pour ChestView, en particulier pour la gestion des données avec des annotations partielles. Deuxièmement, l'efficacité accrue dans le temps de signalement avec l'assistance de l'IA pourrait être bénéfique pour ChestView, en visant à réduire le temps d'analyse sans compromettre la précision.\n\nEn outre, les techniques et méthodes utilisées dans cette étude pour évaluer l'impact de l'IA sur la performance des radiologues pourraient servir de modèle pour tester et intégrer de nouvelles fonctionnalités ou améliorations dans ChestView. Cela inclut l'adoption de stratégies similaires pour tester l'efficacité des algorithmes d'IA dans un environnement qui imite de près les conditions réelles de travail des professionnels de la santé.\n\nBien que l'article apporte des informations précieuses sur l'intégration de l'IA dans l'interprétation des radiographies thoraciques, il existe des limites spécifiques à prendre en compte. La généralisabilité des résultats est une préoccupation majeure, car l'étude se concentre sur un ensemble spécifique d'anomalies thoraciques et ne traite pas de la performance de l'IA pour d'autres pathologies ou dans des contextes variés, ce qui est crucial pour ChestView qui vise une application plus large. De plus, l'étude se limite à des données provenant de deux hôpitaux universitaires à Boston, ce qui soulève des questions sur la diversité des données. Cette limitation est particulièrement pertinente pour ChestView, où il est essentiel d'entraîner et de tester l'IA sur des ensembles de données variés pour garantir son efficacité sur différentes populations de patients et des équipements d'imagerie variés.\n\nEn outre, l'article ne traite pas spécifiquement des défis liés à la gestion des données avec des annotations partielles, comme les radiographies incorrectement annotées, un aspect clé pour ChestView. La question de la validation clinique complète et de l'intégration de l'IA dans les flux de travail cliniques existants, essentielle pour l'adoption et l'efficacité de ChestView dans des contextes réels, n'est pas non plus abordée. Un autre point important est la réduction des faux positifs ; l'étude montre une amélioration de la sensibilité sans affecter la spécificité, mais ne propose pas de stratégies spécifiques pour minimiser les faux positifs, un aspect crucial pour éviter les diagnostics erronés dans ChestView, en particulier pour les nouvelles pathologies ajoutées. \n\nL'article intitulé \"Diagnostic accuracy of a commercially available deep-learning algorithm in supine chest radiographs following trauma\" par Jacob Gipson et al. [2] présente une étude sur la précision diagnostique d'un algorithme d'apprentissage profond, Annalise CXR V1.2, pour détecter les blessures traumatiques dans les radiographies thoraciques en position supine. L'étude a été menée en utilisant des radiographies de la poitrine et des rapports de tomodensitométrie (CT) comme référence. L'objectif était de valider indépendamment la performance de l'algorithme en comparaison avec l'interprétation radiologique, en utilisant la CT comme vérité terrain.\n\nLes auteurs soulignent la complexité de détecter des pathologies subtiles et critiques dans les radiographies thoraciques post-traumatiques. Ils proposent l'utilisation d'une intelligence artificielle (IA) pour aider dans l'interprétation précise, l'identification rapide et la priorisation des listes de travail en radiologie. Cette étude visait à évaluer la performance de l'algorithme Annalise CXR V1.2 dans la détection des lésions traumatiques sur des radiographies thoraciques en supine.\n\nL'étude était rétrospective et a été menée dans un centre de traumatologie de niveau un. \n\n \nFigure 3 : Diagramme de flux de patients \nLes radiographies thoraciques et les CT effectuées dans les 24 heures suivant un traumatisme ont été incluses. L'évaluation de l'algorithme IA a été comparée aux rapports radiologiques, avec les rapports de CT servant de référence. Les pathologies ciblées incluaient pneumothorax, fractures des côtes, de la clavicule, de l'humérus, de la scapula, l'effondrement lobaire/segmentaire et le pneumomédiastin. La performance de l'IA et des radiologues a été mesurée à l'aide de la statistique Kappa de Cohen et des calculs de sensibilité/spécificité.\n\n \nFigure 4 : Courbes ROC pour différentes pathologies\n\nL’étude a abouti sur plusieurs résultats. En effet, l'IA a montré une performance supérieure à celle des radiologues pour identifier le pneumothorax et l'effondrement segmentaire sur la radiographie thoracique. \n\n \nFigure 5 : Exemple de faux positif par l’IA\nDe plus, les radiologues ont mieux performé que l'IA pour la détection des fractures du clavicule, de l'humérus et de la scapula. \n\n \nFigure 6 : Pneumothorax latéral gauche manqué par le radiologue mais identifié par l’IA\nAucune différence statistique n'a été trouvée pour l'identification des fractures des côtes et du pneumomédiastinum. Enfin, un modèle hybride proposé, combinant les interprétations de l'IA et du radiologue, a montré une sensibilité améliorée pour toutes les pathologies.\n\nL'algorithme Annalise CXR V1.2 démontre une capacité notable à identifier des pathologies spécifiques dans des radiographies thoraciques en position supine, ce qui est pertinent pour le projet ChestView visant à améliorer la précision diagnostique. Les techniques d'apprentissage profond utilisées pourraient être examinées pour leur application dans ChestView, en particulier pour la détection des pneumothorax et des effondrements lobaires/segmentaires.\n\nL’article présente plusieurs éléments pertinents pour nos travaux de R&D. En effet, l'étude fournit un cadre de comparaison entre l'IA et les interprétations humaines, mettant en évidence l'importance de la qualité des annotations et des données d'entraînement. Pour ChestView, cela souligne l'importance de disposer de données bien annotées, en particulier pour les cas complexes ou atypiques.\nDe plus, l'approche de l'IA dans l'étude montre une réduction des faux positifs dans certaines catégories. Cette méthodologie pourrait être adaptée pour ChestView afin de minimiser la détection incorrecte de pathologies, comme les nodules.\nLe concept de modèle hybride, où l'IA complète l'analyse du radiologue, justifie notre approche globale de ChestView comme un outil de support pour les radiologues, améliorant ainsi la précision globale du diagnostic.\n\nBien que cet article apporte plusieurs éléments intéressants pour nos travaux de R&D, plusieurs limites se dessinent par rapport à votre projet ChestView.\nTout d'abord, l'étude se concentre spécifiquement sur les traumatismes et utilise des radiographies thoraciques en position supine. Ce contexte spécifique limite l'applicabilité directe des résultats à ChestView car nous visons une gamme plus large de pathologies ou des positions de radiographie différentes. La performance de l'algorithme dans des situations cliniques variées, notamment avec des patients en position debout ou assise, n'est pas abordée, ce qui est une considération importante pour nos travaux.\nEnsuite, l'algorithme a été évalué principalement pour la détection de pathologies spécifiques liées aux traumatismes, telles que les pneumothorax et les fractures alors que nous envisageons d'inclure un spectre plus large de pathologies thoraciques, comme les maladies pulmonaires ou cardiaques. Ainsi, les résultats de cette étude pourraient ne pas être entièrement transférables.\nDe plus, l'étude met en lumière l'importance de la qualité des annotations et des données d'entraînement. Pour ChestView, cela signifie que la qualité et la diversité des données d'entraînement seront cruciales pour la performance générale de l'algorithme, surtout dans les cas atypiques ou complexes qui pourraient ne pas être bien représentés dans l'ensemble de données de l'étude.\nIl est également important de noter que l'étude a utilisé la tomodensitométrie (CT) comme référence pour évaluer la précision de l'algorithme. Cette méthodologie peut ne pas être directement applicable à notre cas. De plus, l'étude ne discute pas de la façon dont l'algorithme pourrait s'intégrer dans les flux de travail cliniques existants, ce qui est un aspect essentiel.\nEnfin, les aspects techniques spécifiques de l'algorithme Annalise CXR V1.2, tels que sa structure de réseau de neurones et ses méthodes d'entraînement, ne sont pas détaillés. Pour nos travaux de R&D, comprendre et adapter ces aspects techniques est crucial pour développer un outil qui répond précisément aux besoins de ChestView.\n\nL'article de Hwang et al. intitulé \"Deep Learning for Detection of Pulmonary Metastasis on Chest Radiographs\" [3] explore l'utilisation de systèmes de détection assistée par ordinateur (CAD) basés sur l'apprentissage profond (DL) pour améliorer le diagnostic des métastases pulmonaires sur les radiographies thoraciques. Cette étude est pertinente pour votre projet ChestView, notamment dans l'optimisation des performances d'analyse médicale et la gestion des données avec des annotations partielles.\n\nHwang et al. se concentrent sur l'évaluation d'un système CAD basé sur le DL pour améliorer le rendement diagnostique des métastases pulmonaires nouvellement visibles sur les radiographies thoraciques chez les patients atteints de cancer. Leur objectif est de surmonter les limites de la radiographie thoracique conventionnelle dans la détection des nodules pulmonaires, qui est souvent insatisfaisante.\n\nL'étude a été menée de manière rétrospective en utilisant un système CAD réglementé pour l'interprétation des radiographies thoraciques. Ils ont comparé les radiographies interprétées avec l'aide du CAD (janvier à avril 2019) avec celles interprétées avant l'implémentation du CAD (septembre à décembre 2018).\n\n[Insérer ici la Figure 1 page 157 du PDF]\nDiagramme de flux pour l'inclusion des radiographies thoraciques dans l'étude.\n\nLes résultats ont montré une amélioration significative du rendement diagnostique pour les métastases nouvellement visibles dans le groupe assisté par CAD. La sensibilité des radiologues dans l'identification des radiographies avec des nodules métastatiques a également été améliorée avec l'assistance du CAD, sans compromettre la spécificité.\n\n[Insérer ici la Figure 2 page 166 du PDF]\nImages d'un patient de 66 ans avec un angiosarcome de l'aorte abdominale, montrant les nodules détectés par le CAD.\n\nL'étude conclut que le système CAD basé sur le DL améliore le rendement diagnostique pour les métastases nouvellement visibles sur les radiographies thoraciques, avec un taux de fausses références similaire. Cela suggère que le CAD peut améliorer la qualité de la surveillance des métastases pulmonaires, en particulier dans les installations cliniques avec un accès limité au CT.\n\nConcernant nos travaux R&D, cette étude est particulièrement pertinente dans le contexte de l'amélioration de l'interprétation des radiographies thoraciques. Les techniques et méthodologies utilisées, telles que l'intégration du système CAD basé sur le DL et l'analyse comparative de son efficacité par rapport aux méthodes conventionnelles, peuvent être adaptées ou inspirer des améliorations dans ChestView. L'accent mis sur la réduction des faux positifs et l'amélioration de la sensibilité des radiologues par le biais de l'assistance CAD aligne également bien avec les objectifs de ChestView.\n\n[Insérer ici la Figure 3 page 285 du PDF]\nRadiographies d'un patient de 75 ans avec un cancer du côlon, où un petit nodule est identifié par le CAD.\n\nL'étude de Hwang et al. (2021) offre des insights précieux pour notre projet ChestView, cependant, elle présente certaines limites qui doivent être prises en compte. Tout d'abord, l'étude se concentre principalement sur la détection des métastases pulmonaires, un domaine spécifique qui, bien que pertinent, ne couvre pas l'ensemble des pathologies thoraciques que ChestView vise à détecter. Par conséquent, l'application directe des méthodes et des résultats de cette étude pourrait ne pas être entièrement transposable à la diversité des cas cliniques que ChestView pourrait rencontrer.\nEn outre, l'étude utilise une approche rétrospective basée sur des données historiques, ce qui pourrait limiter la généralisation des résultats dans des contextes cliniques actuels ou futurs. Cette approche rétrospective ne tient pas compte des évolutions récentes en imagerie médicale ou des changements dans les protocoles de traitement des patients, qui peuvent influencer significativement la présentation des pathologies thoraciques.\nDe plus, bien que l'étude mette en évidence l'efficacité du système CAD dans la réduction des faux positifs, elle ne discute pas en détail de la gestion des faux négatifs, un aspect crucial pour ChestView. La détection précise des cas pathologiques est aussi importante que la réduction des faux positifs, surtout dans le contexte de maladies potentiellement mortelles comme le cancer.\nUne autre limitation concerne la spécificité de la population étudiée. L'étude de Hwang et al. se concentre sur des patients atteints de cancer, ce qui pourrait ne pas refléter la variété des patients et des conditions que ChestView pourrait rencontrer dans un cadre clinique plus général. La capacité de notre système à s'adapter et à être précis à travers un large éventail de conditions et de populations est essentielle pour son succès et son applicabilité clinique.\nEnfin, l'étude n'aborde pas les défis techniques spécifiques liés à l'intégration de tels systèmes CAD dans les flux de travail cliniques existants. Pour ChestView, il est crucial de développer des solutions qui s'intègrent harmonieusement dans les pratiques cliniques actuelles, garantissant une adoption et une utilité maximales parmi les professionnels de la santé.\nCes limitations mettent en évidence l'importance d'une approche holistique dans le développement de ChestView, où non seulement les aspects techniques de la détection des pathologies sont pris en compte, mais aussi l'adaptabilité, la généralisabilité, et l'intégration dans les pratiques cliniques, pour assurer que notre système soit efficace, fiable et utile dans un large éventail de scénarios cliniques."}]}
